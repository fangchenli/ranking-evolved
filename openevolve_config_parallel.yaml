# OpenEvolve configuration for Parallelized Full-Benchmark Evaluation
#
# Uses evaluator_parallel.py which evaluates on ALL 31 datasets:
# - BRIGHT: 12 reasoning-intensive retrieval domains
# - BEIR: 17 heterogeneous IR datasets
# - TREC DL: 2 Deep Learning Track datasets (DL19, DL20)
#
# Parallelization Strategy (112 cores, 440GB RAM):
# - Level 1: Dataset-level parallelism (ProcessPool)
# - Level 2: Tokenization parallelism (ThreadPool within each worker)
# - Level 3: Vectorized BM25 (NumPy BLAS)
# - Memory-aware scheduling: small/medium/large batching
#
# Metrics returned to LLM:
# - Per-dataset: nDCG@10, Recall@100, index_time_ms, query_time_ms
# - Aggregate: avg_nDCG@10, avg_Recall@100, combined_score
# - Timing: total_index_time_ms, total_query_time_ms
#
# Run with:
#   export OPENAI_API_KEY="your-key"
#   EVAL_SAMPLE_QUERIES=20 uv run openevolve-run src/ranking_evolved/bm25_classic.py evaluator_parallel.py --config openevolve_config_parallel.yaml
#
# Environment variables for evaluation:
#   EVAL_BENCHMARKS=all          # all | bright | beir | bright+beir
#   EVAL_SAMPLE_QUERIES=20       # Sample N queries per dataset (0 = all)
#   EVAL_TOKENIZER=lucene        # simple or lucene
#   EVAL_MAX_WORKERS=0           # 0 = auto (based on cores/memory)
#   EVAL_THREADS_PER_WORKER=8    # Threads for tokenization

max_iterations: 200
log_level: "INFO"
random_seed: 42
diff_based_evolution: true
max_code_length: 30000

llm:
  models:
    - name: "gpt-4o"
      weight: 1.0
  api_base: "https://api.openai.com/v1"
  api_key: null
  temperature: 0.7
  top_p: 0.95
  max_tokens: 8192
  timeout: 180
  retries: 3
  retry_delay: 5

prompt:
  system_message: |
    You are evolving a BM25 ranking function evaluated on IR benchmarks.

    ## Evaluation Metrics
    You receive detailed metrics for each dataset:
    - nDCG@10: Normalized Discounted Cumulative Gain (ranking quality)
    - Recall@100: Coverage metric (retrieval completeness)
    - index_time_ms: Time to tokenize corpus and build index
    - query_time_ms: Time to evaluate all queries
    
    Aggregate metrics:
    - avg_ndcg@10: Macro-average across all datasets
    - avg_recall@100: Macro-average across all datasets
    - combined_score: (avg_ndcg@10 + avg_recall@100) / 2

    ## Evaluation Scope (configurable via EVAL_BENCHMARKS env var)
    - BRIGHT: 12 reasoning-intensive domains (biology, earth_science, economics, ...)
    - BEIR: 17 heterogeneous datasets (scifact, nfcorpus, trec-covid, ...)
    - TREC DL: 2 Deep Learning Track datasets (DL19, DL20)
    
    Use per-dataset metrics to identify weak spots and optimize accordingly.

    ## Evolution Targets (in bm25_classic.py)
    1. `ClassicParameters` - k1, b, k3
    2. `ClassicStopwords` - extra_stopwords, keep_words sets
    3. `ClassicStemmer` - enabled, custom_stems, strip_suffixes
    4. `ClassicTokenizer` - min/max_token_length, filter_numbers, split_pattern
    5. `ClassicIDF.compute()` - IDF formula
    6. `ClassicTF.compute()` - TF saturation formula
    7. `ClassicLengthNorm.compute()` - length normalization formula
    8. `ClassicQueryWeighting` - mode (unique/sum_all/saturated), compute_weights()
    9. `ClassicScoreAggregation` - mode (sum/weighted_sum/max/mean), aggregate()
    10. `BM25.score_kernel()` - main scoring function

    ## Current Starting Point (Vanilla Robertson BM25)
    - IDF: log((N - df + 0.5) / (df + 0.5))
    - TF: (tf * (k1 + 1)) / (tf + k1 * norm)
    - LengthNorm: 1 - b + b*(dl/avgdl)
    - Parameters: k1=1.5, b=0.75, k3=8.0
    - Query mode: unique (bag-of-words)
    - Aggregation: sum

    ## Guidelines
    - Keep changes minimal and targeted
    - Preserve BM25 interface (Corpus, rank signatures)
    - No new dependencies - only numpy and re are available
    - Ensure numerical stability (avoid division by zero)
    - Consider both ranking quality (nDCG) AND speed (index/query time)
    - Look at per-dataset metrics to identify weak spots

    ## Areas to Explore
    - Different IDF smoothing/scaling (log1p, sqrt, clipping)
    - Alternative TF saturation curves (log damping, double saturation)
    - Non-linear score combinations
    - Query weighting modes (sum_all, saturated)
    - Score aggregation strategies (max, mean vs sum)
    - Parameter tuning (k1, b, k3) - optimal varies by domain
    - Efficiency: vectorized operations, avoid Python loops

  evaluator_system_message: |
    You review candidate BM25 ranking code evaluated on 31 datasets.
    
    Check for:
    - Numerical stability (no NaN, no division by zero)
    - Interface compatibility (BM25.rank() returns correct types)
    - Meaningful ranking behavior (not all zeros)
    - Efficiency (prefer vectorized operations)
    
    The LLM receives per-dataset metrics. Look for:
    - Datasets where nDCG improved/degraded significantly
    - Balance between ranking quality and speed
    - Generalization across benchmark types (BRIGHT vs BEIR vs TREC DL)

  num_top_programs: 3
  num_diverse_programs: 2
  use_template_stochasticity: true
  include_artifacts: true

database:
  in_memory: true
  population_size: 200
  archive_size: 50
  num_islands: 3
  migration_interval: 20
  migration_rate: 0.1
  feature_dimensions:
    - "complexity"
    - "diversity"
  feature_bins: 10
  log_prompts: true

evaluator:
  timeout: 3600  # 1 hour - 31 datasets can take time even with parallelism
  max_retries: 2
  cascade_evaluation: false
  parallel_evaluations: 1  # Evaluator handles parallelism internally

evolution_trace:
  enabled: true
  format: "jsonl"
  include_code: true
  include_prompts: false
