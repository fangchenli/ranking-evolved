# OpenEvolve configuration for Freeform Lexical Retrieval Seed
#
# Seed: src/ranking_evolved/bm25_freeform_fast.py (concise: doc repr + query repr + scoring method)
#
# GOAL: Discover a new lexical retrieval method. Maximum freedom: document representation,
# query representation, and the scoring formula. We want novel formulations with deep,
# fundamental, and intuitive justification—not just tuning BM25. GPT-5.2 should be creative
# and exploratory. Only the evaluator interface is fixed (BM25, Corpus, tokenize,
# LuceneTokenizer, rank(), score()); everything else is evolvable.
#
# Run (align with commands.txt):
#   export EVAL_EXCLUDE_DATASETS="dl19,dl20,fever,..."
#   uv run python -m openevolve.cli src/ranking_evolved/bm25_freeform_fast.py evaluator_parallel.py --config openevolve_config_freeform.yaml --output openevolve_output_freeform_fast

max_iterations: 70
checkpoint_interval: 5
log_level: "DEBUG"
random_seed: 42
diff_based_evolution: true
max_code_length: 30000

llm:
  models:
    - name: "gpt-5.2"
      weight: 1.0
  api_base: "https://api.openai.com/v1"
  api_key: null  # falls back to OPENAI_API_KEY env var
  temperature: 0.85  # Higher temperature for maximum exploration
  top_p: 0.95
  max_tokens: 8192
  timeout: 180
  retries: 3
  retry_delay: 5

prompt:
  system_message: |
    You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**—not just BM25 with extra knobs. Be creative and exploratory.

    ## Goal

    - **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 × avg_recall@100 + 0.2 × avg_ndcg@10 (higher is better).
    - **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.

    ## What you can change (evolve)

    1. **Config** — Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).
    2. **idf(df, N)** — How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.
    3. **DocumentRepr** — What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.
    4. **QueryRepr** — How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.
    5. **retrieval_score(...)** — **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.
    6. **score_document(query, doc_idx, corpus)** — Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.
    7. **BM25._score_candidates_vectorized** — Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).

    Use **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.

    Use **per-dataset metrics** to see where the method is weak and target those benchmarks.

    ## What you must keep (evaluator contract)

    - The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.
    - **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.
    - **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.
    - Avoid division by zero and NaNs (use Config.epsilon or similar).

    ## Guidelines

    - Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.
    - Explain in comments or structure *why* a formulation is reasonable (e.g. "saturate TF because repeated terms matter less" or "penalize length to favor focused docs").
    - If you add new parameters or signals, give them meaningful names and clear roles.
    - Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).

  evaluator_system_message: |
    You review candidate lexical retrieval code evaluated on IR benchmarks.

    Check:
    - Code runs without errors and returns valid scores (no NaN, not all zeros).
    - **Interface**: BM25, Corpus, tokenize, LuceneTokenizer present; BM25 has rank() and score().
    - Numerical stability (no division by zero, bounded outputs).
    - Novel formulations are conceptually sensible (e.g. formulas are well-defined and have a plausible justification).

    Be **tolerant of unconventional approaches**—we want creative, well-reasoned retrieval methods, not only small BM25 variants. Prefer approving edits that have a clear intuitive or theoretical basis; flag only obvious bugs or interface violations.

  num_top_programs: 4  # Keep more top programs for diversity
  num_diverse_programs: 4  # Maximum diversity for exploration
  use_template_stochasticity: true
  include_artifacts: true

database:
  in_memory: true
  population_size: 300  # Largest population for maximum exploration
  archive_size: 80
  num_islands: 5  # Most islands for maximum diversity
  migration_interval: 30
  migration_rate: 0.15  # Higher migration for cross-pollination
  feature_dimensions:
    - "complexity"
    - "diversity"
  feature_bins: 12
  log_prompts: true

evaluator:
  timeout: 1800  # 30 min - fast set (12 datasets) when EVAL_EXCLUDE_DATASETS set
  max_retries: 3  # More retries since code may be less stable
  cascade_evaluation: false
  parallel_evaluations: 1  # Evaluator handles parallelism internally

evolution_trace:
  enabled: true
  format: "jsonl"
  include_code: true
  include_prompts: true
