{"id": "08ef8c69-e2e1-4715-9276-8bcf500ade9d", "code": "\"\"\"\nConstrained BM25 seed \u2014 safe search over known primitives.\n\nStay within BM25: tune hyperparameters (k1, b, k3), swap IDF/TF/length-norm\nformulas for known alternatives, and combine them meaningfully. No exploration\nof novel retrieval ideas; efficient grid-search over a known search space.\n\nEvaluator contract: BM25, Corpus, tokenize, LuceneTokenizer; BM25.rank(), BM25.score().\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import LuceneTokenizer as _BaseLuceneTokenizer\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = min(int(os.environ.get(\"BM25_QUERY_WORKERS\", 32)), 32)\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Parameters \u2014 EVOLVE: k1, b, k3 (e.g. k1 in [0.5, 2.0], b in [0, 1])\n# -----------------------------------------------------------------------------\n\nclass Parameters:\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: swap for known formulas (Lucene, Robertson, ATIRE, BM25L, BM25+)\n# -----------------------------------------------------------------------------\n\ndef idf(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n    \"\"\"EVOLVE: Lucene default; alternatives: Robertson log((N-df+0.5)/(df+0.5)), ATIRE log(N/df), BM25L log((N+1)/(df+0.5)), BM25+ log((N+1)/df).\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# TF saturation \u2014 EVOLVE: swap for known formulas (Lucene, Robertson, log, etc.)\n# -----------------------------------------------------------------------------\n\ndef tf_saturated(tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]) -> NDArray[np.float64]:\n    \"\"\"EVOLVE: Lucene tf/(tf+k1*norm); Robertson (k1+1)*tf/(tf+k1*norm); log log(1+tf)/(tf+k1*norm).\"\"\"\n    return (k1 + 1.0) * tf / (tf + k1 * norm + 1e-9)\n\n\ndef tf_saturated_scalar(tf: float, k1: float, norm: float) -> float:\n    return (k1 + 1.0) * tf / (tf + k1 * norm + 1e-9)\n\n\n# -----------------------------------------------------------------------------\n# Length norm \u2014 EVOLVE: swap for known formulas (pivoted, none, log, sqrt)\n# -----------------------------------------------------------------------------\n\ndef length_norm(doc_lengths: NDArray[np.float64], avgdl: float, b: float) -> NDArray[np.float64]:\n    \"\"\"EVOLVE: Pivoted 1-b+b*dl/avgdl; none 1.0; log 1/log(e+dl); sqrt 1/sqrt(dl).\"\"\"\n    return 1.0 - b + b * (doc_lengths / max(avgdl, 1.0))\n\n\n# -----------------------------------------------------------------------------\n# Query term weights \u2014 EVOLVE: unique (1 per term), count (qtf), saturated (k3)\n# -----------------------------------------------------------------------------\n\ndef query_weights(\n    query: list[str], k3: float, mode: str = \"count\"\n) -> tuple[list[str], NDArray[np.float64]]:\n    \"\"\"EVOLVE: unique (bag-of-words), count (qtf), saturated (k3+1)*qtf/(k3+qtf). Default count matches Pyserini.\"\"\"\n    if not query:\n        return [], np.array([], dtype=np.float64)\n    cnt = Counter(query)\n    terms = list(cnt.keys())\n    if mode == \"unique\":\n        w = np.ones(len(terms), dtype=np.float64)\n    elif mode == \"count\":\n        w = np.array([float(cnt[t]) for t in terms], dtype=np.float64)\n    elif mode == \"saturated\":\n        qtf = np.array([float(cnt[t]) for t in terms], dtype=np.float64)\n        w = (k3 + 1.0) * qtf / (k3 + qtf)\n    else:\n        w = np.array([float(cnt[t]) for t in terms], dtype=np.float64)\n    return terms, w\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        V = len(self._vocab)\n\n        tf_lil = lil_matrix((V, self.N), dtype=np.float64)\n        self._posting_lists: dict[int, list[int]] = {i: [] for i in range(V)}\n        self._df = np.zeros(V, dtype=np.float64)\n\n        for doc_idx, doc in enumerate(documents):\n            seen = set()\n            for term, count in Counter(doc).items():\n                tid = self._vocab[term]\n                tf_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._posting_lists[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_lil)\n        self._posting_lists = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._posting_lists.items()\n            if doc_ids\n        }\n\n    def __len__(self) -> int:\n        return self.N\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self._df[tid]) if tid is not None else 0\n\n    def get_df_by_id(self, term_id: int) -> int:\n        return int(self._df[term_id])\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_tf_by_id(self, term_id: int, doc_idx: int) -> float:\n        return float(self.tf_matrix[term_id, doc_idx])\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_posting_list_by_id(self, term_id: int) -> NDArray[np.int64]:\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return len(self._vocab)\n\n    @property\n    def vocab_size(self) -> int:\n        return len(self._vocab)\n\n    @property\n    def idf_array(self) -> NDArray[np.float64]:\n        return idf(self._df, self.N)\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(\n        self,\n        corpus: Corpus,\n        k1: float | None = None,\n        b: float | None = None,\n        k3: float | None = None,\n    ):\n        self.corpus = corpus\n        self.k1 = k1 if k1 is not None else Parameters.k1\n        self.b = b if b is not None else Parameters.b\n        self.k3 = k3 if k3 is not None else Parameters.k3\n        self.idf_array = idf(corpus._df, corpus.N)\n        self.norm_array = length_norm(corpus.doc_lengths, corpus.avgdl, self.b)\n        self._idf_by_term = {term: float(self.idf_array[tid]) for term, tid in corpus._vocab.items()}\n\n    def score_document(self, query_terms: list[str], doc_idx: int) -> float:\n        \"\"\"EVOLVE: same formula as vectorized path (IDF \u00d7 saturated TF, sum).\"\"\"\n        norm = self.norm_array[doc_idx]\n        s = 0.0\n        for term in query_terms:\n            idf_val = self._idf_by_term.get(term, 0.0)\n            if idf_val == 0:\n                continue\n            tf_val = self.corpus.get_tf(doc_idx, term)\n            if tf_val == 0:\n                continue\n            s += idf_val * tf_saturated_scalar(float(tf_val), self.k1, norm)\n        return s\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        norms = self.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        for i, tid in enumerate(query_term_ids):\n            idf_val = self.idf_array[tid]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[tid, candidate_docs].toarray().flatten()\n            scores += w * idf_val * tf_saturated(tf_row, self.k1, norms)\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[NDArray[np.int64], NDArray[np.float64]]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        terms, weights = query_weights(query, self.k3, \"saturated\")\n        term_ids = []\n        w_arr = []\n        for t, w in zip(terms, weights):\n            tid = self.corpus.get_term_id(t)\n            if tid is not None:\n                term_ids.append(tid)\n                w_arr.append(w)\n        if not term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        w_arr = np.array(w_arr, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in term_ids:\n            candidate_set.update(self.corpus.get_posting_list_by_id(tid).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        cand_scores = self._score_candidates_vectorized(term_ids, candidate_docs, w_arr)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = cand_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[NDArray[np.int64], NDArray[np.float64]]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n    def score(self, query: list[str], doc_idx: int) -> float:\n        return self.score_document(query, doc_idx)\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"Parameters\",\n    \"idf\",\n    \"tf_saturated\",\n    \"length_norm\",\n    \"query_weights\",\n]\n", "language": "python", "parent_id": "08095786-23cd-4fc6-80cd-0f0f8c876e4b", "generation": 3, "timestamp": 1770020834.328257, "iteration_found": 0, "metrics": {"beir_nfcorpus_ndcg@10": 0.3201447752481062, "beir_nfcorpus_recall@100": 0.2550849295089316, "beir_nfcorpus_index_time_ms": 3472.8421670151874, "beir_nfcorpus_query_time_ms": 89.23300000606105, "beir_scifact_ndcg@10": 0.6766071215992575, "beir_scifact_recall@100": 0.9286666666666668, "beir_scifact_index_time_ms": 4684.896917024162, "beir_scifact_query_time_ms": 321.7797920224257, "bright_pony_ndcg@10": 0.07774665669833704, "bright_pony_recall@100": 0.24969662721378968, "bright_pony_index_time_ms": 1320.712500018999, "bright_pony_query_time_ms": 411.01854102453217, "beir_arguana_ndcg@10": 0.2950583055884139, "beir_arguana_recall@100": 0.926481084939329, "beir_arguana_index_time_ms": 5636.301249964163, "beir_arguana_query_time_ms": 17326.747125014663, "bright_theoremqa_theorems_ndcg@10": 0.023684274886022504, "bright_theoremqa_theorems_recall@100": 0.1337719298245614, "bright_theoremqa_theorems_index_time_ms": 10497.100959008094, "bright_theoremqa_theorems_query_time_ms": 734.1264170245267, "beir_scidocs_ndcg@10": 0.14974633420630584, "beir_scidocs_recall@100": 0.3478333333333334, "beir_scidocs_index_time_ms": 19156.10291698249, "beir_scidocs_query_time_ms": 2976.8008340033703, "bright_biology_ndcg@10": 0.1850915400882199, "bright_biology_recall@100": 0.43204831845608543, "bright_biology_index_time_ms": 14496.669749962166, "bright_biology_query_time_ms": 1986.196584010031, "bright_economics_ndcg@10": 0.16923988487221636, "bright_economics_recall@100": 0.4016863347391702, "bright_economics_index_time_ms": 13730.00525002135, "bright_economics_query_time_ms": 2027.6329169864766, "beir_fiqa_ndcg@10": 0.23645728262642543, "beir_fiqa_recall@100": 0.5372055285249729, "beir_fiqa_index_time_ms": 32787.72191697499, "beir_fiqa_query_time_ms": 5426.057500008028, "bright_earth_science_ndcg@10": 0.30911780758507307, "bright_earth_science_recall@100": 0.6112948089921775, "bright_earth_science_index_time_ms": 31373.032749979757, "bright_earth_science_query_time_ms": 3382.3957920540124, "bright_stackoverflow_ndcg@10": 0.183431379233003, "bright_stackoverflow_recall@100": 0.469874789493262, "bright_stackoverflow_index_time_ms": 97118.40149998898, "bright_stackoverflow_query_time_ms": 7190.2598749729805, "beir_trec-covid_ndcg@10": 0.6690006474391785, "beir_trec-covid_recall@100": 0.10892541820470526, "beir_trec-covid_index_time_ms": 130890.90387499891, "beir_trec-covid_query_time_ms": 1250.6079580052756, "avg_ndcg@10": 0.27461050083921323, "avg_recall@100": 0.4502141474914154, "combined_score": 0.415093418160975, "total_index_time_ms": 365164.69175193924, "total_query_time_ms": 43122.85633513238, "total_time_ms": 408287.5480870716, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  def tf_saturated(tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]) -> NDArray[np.flo...\n      \"\"\"EVOLVE: Lucene tf/(tf+k1*norm); Robertson (k1+1)*tf/(tf+k1*norm); log log(1+tf)/(tf+k1*nor...\n      return tf / (tf + k1 * norm + 1e-9)\n  \n  \n  def tf_saturated_scalar(tf: float, k1: float, norm: float) -> float:\n      return tf / (tf + k1 * norm + 1e-9)\nwith:\n  def tf_saturated(tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]) -> NDArray[np.flo...\n      \"\"\"EVOLVE: Lucene tf/(tf+k1*norm); Robertson (k1+1)*tf/(tf+k1*norm); log log(1+tf)/(tf+k1*nor...\n      return (k1 + 1.0) * tf / (tf + k1 * norm + 1e-9)\n  \n  \n  def tf_saturated_scalar(tf: float, k1: float, norm: float) -> float:\n      return (k1 + 1.0) * tf / (tf + k1 * norm + 1e-9)\nChange 2: 'terms, weights = query_weights(query, self.k3, \"count\")' to 'terms, weights = query_weights(query, self.k3, \"saturated\")'\nChange 3: Replace:\n  class Parameters:\n      k1: float = 1.2\n      b: float = 0.75\n      k3: float = 8.0\nwith:\n  class Parameters:\n      k1: float = 0.9\n      b: float = 0.4\n      k3: float = 8.0", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.32110337203651507, "beir_nfcorpus_recall@100": 0.2549342934874981, "beir_nfcorpus_index_time_ms": 3477.738249988761, "beir_nfcorpus_query_time_ms": 89.15437501855195, "beir_scifact_ndcg@10": 0.6827830892391584, "beir_scifact_recall@100": 0.9275555555555555, "beir_scifact_index_time_ms": 4675.194041978102, "beir_scifact_query_time_ms": 326.5905419830233, "bright_pony_ndcg@10": 0.020578093313154435, "bright_pony_recall@100": 0.11688022483211882, "bright_pony_index_time_ms": 1346.4263329515234, "bright_pony_query_time_ms": 406.1109580215998, "beir_arguana_ndcg@10": 0.3605484442463974, "beir_arguana_recall@100": 0.9621698786581013, "beir_arguana_index_time_ms": 5742.451041995082, "beir_arguana_query_time_ms": 16956.130209029652, "bright_theoremqa_theorems_ndcg@10": 0.010180337032487937, "bright_theoremqa_theorems_recall@100": 0.09649122807017542, "bright_theoremqa_theorems_index_time_ms": 10320.65850001527, "bright_theoremqa_theorems_query_time_ms": 723.1157089699991, "beir_scidocs_ndcg@10": 0.15574299266443528, "beir_scidocs_recall@100": 0.36238333333333334, "beir_scidocs_index_time_ms": 19042.955666023772, "beir_scidocs_query_time_ms": 2952.867374988273, "bright_economics_ndcg@10": 0.11969268515502328, "bright_economics_recall@100": 0.3617073602735931, "bright_economics_index_time_ms": 13585.0577079691, "bright_economics_query_time_ms": 2016.3212500046939, "bright_biology_ndcg@10": 0.07747216735710574, "bright_biology_recall@100": 0.34127458302215585, "bright_biology_index_time_ms": 14903.13883399358, "bright_biology_query_time_ms": 1936.37679197127, "beir_fiqa_ndcg@10": 0.24981229591034987, "beir_fiqa_recall@100": 0.5514906860971676, "beir_fiqa_index_time_ms": 32161.283499968704, "beir_fiqa_query_time_ms": 5356.184041011147, "bright_earth_science_ndcg@10": 0.1301873933380722, "bright_earth_science_recall@100": 0.4073550598675371, "bright_earth_science_index_time_ms": 30873.63758299034, "bright_earth_science_query_time_ms": 3344.8614589869976, "bright_stackoverflow_ndcg@10": 0.15833579633370207, "bright_stackoverflow_recall@100": 0.46628330857155853, "bright_stackoverflow_index_time_ms": 97109.06708304537, "bright_stackoverflow_query_time_ms": 6900.60525003355, "beir_trec-covid_ndcg@10": 0.680808358541072, "beir_trec-covid_recall@100": 0.11190501506103502, "beir_trec-covid_index_time_ms": 130740.64141698182, "beir_trec-covid_query_time_ms": 1237.089041969739, "avg_ndcg@10": 0.24727041876395614, "avg_recall@100": 0.4133692105691525, "combined_score": 0.38014945220811325, "total_index_time_ms": 363978.2499579014, "total_query_time_ms": 42245.4070019885, "total_time_ms": 406223.6569598899, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}