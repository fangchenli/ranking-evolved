{"id": "20eed624-beae-4d64-958d-4c659077be42", "code": "\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # --- Robust background LM (token-LM tempered, plus tiny uniform) ---\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # --- Discriminative diagnostics (query-independent) ---\n    # EDR: compares df-based LM vs token-based LM to downweight \"everywhere tokens\".\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n    residual_idf_strength: float = 0.90\n\n    # Query term frequency saturation (avoid long queries / repeats dominating)\n    query_tf_power: float = 0.60\n\n    # Missing-term penalty (soft AND) + coordination reward\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n        else:\n            tid = None\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM (tempered) + tiny uniform mixture ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            w = qtf * float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(self.corpus._ridf_qweight[term_id]), Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "615cac7e-e58c-454f-af1e-73810d40c005", "generation": 4, "timestamp": 1770361534.150671, "iteration_found": 64, "metrics": {"beir_nfcorpus_ndcg@10": 0.31458960153542515, "beir_nfcorpus_recall@100": 0.25388181173507823, "beir_nfcorpus_index_time_ms": 3587.0279170339927, "beir_nfcorpus_query_time_ms": 116.1441250005737, "beir_scifact_ndcg@10": 0.6753781226844201, "beir_scifact_recall@100": 0.9182222222222223, "beir_scifact_index_time_ms": 4875.967665924691, "beir_scifact_query_time_ms": 437.8905830672011, "bright_pony_ndcg@10": 0.10060348063268999, "bright_pony_recall@100": 0.332758003220113, "bright_pony_index_time_ms": 1480.8186660520732, "bright_pony_query_time_ms": 583.7641670368612, "beir_arguana_ndcg@10": 0.27302892625523034, "beir_arguana_recall@100": 0.9107780157030693, "beir_arguana_index_time_ms": 5925.305834040046, "beir_arguana_query_time_ms": 26721.95899998769, "bright_theoremqa_theorems_ndcg@10": 0.025975991990398482, "bright_theoremqa_theorems_recall@100": 0.16228070175438594, "bright_theoremqa_theorems_index_time_ms": 10684.2257080134, "bright_theoremqa_theorems_query_time_ms": 1159.4419580651447, "beir_scidocs_ndcg@10": 0.14639769535489, "beir_scidocs_recall@100": 0.34463333333333335, "beir_scidocs_index_time_ms": 19959.073917008936, "beir_scidocs_query_time_ms": 4009.5900000305846, "bright_economics_ndcg@10": 0.14268912408955492, "bright_economics_recall@100": 0.36724238582826746, "bright_economics_index_time_ms": 14228.898457949981, "bright_economics_query_time_ms": 3185.328916995786, "bright_biology_ndcg@10": 0.29738366644260483, "bright_biology_recall@100": 0.5918539814809616, "bright_biology_index_time_ms": 15372.68004193902, "bright_biology_query_time_ms": 2941.454916028306, "beir_fiqa_ndcg@10": 0.2185687218823367, "beir_fiqa_recall@100": 0.514353256482886, "beir_fiqa_index_time_ms": 34555.933958035894, "beir_fiqa_query_time_ms": 6908.0111250514165, "bright_earth_science_ndcg@10": 0.30884341040144275, "bright_earth_science_recall@100": 0.6646287954080605, "bright_earth_science_index_time_ms": 32266.679750056937, "bright_earth_science_query_time_ms": 4939.875958953053, "bright_stackoverflow_ndcg@10": 0.18587385029535528, "bright_stackoverflow_recall@100": 0.4358437538107704, "bright_stackoverflow_index_time_ms": 99308.36404196452, "bright_stackoverflow_query_time_ms": 11363.885374972597, "beir_trec-covid_ndcg@10": 0.615575945029311, "beir_trec-covid_recall@100": 0.10549730995042245, "beir_trec-covid_index_time_ms": 133609.71020907164, "beir_trec-covid_query_time_ms": 1731.5567090408877, "avg_ndcg@10": 0.2754090447161383, "avg_recall@100": 0.4668311309107976, "combined_score": 0.42854671367186575, "total_index_time_ms": 375854.68616709113, "total_query_time_ms": 64098.9028342301, "total_time_ms": 439953.58900132123, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Query term weighting (IDF curvature)\n      q_idf_power: float = 0.70\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.10\n      miss_cap: float = 1.4\n  \n      # NEW: coordination reward (soft AND without harsh penalties)\n      # Encourages documents that match a larger fraction of (weighted) query terms.\n      coord_strength: float = 0.12\n  \n      # NEW: length-normalize the missing-term evidence:\n      # omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\nwith:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # --- Robust background LM (token-LM tempered, plus tiny uniform) ---\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n  \n      # --- Discriminative diagnostics (query-independent) ---\n      # EDR: compares df-based LM vs token-based LM to downweight \"everywhere tokens\".\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n      residual_idf_strength: float = 0.90\n  \n      # Query term frequency saturation (avoid long queries / repeats dominating)\n      query_tf_power: float = 0.60\n  \n      # Missing-term penalty (soft AND) + coordination reward\n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\nChange 2: Replace:\n  def collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n      \"\"\"\n      Collection probability P(w | C) = total frequency / total tokens.\n      EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n      \"\"\"\n      if term not in corpus_term_freq:\n          return Config.epsilon\n      return corpus_term_freq[term] / max(total_tokens, 1)\nwith:\n  def collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n      \"\"\"\n      Collection probability P(w | C).\n  \n      Fallback used outside Corpus precomputation:\n        p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n      Exact renormalization is done in Corpus for speed/consistency.\n      \"\"\"\n      tf = corpus_term_freq.get(term, 0)\n      if tf <= 0 or total_tokens <= 0:\n          return Config.epsilon\n      p = tf / float(total_tokens)\n      tau = float(getattr(Config, \"collection_temper\", 1.0))\n      return max(p ** tau, Config.epsilon)\nChange 3: Replace:\n  # Collection probability array for vectorized scoring\n          self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n          for term, tid in self._vocab.items():\n              self._collection_prob[tid] = collection_probability(\n                  term, self.corpus_term_freq, self.total_tokens\n              )\n  \n          self._posting_lists: dict[int, NDArray[np.int64]] = {\n              tid: np.array(doc_ids, dtype=np.int64)\n              for tid, doc_ids in self._inverted_index.items()\n              if doc_ids\n          }\n          del self._inverted_index\n          self.document_length = self.doc_lengths\nwith:\n  # --- Collection LM (tempered) + tiny uniform mixture ---\n          tau = float(getattr(Config, \"collection_temper\", 1.0))\n          base_p = np.zeros(self.vocab_size, dtype=np.float64)\n          for term, tid in self._vocab.items():\n              base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens...\n  \n          if tau != 1.0:\n              tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n              z = float(np.sum(tmp))\n              p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n          else:\n              p_col = np.maximum(base_p, Config.epsilon)\n  \n          gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n          if gamma > 0.0 and self.vocab_size > 0:\n              p_uni = 1.0 / float(self.vocab_size)\n              self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsi...\n          else:\n              self._collection_prob = p_col\n  \n          # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n          if self.N > 0:\n              p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n          else:\n              p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n  \n          lam = float(getattr(Config, \"edr_strength\", 0.0))\n          clipc = float(getattr(Config, \"edr_clip\", 3.0))\n          ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n  \n  ... (21 more lines)\nChange 4: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      CRAD-QL + Coordination Reward + Length-normalized Coverage Evidence\n  \n      Two principled extensions aimed at improving nDCG@10 without killing recall@100:\n  \n      (1) Length-normalized missing-term penalty:\n          Missing-term evidence is weaker for long docs (they cover more topics, omission is less d...\n          stronger for short docs. This is a simple relevance prior over \"query completeness\" condi...\n  \n      (2) Coordination reward:\n          Add a bounded reward for matching a larger fraction of the query (weighted by qtf).\n          This behaves like a soft-AND that does not require tuning large negative penalties.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  ... (43 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n  \n      Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n      Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n          ridf(w) = max(0, log p_doc(w) - log p_col(w))\n      and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n      (scientific, code/math, web) than raw IDF alone.\n  \n      Keep your:\n        - length-normalized missing-term penalty (soft AND)\n        - coordination reward (bounded)\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n  ... (51 more lines)\nChange 5: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              idf = math.log((N + 1.0) / (df + 1.0) + eps)\n              spec = (np.maximum(idf, 0.0) + 1e-3) ** Config.q_idf_power\n  \n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n              denominator = (doc_lengths + mu_t) / mu_t\n              per_term = np.log(numerator / denominator + eps)\n  \n              qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n  \n              pos = np.maximum(per_term, 0.0)\n              # Length-normalized missing-term penalty (vectorized)\n              avgdl = max(float(self.corpus.avgdl), 1.0)\n              len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n              len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n              miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n              )\n  \n              scores += qtf * (float(spec) * pos + miss)\n  \n  ... (11 more lines)\nwith:\n  alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n  \n          # Hoist length factor (used in missing-term evidence)\n          avgdl = max(float(self.corpus.avgdl), 1.0)\n          len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n          len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n  \n          # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(...\n          total_q = total_q if total_q > 0.0 else 1.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n              denominator = (doc_lengths + mu_t) / mu_t\n              per_term = np.log(numerator / denominator + eps)\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf = float(qtf_raw) ** alpha\n  \n              pos = np.maximum(per_term, 0.0)\n  \n              # Residual-IDF weighting + EDR gate (query-independent)\n  ... (17 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.30562423375150366, "beir_nfcorpus_recall@100": 0.25020778477112793, "beir_nfcorpus_index_time_ms": 3594.5438750786707, "beir_nfcorpus_query_time_ms": 155.31637496314943, "beir_scifact_ndcg@10": 0.6587746648551214, "beir_scifact_recall@100": 0.9208888888888889, "beir_scifact_index_time_ms": 4888.9797910815105, "beir_scifact_query_time_ms": 703.0397499911487, "bright_pony_ndcg@10": 0.042577796436666225, "bright_pony_recall@100": 0.26871676957044927, "bright_pony_index_time_ms": 1400.3671249374747, "bright_pony_query_time_ms": 1014.3714169971645, "beir_arguana_ndcg@10": 0.2880139056532775, "beir_arguana_recall@100": 0.9243397573162027, "beir_arguana_index_time_ms": 5977.833375101909, "beir_arguana_query_time_ms": 47951.679915888235, "bright_theoremqa_theorems_ndcg@10": 0.016931143362813306, "bright_theoremqa_theorems_recall@100": 0.09649122807017543, "bright_theoremqa_theorems_index_time_ms": 10594.975124928169, "bright_theoremqa_theorems_query_time_ms": 1979.1338329669088, "beir_scidocs_ndcg@10": 0.13692535509520376, "beir_scidocs_recall@100": 0.3280666666666667, "beir_scidocs_index_time_ms": 19757.828666013665, "beir_scidocs_query_time_ms": 5740.90441595763, "bright_economics_ndcg@10": 0.14057815852973815, "bright_economics_recall@100": 0.3978628349506645, "bright_economics_index_time_ms": 14338.546958984807, "bright_economics_query_time_ms": 5672.781458939426, "bright_biology_ndcg@10": 0.2834733405903554, "bright_biology_recall@100": 0.5344390039075783, "bright_biology_index_time_ms": 15438.075791927986, "bright_biology_query_time_ms": 5263.706166064367, "beir_fiqa_ndcg@10": 0.2160882249373644, "beir_fiqa_recall@100": 0.49235332996907066, "beir_fiqa_index_time_ms": 33056.028833962046, "beir_fiqa_query_time_ms": 9984.154292033054, "bright_earth_science_ndcg@10": 0.3547307923290461, "bright_earth_science_recall@100": 0.6754621909397318, "bright_earth_science_index_time_ms": 31389.252041932195, "bright_earth_science_query_time_ms": 9047.844958025962, "bright_stackoverflow_ndcg@10": 0.17053439900790404, "bright_stackoverflow_recall@100": 0.44922603156894886, "bright_stackoverflow_index_time_ms": 98741.14954099059, "bright_stackoverflow_query_time_ms": 20812.30979191605, "beir_trec-covid_ndcg@10": 0.5293123081070294, "beir_trec-covid_recall@100": 0.09256970683627117, "beir_trec-covid_index_time_ms": 136884.2298340751, "beir_trec-covid_query_time_ms": 2383.100499981083, "avg_ndcg@10": 0.2619636935546686, "avg_recall@100": 0.45255201612131474, "combined_score": 0.4144343516079856, "total_index_time_ms": 376061.8109590141, "total_query_time_ms": 110708.34287372418, "total_time_ms": 486770.1538327383, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new probabilistic retrieval method** based on Query Likelihood language models. The seed program is a minimal skeleton: document representation, query representation, collection model, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just Dirichlet smoothing with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall@100, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the probabilistic model with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 \u03bc (smoothing parameter), epsilon, and any new parameters you need.\n2. **collection_probability(term, ...)** \u2014 How to compute P(w | C) (collection language model). EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, hierarchical, etc.).\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. Default: Dirichlet smoothing. Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors, query models, multi-field models, term dependencies, etc. You can use multiple sub-signals and combine them, or a single unified formula.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **QL._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **QL**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **QL** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and QL.rank() / QL.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n- Avoid taking log(0) (use max(value, epsilon) before log).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"smoothing prevents zero probabilities\" or \"collection model favors discriminative terms\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different smoothing methods, document priors, query expansion, term dependencies) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n- Probabilistic, information-theoretic, and language modeling principles are good foundations.\n", "user": "# Current Program Information\n- Fitness: 0.4144\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.4141 \u2192 0.4144\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Unknown changes\n- Metrics: beir_nfcorpus_ndcg@10: 0.3085, beir_nfcorpus_recall@100: 0.2517, beir_nfcorpus_index_time_ms: 3613.0808, beir_nfcorpus_query_time_ms: 98.9057, beir_scifact_ndcg@10: 0.6663, beir_scifact_recall@100: 0.9109, beir_scifact_index_time_ms: 4899.2214, beir_scifact_query_time_ms: 374.1062, bright_pony_ndcg@10: 0.0390, bright_pony_recall@100: 0.2574, bright_pony_index_time_ms: 1380.7682, bright_pony_query_time_ms: 498.8933, beir_arguana_ndcg@10: 0.2677, beir_arguana_recall@100: 0.9201, beir_arguana_index_time_ms: 5970.8886, beir_arguana_query_time_ms: 21909.3163, bright_theoremqa_theorems_ndcg@10: 0.0171, bright_theoremqa_theorems_recall@100: 0.0943, bright_theoremqa_theorems_index_time_ms: 10705.6708, bright_theoremqa_theorems_query_time_ms: 909.8287, beir_scidocs_ndcg@10: 0.1368, beir_scidocs_recall@100: 0.3368, beir_scidocs_index_time_ms: 19663.2320, beir_scidocs_query_time_ms: 3342.3262, bright_economics_ndcg@10: 0.1457, bright_economics_recall@100: 0.3872, bright_economics_index_time_ms: 14277.3363, bright_economics_query_time_ms: 2511.1261, bright_biology_ndcg@10: 0.2994, bright_biology_recall@100: 0.5734, bright_biology_index_time_ms: 14902.2635, bright_biology_query_time_ms: 2403.9985, beir_fiqa_ndcg@10: 0.1893, beir_fiqa_recall@100: 0.4803, beir_fiqa_index_time_ms: 33321.0246, beir_fiqa_query_time_ms: 5897.5140, bright_earth_science_ndcg@10: 0.3028, bright_earth_science_recall@100: 0.6630, bright_earth_science_index_time_ms: 31438.2908, bright_earth_science_query_time_ms: 4118.5245, bright_stackoverflow_ndcg@10: 0.1783, bright_stackoverflow_recall@100: 0.4605, bright_stackoverflow_index_time_ms: 98091.5534, bright_stackoverflow_query_time_ms: 8933.1637, beir_trec-covid_ndcg@10: 0.5513, beir_trec-covid_recall@100: 0.1004, beir_trec-covid_index_time_ms: 131890.5413, beir_trec-covid_query_time_ms: 1422.2429, avg_ndcg@10: 0.2585, avg_recall@100: 0.4530, combined_score: 0.4141, total_index_time_ms: 370153.8717, total_query_time_ms: 52419.9462, total_time_ms: 422573.8179, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength (kept; works reasonably across domains)\n      mu_df_power: float = 0.30\n  \n      # Query term weighting\n      q_idf_power: float = 0.75\n  \n      # --- New: coverage-aware soft negative evidence (query completeness) ---\n      # Lucene-style clamp (max(.,0)) can over-reward partial matches (good for recall,\n      # sometimes hurts nDCG@10). We add a *small* bounded penalty for missing query terms,\n      # scaled by IDF so missing rare terms matter more.\n      miss_penalty: float = 0.12\n  \n      # Bound the penalty so long docs aren't overly punished; interpret as a saturated\n      # log Bayes factor against relevance for a missing term.\n      miss_cap: float = 1.5\nwith:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Query term weighting (IDF curvature)\n      q_idf_power: float = 0.70\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.10\n      miss_cap: float = 1.4\n  \n      # NEW: coordination reward (soft AND without harsh penalties)\n      # Encourages documents that match a larger fraction of (weighted) query terms.\n      coord_strength: float = 0.12\n  \n      # NEW: length-normalize the missing-term evidence:\n      # omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Coverage-regularized Adaptive-Dirichlet QL (CRAD-QL)\n  \n      Keeps the strong part of Lucene-style LMDirichlet (only positive per-term evidence),\n      *but* adds a bounded, IDF-shaped penalty for query terms that are absent in the doc.\n  \n      Intuition:\n      - Positive evidence answers: \"does this doc generate the observed query term unusually well?\"\n      - Missing-term penalty answers: \"how surprising is it that an actually relevant doc would omit\n        this informative term?\" (soft AND / coverage prior)\n      This tends to improve early precision (nDCG@10) without destroying recall@100 because the\n      penalty is small and capped.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      score = 0.0\n      N = float(corpus.N) if corpus is not None else 1.0\n  \n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0.0))\n          p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n  \n          df = float(corpus.get_df(term)) if corpus is not None else 1.0\n  ... (25 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      CRAD-QL + Coordination Reward + Length-normalized Coverage Evidence\n  \n      Two principled extensions aimed at improving nDCG@10 without killing recall@100:\n  \n      (1) Length-normalized missing-term penalty:\n          Missing-term evidence is weaker for long docs (they cover more topics, omission is less d...\n          stronger for short docs. This is a simple relevance prior over \"query completeness\" condi...\n  \n      (2) Coordination reward:\n          Add a bounded reward for matching a larger fraction of the query (weighted by qtf).\n          This behaves like a soft-AND that does not require tuning large negative penalties.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  ... (43 more lines)\nChange 3: Replace:\n  miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -Config.miss_penalty * min(float(spec), Config.miss_cap),\n              )\n  \n              scores += qtf * (float(spec) * pos + miss)\n  \n          return scores\nwith:\n  # Length-normalized missing-term penalty (vectorized)\n              avgdl = max(float(self.corpus.avgdl), 1.0)\n              len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n              len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n              miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n              )\n  \n              scores += qtf * (float(spec) * pos + miss)\n  \n          # Coordination reward on candidates (weighted fraction of query terms matched)\n          if query_term_weights is not None and len(query_term_ids) > 0:\n              matched = np.zeros(len(candidate_docs), dtype=np.float64)\n              total_q = float(np.sum(query_term_weights)) if float(np.sum(query_term_weights)) > 0 ...\n              for i, term_id in enumerate(query_term_ids):\n                  tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n                  matched += (query_term_weights[i]) * (tf_row > 0.0)\n              coord = matched / total_q\n              scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n  \n          return scores\n- Metrics: beir_nfcorpus_ndcg@10: 0.3056, beir_nfcorpus_recall@100: 0.2502, beir_nfcorpus_index_time_ms: 3594.5439, beir_nfcorpus_query_time_ms: 155.3164, beir_scifact_ndcg@10: 0.6588, beir_scifact_recall@100: 0.9209, beir_scifact_index_time_ms: 4888.9798, beir_scifact_query_time_ms: 703.0397, bright_pony_ndcg@10: 0.0426, bright_pony_recall@100: 0.2687, bright_pony_index_time_ms: 1400.3671, bright_pony_query_time_ms: 1014.3714, beir_arguana_ndcg@10: 0.2880, beir_arguana_recall@100: 0.9243, beir_arguana_index_time_ms: 5977.8334, beir_arguana_query_time_ms: 47951.6799, bright_theoremqa_theorems_ndcg@10: 0.0169, bright_theoremqa_theorems_recall@100: 0.0965, bright_theoremqa_theorems_index_time_ms: 10594.9751, bright_theoremqa_theorems_query_time_ms: 1979.1338, beir_scidocs_ndcg@10: 0.1369, beir_scidocs_recall@100: 0.3281, beir_scidocs_index_time_ms: 19757.8287, beir_scidocs_query_time_ms: 5740.9044, bright_economics_ndcg@10: 0.1406, bright_economics_recall@100: 0.3979, bright_economics_index_time_ms: 14338.5470, bright_economics_query_time_ms: 5672.7815, bright_biology_ndcg@10: 0.2835, bright_biology_recall@100: 0.5344, bright_biology_index_time_ms: 15438.0758, bright_biology_query_time_ms: 5263.7062, beir_fiqa_ndcg@10: 0.2161, beir_fiqa_recall@100: 0.4924, beir_fiqa_index_time_ms: 33056.0288, beir_fiqa_query_time_ms: 9984.1543, bright_earth_science_ndcg@10: 0.3547, bright_earth_science_recall@100: 0.6755, bright_earth_science_index_time_ms: 31389.2520, bright_earth_science_query_time_ms: 9047.8450, bright_stackoverflow_ndcg@10: 0.1705, bright_stackoverflow_recall@100: 0.4492, bright_stackoverflow_index_time_ms: 98741.1495, bright_stackoverflow_query_time_ms: 20812.3098, beir_trec-covid_ndcg@10: 0.5293, beir_trec-covid_recall@100: 0.0926, beir_trec-covid_index_time_ms: 136884.2298, beir_trec-covid_query_time_ms: 2383.1005, avg_ndcg@10: 0.2620, avg_recall@100: 0.4526, combined_score: 0.4144, total_index_time_ms: 376061.8110, total_query_time_ms: 110708.3429, total_time_ms: 486770.1538, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Tempered background LM p_t(w) \u221d p(w)^tau\n      collection_temper: float = 0.85\n  \n      # Query term burstiness saturation (qtf^alpha)\n      query_tf_power: float = 0.6\n  \n      # Document length prior (log-normal-ish); keep small to avoid recall loss\n      length_prior_strength: float = 0.06\n  \n      # EDR gate: token-vs-document spread mismatch\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Residual-IDF query weighting (df/N vs token LM)\n      residual_idf_strength: float = 0.9\n  \n      # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n      burstiness_strength: float = 0.35\n  \n      # Two-stage background (collection + uniform over vocab).\n      uniform_bg_mass: float = 0.03  # 0 disables\n  \n      # Soft-AND query aggregation.\n      and_strength: float = 0.15  # 0 disables\n      and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n  \n  ... (9 more lines)\nwith:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Tempered background LM p_t(w) \u221d p(w)^tau\n      collection_temper: float = 0.85\n  \n      # Query term burstiness saturation (qtf^alpha)\n      query_tf_power: float = 0.6\n  \n      # Document length prior (log-normal-ish); keep small to avoid recall loss\n      length_prior_strength: float = 0.06\n  \n      # EDR gate: token-vs-document spread mismatch\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Residual-IDF query weighting (df/N vs token LM)\n      residual_idf_strength: float = 0.9\n  \n      # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n      burstiness_strength: float = 0.35\n  \n      # Two-stage background (collection + uniform over vocab).\n      uniform_bg_mass: float = 0.03  # 0 disables\n  \n      # Soft-AND query aggregation.\n      and_strength: float = 0.15  # 0 disables\n      and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n  \n  ... (13 more lines)\nChange 2: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              beta = self.corpus._tf_beta[term_id]\n              tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n  \n              per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / ...\n              per_term *= self.corpus._edr_gate[term_id]\n  \n              w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n              w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n  \n              present = np.maximum(per_term, 0.0)\n              contrib = w * present\n              scores += contrib\n  \n              if miss_s > 0.0:\n                  miss = (tf_row <= 0.0).astype(np.float64)\n                  if np.any(miss):\n                      base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                      scores += miss_s * w * miss * base0\n  \n              if and_acc is not None:\n                  and_acc += np.tanh(contrib / max(and_sat, eps))\n  \n              if scope_num is not None:\n                  # Stable per-term \"information mass\" proxy derived from residual IDF weights.\n                  info = float(np.log1p(self.corpus._ridf_qweight[term_id]))\n                  scope_den += info\n  ... (1 more lines)\nwith:\n  # Optional risk-sensitive aggregator accumulator\n          risk_s = float(getattr(Config, \"risk_strength\", 0.0))\n          risk_p = float(getattr(Config, \"risk_power\", 0.55))\n          risk_sat = float(getattr(Config, \"risk_saturation\", 6.0))\n          risk_acc = np.zeros(len(candidate_docs), dtype=np.float64) if risk_s > 0.0 else None\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              beta = self.corpus._tf_beta[term_id]\n              tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n  \n              per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / ...\n              per_term *= self.corpus._edr_gate[term_id]\n  \n              w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n              w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n  \n              present = np.maximum(per_term, 0.0)\n              contrib = w * present\n              scores += contrib\n  \n              if miss_s > 0.0:\n                  miss = (tf_row <= 0.0).astype(np.float64)\n                  if np.any(miss):\n                      base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                      scores += miss_s * w * miss * base0\n  \n              if and_acc is not None:\n  ... (13 more lines)\nChange 3: Replace:\n  if and_acc is not None and len(query_term_ids) > 0:\n              scores += and_strength * (and_acc / float(len(query_term_ids)))\n  \n          if scope_num is not None:\n              denom = max(scope_den, eps)\n              scope_frac = np.maximum(scope_num / denom, eps)\n              scores += scope_s * np.log(scope_frac)\n  \n          scores += self.corpus._length_prior[candidate_docs]\n          return scores\nwith:\n  if and_acc is not None and len(query_term_ids) > 0:\n              scores += and_strength * (and_acc / float(len(query_term_ids)))\n  \n          if scope_num is not None:\n              denom = max(scope_den, eps)\n              scope_frac = np.maximum(scope_num / denom, eps)\n              scores += scope_s * np.log(scope_frac)\n  \n          if risk_acc is not None and len(query_term_ids) > 0:\n              # Turn accumulator into a normalized power mean (scale-stable across query length).\n              pm = np.power(risk_acc / float(len(query_term_ids)), 1.0 / max(risk_p, eps))\n              scores += risk_s * pm\n  \n          scores += self.corpus._length_prior[candidate_docs]\n          return scores\n- Metrics: beir_nfcorpus_ndcg@10: 0.3099, beir_nfcorpus_recall@100: 0.2510, beir_nfcorpus_index_time_ms: 3587.2570, beir_nfcorpus_query_time_ms: 136.6278, beir_scifact_ndcg@10: 0.6731, beir_scifact_recall@100: 0.9253, beir_scifact_index_time_ms: 4847.7871, beir_scifact_query_time_ms: 625.4840, bright_pony_ndcg@10: 0.1070, bright_pony_recall@100: 0.3344, bright_pony_index_time_ms: 1377.6150, bright_pony_query_time_ms: 954.1228, beir_arguana_ndcg@10: 0.2579, beir_arguana_recall@100: 0.8929, beir_arguana_index_time_ms: 5882.7471, beir_arguana_query_time_ms: 46895.2606, bright_theoremqa_theorems_ndcg@10: 0.0336, bright_theoremqa_theorems_recall@100: 0.1689, bright_theoremqa_theorems_index_time_ms: 10962.1902, bright_theoremqa_theorems_query_time_ms: 1951.3770, beir_scidocs_ndcg@10: 0.1410, beir_scidocs_recall@100: 0.3394, beir_scidocs_index_time_ms: 19959.1565, beir_scidocs_query_time_ms: 5892.9617, bright_economics_ndcg@10: 0.1283, bright_economics_recall@100: 0.3688, bright_economics_index_time_ms: 14401.7161, bright_economics_query_time_ms: 5654.2075, bright_biology_ndcg@10: 0.2881, bright_biology_recall@100: 0.5996, bright_biology_index_time_ms: 14995.1182, bright_biology_query_time_ms: 5431.8961, beir_fiqa_ndcg@10: 0.2087, beir_fiqa_recall@100: 0.5024, beir_fiqa_index_time_ms: 33353.8632, beir_fiqa_query_time_ms: 10785.5394, bright_earth_science_ndcg@10: 0.2876, bright_earth_science_recall@100: 0.6603, bright_earth_science_index_time_ms: 32194.4714, bright_earth_science_query_time_ms: 8753.9237, bright_stackoverflow_ndcg@10: 0.1902, bright_stackoverflow_recall@100: 0.4512, bright_stackoverflow_index_time_ms: 98778.2065, bright_stackoverflow_query_time_ms: 20209.4303, beir_trec-covid_ndcg@10: 0.6108, beir_trec-covid_recall@100: 0.1025, beir_trec-covid_index_time_ms: 133951.1087, beir_trec-covid_query_time_ms: 2545.5148, avg_ndcg@10: 0.2697, avg_recall@100: 0.4664, combined_score: 0.4271, total_index_time_ms: 374291.2369, total_query_time_ms: 109836.3455, total_time_ms: 484127.5824, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4273)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n    and_strength: float = 0.14  # 0 disables\n    and_saturation: float = 3.0\n\n    # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n    missing_strength: float = 0.07  # keep small to protect recall@100\n\n    # NEW: TF burstiness normalization (per-term exponent) to reduce domination by very common terms.\n    # Probabilistic story: approximate a sublinear TF model (mixture of \"aboutness\" + repetition noise).\n    # Common terms (low idf) get stronger saturation.\n    burstiness_strength: float = 0.30  # 0 disables; keep modest for recall\n\n    # NEW: Add back a tiny amount of negative evidence for \"weak hits\":\n    # when a query term appears but below its expectation under the doc+prior mixture,\n    # we keep a small fraction of the negative log-likelihood ratio.\n    # This reduces ranking of boilerplate/long docs that mention many terms once.\n    neg_strength: float = 0.12  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        base = math.log(numerator / denominator + eps)\n        base = max(base, 0.0)\n\n        # Fallback term diagnostic: use a capped surprisal of the collection LM.\n        # (True residual-IDF uses df/N; we do that in vectorized rank()).\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n\n        # Approx residual-IDF in slow path: reuse spec as a proxy.\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with an optional uniform mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n        # Common terms saturate more: tf -> tf^beta(w).\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            # TF burstiness saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Apply EDR gate (query-independent)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            # Keep most of the classic \"only reward positive LLR\", but leak a small fraction\n            # of negative evidence for weak hits to improve early precision.\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += w * per_term  # includes leaked negatives if enabled\n\n            # Missing-term anti-evidence (scaled tf=0 Dirichlet contribution) within candidates.\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += miss_s * w * miss * base0\n\n            # Soft-AND: saturating coverage reward (encourages matching more query terms)\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3105), Performs well on beir_nfcorpus_recall@100 (0.2510), Performs well on beir_nfcorpus_index_time_ms (3585.8080), Performs well on beir_nfcorpus_query_time_ms (120.6080), Performs well on beir_scifact_ndcg@10 (0.6729), Performs well on beir_scifact_recall@100 (0.9253), Performs well on beir_scifact_index_time_ms (4856.9142), Performs well on beir_scifact_query_time_ms (534.4172), Performs well on bright_pony_ndcg@10 (0.1097), Performs well on bright_pony_recall@100 (0.3311), Performs well on bright_pony_index_time_ms (1473.4872), Performs well on bright_pony_query_time_ms (790.7443), Performs well on beir_arguana_ndcg@10 (0.2633), Performs well on beir_arguana_recall@100 (0.9051), Performs well on beir_arguana_index_time_ms (5881.2448), Performs well on beir_arguana_query_time_ms (38038.7680), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0339), Performs well on bright_theoremqa_theorems_recall@100 (0.1557), Performs well on bright_theoremqa_theorems_index_time_ms (10631.9319), Performs well on bright_theoremqa_theorems_query_time_ms (1605.4113), Performs well on beir_scidocs_ndcg@10 (0.1420), Performs well on beir_scidocs_recall@100 (0.3402), Performs well on beir_scidocs_index_time_ms (19643.1758), Performs well on beir_scidocs_query_time_ms (4853.3809), Performs well on bright_economics_ndcg@10 (0.1318), Performs well on bright_economics_recall@100 (0.3749), Performs well on bright_economics_index_time_ms (14090.7396), Performs well on bright_economics_query_time_ms (4476.3081), Performs well on bright_biology_ndcg@10 (0.2941), Performs well on bright_biology_recall@100 (0.5905), Performs well on bright_biology_index_time_ms (15207.7363), Performs well on bright_biology_query_time_ms (4380.0538), Performs well on beir_fiqa_ndcg@10 (0.2085), Performs well on beir_fiqa_recall@100 (0.5022), Performs well on beir_fiqa_index_time_ms (33344.9355), Performs well on beir_fiqa_query_time_ms (8908.5117), Performs well on bright_earth_science_ndcg@10 (0.2904), Performs well on bright_earth_science_recall@100 (0.6650), Performs well on bright_earth_science_index_time_ms (31310.2214), Performs well on bright_earth_science_query_time_ms (7206.8459), Performs well on bright_stackoverflow_ndcg@10 (0.1888), Performs well on bright_stackoverflow_recall@100 (0.4482), Performs well on bright_stackoverflow_index_time_ms (99579.9722), Performs well on bright_stackoverflow_query_time_ms (17001.6652), Performs well on beir_trec-covid_ndcg@10 (0.6172), Performs well on beir_trec-covid_recall@100 (0.1038), Performs well on beir_trec-covid_index_time_ms (137644.1000), Performs well on beir_trec-covid_query_time_ms (2097.8336), Performs well on avg_ndcg@10 (0.2719), Performs well on avg_recall@100 (0.4661), Performs well on combined_score (0.4273), Performs well on total_index_time_ms (377250.2669), Performs well on total_query_time_ms (90014.5479), Performs well on total_time_ms (467264.8148), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4271)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n    burstiness_strength: float = 0.35\n\n    # Two-stage background (collection + uniform over vocab).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND query aggregation.\n    and_strength: float = 0.15  # 0 disables\n    and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n\n    # Dirichlet \"anti-evidence\" for missing query terms (tf=0).\n    # Keep slightly smaller by default: too much anti-evidence can hurt recall@100.\n    missing_strength: float = 0.06\n\n    # Query-scope prior: reward explaining a larger fraction of the query's information mass.\n    query_scope_strength: float = 0.07  # 0 disables\n\n    # NEW: Risk-sensitive aggregation (R\u00e9nyi / power mean) over per-term evidence.\n    # Add a small concave utility on term evidence to prefer \"balanced\" multi-term matches\n    # over one-term spikes (improves early precision on multi-aspect queries).\n    risk_strength: float = 0.12  # 0 disables\n    risk_power: float = 0.55     # in (0,1]: smaller => more \"AND-like\"\n    risk_saturation: float = 6.0 # scale inside utility; keeps it stable across corpora\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n    scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        if burst_s > 0.0:\n            idf01_proxy = 1.0 / (1.0 + max(0.0, math.log(1.0 / max(p_collection, eps))))\n            beta = 1.0 - burst_s * (1.0 - idf01_proxy)\n            term_count = term_count ** beta if beta != 1.0 else term_count\n\n        base = math.log((1.0 + term_count / (mu * p_collection + eps)) / ((doc_length + mu) / mu) + eps)\n        base = max(base, 0.0)\n\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n        if scope_s > 0.0:\n            info = math.log1p(1.0 + ridf_s * (spec / max(clipc, eps)))\n            scope_den += info\n            if term_count > 0.0:\n                scope_num += info\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) for Dirichlet smoothing\n        # plus a small uniform background mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V)\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob[:] = (1.0 - gamma) * p_col + gamma * p_uni\n            self._collection_prob[:] = np.maximum(self._collection_prob, Config.epsilon)\n        else:\n            self._collection_prob[:] = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: IDF-in-[0,1] and burstiness exponent beta(w) for tf saturation (common terms saturate more).\n        # idf = log((N+1)/(df+1)) is classic; we normalize by max idf to get [0,1] without tuning per dataset.\n        if self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n        else:\n            idf01 = np.zeros(self.vocab_size, dtype=np.float64)\n\n        if burst_s > 0.0:\n            # beta in [1-burst_s, 1]; common terms (low idf01) -> smaller beta -> more saturation\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query-scope accumulators: per-doc matched specificity mass vs total query specificity mass.\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Optional risk-sensitive aggregator accumulator\n        risk_s = float(getattr(Config, \"risk_strength\", 0.0))\n        risk_p = float(getattr(Config, \"risk_power\", 0.55))\n        risk_sat = float(getattr(Config, \"risk_saturation\", 6.0))\n        risk_acc = np.zeros(len(candidate_docs), dtype=np.float64) if risk_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += contrib\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                    scores += miss_s * w * miss * base0\n\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n            if scope_num is not None:\n                # Stable per-term \"information mass\" proxy derived from residual IDF weights.\n                info = float(np.log1p(self.corpus._ridf_qweight[term_id]))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if risk_acc is not None:\n                # Concave utility of evidence; then power-mean across terms.\n                # u(x)=log(1+x/sat) is information-like (diminishing returns).\n                u = np.log1p(contrib / max(risk_sat, eps))\n                risk_acc += np.power(np.maximum(u, 0.0), risk_p)\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        if scope_num is not None:\n            denom = max(scope_den, eps)\n            scope_frac = np.maximum(scope_num / denom, eps)\n            scores += scope_s * np.log(scope_frac)\n\n        if risk_acc is not None and len(query_term_ids) > 0:\n            # Turn accumulator into a normalized power mean (scale-stable across query length).\n            pm = np.power(risk_acc / float(len(query_term_ids)), 1.0 / max(risk_p, eps))\n            scores += risk_s * pm\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3099), Performs well on beir_nfcorpus_recall@100 (0.2510), Performs well on beir_nfcorpus_index_time_ms (3587.2570), Performs well on beir_nfcorpus_query_time_ms (136.6278), Performs well on beir_scifact_ndcg@10 (0.6731), Performs well on beir_scifact_recall@100 (0.9253), Performs well on beir_scifact_index_time_ms (4847.7871), Performs well on beir_scifact_query_time_ms (625.4840), Performs well on bright_pony_ndcg@10 (0.1070), Performs well on bright_pony_recall@100 (0.3344), Performs well on bright_pony_index_time_ms (1377.6150), Performs well on bright_pony_query_time_ms (954.1228), Performs well on beir_arguana_ndcg@10 (0.2579), Performs well on beir_arguana_recall@100 (0.8929), Performs well on beir_arguana_index_time_ms (5882.7471), Performs well on beir_arguana_query_time_ms (46895.2606), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0336), Performs well on bright_theoremqa_theorems_recall@100 (0.1689), Performs well on bright_theoremqa_theorems_index_time_ms (10962.1902), Performs well on bright_theoremqa_theorems_query_time_ms (1951.3770), Performs well on beir_scidocs_ndcg@10 (0.1410), Performs well on beir_scidocs_recall@100 (0.3394), Performs well on beir_scidocs_index_time_ms (19959.1565), Performs well on beir_scidocs_query_time_ms (5892.9617), Performs well on bright_economics_ndcg@10 (0.1283), Performs well on bright_economics_recall@100 (0.3688), Performs well on bright_economics_index_time_ms (14401.7161), Performs well on bright_economics_query_time_ms (5654.2075), Performs well on bright_biology_ndcg@10 (0.2881), Performs well on bright_biology_recall@100 (0.5996), Performs well on bright_biology_index_time_ms (14995.1182), Performs well on bright_biology_query_time_ms (5431.8961), Performs well on beir_fiqa_ndcg@10 (0.2087), Performs well on beir_fiqa_recall@100 (0.5024), Performs well on beir_fiqa_index_time_ms (33353.8632), Performs well on beir_fiqa_query_time_ms (10785.5394), Performs well on bright_earth_science_ndcg@10 (0.2876), Performs well on bright_earth_science_recall@100 (0.6603), Performs well on bright_earth_science_index_time_ms (32194.4714), Performs well on bright_earth_science_query_time_ms (8753.9237), Performs well on bright_stackoverflow_ndcg@10 (0.1902), Performs well on bright_stackoverflow_recall@100 (0.4512), Performs well on bright_stackoverflow_index_time_ms (98778.2065), Performs well on bright_stackoverflow_query_time_ms (20209.4303), Performs well on beir_trec-covid_ndcg@10 (0.6108), Performs well on beir_trec-covid_recall@100 (0.1025), Performs well on beir_trec-covid_index_time_ms (133951.1087), Performs well on beir_trec-covid_query_time_ms (2545.5148), Performs well on avg_ndcg@10 (0.2697), Performs well on avg_recall@100 (0.4664), Performs well on combined_score (0.4271), Performs well on total_index_time_ms (374291.2369), Performs well on total_query_time_ms (109836.3455), Performs well on total_time_ms (484127.5824), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4144)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Query term weighting (IDF curvature)\n    q_idf_power: float = 0.70\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.10\n    miss_cap: float = 1.4\n\n    # NEW: coordination reward (soft AND without harsh penalties)\n    # Encourages documents that match a larger fraction of (weighted) query terms.\n    coord_strength: float = 0.12\n\n    # NEW: length-normalize the missing-term evidence:\n    # omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    CRAD-QL + Coordination Reward + Length-normalized Coverage Evidence\n\n    Two principled extensions aimed at improving nDCG@10 without killing recall@100:\n\n    (1) Length-normalized missing-term penalty:\n        Missing-term evidence is weaker for long docs (they cover more topics, omission is less diagnostic),\n        stronger for short docs. This is a simple relevance prior over \"query completeness\" conditioned on L_D.\n\n    (2) Coordination reward:\n        Add a bounded reward for matching a larger fraction of the query (weighted by qtf).\n        This behaves like a soft-AND that does not require tuning large negative penalties.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # In (0,1] typically: shrinks missing-term penalty for long docs.\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf = float(query_repr.term_weights.get(term, 1.0))\n        total_qw += qtf\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        pos = max(per_term, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(spec, Config.miss_cap)\n\n        score += qtf * (spec * pos + miss)\n\n    # Coordination reward: bounded, encourages fuller matches.\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (np.maximum(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            pos = np.maximum(per_term, 0.0)\n            # Length-normalized missing-term penalty (vectorized)\n            avgdl = max(float(self.corpus.avgdl), 1.0)\n            len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n            len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n            )\n\n            scores += qtf * (float(spec) * pos + miss)\n\n        # Coordination reward on candidates (weighted fraction of query terms matched)\n        if query_term_weights is not None and len(query_term_ids) > 0:\n            matched = np.zeros(len(candidate_docs), dtype=np.float64)\n            total_q = float(np.sum(query_term_weights)) if float(np.sum(query_term_weights)) > 0 else 1.0\n            for i, term_id in enumerate(query_term_ids):\n                tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n                matched += (query_term_weights[i]) * (tf_row > 0.0)\n            coord = matched / total_q\n            scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3056), Performs well on beir_nfcorpus_recall@100 (0.2502), Performs well on beir_nfcorpus_index_time_ms (3594.5439), Performs well on beir_nfcorpus_query_time_ms (155.3164), Performs well on beir_scifact_ndcg@10 (0.6588), Performs well on beir_scifact_recall@100 (0.9209), Performs well on beir_scifact_index_time_ms (4888.9798), Performs well on beir_scifact_query_time_ms (703.0397), Performs well on bright_pony_ndcg@10 (0.0426), Performs well on bright_pony_recall@100 (0.2687), Performs well on bright_pony_index_time_ms (1400.3671), Performs well on bright_pony_query_time_ms (1014.3714), Performs well on beir_arguana_ndcg@10 (0.2880), Performs well on beir_arguana_recall@100 (0.9243), Performs well on beir_arguana_index_time_ms (5977.8334), Performs well on beir_arguana_query_time_ms (47951.6799), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0169), Performs well on bright_theoremqa_theorems_recall@100 (0.0965), Performs well on bright_theoremqa_theorems_index_time_ms (10594.9751), Performs well on bright_theoremqa_theorems_query_time_ms (1979.1338), Performs well on beir_scidocs_ndcg@10 (0.1369), Performs well on beir_scidocs_recall@100 (0.3281), Performs well on beir_scidocs_index_time_ms (19757.8287), Performs well on beir_scidocs_query_time_ms (5740.9044), Performs well on bright_economics_ndcg@10 (0.1406), Performs well on bright_economics_recall@100 (0.3979), Performs well on bright_economics_index_time_ms (14338.5470), Performs well on bright_economics_query_time_ms (5672.7815), Performs well on bright_biology_ndcg@10 (0.2835), Performs well on bright_biology_recall@100 (0.5344), Performs well on bright_biology_index_time_ms (15438.0758), Performs well on bright_biology_query_time_ms (5263.7062), Performs well on beir_fiqa_ndcg@10 (0.2161), Performs well on beir_fiqa_recall@100 (0.4924), Performs well on beir_fiqa_index_time_ms (33056.0288), Performs well on beir_fiqa_query_time_ms (9984.1543), Performs well on bright_earth_science_ndcg@10 (0.3547), Performs well on bright_earth_science_recall@100 (0.6755), Performs well on bright_earth_science_index_time_ms (31389.2520), Performs well on bright_earth_science_query_time_ms (9047.8450), Performs well on bright_stackoverflow_ndcg@10 (0.1705), Performs well on bright_stackoverflow_recall@100 (0.4492), Performs well on bright_stackoverflow_index_time_ms (98741.1495), Performs well on bright_stackoverflow_query_time_ms (20812.3098), Performs well on beir_trec-covid_ndcg@10 (0.5293), Performs well on beir_trec-covid_recall@100 (0.0926), Performs well on beir_trec-covid_index_time_ms (136884.2298), Performs well on beir_trec-covid_query_time_ms (2383.1005), Performs well on avg_ndcg@10 (0.2620), Performs well on avg_recall@100 (0.4526), Performs well on combined_score (0.4144), Performs well on total_index_time_ms (376061.8110), Performs well on total_query_time_ms (110708.3429), Performs well on total_time_ms (486770.1538), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4141)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 2000.0  # Dirichlet smoothing parameter\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Score one document for one query using Query Likelihood with Dirichlet smoothing.\n\n    Formula (Lucene/Pyserini variant):\n        Score(D, Q) = \u03a3_{w in Q} max(0, log(1 + c(w,D)/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc)))\n\n    This matches Pyserini's LMDirichletSimilarity which clamps per-term scores to 0.\n    This differs from academic QL which allows negative per-term scores.\n\n    Default behavior: Matches Pyserini's LMDirichletSimilarity with \u03bc=2000.\n    EVOLVE: design a probabilistic formulation with deep, fundamental, intuitive justification.\n    Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors,\n    term dependencies, multi-field models, etc.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n\n    for term in query_repr.terms:\n        # c(w, D): term count in document\n        term_count = float(doc_tf.get(term, 0))\n\n        # P(w | C): collection probability\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        # Lucene/Pyserini formula: log(1 + freq/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc))\n        # Clamp per-term score to 0 (Lucene's behavior for negative term scores)\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        per_term_score = math.log(numerator / denominator + eps)\n\n        # Apply query term weight and clamp to 0\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * max(per_term_score, 0.0)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            # Get collection probability for this term\n            p_collection = self.corpus._collection_prob[term_id]\n\n            # Get term frequencies for candidates\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n\n            # Lucene/Pyserini formula: log(1 + freq/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc))\n            numerator = 1.0 + tf_row / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term_scores = np.log(numerator / denominator + eps)\n\n            # Apply query term weight and clamp to 0\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            scores += w * np.maximum(per_term_scores, 0.0)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3085), Performs well on beir_nfcorpus_recall@100 (0.2517), Performs well on beir_nfcorpus_index_time_ms (3613.0808), Performs well on beir_nfcorpus_query_time_ms (98.9057), Performs well on beir_scifact_ndcg@10 (0.6663), Performs well on beir_scifact_recall@100 (0.9109), Performs well on beir_scifact_index_time_ms (4899.2214), Performs well on beir_scifact_query_time_ms (374.1062), Performs well on bright_pony_ndcg@10 (0.0390), Performs well on bright_pony_recall@100 (0.2574), Performs well on bright_pony_index_time_ms (1380.7682), Performs well on bright_pony_query_time_ms (498.8933), Performs well on beir_arguana_ndcg@10 (0.2677), Performs well on beir_arguana_recall@100 (0.9201), Performs well on beir_arguana_index_time_ms (5970.8886), Performs well on beir_arguana_query_time_ms (21909.3163), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0171), Performs well on bright_theoremqa_theorems_recall@100 (0.0943), Performs well on bright_theoremqa_theorems_index_time_ms (10705.6708), Performs well on bright_theoremqa_theorems_query_time_ms (909.8287), Performs well on beir_scidocs_ndcg@10 (0.1368), Performs well on beir_scidocs_recall@100 (0.3368), Performs well on beir_scidocs_index_time_ms (19663.2320), Performs well on beir_scidocs_query_time_ms (3342.3262), Performs well on bright_economics_ndcg@10 (0.1457), Performs well on bright_economics_recall@100 (0.3872), Performs well on bright_economics_index_time_ms (14277.3363), Performs well on bright_economics_query_time_ms (2511.1261), Performs well on bright_biology_ndcg@10 (0.2994), Performs well on bright_biology_recall@100 (0.5734), Performs well on bright_biology_index_time_ms (14902.2635), Performs well on bright_biology_query_time_ms (2403.9985), Performs well on beir_fiqa_ndcg@10 (0.1893), Performs well on beir_fiqa_recall@100 (0.4803), Performs well on beir_fiqa_index_time_ms (33321.0246), Performs well on beir_fiqa_query_time_ms (5897.5140), Performs well on bright_earth_science_ndcg@10 (0.3028), Performs well on bright_earth_science_recall@100 (0.6630), Performs well on bright_earth_science_index_time_ms (31438.2908), Performs well on bright_earth_science_query_time_ms (4118.5245), Performs well on bright_stackoverflow_ndcg@10 (0.1783), Performs well on bright_stackoverflow_recall@100 (0.4605), Performs well on bright_stackoverflow_index_time_ms (98091.5534), Performs well on bright_stackoverflow_query_time_ms (8933.1637), Performs well on beir_trec-covid_ndcg@10 (0.5513), Performs well on beir_trec-covid_recall@100 (0.1004), Performs well on beir_trec-covid_index_time_ms (131890.5413), Performs well on beir_trec-covid_query_time_ms (1422.2429), Performs well on avg_ndcg@10 (0.2585), Performs well on avg_recall@100 (0.4530), Performs well on combined_score (0.4141), Performs well on total_index_time_ms (370153.8717), Performs well on total_query_time_ms (52419.9462), Performs well on total_time_ms (422573.8179), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4141)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength (kept; works reasonably across domains)\n    mu_df_power: float = 0.30\n\n    # Query term weighting\n    q_idf_power: float = 0.75\n\n    # --- New: coverage-aware soft negative evidence (query completeness) ---\n    # Lucene-style clamp (max(.,0)) can over-reward partial matches (good for recall,\n    # sometimes hurts nDCG@10). We add a *small* bounded penalty for missing query terms,\n    # scaled by IDF so missing rare terms matter more.\n    miss_penalty: float = 0.12\n\n    # Bound the penalty so long docs aren't overly punished; interpret as a saturated\n    # log Bayes factor against relevance for a missing term.\n    miss_cap: float = 1.5\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Coverage-regularized Adaptive-Dirichlet QL (CRAD-QL)\n\n    Keeps the strong part of Lucene-style LMDirichlet (only positive per-term evidence),\n    *but* adds a bounded, IDF-shaped penalty for query terms that are absent in the doc.\n\n    Intuition:\n    - Positive evidence answers: \"does this doc generate the observed query term unusually well?\"\n    - Missing-term penalty answers: \"how surprising is it that an actually relevant doc would omit\n      this informative term?\" (soft AND / coverage prior)\n    This tends to improve early precision (nDCG@10) without destroying recall@100 because the\n    penalty is small and capped.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    score = 0.0\n    N = float(corpus.N) if corpus is not None else 1.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n        # Term-adaptive smoothing mass in (0, base_mu]\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        qtf = query_repr.term_weights.get(term, 1.0)\n\n        # Positive evidence (Lucene clamp)\n        pos = max(per_term, 0.0)\n\n        # Soft missing-term penalty (bounded): only apply when tf==0.\n        # Using spec keeps it probabilistic/information-theoretic: missing rare terms costs more bits.\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -Config.miss_penalty * min(spec, Config.miss_cap)\n\n        score += qtf * (spec * pos + miss)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (np.maximum(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            pos = np.maximum(per_term, 0.0)\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -Config.miss_penalty * min(float(spec), Config.miss_cap),\n            )\n\n            scores += qtf * (float(spec) * pos + miss)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4104)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 2000.0  # Dirichlet smoothing parameter\n    epsilon: float = 1e-9\n\n    # --- Novel: discriminative query-likelihood (DQL) ---\n    # Mix standard QL evidence with a \"specificity\" prior on terms derived from df.\n    # Intuition: a query term is informative only if it is surprising under the collection\n    # (high IDF). This downweights spurious matches on common terms, helping nDCG@10.\n    idf_power: float = 0.6  # curvature of specificity weight\n\n    # Soften Lucene's hard clamp: keep a small amount of negative evidence to reward\n    # documents that cover *more* of the query and punish verbosity-mismatch less abruptly.\n    # This often helps recall-sensitive datasets.\n    neg_evidence_weight: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Discriminative Query Likelihood (DQL)\n\n    View standard QL term evidence as a log Bayes factor:\n        log P(w|D) - log P(w|C)  (with Dirichlet smoothing)\n\n    We then *gate* this evidence by the term's specificity (IDF), interpreting IDF as the\n    expected information content of observing the term in a random document.\n    This is not just \"more knobs\": it changes the relevance story from pure generation\n    to *discriminative* evidence accumulation.\n\n    Additionally, we soften Lucene's hard clamp by keeping a small amount of negative\n    evidence (scaled by neg_evidence_weight). This reduces brittle behavior where a\n    single missing term produces no penalty at all, helping both early precision and recall.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n    N = float(corpus.N) if corpus is not None else 1.0\n\n    for term in query_repr.terms:\n        term_count = float(doc_tf.get(term, 0.0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        per_term = math.log(numerator / denominator + eps)\n\n        # Specificity weight from df (if available). Falls back gracefully.\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n            df = float(corpus._df[tid]) if tid is not None else 1.0\n        else:\n            df = 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n\n        # Soft clamp: positive evidence + small negative evidence\n        pos = max(per_term, 0.0)\n        neg = min(per_term, 0.0)\n        soft = pos + Config.neg_evidence_weight * neg\n\n        wq = query_repr.term_weights.get(term, 1.0)\n        score += wq * spec * soft\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            numerator = 1.0 + tf_row / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Specificity gate\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n\n            # Soft clamp: keep some negative evidence\n            pos = np.maximum(per_term, 0.0)\n            neg = np.minimum(per_term, 0.0)\n            soft = pos + Config.neg_evidence_weight * neg\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n            scores += (qtf * spec) * soft\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4117)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Dirichlet smoothing mass (close to strong baselines)\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Specificity transform for IDF -> evidence weight\n    idf_power: float = 0.70\n\n    # Robust background mixing (token-LM + df-LM)\n    collection_df_alpha: float = 0.10\n\n    # Soft AND: bounded missing-term penalty, length-normalized\n    miss_penalty: float = 0.09\n    miss_cap: float = 1.4\n    coverage_len_power: float = 0.35\n\n    # Soft AND without harsh penalties: reward matching a larger fraction of query terms\n    coord_strength: float = 0.12\n\n    # NEW: two-population query generator (content vs. noise tokens).\n    # Very common query terms are likely generated from a background/noise process,\n    # so they should contribute less evidence (improves early precision).\n    noise_idf_cut: float = 0.25        # below this IDF => mostly noise\n    noise_slope: float = 6.0           # sharpness of transition\n    noise_floor: float = 0.20          # minimum weight kept even for noisy terms\n\n    # NEW: avoid one ultra-rare token dominating ranking (esp. theorem/code corpora).\n    spec_cap: float = 3.0\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Iterate unique terms; store query term frequency (qtf) as weight.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"Multinomial query view: unique support + qtf weights (aligns score() with rank()).\"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Robust LMDirichlet + Length-normalized Coverage Prior + Coordination Reward\n\n    - Keep the LMDirichlet \"positive evidence\" backbone (Lucene-style clamp).\n      This preserves the generative interpretation of counts under a smoothed doc LM.\n    - Use a robust background model:\n          P_bg(w) = (1-a) * tf(w)/|C| + a * df(w)/N\n      (df-channel is more stable against very long/bursty docs).\n    - Add a small, capped missing-term penalty (soft AND), but *shrink it for long docs*:\n      omission is less diagnostic when documents are long/multi-topic.\n    - Add a bounded coordination reward to prefer documents that cover more query terms.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    inv_N = 1.0 / max(N, 1.0)\n\n    # In (0,1]: reduce missing-term penalty for long docs\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf = float(query_repr.term_weights.get(term, 1.0))\n        total_qw += qtf\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf\n\n        # df/idf -> specificity (information content)\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n\n        # Specificity as \"information content\", but cap it to prevent single-token domination.\n        spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n        spec = min(spec, Config.spec_cap)\n\n        # Query-noise gate: interpret IDF as evidence the token is content-bearing.\n        # Weight in (noise_floor, 1], smoothly increasing with IDF.\n        q_gate = Config.noise_floor + (1.0 - Config.noise_floor) / (\n            1.0 + math.exp(-Config.noise_slope * (idf - Config.noise_idf_cut))\n        )\n\n        # robust background\n        p_tf = collection_probability(term, corpus_term_freq, total_tokens)\n        p_df = max(df * inv_N, eps)\n        p_bg = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n        p_bg = max(p_bg, eps)\n\n        # LMDirichlet positive evidence (keep counts; do not squash tf inside the Bayes factor)\n        numerator = 1.0 + tf / (mu * p_bg + eps)\n        denominator = (doc_length + mu) / mu\n        per_term = math.log(numerator / denominator + eps)\n        pos = max(per_term, 0.0)\n\n        # missing-term penalty (bounded + length-normalized)\n        miss = 0.0 if tf > 0.0 else -(Config.miss_penalty * len_factor) * min(spec, Config.miss_cap)\n\n        score += qtf * q_gate * (spec * pos + miss)\n\n    # coordination reward (bounded soft-AND)\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n        inv_N = 1.0 / max(N, 1.0)\n\n        # length-normalized coverage factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # for coordination reward\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n\n            spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n            spec = min(spec, Config.spec_cap)\n\n            q_gate = Config.noise_floor + (1.0 - Config.noise_floor) / (\n                1.0 + np.exp(-Config.noise_slope * (idf - Config.noise_idf_cut))\n            )\n\n            # robust background P_bg\n            p_tf = float(self.corpus._collection_prob[term_id])\n            p_df = max(df * inv_N, eps)\n            p_bg = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n            p_bg = max(p_bg, eps)\n\n            numerator = 1.0 + tf_row / (mu * p_bg + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n            pos = np.maximum(per_term, 0.0)\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n            )\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n            # Apply query-noise gate uniformly to positive and missing-term evidence.\n            scores += (qtf * q_gate) * (float(spec) * pos + miss)\n            matched += (qtf * q_gate) * (tf_row > 0.0)\n\n        # coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4118)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1500.0\n    epsilon: float = 1e-9\n\n    # Novelty: make smoothing adaptive to term rarity (df) so rare terms rely less on the\n    # collection model (less \"washed out\"), improving early precision on technical datasets.\n    # mu_t = mu * ( (df+1)/(N+1) )^mu_df_power  in (0, mu]\n    mu_df_power: float = 0.35\n\n    # Query term weighting: probabilistic IDF-like weight from collection evidence\n    # w_q(t) = (log((N+1)/(df+1)))^q_idf_power\n    q_idf_power: float = 0.8\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Adaptive-Dirichlet Query Likelihood (AD-QL)\n\n    Key idea (not \"Dirichlet with extra knobs\" but a different generative story):\n    treat each query term as having its own prior strength \u03bc_t based on how reliable the\n    collection model is for that term. For rare terms (small df), the collection estimate\n    is noisy/unhelpful, so we trust the document evidence more (smaller \u03bc_t). For common\n    terms, we smooth more (\u03bc_t closer to \u03bc).\n\n        \u03bc_t = \u03bc * ((df_t+1)/(N+1))^a,  a>0\n\n    Then use the Lucene-style positive-evidence QL contribution, but with \u03bc_t and with an\n    IDF-shaped query weight to reduce over-counting common query tokens.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    score = 0.0\n    N = float(corpus.N) if corpus is not None else 1.0\n\n    for term in query_repr.terms:\n        term_count = float(doc_tf.get(term, 0.0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        if corpus is not None:\n            df = float(corpus.get_df(term))\n        else:\n            df = 1.0\n\n        # Term-adaptive smoothing mass in (0, base_mu]\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        # Lucene/Pyserini positive-evidence term (but with \u03bc_t)\n        numerator = 1.0 + term_count / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term_score = math.log(numerator / denominator + eps)\n\n        # Query weight: combine provided qtf with an IDF-like factor\n        # (keeps probabilistic flavor: rarer terms contribute more bits of evidence)\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        q_w = (max(idf, 0.0) + 1e-3) ** Config.q_idf_power\n        w = query_repr.term_weights.get(term, 1.0) * q_w\n\n        score += w * max(per_term_score, 0.0)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term_scores = np.log(numerator / denominator + eps)\n\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            q_w_idf = (max(idf, 0.0) + 1e-3) ** Config.q_idf_power\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = qtf * q_w_idf\n\n            scores += w * np.maximum(per_term_scores, 0.0)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3545, Type: Exploratory)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1900.0  # slightly more smoothing tends to stabilize across BEIR corpora\n    epsilon: float = 1e-9\n\n    # Specificity transform for IDF -> evidence weight (kept mild to preserve recall)\n    idf_power: float = 0.55\n\n    # Keep a little negative evidence, but not too much (avoid harming recall@100)\n    neg_evidence_weight: float = 0.08\n\n    # Burstiness-aware saturation (diminishing returns of repetition)\n    k_tf: float = 1.6\n\n    # Robust background: mix token-LM with df-LM (but keep mix modest; large alpha can distort QL)\n    collection_df_alpha: float = 0.15\n\n    # --- New: query completeness prior (soft AND / coverage regularizer) ---\n    # Lucene-style positive-only evidence can over-reward partial matches.\n    # Add small bounded *negative evidence* for missing informative query terms.\n    miss_penalty: float = 0.10\n    miss_cap: float = 1.3\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    BR-DQL + Completeness Prior (soft AND)\n\n    Keep the two robustness ideas:\n      (1) burstiness-aware tf saturation tf_eff=tf/(tf+k)\n      (2) robust background P_mix=(1-a)P_tf+aP_df\n\n    Add a *query completeness prior*:\n      - if a (specific) query term is absent, add a small bounded penalty.\n    This regularizes against high-scoring partial matches, often improving nDCG@10\n    while keeping recall@100 stable (penalty is small + capped).\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n    N = float(corpus.N) if corpus is not None else 1.0\n    inv_N = 1.0 / max(N, 1.0)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        tf_eff = tf / (tf + Config.k_tf) if tf > 0.0 else 0.0\n\n        p_tf = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n\n        p_df = max(df * inv_N, eps)\n        p_collection = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n        p_collection = max(p_collection, eps)\n\n        numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        per_term = math.log(numerator / denominator + eps)\n\n        pos = max(per_term, 0.0)\n        neg = min(per_term, 0.0)\n        soft = pos + Config.neg_evidence_weight * neg\n\n        # completeness prior: only when missing\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -Config.miss_penalty * min(spec, Config.miss_cap)\n\n        wq = query_repr.term_weights.get(term, 1.0)\n        score += wq * (spec * soft + miss)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            # df-based collection component\n            df = float(self.corpus._df[term_id])\n            p_df = max(df / max(N, 1.0), eps)\n\n            # token-frequency collection component (precomputed)\n            p_tf = float(self.corpus._collection_prob[term_id])\n\n            # mixed collection model\n            p_collection = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n            p_collection = max(p_collection, eps)\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            tf_eff = tf_row / (tf_row + Config.k_tf)\n\n            numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n\n            pos = np.maximum(per_term, 0.0)\n            neg = np.minimum(per_term, 0.0)\n            soft = pos + Config.neg_evidence_weight * neg\n\n            # completeness prior (vectorized): bounded penalty when missing\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -Config.miss_penalty * min(float(spec), Config.miss_cap),\n            )\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n            scores += qtf * (float(spec) * soft + miss)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4048, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Dirichlet smoothing mass (close to strong baselines)\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Specificity transform for IDF -> evidence weight\n    idf_power: float = 0.70\n\n    # Robust background mixing (token-LM + df-LM)\n    collection_df_alpha: float = 0.10\n\n    # Soft AND: bounded missing-term penalty, length-normalized\n    miss_penalty: float = 0.09\n    miss_cap: float = 1.4\n    coverage_len_power: float = 0.35\n\n    # Soft AND without harsh penalties: reward matching a larger fraction of query terms\n    coord_strength: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Iterate unique terms; store query term frequency (qtf) as weight.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"Multinomial query view: unique support + qtf weights (aligns score() with rank()).\"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Robust LMDirichlet + Length-normalized Coverage Prior + Coordination Reward\n\n    - Keep the LMDirichlet \"positive evidence\" backbone (Lucene-style clamp).\n      This preserves the generative interpretation of counts under a smoothed doc LM.\n    - Use a robust background model:\n          P_bg(w) = (1-a) * tf(w)/|C| + a * df(w)/N\n      (df-channel is more stable against very long/bursty docs).\n    - Add a small, capped missing-term penalty (soft AND), but *shrink it for long docs*:\n      omission is less diagnostic when documents are long/multi-topic.\n    - Add a bounded coordination reward to prefer documents that cover more query terms.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    inv_N = 1.0 / max(N, 1.0)\n\n    # In (0,1]: reduce missing-term penalty for long docs\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf = float(query_repr.term_weights.get(term, 1.0))\n        total_qw += qtf\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf\n\n        # df/idf -> specificity (information content)\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n\n        # robust background\n        p_tf = collection_probability(term, corpus_term_freq, total_tokens)\n        p_df = max(df * inv_N, eps)\n        p_bg = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n        p_bg = max(p_bg, eps)\n\n        # LMDirichlet positive evidence (keep counts; do not squash tf inside the Bayes factor)\n        numerator = 1.0 + tf / (mu * p_bg + eps)\n        denominator = (doc_length + mu) / mu\n        per_term = math.log(numerator / denominator + eps)\n        pos = max(per_term, 0.0)\n\n        # missing-term penalty (bounded + length-normalized)\n        miss = 0.0 if tf > 0.0 else -(Config.miss_penalty * len_factor) * min(spec, Config.miss_cap)\n\n        score += qtf * (spec * pos + miss)\n\n    # coordination reward (bounded soft-AND)\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n        inv_N = 1.0 / max(N, 1.0)\n\n        # length-normalized coverage factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # for coordination reward\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n\n            # robust background P_bg\n            p_tf = float(self.corpus._collection_prob[term_id])\n            p_df = max(df * inv_N, eps)\n            p_bg = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n            p_bg = max(p_bg, eps)\n\n            numerator = 1.0 + tf_row / (mu * p_bg + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n            pos = np.maximum(per_term, 0.0)\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n            )\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n            scores += qtf * (float(spec) * pos + miss)\n            matched += qtf * (tf_row > 0.0)\n\n        # coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4010, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Query term weighting\n    q_idf_power: float = 0.70\n\n    # Coverage-aware soft negative evidence (query completeness)\n    # (slightly reduced; penalties + coord can over-regularize and hurt recall@100)\n    miss_penalty: float = 0.09\n    miss_cap: float = 1.4\n\n    # Robust background LM mixing: token-LM mixed with df-LM\n    # Lowering alpha reduces over-smoothing rare terms (helps technical domains / recall)\n    collection_df_alpha: float = 0.07\n\n    # Coordination reward (soft AND without harsh penalties).\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # NEW: \"soft clamp\" negative evidence fraction.\n    # Lucene hard-clamp ignores negative evidence entirely; keeping a small fraction helps\n    # discriminate partial matches and can improve nDCG on some datasets.\n    neg_evidence_weight: float = 0.08\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Token-frequency collection LM (kept as a primitive).\n    Note: retrieval_score() may mix this with df/N when corpus stats are available.\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    CRAD-QL++ with *soft negative evidence*.\n\n    We keep the earlier cohesive story (robust background, length-normalized completeness, coord),\n    but replace the hard Lucene clamp with a \"soft clamp\":\n\n        soft(per_term) = max(per_term, 0) + \u03bb * min(per_term, 0)\n\n    Interpretable as: most of the time we only count \"surplus\" evidence over the background,\n    yet we allow a small fraction of counter-evidence to matter, improving discrimination among\n    partial matches (often helps nDCG@10) while preserving recall (\u03bb small).\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    inv_N = 1.0 / max(N, 1.0)\n\n    # In (0,1] typically: shrink missing-term penalty for long docs.\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf = float(query_repr.term_weights.get(term, 1.0))\n        total_qw += qtf\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf\n\n        # df/idf\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n        # Robust collection probability: token-LM mixed with df-LM\n        p_tf = collection_probability(term, corpus_term_freq, total_tokens)\n        p_df = max(df * inv_N, eps)\n        p_collection = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n        p_collection = max(p_collection, eps)\n\n        # Term-adaptive smoothing mass in (0, base_mu]\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Soft clamp: keep a small amount of negative evidence\n        pos = max(per_term, 0.0)\n        neg = min(per_term, 0.0)\n        soft = pos + Config.neg_evidence_weight * neg\n\n        # Soft missing-term penalty (bounded), length-normalized\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(spec, Config.miss_cap)\n\n        score += qtf * (spec * soft + miss)\n\n    # Coordination reward: bounded, encourages fuller matches without big negative penalties.\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n        inv_N = 1.0 / max(N, 1.0)\n\n        # length factor for missing-term evidence\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (np.maximum(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n            # Robust collection probability: token-LM mixed with df-LM\n            p_tf = float(self.corpus._collection_prob[term_id])\n            p_df = max(df * inv_N, eps)\n            p_collection = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n            p_collection = max(p_collection, eps)\n\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            pos = np.maximum(per_term, 0.0)\n            neg = np.minimum(per_term, 0.0)\n            soft = pos + Config.neg_evidence_weight * neg\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n            )\n\n            scores += qtf * (float(spec) * soft + miss)\n            matched += (qtf) * (tf_row > 0.0)\n\n        # Coordination reward\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4104, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 2000.0  # Dirichlet smoothing parameter\n    epsilon: float = 1e-9\n\n    # --- Novel: discriminative query-likelihood (DQL) ---\n    # Mix standard QL evidence with a \"specificity\" prior on terms derived from df.\n    # Intuition: a query term is informative only if it is surprising under the collection\n    # (high IDF). This downweights spurious matches on common terms, helping nDCG@10.\n    idf_power: float = 0.6  # curvature of specificity weight\n\n    # Soften Lucene's hard clamp: keep a small amount of negative evidence to reward\n    # documents that cover *more* of the query and punish verbosity-mismatch less abruptly.\n    # This often helps recall-sensitive datasets.\n    neg_evidence_weight: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Discriminative Query Likelihood (DQL)\n\n    View standard QL term evidence as a log Bayes factor:\n        log P(w|D) - log P(w|C)  (with Dirichlet smoothing)\n\n    We then *gate* this evidence by the term's specificity (IDF), interpreting IDF as the\n    expected information content of observing the term in a random document.\n    This is not just \"more knobs\": it changes the relevance story from pure generation\n    to *discriminative* evidence accumulation.\n\n    Additionally, we soften Lucene's hard clamp by keeping a small amount of negative\n    evidence (scaled by neg_evidence_weight). This reduces brittle behavior where a\n    single missing term produces no penalty at all, helping both early precision and recall.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n    N = float(corpus.N) if corpus is not None else 1.0\n\n    for term in query_repr.terms:\n        term_count = float(doc_tf.get(term, 0.0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        per_term = math.log(numerator / denominator + eps)\n\n        # Specificity weight from df (if available). Falls back gracefully.\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n            df = float(corpus._df[tid]) if tid is not None else 1.0\n        else:\n            df = 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n\n        # Soft clamp: positive evidence + small negative evidence\n        pos = max(per_term, 0.0)\n        neg = min(per_term, 0.0)\n        soft = pos + Config.neg_evidence_weight * neg\n\n        wq = query_repr.term_weights.get(term, 1.0)\n        score += wq * spec * soft\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            numerator = 1.0 + tf_row / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Specificity gate\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n\n            # Soft clamp: keep some negative evidence\n            pos = np.maximum(per_term, 0.0)\n            neg = np.minimum(per_term, 0.0)\n            soft = pos + Config.neg_evidence_weight * neg\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n            scores += (qtf * spec) * soft\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Query term weighting (IDF curvature)\n    q_idf_power: float = 0.70\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.10\n    miss_cap: float = 1.4\n\n    # NEW: coordination reward (soft AND without harsh penalties)\n    # Encourages documents that match a larger fraction of (weighted) query terms.\n    coord_strength: float = 0.12\n\n    # NEW: length-normalize the missing-term evidence:\n    # omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    CRAD-QL + Coordination Reward + Length-normalized Coverage Evidence\n\n    Two principled extensions aimed at improving nDCG@10 without killing recall@100:\n\n    (1) Length-normalized missing-term penalty:\n        Missing-term evidence is weaker for long docs (they cover more topics, omission is less diagnostic),\n        stronger for short docs. This is a simple relevance prior over \"query completeness\" conditioned on L_D.\n\n    (2) Coordination reward:\n        Add a bounded reward for matching a larger fraction of the query (weighted by qtf).\n        This behaves like a soft-AND that does not require tuning large negative penalties.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # In (0,1] typically: shrinks missing-term penalty for long docs.\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf = float(query_repr.term_weights.get(term, 1.0))\n        total_qw += qtf\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        pos = max(per_term, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(spec, Config.miss_cap)\n\n        score += qtf * (spec * pos + miss)\n\n    # Coordination reward: bounded, encourages fuller matches.\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (np.maximum(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            pos = np.maximum(per_term, 0.0)\n            # Length-normalized missing-term penalty (vectorized)\n            avgdl = max(float(self.corpus.avgdl), 1.0)\n            len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n            len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n            )\n\n            scores += qtf * (float(spec) * pos + miss)\n\n        # Coordination reward on candidates (weighted fraction of query terms matched)\n        if query_term_weights is not None and len(query_term_ids) > 0:\n            matched = np.zeros(len(candidate_docs), dtype=np.float64)\n            total_q = float(np.sum(query_term_weights)) if float(np.sum(query_term_weights)) > 0 else 1.0\n            for i, term_id in enumerate(query_term_ids):\n                tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n                matched += (query_term_weights[i]) * (tf_row > 0.0)\n            coord = matched / total_q\n            scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Below are targeted, high-leverage changes that (1) restore a stronger baseline backbone (the best programs in the history use tempered collection + df/token \u201cresidual-IDF\u201d diagnostics), and (2) keep your current \u201ccoverage prior\u201d idea but make it cheaper and less brittle.\n\nKey idea: **use a more robust background LM** and **use a discriminative term diagnostic** (p_doc vs p_col) to weight/gate evidence, rather than purely IDF-based penalties. This tends to help nDCG@10 without collapsing recall@100.\n\n---\n\n### Change 1 \u2014 Upgrade Config (add tempered background + robust gates; reduce over-regularization)\nThis brings you closer to the top-performing \u201cProgram 1\u201d family while keeping your coord/missing components.\n\n<<<<<<< SEARCH\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Query term weighting (IDF curvature)\n    q_idf_power: float = 0.70\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.10\n    miss_cap: float = 1.4\n\n    # NEW: coordination reward (soft AND without harsh penalties)\n    # Encourages documents that match a larger fraction of (weighted) query terms.\n    coord_strength: float = 0.12\n\n    # NEW: length-normalize the missing-term evidence:\n    # omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n=======\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # --- Robust background LM (token-LM tempered, plus tiny uniform) ---\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # --- Discriminative diagnostics (query-independent) ---\n    # EDR: compares df-based LM vs token-based LM to downweight \"everywhere tokens\".\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n    residual_idf_strength: float = 0.90\n\n    # Query term frequency saturation (avoid long queries / repeats dominating)\n    query_tf_power: float = 0.60\n\n    # Missing-term penalty (soft AND) + coordination reward\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n>>>>>>> REPLACE\n\n---\n\n### Change 2 \u2014 Tempered collection_probability fallback\nThis makes the \u201cslow path\u201d consistent with the precomputed corpus LM and reduces dominance of very frequent tokens.\n\n<<<<<<< SEARCH\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n=======\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n>>>>>>> REPLACE\n\n---\n\n### Change 3 \u2014 Precompute robust collection LM + gates in Corpus\nThis is the main \u201cengine upgrade\u201d: build a **proper tempered+uniform collection LM**, plus **EDR gate** and **residual-IDF query weights** based on p_doc vs p_col.\n\n<<<<<<< SEARCH\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n=======\n        # --- Collection LM (tempered) + tiny uniform mixture ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n>>>>>>> REPLACE\n\n---\n\n### Change 4 \u2014 Retrieval score: use residual-IDF weights + EDR gate; keep your coverage+coord\nThis replaces the pure IDF-shaped \u201cspec\u201d with a more stable **ridf-based query weight** (computed from p_doc/p_col), while still allowing your missing penalty and coordination reward.\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    CRAD-QL + Coordination Reward + Length-normalized Coverage Evidence\n\n    Two principled extensions aimed at improving nDCG@10 without killing recall@100:\n\n    (1) Length-normalized missing-term penalty:\n        Missing-term evidence is weaker for long docs (they cover more topics, omission is less diagnostic),\n        stronger for short docs. This is a simple relevance prior over \"query completeness\" conditioned on L_D.\n\n    (2) Coordination reward:\n        Add a bounded reward for matching a larger fraction of the query (weighted by qtf).\n        This behaves like a soft-AND that does not require tuning large negative penalties.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # In (0,1] typically: shrinks missing-term penalty for long docs.\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf = float(query_repr.term_weights.get(term, 1.0))\n        total_qw += qtf\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        pos = max(per_term, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(spec, Config.miss_cap)\n\n        score += qtf * (spec * pos + miss)\n\n    # Coordination reward: bounded, encourages fuller matches.\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n        else:\n            tid = None\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n>>>>>>> REPLACE\n\n---\n\n### Change 5 \u2014 Vectorized scorer: remove extra tf_matrix passes for coord; keep in-loop coord accumulation\nYour current vectorized code does a **second pass over terms** for coordination reward (expensive). Accumulating `matched` during the main loop improves query time and usually fitness (since walltime is tracked), and it also keeps behavior identical.\n\n<<<<<<< SEARCH\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (np.maximum(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            pos = np.maximum(per_term, 0.0)\n            # Length-normalized missing-term penalty (vectorized)\n            avgdl = max(float(self.corpus.avgdl), 1.0)\n            len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n            len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n            )\n\n            scores += qtf * (float(spec) * pos + miss)\n\n        # Coordination reward on candidates (weighted fraction of query terms matched)\n        if query_term_weights is not None and len(query_term_ids) > 0:\n            matched = np.zeros(len(candidate_docs), dtype=np.float64)\n            total_q = float(np.sum(query_term_weights)) if float(np.sum(query_term_weights)) > 0 else 1.0\n            for i, term_id in enumerate(query_term_ids):\n                tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n                matched += (query_term_weights[i]) * (tf_row > 0.0)\n            coord = matched / total_q\n            scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n=======\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            w = qtf * float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(self.corpus._ridf_qweight[term_id]), Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n>>>>>>> REPLACE\n\n---\n\n#### Why this should improve FITNESS\n- **nDCG@10**: residual-IDF + EDR gate penalizes \u201cgeneric\u201d terms and rewards terms that are token-common but doc-rare (classic hard case for scientific/code corpora), improving early precision.\n- **recall@100**: backbone is still LMDirichlet positive evidence; missing penalty is reduced slightly and bounded by ridf weight rather than raw IDF spikes.\n- **speed**: coordination reward no longer requires an extra per-term sparse lookup pass.\n\nIf you want an even smaller/safer change set, apply only Change 5 (speed) + Change 3 (precompute gates) + Change 5 (vectorized usage), but the full set is most likely to move fitness upward."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}