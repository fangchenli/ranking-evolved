{"id": "b23ddda3-f86e-49ee-b72d-b609b39ca4cb", "code": "\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n    doclen_prior_strength: float = 0.06\n\n    # Term TF saturation.\n    # This can help on bursty/forum corpora, but it can also hurt recall on scientific/news corpora.\n    # Default to 0 (off) for robustness; enable during evolution if beneficial.\n    tf_saturation_strength: float = 0.0\n    min_tf_beta: float = 0.65\n\n    # NEW: anti-EDR gate (penalize corpus-wide boilerplate markers where p_doc >> p_col)\n    anti_edr_strength: float = 0.12  # 0 disables\n\n    # Query-conditioned specificity agreement prior.\n    # Penalize docs whose average token surprisal under p_col differs too much from the query's.\n    # Helps early precision for specific queries; keep small to avoid recall loss.\n    spec_agreement_strength: float = 0.05  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior: long docs tend to match many terms by chance.\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Prefer precomputed, renormalized collection LM when possible (faster + consistent).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        # Apply the same per-term TF saturation as the vectorized rank() path.\n        if corpus is not None and tid is not None and hasattr(corpus, \"_tf_beta\"):\n            beta = float(corpus._tf_beta[tid])\n            tf_eff = tf ** beta if beta != 1.0 else tf\n        else:\n            tf_eff = tf\n        numerator = 1.0 + tf_eff / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None and tid is not None:\n            gate = float(corpus._edr_gate[tid]) * float(getattr(corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            gate, ridf_w = 1.0, 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    # Query-conditioned specificity agreement prior (slow path).\n    s_ag = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n    if s_ag > 0.0 and corpus is not None and hasattr(corpus, \"_doc_avg_surprisal\"):\n        tids = [corpus.get_term_id(t) for t in query_repr.terms]\n        tids = [t for t in tids if t is not None]\n        if tids:\n            q_avg = float(np.mean(-np.log(np.maximum(corpus._collection_prob[np.array(tids, dtype=np.int64)], eps))))\n            score += -s_ag * (float(corpus._doc_avg_surprisal[doc_tf is None and 0 or 0]) if False else 0.0)\n            # (the above line is intentionally dead-code-free; compute per-doc below)\n            # doc specificity:\n            # We don't have doc_idx here; approximate from doc_tf + p_col:\n            acc = 0.0\n            for term, c in doc_tf.items():\n                tid = corpus.get_term_id(term)\n                if tid is None:\n                    continue\n                acc += float(c) * (-math.log(max(float(corpus._collection_prob[tid]), eps)))\n            d_avg = acc / max(doc_length, 1.0)\n            score += -s_ag * (d_avg - q_avg) ** 2\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if self.N > 0:\n            ratio_full = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio_clip = np.clip(ratio_full, -clipc, clipc)\n        else:\n            ratio_full = np.zeros(self.vocab_size, dtype=np.float64)\n            ratio_clip = ratio_full\n\n        if lam > 0.0 and self.N > 0:\n            self._edr_gate = 1.0 + lam * ratio_clip\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Anti-EDR: if p_doc >> p_col, term is \"too spread\" (often boilerplate); downweight gently.\n        anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n        if anti > 0.0 and self.N > 0:\n            self._anti_edr_gate = 1.0 / (1.0 + anti * np.maximum(ratio_full, 0.0))\n        else:\n            self._anti_edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.maximum(ratio_full, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Per-term TF saturation: interpolate between surprisal-based and df-stability-based signals.\n        # Intuition: in bursty corpora, p_col may be distorted by a few long docs; df/N is stabler.\n        sat_s = float(getattr(Config, \"tf_saturation_strength\", 0.0))\n        minb = float(getattr(Config, \"min_tf_beta\", 0.60))\n        if sat_s > 0.0:\n            surpr = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n            surpr01 = surpr / max(float(np.max(surpr)), Config.epsilon)\n            # df-based idf01 proxy\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            mix = 0.5  # fixed: half surprisal, half df-stability (keeps code size small)\n            sat01 = mix * surpr01 + (1.0 - mix) * idf01\n            beta = 1.0 - sat_s * (1.0 - sat01)\n            self._tf_beta = np.clip(beta, minb, 1.0)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # --- Query-independent doc specificity proxy ---\n        # avg_surprisal_d = E_{w~doc}[-log p_col(w)] (computed on raw tf distribution).\n        # Used for a small query-conditioned prior that prefers specificity-matched docs.\n        log_pcol = np.log(np.maximum(self._collection_prob, Config.epsilon))\n        self._doc_avg_surprisal = np.zeros(self.N, dtype=np.float64)\n        for di, tfc in enumerate(self._doc_tf_dicts):\n            L = float(self.doc_lengths[di])\n            if L <= 0.0:\n                continue\n            acc = 0.0\n            for term, c in tfc.items():\n                tid = self._vocab.get(term)\n                if tid is None:\n                    continue\n                acc += float(c) * (-float(log_pcol[tid]))\n            self._doc_avg_surprisal[di] = acc / max(L, 1.0)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: query-scope (information-mass) coverage accumulators.\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            # Apply per-term TF saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = float(getattr(self.corpus, \"_tf_beta\", np.ones(1, dtype=np.float64))[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n            numerator = 1.0 + tf_eff / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Optional small leak of negative evidence (prevents \"mention once everywhere\" docs).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + (EDR * anti-EDR) gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id]) * float(getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            # NOTE: per_term already contains leaked negatives (if enabled).\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            # Query-scope prior: reward covering the query's \"information mass\".\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        # Query-conditioned specificity agreement prior (bounded, candidate-only).\n        s_ag = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n        if s_ag > 0.0 and len(query_term_ids) > 0 and hasattr(self.corpus, \"_doc_avg_surprisal\"):\n            q_avg = float(\n                np.mean(-np.log(np.maximum(self.corpus._collection_prob[np.array(query_term_ids, dtype=np.int64)], eps)))\n            )\n            d_avg = self.corpus._doc_avg_surprisal[candidate_docs]\n            scores += -s_ag * np.square(d_avg - q_avg)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "d8957b1a-48ef-4215-8e03-2d695eeef524", "generation": 9, "timestamp": 1770388498.712563, "iteration_found": 173, "metrics": {"beir_nfcorpus_ndcg@10": 0.3224880346438961, "beir_nfcorpus_recall@100": 0.2557580606291045, "beir_nfcorpus_index_time_ms": 3646.021332941018, "beir_nfcorpus_query_time_ms": 125.55933301337063, "beir_scifact_ndcg@10": 0.6836264257131559, "beir_scifact_recall@100": 0.9242222222222222, "beir_scifact_index_time_ms": 5000.611790921539, "beir_scifact_query_time_ms": 486.7388750426471, "bright_pony_ndcg@10": 0.0825050115217552, "bright_pony_recall@100": 0.3016996677321232, "bright_pony_index_time_ms": 1495.0357499765232, "bright_pony_query_time_ms": 657.8698750818148, "beir_arguana_ndcg@10": 0.28575435755749257, "beir_arguana_recall@100": 0.9314775160599572, "beir_arguana_index_time_ms": 6050.678750034422, "beir_arguana_query_time_ms": 30221.13837499637, "bright_theoremqa_theorems_ndcg@10": 0.028329786916867487, "bright_theoremqa_theorems_recall@100": 0.15789473684210525, "bright_theoremqa_theorems_index_time_ms": 10864.578417036682, "bright_theoremqa_theorems_query_time_ms": 1286.7425410076976, "beir_scidocs_ndcg@10": 0.1464540688247322, "beir_scidocs_recall@100": 0.3495, "beir_scidocs_index_time_ms": 20239.486000034958, "beir_scidocs_query_time_ms": 4430.96637504641, "bright_economics_ndcg@10": 0.15334158277835344, "bright_economics_recall@100": 0.38884009648662604, "bright_economics_index_time_ms": 14552.349667064846, "bright_economics_query_time_ms": 3626.8738330109045, "bright_biology_ndcg@10": 0.3018932922720273, "bright_biology_recall@100": 0.5781668510334836, "bright_biology_index_time_ms": 15711.83716703672, "bright_biology_query_time_ms": 3365.1452919002622, "beir_fiqa_ndcg@10": 0.22426822786629744, "beir_fiqa_recall@100": 0.5153868679331642, "beir_fiqa_index_time_ms": 34489.950124989264, "beir_fiqa_query_time_ms": 7871.075583039783, "bright_earth_science_ndcg@10": 0.32832823563778224, "bright_earth_science_recall@100": 0.6657766657369651, "bright_earth_science_index_time_ms": 32950.6260829512, "bright_earth_science_query_time_ms": 5813.032875070348, "bright_stackoverflow_ndcg@10": 0.18720642666262266, "bright_stackoverflow_recall@100": 0.4910063148769432, "bright_stackoverflow_index_time_ms": 99565.07029104978, "bright_stackoverflow_query_time_ms": 12892.829000018537, "beir_trec-covid_ndcg@10": 0.655905275130741, "beir_trec-covid_recall@100": 0.11126994649411437, "beir_trec-covid_index_time_ms": 136454.6636659652, "beir_trec-covid_query_time_ms": 1903.37808302138, "avg_ndcg@10": 0.2833417271271436, "avg_recall@100": 0.4725832455039007, "combined_score": 0.4347349418285493, "total_index_time_ms": 381020.90904000215, "total_query_time_ms": 72681.35004024953, "total_time_ms": 453702.2590802517, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  # Term TF saturation (kept, but computed from both surprisal and df-stability; see Corpus)\n      tf_saturation_strength: float = 0.22\n      min_tf_beta: float = 0.65\nwith:\n  # Term TF saturation.\n      # This can help on bursty/forum corpora, but it can also hurt recall on scientific/news corpora.\n      # Default to 0 (off) for robustness; enable during evolution if beneficial.\n      tf_saturation_strength: float = 0.0\n      min_tf_beta: float = 0.65\nChange 2: Replace:\n  # NEW: anti-EDR gate (penalize corpus-wide boilerplate markers where p_doc >> p_col)\n      anti_edr_strength: float = 0.12  # 0 disables\nwith:\n  # NEW: anti-EDR gate (penalize corpus-wide boilerplate markers where p_doc >> p_col)\n      anti_edr_strength: float = 0.12  # 0 disables\n  \n      # Query-conditioned specificity agreement prior.\n      # Penalize docs whose average token surprisal under p_col differs too much from the query's.\n      # Helps early precision for specific queries; keep small to avoid recall loss.\n      spec_agreement_strength: float = 0.05  # 0 disables\nChange 3: Replace:\n  else:\n              self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n  \n          self._posting_lists: dict[int, NDArray[np.int64]] = {\nwith:\n  else:\n              self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n  \n          # --- Query-independent doc specificity proxy ---\n          # avg_surprisal_d = E_{w~doc}[-log p_col(w)] (computed on raw tf distribution).\n          # Used for a small query-conditioned prior that prefers specificity-matched docs.\n          log_pcol = np.log(np.maximum(self._collection_prob, Config.epsilon))\n          self._doc_avg_surprisal = np.zeros(self.N, dtype=np.float64)\n          for di, tfc in enumerate(self._doc_tf_dicts):\n              L = float(self.doc_lengths[di])\n              if L <= 0.0:\n                  continue\n              acc = 0.0\n              for term, c in tfc.items():\n                  tid = self._vocab.get(term)\n                  if tid is None:\n                      continue\n                  acc += float(c) * (-float(log_pcol[tid]))\n              self._doc_avg_surprisal[di] = acc / max(L, 1.0)\n  \n          self._posting_lists: dict[int, NDArray[np.int64]] = {\nChange 4: Replace:\n  # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n          if scope_num is not None and scope_den > 0.0:\n              frac = np.maximum(scope_num / max(scope_den, eps), eps)\n              scores += scope_s * np.log(frac)\n  \n          return scores\nwith:\n  # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n          if scope_num is not None and scope_den > 0.0:\n              frac = np.maximum(scope_num / max(scope_den, eps), eps)\n              scores += scope_s * np.log(frac)\n  \n          # Query-conditioned specificity agreement prior (bounded, candidate-only).\n          s_ag = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n          if s_ag > 0.0 and len(query_term_ids) > 0 and hasattr(self.corpus, \"_doc_avg_surprisal\"):\n              q_avg = float(\n                  np.mean(-np.log(np.maximum(self.corpus._collection_prob[np.array(query_term_ids, ...\n              )\n              d_avg = self.corpus._doc_avg_surprisal[candidate_docs]\n              scores += -s_ag * np.square(d_avg - q_avg)\n  \n          return scores\nChange 5: Replace:\n  # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      if scope_s > 0.0 and corpus is not None:\n          scope_num = 0.0\n          scope_den = 0.0\n          for term in query_repr.terms:\n              tid = corpus.get_term_id(term)\n              if tid is None:\n                  continue\n              info = math.log1p(float(corpus._ridf_qweight[tid]))\n              scope_den += info\n              if float(doc_tf.get(term, 0.0)) > 0.0:\n                  scope_num += info\n          if scope_den > 0.0:\n              frac = max(scope_num / scope_den, eps)\n              score += scope_s * math.log(frac)\n  \n      return score\nwith:\n  # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      if scope_s > 0.0 and corpus is not None:\n          scope_num = 0.0\n          scope_den = 0.0\n          for term in query_repr.terms:\n              tid = corpus.get_term_id(term)\n              if tid is None:\n                  continue\n              info = math.log1p(float(corpus._ridf_qweight[tid]))\n              scope_den += info\n              if float(doc_tf.get(term, 0.0)) > 0.0:\n                  scope_num += info\n          if scope_den > 0.0:\n              frac = max(scope_num / scope_den, eps)\n              score += scope_s * math.log(frac)\n  \n      # Query-conditioned specificity agreement prior (slow path).\n      s_ag = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n      if s_ag > 0.0 and corpus is not None and hasattr(corpus, \"_doc_avg_surprisal\"):\n          tids = [corpus.get_term_id(t) for t in query_repr.terms]\n          tids = [t for t in tids if t is not None]\n          if tids:\n              q_avg = float(np.mean(-np.log(np.maximum(corpus._collection_prob[np.array(tids, dtype...\n              score += -s_ag * (float(corpus._doc_avg_surprisal[doc_tf is None and 0 or 0]) if Fals...\n              # (the above line is intentionally dead-code-free; compute per-doc below)\n              # doc specificity:\n              # We don't have doc_idx here; approximate from doc_tf + p_col:\n              acc = 0.0\n              for term, c in doc_tf.items():\n  ... (8 more lines)\nChange 6: Replace:\n  # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      if scope_s > 0.0 and corpus is not None:\n          scope_num = 0.0\n          scope_den = 0.0\n          for term in query_repr.terms:\n              tid = corpus.get_term_id(term)\n              if tid is None:\n                  continue\n              info = math.log1p(float(corpus._ridf_qweight[tid]))\n              scope_den += info\n              if float(doc_tf.get(term, 0.0)) > 0.0:\n                  scope_num += info\n          if scope_den > 0.0:\n              frac = max(scope_num / scope_den, eps)\n              score += scope_s * math.log(frac)\n  \n      return score\nwith:\n  # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      if scope_s > 0.0 and corpus is not None:\n          scope_num = 0.0\n          scope_den = 0.0\n          for term in query_repr.terms:\n              tid = corpus.get_term_id(term)\n              if tid is None:\n                  continue\n              info = math.log1p(float(corpus._ridf_qweight[tid]))\n              scope_den += info\n              if float(doc_tf.get(term, 0.0)) > 0.0:\n                  scope_num += info\n          if scope_den > 0.0:\n              frac = max(scope_num / scope_den, eps)\n              score += scope_s * math.log(frac)\n  \n      # Query-conditioned specificity agreement prior (slow path; approximate doc specificity from ...\n      s_ag = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n      if s_ag > 0.0 and corpus is not None:\n          tids = [corpus.get_term_id(t) for t in query_repr.terms]\n          tids = [t for t in tids if t is not None]\n          if tids:\n              q_avg = float(np.mean(-np.log(np.maximum(corpus._collection_prob[np.array(tids, dtype...\n              acc = 0.0\n              for t, c in doc_tf.items():\n                  tid = corpus.get_term_id(t)\n                  if tid is None:\n                      continue\n                  acc += float(c) * (-math.log(max(float(corpus._collection_prob[tid]), eps)))\n  ... (4 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.32096930724818107, "beir_nfcorpus_recall@100": 0.25504790967553115, "beir_nfcorpus_index_time_ms": 3619.290624978021, "beir_nfcorpus_query_time_ms": 124.64937497861683, "beir_scifact_ndcg@10": 0.681847453812571, "beir_scifact_recall@100": 0.9208888888888889, "beir_scifact_index_time_ms": 4931.624041986652, "beir_scifact_query_time_ms": 523.5092079965398, "bright_pony_ndcg@10": 0.08631613688339147, "bright_pony_recall@100": 0.3057097730368713, "bright_pony_index_time_ms": 1510.6735830195248, "bright_pony_query_time_ms": 719.5090840104967, "beir_arguana_ndcg@10": 0.2905215597130342, "beir_arguana_recall@100": 0.9336188436830836, "beir_arguana_index_time_ms": 5989.3930410034955, "beir_arguana_query_time_ms": 33674.07375003677, "bright_theoremqa_theorems_ndcg@10": 0.02897711702039787, "bright_theoremqa_theorems_recall@100": 0.13815789473684212, "bright_theoremqa_theorems_index_time_ms": 10662.93874999974, "bright_theoremqa_theorems_query_time_ms": 1436.046416987665, "beir_scidocs_ndcg@10": 0.14676800746102323, "beir_scidocs_recall@100": 0.3471, "beir_scidocs_index_time_ms": 19933.090457925573, "beir_scidocs_query_time_ms": 4746.8968339962885, "bright_economics_ndcg@10": 0.14865222799650907, "bright_economics_recall@100": 0.40009269236105294, "bright_economics_index_time_ms": 14476.50629200507, "bright_economics_query_time_ms": 4096.389457932673, "bright_biology_ndcg@10": 0.2951845888467541, "bright_biology_recall@100": 0.5703743114320529, "bright_biology_index_time_ms": 15461.673291982152, "bright_biology_query_time_ms": 3764.784749946557, "beir_fiqa_ndcg@10": 0.23267697772887666, "beir_fiqa_recall@100": 0.523539029557548, "beir_fiqa_index_time_ms": 33887.39570893813, "beir_fiqa_query_time_ms": 8543.942541931756, "bright_earth_science_ndcg@10": 0.3493439732901902, "bright_earth_science_recall@100": 0.6669553242432099, "bright_earth_science_index_time_ms": 32285.756540950388, "bright_earth_science_query_time_ms": 6387.715624994598, "bright_stackoverflow_ndcg@10": 0.18570234284965953, "bright_stackoverflow_recall@100": 0.4877823357077228, "bright_stackoverflow_index_time_ms": 98482.40370803978, "bright_stackoverflow_query_time_ms": 14422.33204189688, "beir_trec-covid_ndcg@10": 0.628456998388598, "beir_trec-covid_recall@100": 0.10908109501703159, "beir_trec-covid_index_time_ms": 133693.49808292463, "beir_trec-covid_query_time_ms": 2109.572791028768, "avg_ndcg@10": 0.2829513909365988, "avg_recall@100": 0.4715290081949863, "combined_score": 0.43381348474330883, "total_index_time_ms": 374934.24412375316, "total_query_time_ms": 80549.42187573761, "total_time_ms": 455483.66599949077, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1}, "prompts": {"diff_user": {"system": "You are discovering a **new probabilistic retrieval method** based on Query Likelihood language models. The seed program is a minimal skeleton: document representation, query representation, collection model, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just Dirichlet smoothing with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall@100, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the probabilistic model with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 \u03bc (smoothing parameter), epsilon, and any new parameters you need.\n2. **collection_probability(term, ...)** \u2014 How to compute P(w | C) (collection language model). EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, hierarchical, etc.).\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. Default: Dirichlet smoothing. Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors, query models, multi-field models, term dependencies, etc. You can use multiple sub-signals and combine them, or a single unified formula.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **QL._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **QL**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **QL** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and QL.rank() / QL.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n- Avoid taking log(0) (use max(value, epsilon) before log).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"smoothing prevents zero probabilities\" or \"collection model favors discriminative terms\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different smoothing methods, document priors, query expansion, term dependencies) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n- Probabilistic, information-theoretic, and language modeling principles are good foundations.\n", "user": "# Current Program Information\n- Fitness: 0.4338\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4353 \u2192 0.4338. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      mu: float = 1750.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      # Background LM robustness\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      # Mix token-LM background with a df-based \"presence LM\" to reduce burstiness bias\n      # in corpora with very long / code-heavy docs (StackOverflow / web dumps).\n      collection_df_alpha: float = 0.10  # 0 disables\n  \n      # Query-independent term diagnostics\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      # Query TF saturation\n      query_tf_power: float = 0.60\n  \n      # Soft-AND signals\n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n  \n      # Query-scope coverage prior over \"information mass\"\n      scope_strength: float = 0.07  # 0 disables\n  \n      # Leak a tiny fraction of negative evidence (still mostly surplus-evidence)\n  ... (4 more lines)\nwith:\n  class Config:\n      mu: float = 1750.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      collection_df_alpha: float = 0.10  # 0 disables\n  \n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      query_tf_power: float = 0.60\n  \n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n      scope_strength: float = 0.07  # 0 disables\n      neg_strength: float = 0.06  # 0 disables\n      doclen_prior_strength: float = 0.06  # 0 disables\n  \n      # Penalize \"over-spread\" terms where p_doc >> p_col (often boilerplate markers).\n      anti_edr_strength: float = 0.12  # 0 disables; keep small\n  \n      # Aboutness prior: reward docs that allocate more *raw tf mass* to the query.\n      # Helps nDCG by demoting very long docs with a single mention.\n      focus_strength: float = 0.08  # 0 disables\nChange 2: Replace:\n  # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n          if self.N > 0:\n              p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n          else:\n              p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n  \n          lam = float(getattr(Config, \"edr_strength\", 0.0))\n          clipc = float(getattr(Config, \"edr_clip\", 3.0))\n          ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n  \n          if lam > 0.0 and self.N > 0:\n              ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), ...\n              ratio = np.clip(ratio, -clipc, clipc)\n              self._edr_gate = 1.0 + lam * ratio\n          else:\n              self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n  \n          if ridf_s > 0.0 and self.N > 0:\n              ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), C...\n              ridf = np.maximum(ridf, 0.0)\n              self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n          else:\n              self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\nwith:\n  # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n          if self.N > 0:\n              p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n          else:\n              p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n  \n          lam = float(getattr(Config, \"edr_strength\", 0.0))\n          clipc = float(getattr(Config, \"edr_clip\", 3.0))\n          ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n          anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n  \n          if self.N > 0:\n              ratio_full = np.log(\n                  np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsi...\n              )\n              ratio_clip = np.clip(ratio_full, -clipc, clipc)\n          else:\n              ratio_full = np.zeros(self.vocab_size, dtype=np.float64)\n              ratio_clip = ratio_full\n  \n          if lam > 0.0 and self.N > 0:\n              self._edr_gate = 1.0 + lam * ratio_clip\n          else:\n              self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n  \n          # Anti-EDR: softly downweight only the positive side (p_doc >> p_col), bounded and monotone.\n          if anti > 0.0 and self.N > 0:\n              self._anti_edr_gate = 1.0 / (1.0 + anti * np.maximum(ratio_full, 0.0))\n          else:\n              self._anti_edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n  ... (6 more lines)\nChange 3: Replace:\n  gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n          ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) els...\nwith:\n  gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n          if corpus is not None and tid is not None and hasattr(corpus, \"_anti_edr_gate\"):\n              gate *= float(corpus._anti_edr_gate[tid])\n          ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) els...\nChange 4: Replace:\n  # Query-scope info-mass accumulators (slow path; score() isn't the bottleneck)\n      scope_num = 0.0\n      scope_den = 0.0\nwith:\n  # Query-scope info-mass accumulators (slow path; score() isn't the bottleneck)\n      scope_num = 0.0\n      scope_den = 0.0\n  \n      # Aboutness accumulator: fraction of doc mass assigned to query terms (raw tf / L).\n      focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n      focus_acc = 0.0\nChange 5: Replace:\n  if scope_s > 0.0 and ridf_w > 0.0:\n              # ridf_w is already bounded in Corpus; log1p makes \"info mass\" robust.\n              info = math.log1p(ridf_w)\n              scope_den += info\n              if tf > 0.0:\n                  scope_num += info\nwith:\n  if scope_s > 0.0 and ridf_w > 0.0:\n              info = math.log1p(ridf_w)\n              scope_den += info\n              if tf > 0.0:\n                  scope_num += info\n  \n          if focus_s > 0.0 and doc_length > 0.0 and tf > 0.0:\n              focus_acc += qtf_raw * (tf / doc_length)\nChange 6: Replace:\n  if scope_s > 0.0 and scope_den > 0.0:\n          score += scope_s * math.log(max(scope_num / scope_den, eps))\n  \n      return score\nwith:\n  if scope_s > 0.0 and scope_den > 0.0:\n          score += scope_s * math.log(max(scope_num / scope_den, eps))\n  \n      # Aboutness prior: concave transform to keep scale stable across queries.\n      if focus_s > 0.0 and total_qw > 0.0:\n          score += focus_s * math.log1p(max(focus_acc / max(total_qw, eps), 0.0))\n  \n      return score\nChange 7: Replace:\n  ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id])\nwith:\n  ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id]) * float(\n                  getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id]\n              )\nChange 8: Replace:\n  scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n          scope_den = 0.0\nwith:\n  scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n          scope_den = 0.0\n  \n          focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n          focus_acc = np.zeros(len(candidate_docs), dtype=np.float64) if focus_s > 0.0 else None\nChange 9: Replace:\n  if scope_num is not None:\n                  info = float(np.log1p(ridf_w))\n                  scope_den += info\n                  scope_num += (tf_row > 0.0).astype(np.float64) * info\nwith:\n  if scope_num is not None:\n                  info = float(np.log1p(ridf_w))\n                  scope_den += info\n                  scope_num += (tf_row > 0.0).astype(np.float64) * info\n  \n              if focus_acc is not None:\n                  focus_acc += (qtf_raw * tf_row) / np.maximum(doc_lengths, 1.0)\nChange 10: Replace:\n  if scope_num is not None and scope_den > 0.0:\n              frac = np.maximum(scope_num / max(scope_den, eps), eps)\n              scores += scope_s * np.log(frac)\n  \n          return scores\nwith:\n  if scope_num is not None and scope_den > 0.0:\n              frac = np.maximum(scope_num / max(scope_den, eps), eps)\n              scores += scope_s * np.log(frac)\n  \n          if focus_acc is not None and focus_s > 0.0:\n              scores += focus_s * np.log1p(np.maximum(focus_acc / max(total_q, eps), 0.0))\n  \n          return scores\n- Metrics: beir_nfcorpus_ndcg@10: 0.3228, beir_nfcorpus_recall@100: 0.2556, beir_nfcorpus_index_time_ms: 3589.6773, beir_nfcorpus_query_time_ms: 124.1974, beir_scifact_ndcg@10: 0.6842, beir_scifact_recall@100: 0.9242, beir_scifact_index_time_ms: 4946.9265, beir_scifact_query_time_ms: 501.9903, bright_pony_ndcg@10: 0.0825, bright_pony_recall@100: 0.3143, bright_pony_index_time_ms: 1467.4110, bright_pony_query_time_ms: 700.0621, beir_arguana_ndcg@10: 0.2777, beir_arguana_recall@100: 0.9236, beir_arguana_index_time_ms: 5952.2930, beir_arguana_query_time_ms: 31319.8932, bright_theoremqa_theorems_ndcg@10: 0.0302, bright_theoremqa_theorems_recall@100: 0.1513, bright_theoremqa_theorems_index_time_ms: 10567.6632, bright_theoremqa_theorems_query_time_ms: 1332.7658, beir_scidocs_ndcg@10: 0.1461, beir_scidocs_recall@100: 0.3495, beir_scidocs_index_time_ms: 19775.6025, beir_scidocs_query_time_ms: 4512.2918, bright_economics_ndcg@10: 0.1514, bright_economics_recall@100: 0.3881, bright_economics_index_time_ms: 14380.2947, bright_economics_query_time_ms: 3687.5617, bright_biology_ndcg@10: 0.3044, bright_biology_recall@100: 0.5854, bright_biology_index_time_ms: 15362.4323, bright_biology_query_time_ms: 3588.7763, beir_fiqa_ndcg@10: 0.2241, beir_fiqa_recall@100: 0.5142, beir_fiqa_index_time_ms: 32634.9129, beir_fiqa_query_time_ms: 7991.0925, bright_earth_science_ndcg@10: 0.3263, bright_earth_science_recall@100: 0.6682, bright_earth_science_index_time_ms: 31922.3339, bright_earth_science_query_time_ms: 5982.0736, bright_stackoverflow_ndcg@10: 0.1937, bright_stackoverflow_recall@100: 0.4949, bright_stackoverflow_index_time_ms: 99055.4851, bright_stackoverflow_query_time_ms: 13381.8923, beir_trec-covid_ndcg@10: 0.6532, beir_trec-covid_recall@100: 0.1109, beir_trec-covid_index_time_ms: 134138.2657, beir_trec-covid_query_time_ms: 2040.2857, avg_ndcg@10: 0.2831, avg_recall@100: 0.4734, combined_score: 0.4353, total_index_time_ms: 373793.2981, total_query_time_ms: 75162.8826, total_time_ms: 448956.1807, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Tempered background LM p_t(w) \u221d p(w)^tau\n      collection_temper: float = 0.85\n  \n      # Query term burstiness saturation (qtf^alpha)\n      query_tf_power: float = 0.6\n  \n      # Document length prior (log-normal-ish); keep small to avoid recall loss\n      length_prior_strength: float = 0.06\n  \n      # EDR gate: token-vs-document spread mismatch\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Residual-IDF query weighting (df/N vs token LM)\n      residual_idf_strength: float = 0.9\n  \n      # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n      burstiness_strength: float = 0.35\n  \n      # Two-stage background (collection + uniform over vocab).\n      uniform_bg_mass: float = 0.03  # 0 disables\n  \n      # Soft-AND query aggregation.\n      and_strength: float = 0.15  # 0 disables\n      and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n  \n  ... (22 more lines)\nwith:\n  class Config:\n      # Dirichlet smoothing mass\n      mu: float = 1750.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive smoothing: mu_t = mu * ((df+1)/(N+1))^a\n      mu_df_power: float = 0.30\n  \n      # Background LM robustness\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      collection_df_alpha: float = 0.10  # mix token-LM with df-LM\n  \n      # Query weighting + query-independent term diagnostics\n      query_tf_power: float = 0.60\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n      anti_edr_strength: float = 0.12  # penalize \"over-spread\" boilerplate markers\n  \n      # Gentle completeness (soft AND) + coordination\n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      scope_strength: float = 0.07\n      coverage_len_power: float = 0.35\n  \n      # Mild verbosity prior\n      length_prior_strength: float = 0.06\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n  ) -> float:\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\nChange 3: 'return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)' to 'return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus)'\nChange 4: Replace:\n  \"\"\"\n      Dirichlet QL with two *information-diagnostic* modifiers:\n  \n      1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n      2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n         yet not widely spread across documents.\n  \n      The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n      In the vectorized path we can compute p_doc exactly and apply it per term id.\n      \"\"\"\n      mu, eps = Config.mu, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      lam = float(getattr(Config, \"edr_strength\", 0.0))\n      clipc = float(getattr(Config, \"edr_clip\", 3.0))\n      ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n  \n      score = 0.0\n      qtf = Counter(query_repr.terms)\n  \n      burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n  \n      scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n      scope_num = 0.0\n      scope_den = 0.0\n  \n      for term, c_q in qtf.items():\n          term_count = float(doc_tf.get(term, 0))\n          p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n  \n          if burst_s > 0.0:\n  ... (26 more lines)\nwith:\n  \"\"\"\n      Residual-IDF + (EDR * anti-EDR) gated LMDirichlet with gentle completeness.\n  \n      Rationale:\n        - Keep a recall-friendly LMDirichlet \"surplus evidence\" backbone.\n        - Use query-independent term diagnostics based on p_doc vs p_col:\n            ridf(w) = max(0, log(p_doc/p_col))  (query weighting)\n            edr gate = 1 + \u03bb * clip(log(p_doc/p_col))  (evidence shaping)\n            anti-edr gate = 1/(1 + a * max(log(p_doc/p_col),0)) (boilerplate damping)\n        - Add mild soft-AND (missing-term penalty + scope + coordination) to improve early precision.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n  \n      score = 0.0\n      qtf = Counter(query_repr.terms)\n  \n      # need avgdl for a stable verbosity/coverage normalization in the slow path\n      if corpus is not None and corpus.N > 0:\n          avgdl = float(corpus.avgdl)\n          N = float(corpus.N)\n      else:\n          avgdl = max(doc_length, 1.0)\n          N = 1.0\n  \n      # Mild verbosity prior\n      prior_s = float(getattr(Config, \"length_prior_strength\", 0.0))\n      if prior_s > 0.0:\n          m = math.log(max(avgdl, 1.0))\n          score -= prior_s * (math.log(max(doc_length, 1.0)) - m) ** 2\n  ... (63 more lines)\nChange 5: Replace:\n  # Collection LM (tempered) for Dirichlet smoothing\n          # plus a small uniform background mixture to reduce peakiness.\n          tau = float(getattr(Config, \"collection_temper\", 1.0))\n          self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n          base_p = np.zeros(self.vocab_size, dtype=np.float64)\n          for term, tid in self._vocab.items():\n              tf = float(self.corpus_term_freq.get(term, 0))\n              base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n  \n          if tau != 1.0:\n              tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n              z = float(np.sum(tmp))\n              p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n          else:\n              p_col = np.maximum(base_p, Config.epsilon)\n  \n          # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V)\n          gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n          if gamma > 0.0 and self.vocab_size > 0:\n              p_uni = 1.0 / float(self.vocab_size)\n              self._collection_prob[:] = (1.0 - gamma) * p_col + gamma * p_uni\n              self._collection_prob[:] = np.maximum(self._collection_prob, Config.epsilon)\n          else:\n              self._collection_prob[:] = p_col\n  \n          # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n          lam = float(getattr(Config, \"edr_strength\", 0.0))\n          clipc = float(getattr(Config, \"edr_clip\", 3.0))\n          ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n          burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n  ... (39 more lines)\nwith:\n  # --- Robust collection LM: tempered token-LM mixed with df-LM, plus tiny uniform ---\n          tau = float(getattr(Config, \"collection_temper\", 1.0))\n          base_p = np.zeros(self.vocab_size, dtype=np.float64)\n          for term, tid in self._vocab.items():\n              base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens...\n  \n          if tau != 1.0:\n              tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n              p_tf = np.maximum(tmp / max(float(np.sum(tmp)), Config.epsilon), Config.epsilon)\n          else:\n              p_tf = np.maximum(base_p, Config.epsilon)\n  \n          p_df = np.maximum(self._df / float(self.N), Config.epsilon) if self.N > 0 else np.full(se...\n          mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n          p_col = (1.0 - mix) * p_tf + mix * p_df if mix > 0.0 else p_tf\n          p_col = np.maximum(p_col / max(float(np.sum(p_col)), Config.epsilon), Config.epsilon)\n  \n          gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n          if gamma > 0.0 and self.vocab_size > 0:\n              p_uni = 1.0 / float(self.vocab_size)\n              self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsi...\n          else:\n              self._collection_prob = p_col\n  \n          # --- Query-independent term diagnostics: p_doc vs p_col ---\n          p_doc = p_df  # df-based doc-spread LM\n  \n          clipc = float(getattr(Config, \"edr_clip\", 3.0))\n          ratio_full = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon),...\n          ratio_clip = np.clip(ratio_full, -clipc, clipc)\n  ... (24 more lines)\nChange 6: Replace:\n  mu, eps = Config.mu, Config.epsilon\n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          and_strength = float(getattr(Config, \"and_strength\", 0.0))\n          and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n          miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n          scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n  \n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n  \n          # Query-scope accumulators: per-doc matched specificity mass vs total query specificity m...\n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n          scope_den = 0.0\n  \n          # Query clarity (mean capped IDF over query terms). Used to modulate \"AND-ness\" safely:\n          # generic queries -> less missing-term penalty / less AND reward; specific queries -> more.\n          idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n          q_idf = np.array(\n              [min(idf_clip, float(self.corpus._idf[tid])) for tid in query_term_ids],\n              dtype=np.float64,\n          )\n          q_clarity = float(np.mean(q_idf)) / max(idf_clip, eps) if len(q_idf) > 0 else 0.0  # in [...\n  \n          miss_gain = float(getattr(Config, \"clarity_missing_gain\", 0.0))\n          and_gain = float(getattr(Config, \"clarity_and_gain\", 0.0))\n          miss_s_eff = miss_s * (1.0 + miss_gain * q_clarity)\n          and_strength_eff = and_strength * (1.0 + and_gain * q_clarity)\n  \n  ... (5 more lines)\nwith:\n  base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n  \n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          avgdl = max(float(self.corpus.avgdl), 1.0)\n          len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** float(getattr(Config, \"coverage_le...\n          len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n  \n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n          scope_den = 0.0\n  \n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(...\n          total_q = total_q if total_q > 0.0 else 1.0\nChange 7: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              beta = self.corpus._tf_beta[term_id]\n              tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n  \n              per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / ...\n              per_term *= self.corpus._edr_gate[term_id]\n  \n              w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n              w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n  \n              present = np.maximum(per_term, 0.0)\n              contrib = w * present\n              scores += contrib\n  \n              if miss_s_eff > 0.0:\n                  miss = (tf_row <= 0.0).astype(np.float64)\n                  if np.any(miss):\n                      base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                      scores += miss_s_eff * w * miss * base0\n  \n              if and_acc is not None:\n                  and_acc += np.tanh(contrib / max(and_sat, eps))\n  \n              if scope_num is not None:\n                  # Stable per-term \"information mass\" proxy derived from residual IDF weights.\n                  info = float(np.log1p(self.corpus._ridf_qweight[term_id]))\n                  scope_den += info\n  ... (23 more lines)\nwith:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (float(self.corpus.N) + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              per_term = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  \n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id]) * float(self.corpus._anti_edr_gate[term_...\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf_w = float(qtf_raw) ** alpha\n  \n              pos = np.maximum(per_term, 0.0)\n              miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -(float(getattr(Config, \"miss_penalty\", 0.0)) * len_factor) * min(ridf_w, float(g...\n              )\n  \n              scores += (qtf_w * ridf_w * gate) * pos + (qtf_w * miss)\n  \n              matched += float(qtf_raw) * (tf_row > 0.0)\n  \n              if scope_num is not None:\n  ... (15 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3226, beir_nfcorpus_recall@100: 0.2558, beir_nfcorpus_index_time_ms: 3672.3212, beir_nfcorpus_query_time_ms: 114.0320, beir_scifact_ndcg@10: 0.6841, beir_scifact_recall@100: 0.9242, beir_scifact_index_time_ms: 4858.3952, beir_scifact_query_time_ms: 461.4300, bright_pony_ndcg@10: 0.0831, bright_pony_recall@100: 0.3137, bright_pony_index_time_ms: 1375.8632, bright_pony_query_time_ms: 614.1837, beir_arguana_ndcg@10: 0.2779, beir_arguana_recall@100: 0.9229, beir_arguana_index_time_ms: 5966.6562, beir_arguana_query_time_ms: 29041.2603, bright_theoremqa_theorems_ndcg@10: 0.0302, bright_theoremqa_theorems_recall@100: 0.1513, bright_theoremqa_theorems_index_time_ms: 10728.7035, bright_theoremqa_theorems_query_time_ms: 1223.0532, beir_scidocs_ndcg@10: 0.1458, beir_scidocs_recall@100: 0.3493, beir_scidocs_index_time_ms: 19703.6281, beir_scidocs_query_time_ms: 4120.4590, bright_economics_ndcg@10: 0.1515, bright_economics_recall@100: 0.3881, bright_economics_index_time_ms: 14382.7540, bright_economics_query_time_ms: 3434.3833, bright_biology_ndcg@10: 0.3011, bright_biology_recall@100: 0.5854, bright_biology_index_time_ms: 15247.5872, bright_biology_query_time_ms: 3231.5335, beir_fiqa_ndcg@10: 0.2246, beir_fiqa_recall@100: 0.5156, beir_fiqa_index_time_ms: 34280.9993, beir_fiqa_query_time_ms: 7240.5552, bright_earth_science_ndcg@10: 0.3265, bright_earth_science_recall@100: 0.6687, bright_earth_science_index_time_ms: 32276.5981, bright_earth_science_query_time_ms: 5271.3585, bright_stackoverflow_ndcg@10: 0.1938, bright_stackoverflow_recall@100: 0.4956, bright_stackoverflow_index_time_ms: 98609.4415, bright_stackoverflow_query_time_ms: 11942.4517, beir_trec-covid_ndcg@10: 0.6517, beir_trec-covid_recall@100: 0.1111, beir_trec-covid_index_time_ms: 134423.2385, beir_trec-covid_query_time_ms: 1747.8277, avg_ndcg@10: 0.2827, avg_recall@100: 0.4735, combined_score: 0.4353, total_index_time_ms: 375526.1860, total_query_time_ms: 68442.5280, total_time_ms: 443968.7140, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      mu: float = 1750.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      collection_df_alpha: float = 0.10\n  \n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      query_tf_power: float = 0.60\n  \n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n      scope_strength: float = 0.07\n      neg_strength: float = 0.06\n      doclen_prior_strength: float = 0.06\n  \n      clarity_idf_clip: float = 6.0\n      clarity_gain: float = 0.8\n      miss_clarity_boost: float = 0.60\n      scope_clarity_boost: float = 0.60\n  \n      # Aboutness / topicality prior (query mass under doc LM).\n      focus_strength: float = 0.08  # 0 disables\n  ... (6 more lines)\nwith:\n  class Config:\n      mu: float = 1750.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      collection_df_alpha: float = 0.10\n  \n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      query_tf_power: float = 0.60\n  \n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n      scope_strength: float = 0.07\n      neg_strength: float = 0.06\n      doclen_prior_strength: float = 0.06\n  \n      clarity_idf_clip: float = 6.0\n      clarity_gain: float = 0.8\n      miss_clarity_boost: float = 0.60\n      scope_clarity_boost: float = 0.60\n  \n      focus_strength: float = 0.08  # 0 disables\n  \n  ... (9 more lines)\nChange 2: Replace:\n  # Residual-IDF weighting + (EDR * anti-EDR) gate (query-independent)\n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              w = qtf * ridf_w\n              gate = float(self.corpus._edr_gate[term_id]) * float(getattr(self.corpus, \"_anti_edr_...\nwith:\n  # Residual-IDF weighting + (EDR * anti-EDR) gate (query-independent)\n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n  \n              # Add a tiny query-term surprisal prior (bounded) to favor informative query terms.\n              # This is a *query prior* (not a doc-dependent IDF hack): it scales how much evidence\n              # we ask for from each query term based on how surprising it is in the collection.\n              s_q = float(getattr(Config, \"surprisal_q_strength\", 0.0))\n              if s_q > 0.0:\n                  clip_s = float(getattr(Config, \"surprisal_q_clip\", 8.0))\n                  surpr = min(clip_s, float(-np.log(max(p_collection, eps))))\n                  # concave -> stable across domains; 1+... keeps backward compatibility at s_q=0\n                  q_prior_w = 1.0 + s_q * np.log1p(surpr)\n              else:\n                  q_prior_w = 1.0\n  \n              w = qtf * ridf_w * q_prior_w\n              gate = float(self.corpus._edr_gate[term_id]) * float(\n                  getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id]\n              )\nChange 3: Replace:\n  # Discriminative term weighting/gating (use precomputed signals when available)\n          gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n          ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) els...\n  \n          w = qtf * ridf_w\nwith:\n  # Discriminative term weighting/gating (use precomputed signals when available)\n          gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n          ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) els...\n  \n          # Tiny query-term surprisal prior (bounded): prefer evidence on rarer query terms.\n          s_q = float(getattr(Config, \"surprisal_q_strength\", 0.0))\n          if s_q > 0.0:\n              clip_s = float(getattr(Config, \"surprisal_q_clip\", 8.0))\n              surpr = min(clip_s, -math.log(max(p_collection, eps)))\n              q_prior_w = 1.0 + s_q * math.log1p(surpr)\n          else:\n              q_prior_w = 1.0\n  \n          w = qtf * ridf_w * q_prior_w\n- Metrics: beir_nfcorpus_ndcg@10: 0.3226, beir_nfcorpus_recall@100: 0.2559, beir_nfcorpus_index_time_ms: 3598.4533, beir_nfcorpus_query_time_ms: 128.9235, beir_scifact_ndcg@10: 0.6844, beir_scifact_recall@100: 0.9242, beir_scifact_index_time_ms: 4885.9827, beir_scifact_query_time_ms: 497.5847, bright_pony_ndcg@10: 0.0865, bright_pony_recall@100: 0.3125, bright_pony_index_time_ms: 1477.9503, bright_pony_query_time_ms: 685.3869, beir_arguana_ndcg@10: 0.2848, beir_arguana_recall@100: 0.9315, beir_arguana_index_time_ms: 5982.1123, beir_arguana_query_time_ms: 30907.6471, bright_theoremqa_theorems_ndcg@10: 0.0308, bright_theoremqa_theorems_recall@100: 0.1579, bright_theoremqa_theorems_index_time_ms: 10908.5364, bright_theoremqa_theorems_query_time_ms: 1272.2342, beir_scidocs_ndcg@10: 0.1465, beir_scidocs_recall@100: 0.3497, beir_scidocs_index_time_ms: 20013.1345, beir_scidocs_query_time_ms: 4485.1668, bright_economics_ndcg@10: 0.1521, bright_economics_recall@100: 0.3890, bright_economics_index_time_ms: 13681.9130, bright_economics_query_time_ms: 3611.0312, bright_biology_ndcg@10: 0.3005, bright_biology_recall@100: 0.5819, bright_biology_index_time_ms: 15101.7301, bright_biology_query_time_ms: 3434.8104, beir_fiqa_ndcg@10: 0.2238, beir_fiqa_recall@100: 0.5169, beir_fiqa_index_time_ms: 33815.7202, beir_fiqa_query_time_ms: 7771.4095, bright_earth_science_ndcg@10: 0.3247, bright_earth_science_recall@100: 0.6643, bright_earth_science_index_time_ms: 31735.4798, bright_earth_science_query_time_ms: 5855.5519, bright_stackoverflow_ndcg@10: 0.1872, bright_stackoverflow_recall@100: 0.4906, bright_stackoverflow_index_time_ms: 98382.1684, bright_stackoverflow_query_time_ms: 13390.0830, beir_trec-covid_ndcg@10: 0.6524, beir_trec-covid_recall@100: 0.1108, beir_trec-covid_index_time_ms: 133506.8257, beir_trec-covid_query_time_ms: 1862.6364, avg_ndcg@10: 0.2830, avg_recall@100: 0.4738, combined_score: 0.4356, total_index_time_ms: 373090.0066, total_query_time_ms: 73902.4654, total_time_ms: 446992.4720, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4356)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n    doclen_prior_strength: float = 0.06\n\n    clarity_idf_clip: float = 6.0\n    clarity_gain: float = 0.8\n    miss_clarity_boost: float = 0.60\n    scope_clarity_boost: float = 0.60\n\n    # Aboutness / topicality prior (query mass under doc LM).\n    focus_strength: float = 0.08  # 0 disables\n\n    # NEW: \"excess spread\" gate.\n    # If a term is *more* spread across docs than expected from its token frequency (p_doc >> p_col),\n    # it behaves like a corpus-wide boilerplate marker. Penalize its evidence slightly.\n    # This targets early precision (nDCG@10) on web/forum corpora without hurting recall much.\n    anti_edr_strength: float = 0.12  # 0 disables; keep small\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior: long docs tend to match many terms by chance.\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Prefer precomputed, renormalized collection LM when possible (faster + consistent).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # Query-scope prior in slow path, with the same clarity-adaptive scaling as rank().\n    scope_s0 = float(getattr(Config, \"scope_strength\", 0.0))\n    if corpus is not None and scope_s0 > 0.0:\n        idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n        gain = float(getattr(Config, \"clarity_gain\", 0.0))\n        scope_boost = float(getattr(Config, \"scope_clarity_boost\", 0.0))\n\n        # clarity in [0,1] from mean clipped IDF of query terms\n        tids = [corpus.get_term_id(t) for t in query_repr.terms]\n        tids = [t for t in tids if t is not None]\n        if tids and hasattr(corpus, \"_idf\"):\n            q_idf = [min(idf_clip, float(corpus._idf[t])) for t in tids]\n            clarity = (sum(q_idf) / float(len(q_idf))) / max(idf_clip, eps)\n        else:\n            clarity = 0.0\n\n        cfac = clarity ** max(gain, 0.0) if gain > 0.0 else clarity\n        scope_s = scope_s0 * (1.0 + scope_boost * cfac)\n\n        if scope_s > 0.0:\n            scope_num = 0.0\n            scope_den = 0.0\n            for term in query_repr.terms:\n                tid = corpus.get_term_id(term)\n                if tid is None:\n                    continue\n                info = math.log1p(float(corpus._ridf_qweight[tid]))\n                scope_den += info\n                if float(doc_tf.get(term, 0.0)) > 0.0:\n                    scope_num += info\n            if scope_den > 0.0:\n                frac = max(scope_num / scope_den, eps)\n                score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Store IDF for query-clarity modulation (cheap + query-time only).\n        if self.N > 0:\n            self._idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n        else:\n            self._idf = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n        else:\n            ratio = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Positive EDR: boost terms whose document-spread is higher than token-LM predicts (often discriminative).\n        if lam > 0.0 and self.N > 0:\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Anti-EDR: softly *downweight* extreme p_doc >> p_col terms (global boilerplate markers).\n        anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n        if anti > 0.0 and self.N > 0:\n            # penalize only the positive side; keep bounded and monotone\n            self._anti_edr_gate = 1.0 / (1.0 + anti * np.maximum(ratio, 0.0))\n        else:\n            self._anti_edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: clarity-adaptive completeness.\n        # clarity in [0,1] from mean clipped IDF of query terms.\n        scope_s0 = float(getattr(Config, \"scope_strength\", 0.0))\n        miss_boost = float(getattr(Config, \"miss_clarity_boost\", 0.0))\n        scope_boost = float(getattr(Config, \"scope_clarity_boost\", 0.0))\n        idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n        gain = float(getattr(Config, \"clarity_gain\", 0.0))\n\n        if len(query_term_ids) > 0 and hasattr(self.corpus, \"_idf\"):\n            q_idf = np.minimum(self.corpus._idf[np.array(query_term_ids, dtype=np.int64)], idf_clip)\n            clarity = float(np.mean(q_idf)) / max(idf_clip, eps)\n        else:\n            clarity = 0.0\n\n        cfac = clarity ** max(gain, 0.0) if gain > 0.0 else clarity\n        miss_penalty_eff = Config.miss_penalty * (1.0 + miss_boost * cfac)\n\n        scope_s = scope_s0 * (1.0 + scope_boost * cfac)\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n        focus_acc = np.zeros(len(candidate_docs), dtype=np.float64) if focus_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            # Residual-IDF weighting + (EDR * anti-EDR) gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id]) * float(getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(miss_penalty_eff * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            # Focus/aboutness: fraction of doc LM mass assigned to query terms.\n            # Uses raw tf (not smoothed) on purpose: penalizes \"one hit in huge doc\".\n            if focus_acc is not None:\n                focus_acc += (qtf_raw * tf_row) / np.maximum(doc_lengths, 1.0)\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0 and scope_s > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        # Focus prior: log of query-mass under doc LM (aboutness).\n        # Use a concave transform log1p to reduce sensitivity to long queries and repeated tokens.\n        if focus_acc is not None and focus_s > 0.0:\n            scores += focus_s * np.log1p(np.maximum(focus_acc / max(total_q, eps), 0.0))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3226), Performs well on beir_nfcorpus_recall@100 (0.2559), Performs well on beir_nfcorpus_index_time_ms (3578.6159), Performs well on beir_nfcorpus_query_time_ms (128.4989), Performs well on beir_scifact_ndcg@10 (0.6844), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4894.2152), Performs well on beir_scifact_query_time_ms (494.1545), Performs well on bright_pony_ndcg@10 (0.0865), Performs well on bright_pony_recall@100 (0.3125), Performs well on bright_pony_index_time_ms (1398.5920), Performs well on bright_pony_query_time_ms (684.5597), Performs well on beir_arguana_ndcg@10 (0.2848), Performs well on beir_arguana_recall@100 (0.9315), Performs well on beir_arguana_index_time_ms (5939.9303), Performs well on beir_arguana_query_time_ms (30934.9954), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0308), Performs well on bright_theoremqa_theorems_recall@100 (0.1579), Performs well on bright_theoremqa_theorems_index_time_ms (10768.6291), Performs well on bright_theoremqa_theorems_query_time_ms (1337.9200), Performs well on beir_scidocs_ndcg@10 (0.1465), Performs well on beir_scidocs_recall@100 (0.3497), Performs well on beir_scidocs_index_time_ms (20005.2435), Performs well on beir_scidocs_query_time_ms (4488.5124), Performs well on bright_economics_ndcg@10 (0.1521), Performs well on bright_economics_recall@100 (0.3890), Performs well on bright_economics_index_time_ms (14109.5551), Performs well on bright_economics_query_time_ms (3595.6228), Performs well on bright_biology_ndcg@10 (0.3005), Performs well on bright_biology_recall@100 (0.5819), Performs well on bright_biology_index_time_ms (15447.3350), Performs well on bright_biology_query_time_ms (3506.5857), Performs well on beir_fiqa_ndcg@10 (0.2238), Performs well on beir_fiqa_recall@100 (0.5169), Performs well on beir_fiqa_index_time_ms (34184.5676), Performs well on beir_fiqa_query_time_ms (7897.7770), Performs well on bright_earth_science_ndcg@10 (0.3247), Performs well on bright_earth_science_recall@100 (0.6643), Performs well on bright_earth_science_index_time_ms (31362.2759), Performs well on bright_earth_science_query_time_ms (5929.4434), Performs well on bright_stackoverflow_ndcg@10 (0.1872), Performs well on bright_stackoverflow_recall@100 (0.4906), Performs well on bright_stackoverflow_index_time_ms (98894.1449), Performs well on bright_stackoverflow_query_time_ms (12960.8587), Performs well on beir_trec-covid_ndcg@10 (0.6524), Performs well on beir_trec-covid_recall@100 (0.1108), Performs well on beir_trec-covid_index_time_ms (133741.1315), Performs well on beir_trec-covid_query_time_ms (1932.4787), Performs well on avg_ndcg@10 (0.2830), Performs well on avg_recall@100 (0.4738), Performs well on combined_score (0.4356), Performs well on total_index_time_ms (374324.2358), Performs well on total_query_time_ms (73891.4070), Performs well on total_time_ms (448215.6429), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4356)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n    doclen_prior_strength: float = 0.06\n\n    clarity_idf_clip: float = 6.0\n    clarity_gain: float = 0.8\n    miss_clarity_boost: float = 0.60\n    scope_clarity_boost: float = 0.60\n\n    focus_strength: float = 0.08  # 0 disables\n\n    anti_edr_strength: float = 0.12  # 0 disables; keep small\n\n    # NEW: \"informativeness prior\" for query terms, derived from their collection surprisal.\n    # Intuition: the LM surplus-evidence already accounts for tf vs background, but for very short\n    # (often keyword) queries, a small surprisal prior helps early precision by preferring docs\n    # that match rarer query terms.\n    # We keep it bounded + concave (log1p) to avoid crushing recall.\n    surprisal_q_strength: float = 0.08  # 0 disables\n    surprisal_q_clip: float = 8.0\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior: long docs tend to match many terms by chance.\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Prefer precomputed, renormalized collection LM when possible (faster + consistent).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        # Tiny query-term surprisal prior (bounded): prefer evidence on rarer query terms.\n        s_q = float(getattr(Config, \"surprisal_q_strength\", 0.0))\n        if s_q > 0.0:\n            clip_s = float(getattr(Config, \"surprisal_q_clip\", 8.0))\n            surpr = min(clip_s, -math.log(max(p_collection, eps)))\n            q_prior_w = 1.0 + s_q * math.log1p(surpr)\n        else:\n            q_prior_w = 1.0\n\n        w = qtf * ridf_w * q_prior_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # Query-scope prior in slow path, with the same clarity-adaptive scaling as rank().\n    scope_s0 = float(getattr(Config, \"scope_strength\", 0.0))\n    if corpus is not None and scope_s0 > 0.0:\n        idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n        gain = float(getattr(Config, \"clarity_gain\", 0.0))\n        scope_boost = float(getattr(Config, \"scope_clarity_boost\", 0.0))\n\n        # clarity in [0,1] from mean clipped IDF of query terms\n        tids = [corpus.get_term_id(t) for t in query_repr.terms]\n        tids = [t for t in tids if t is not None]\n        if tids and hasattr(corpus, \"_idf\"):\n            q_idf = [min(idf_clip, float(corpus._idf[t])) for t in tids]\n            clarity = (sum(q_idf) / float(len(q_idf))) / max(idf_clip, eps)\n        else:\n            clarity = 0.0\n\n        cfac = clarity ** max(gain, 0.0) if gain > 0.0 else clarity\n        scope_s = scope_s0 * (1.0 + scope_boost * cfac)\n\n        if scope_s > 0.0:\n            scope_num = 0.0\n            scope_den = 0.0\n            for term in query_repr.terms:\n                tid = corpus.get_term_id(term)\n                if tid is None:\n                    continue\n                info = math.log1p(float(corpus._ridf_qweight[tid]))\n                scope_den += info\n                if float(doc_tf.get(term, 0.0)) > 0.0:\n                    scope_num += info\n            if scope_den > 0.0:\n                frac = max(scope_num / scope_den, eps)\n                score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Store IDF for query-clarity modulation (cheap + query-time only).\n        if self.N > 0:\n            self._idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n        else:\n            self._idf = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n        else:\n            ratio = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Positive EDR: boost terms whose document-spread is higher than token-LM predicts (often discriminative).\n        if lam > 0.0 and self.N > 0:\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Anti-EDR: softly *downweight* extreme p_doc >> p_col terms (global boilerplate markers).\n        anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n        if anti > 0.0 and self.N > 0:\n            # penalize only the positive side; keep bounded and monotone\n            self._anti_edr_gate = 1.0 / (1.0 + anti * np.maximum(ratio, 0.0))\n        else:\n            self._anti_edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: clarity-adaptive completeness.\n        # clarity in [0,1] from mean clipped IDF of query terms.\n        scope_s0 = float(getattr(Config, \"scope_strength\", 0.0))\n        miss_boost = float(getattr(Config, \"miss_clarity_boost\", 0.0))\n        scope_boost = float(getattr(Config, \"scope_clarity_boost\", 0.0))\n        idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n        gain = float(getattr(Config, \"clarity_gain\", 0.0))\n\n        if len(query_term_ids) > 0 and hasattr(self.corpus, \"_idf\"):\n            q_idf = np.minimum(self.corpus._idf[np.array(query_term_ids, dtype=np.int64)], idf_clip)\n            clarity = float(np.mean(q_idf)) / max(idf_clip, eps)\n        else:\n            clarity = 0.0\n\n        cfac = clarity ** max(gain, 0.0) if gain > 0.0 else clarity\n        miss_penalty_eff = Config.miss_penalty * (1.0 + miss_boost * cfac)\n\n        scope_s = scope_s0 * (1.0 + scope_boost * cfac)\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n        focus_acc = np.zeros(len(candidate_docs), dtype=np.float64) if focus_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            # Residual-IDF weighting + (EDR * anti-EDR) gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id]) * float(getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(miss_penalty_eff * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            # Focus/aboutness: fraction of doc LM mass assigned to query terms.\n            # Uses raw tf (not smoothed) on purpose: penalizes \"one hit in huge doc\".\n            if focus_acc is not None:\n                focus_acc += (qtf_raw * tf_row) / np.maximum(doc_lengths, 1.0)\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0 and scope_s > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        # Focus prior: log of query-mass under doc LM (aboutness).\n        # Use a concave transform log1p to reduce sensitivity to long queries and repeated tokens.\n        if focus_acc is not None and focus_s > 0.0:\n            scores += focus_s * np.log1p(np.maximum(focus_acc / max(total_q, eps), 0.0))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3226), Performs well on beir_nfcorpus_recall@100 (0.2559), Performs well on beir_nfcorpus_index_time_ms (3598.4533), Performs well on beir_nfcorpus_query_time_ms (128.9235), Performs well on beir_scifact_ndcg@10 (0.6844), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4885.9827), Performs well on beir_scifact_query_time_ms (497.5847), Performs well on bright_pony_ndcg@10 (0.0865), Performs well on bright_pony_recall@100 (0.3125), Performs well on bright_pony_index_time_ms (1477.9503), Performs well on bright_pony_query_time_ms (685.3869), Performs well on beir_arguana_ndcg@10 (0.2848), Performs well on beir_arguana_recall@100 (0.9315), Performs well on beir_arguana_index_time_ms (5982.1123), Performs well on beir_arguana_query_time_ms (30907.6471), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0308), Performs well on bright_theoremqa_theorems_recall@100 (0.1579), Performs well on bright_theoremqa_theorems_index_time_ms (10908.5364), Performs well on bright_theoremqa_theorems_query_time_ms (1272.2342), Performs well on beir_scidocs_ndcg@10 (0.1465), Performs well on beir_scidocs_recall@100 (0.3497), Performs well on beir_scidocs_index_time_ms (20013.1345), Performs well on beir_scidocs_query_time_ms (4485.1668), Performs well on bright_economics_ndcg@10 (0.1521), Performs well on bright_economics_recall@100 (0.3890), Performs well on bright_economics_index_time_ms (13681.9130), Performs well on bright_economics_query_time_ms (3611.0312), Performs well on bright_biology_ndcg@10 (0.3005), Performs well on bright_biology_recall@100 (0.5819), Performs well on bright_biology_index_time_ms (15101.7301), Performs well on bright_biology_query_time_ms (3434.8104), Performs well on beir_fiqa_ndcg@10 (0.2238), Performs well on beir_fiqa_recall@100 (0.5169), Performs well on beir_fiqa_index_time_ms (33815.7202), Performs well on beir_fiqa_query_time_ms (7771.4095), Performs well on bright_earth_science_ndcg@10 (0.3247), Performs well on bright_earth_science_recall@100 (0.6643), Performs well on bright_earth_science_index_time_ms (31735.4798), Performs well on bright_earth_science_query_time_ms (5855.5519), Performs well on bright_stackoverflow_ndcg@10 (0.1872), Performs well on bright_stackoverflow_recall@100 (0.4906), Performs well on bright_stackoverflow_index_time_ms (98382.1684), Performs well on bright_stackoverflow_query_time_ms (13390.0830), Performs well on beir_trec-covid_ndcg@10 (0.6524), Performs well on beir_trec-covid_recall@100 (0.1108), Performs well on beir_trec-covid_index_time_ms (133506.8257), Performs well on beir_trec-covid_query_time_ms (1862.6364), Performs well on avg_ndcg@10 (0.2830), Performs well on avg_recall@100 (0.4738), Performs well on combined_score (0.4356), Performs well on total_index_time_ms (373090.0066), Performs well on total_query_time_ms (73902.4654), Performs well on total_time_ms (446992.4720), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4353)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Dirichlet smoothing mass\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive smoothing: mu_t = mu * ((df+1)/(N+1))^a\n    mu_df_power: float = 0.30\n\n    # Background LM robustness\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10  # mix token-LM with df-LM\n\n    # Query weighting + query-independent term diagnostics\n    query_tf_power: float = 0.60\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n    anti_edr_strength: float = 0.12  # penalize \"over-spread\" boilerplate markers\n\n    # Gentle completeness (soft AND) + coordination\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    scope_strength: float = 0.07\n    coverage_len_power: float = 0.35\n\n    # Mild verbosity prior\n    length_prior_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + (EDR * anti-EDR) gated LMDirichlet with gentle completeness.\n\n    Rationale:\n      - Keep a recall-friendly LMDirichlet \"surplus evidence\" backbone.\n      - Use query-independent term diagnostics based on p_doc vs p_col:\n          ridf(w) = max(0, log(p_doc/p_col))  (query weighting)\n          edr gate = 1 + \u03bb * clip(log(p_doc/p_col))  (evidence shaping)\n          anti-edr gate = 1/(1 + a * max(log(p_doc/p_col),0)) (boilerplate damping)\n      - Add mild soft-AND (missing-term penalty + scope + coordination) to improve early precision.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    # need avgdl for a stable verbosity/coverage normalization in the slow path\n    if corpus is not None and corpus.N > 0:\n        avgdl = float(corpus.avgdl)\n        N = float(corpus.N)\n    else:\n        avgdl = max(doc_length, 1.0)\n        N = 1.0\n\n    # Mild verbosity prior\n    prior_s = float(getattr(Config, \"length_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        m = math.log(max(avgdl, 1.0))\n        score -= prior_s * (math.log(max(doc_length, 1.0)) - m) ** 2\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** float(getattr(Config, \"coverage_len_power\", 0.35))\n    len_factor = min(1.0, max(0.0, len_factor))\n\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    matched = 0.0\n    total_q = 0.0\n\n    for term, c_q in qtf.items():\n        qtf_raw = float(c_q)\n        qtf_w = qtf_raw ** alpha\n        total_q += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid]) if hasattr(corpus, \"_df\") else 1.0\n            ridf_w = float(corpus._ridf_qweight[tid])\n            gate = float(corpus._edr_gate[tid]) * float(getattr(corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = 1.0\n            ridf_w = 1.0\n            gate = 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n        pos = max(per_term, 0.0)\n\n        # missing-term penalty (bounded by ridf_w proxy mass)\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(float(getattr(Config, \"miss_penalty\", 0.0)) * len_factor) * min(ridf_w, float(getattr(Config, \"miss_cap\", 1.4)))\n\n        score += (qtf_w * ridf_w * gate) * pos + (qtf_w * miss)\n\n        if scope_s > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    # Coordination reward\n    if total_q > 0.0:\n        coord = matched / total_q\n        score += float(getattr(Config, \"coord_strength\", 0.0)) * math.log(1.0 + 9.0 * coord + eps)\n\n    # Scope prior\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Robust collection LM: tempered token-LM mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            p_tf = np.maximum(tmp / max(float(np.sum(tmp)), Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        p_df = np.maximum(self._df / float(self.N), Config.epsilon) if self.N > 0 else np.full(self.vocab_size, Config.epsilon)\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        p_col = (1.0 - mix) * p_tf + mix * p_df if mix > 0.0 else p_tf\n        p_col = np.maximum(p_col / max(float(np.sum(p_col)), Config.epsilon), Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Query-independent term diagnostics: p_doc vs p_col ---\n        p_doc = p_df  # df-based doc-spread LM\n\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ratio_full = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n        ratio_clip = np.clip(ratio_full, -clipc, clipc)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        self._edr_gate = (1.0 + lam * ratio_clip) if (lam > 0.0 and self.N > 0) else np.ones(self.vocab_size, dtype=np.float64)\n\n        anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n        self._anti_edr_gate = (\n            1.0 / (1.0 + anti * np.maximum(ratio_full, 0.0))\n            if (anti > 0.0 and self.N > 0)\n            else np.ones(self.vocab_size, dtype=np.float64)\n        )\n\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.maximum(ratio_full, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Keep IDF for possible query-time modulation (cheap)\n        self._idf = np.log((float(self.N) + 1.0) / (self._df + 1.0)) if self.N > 0 else np.zeros(self.vocab_size)\n\n        # Disable tf saturation in this variant (it interacted badly with the newer risk/AND logic).\n        # The robust background + anti-EDR usually provides the needed stability with fewer knobs.\n        self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** float(getattr(Config, \"coverage_len_power\", 0.35))\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (float(self.corpus.N) + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id]) * float(self.corpus._anti_edr_gate[term_id])\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf_w = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(float(getattr(Config, \"miss_penalty\", 0.0)) * len_factor) * min(ridf_w, float(getattr(Config, \"miss_cap\", 1.4))),\n            )\n\n            scores += (qtf_w * ridf_w * gate) * pos + (qtf_w * miss)\n\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Coordination reward\n        coord = matched / total_q\n        scores += float(getattr(Config, \"coord_strength\", 0.0)) * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior\n        if scope_num is not None and scope_den > 0.0:\n            scores += scope_s * np.log(np.maximum(scope_num / max(scope_den, eps), eps))\n\n        # Verbosity prior\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3226), Performs well on beir_nfcorpus_recall@100 (0.2558), Performs well on beir_nfcorpus_index_time_ms (3672.3212), Performs well on beir_nfcorpus_query_time_ms (114.0320), Performs well on beir_scifact_ndcg@10 (0.6841), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4858.3952), Performs well on beir_scifact_query_time_ms (461.4300), Performs well on bright_pony_ndcg@10 (0.0831), Performs well on bright_pony_recall@100 (0.3137), Performs well on bright_pony_index_time_ms (1375.8632), Performs well on bright_pony_query_time_ms (614.1837), Performs well on beir_arguana_ndcg@10 (0.2779), Performs well on beir_arguana_recall@100 (0.9229), Performs well on beir_arguana_index_time_ms (5966.6562), Performs well on beir_arguana_query_time_ms (29041.2603), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0302), Performs well on bright_theoremqa_theorems_recall@100 (0.1513), Performs well on bright_theoremqa_theorems_index_time_ms (10728.7035), Performs well on bright_theoremqa_theorems_query_time_ms (1223.0532), Performs well on beir_scidocs_ndcg@10 (0.1458), Performs well on beir_scidocs_recall@100 (0.3493), Performs well on beir_scidocs_index_time_ms (19703.6281), Performs well on beir_scidocs_query_time_ms (4120.4590), Performs well on bright_economics_ndcg@10 (0.1515), Performs well on bright_economics_recall@100 (0.3881), Performs well on bright_economics_index_time_ms (14382.7540), Performs well on bright_economics_query_time_ms (3434.3833), Performs well on bright_biology_ndcg@10 (0.3011), Performs well on bright_biology_recall@100 (0.5854), Performs well on bright_biology_index_time_ms (15247.5872), Performs well on bright_biology_query_time_ms (3231.5335), Performs well on beir_fiqa_ndcg@10 (0.2246), Performs well on beir_fiqa_recall@100 (0.5156), Performs well on beir_fiqa_index_time_ms (34280.9993), Performs well on beir_fiqa_query_time_ms (7240.5552), Performs well on bright_earth_science_ndcg@10 (0.3265), Performs well on bright_earth_science_recall@100 (0.6687), Performs well on bright_earth_science_index_time_ms (32276.5981), Performs well on bright_earth_science_query_time_ms (5271.3585), Performs well on bright_stackoverflow_ndcg@10 (0.1938), Performs well on bright_stackoverflow_recall@100 (0.4956), Performs well on bright_stackoverflow_index_time_ms (98609.4415), Performs well on bright_stackoverflow_query_time_ms (11942.4517), Performs well on beir_trec-covid_ndcg@10 (0.6517), Performs well on beir_trec-covid_recall@100 (0.1111), Performs well on beir_trec-covid_index_time_ms (134423.2385), Performs well on beir_trec-covid_query_time_ms (1747.8277), Performs well on avg_ndcg@10 (0.2827), Performs well on avg_recall@100 (0.4735), Performs well on combined_score (0.4353), Performs well on total_index_time_ms (375526.1860), Performs well on total_query_time_ms (68442.5280), Performs well on total_time_ms (443968.7140), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4353)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07  # 0 disables\n    neg_strength: float = 0.06  # 0 disables\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n    # Penalize \"over-spread\" terms where p_doc >> p_col (often boilerplate markers).\n    anti_edr_strength: float = 0.12  # 0 disables; keep small\n\n    # Aboutness prior: reward docs that allocate more *raw tf mass* to the query.\n    # Helps nDCG by demoting very long docs with a single mention.\n    focus_strength: float = 0.08  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization,\n    plus (optional) query-scope info-mass prior and tiny leaked negative evidence.\n\n    Why this helps:\n      - Positive-only surplus evidence is recall-friendly, but can over-rank \"one rare hit\" docs.\n      - A query-scope prior rewards covering more of the query's *information mass* (gentle soft-AND).\n      - Leaking a small fraction of negative LLR demotes boilerplate/weak matches without crushing recall.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish prior against verbosity (helps early precision on web/forum corpora).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Query-scope info-mass accumulators (slow path; score() isn't the bottleneck)\n    scope_num = 0.0\n    scope_den = 0.0\n\n    # Aboutness accumulator: fraction of doc mass assigned to query terms (raw tf / L).\n    focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n    focus_acc = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Prefer precomputed, renormalized collection LM when possible (faster + consistent).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Leak tiny negative evidence (optional): keep mostly surplus-evidence behavior.\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        if corpus is not None and tid is not None and hasattr(corpus, \"_anti_edr_gate\"):\n            gate *= float(corpus._anti_edr_gate[tid])\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n        if focus_s > 0.0 and doc_length > 0.0 and tf > 0.0:\n            focus_acc += qtf_raw * (tf / doc_length)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    # Aboutness prior: concave transform to keep scale stable across queries.\n    if focus_s > 0.0 and total_qw > 0.0:\n        score += focus_s * math.log1p(max(focus_acc / max(total_qw, eps), 0.0))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n\n        if self.N > 0:\n            ratio_full = np.log(\n                np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon)\n            )\n            ratio_clip = np.clip(ratio_full, -clipc, clipc)\n        else:\n            ratio_full = np.zeros(self.vocab_size, dtype=np.float64)\n            ratio_clip = ratio_full\n\n        if lam > 0.0 and self.N > 0:\n            self._edr_gate = 1.0 + lam * ratio_clip\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Anti-EDR: softly downweight only the positive side (p_doc >> p_col), bounded and monotone.\n        if anti > 0.0 and self.N > 0:\n            self._anti_edr_gate = 1.0 / (1.0 + anti * np.maximum(ratio_full, 0.0))\n        else:\n            self._anti_edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.maximum(ratio_full, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Coordination + scope accumulators\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n        focus_acc = np.zeros(len(candidate_docs), dtype=np.float64) if focus_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Leak tiny negative evidence (optional)\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id]) * float(\n                getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id]\n            )\n\n            pos = np.maximum(per_term, 0.0)\n            w = qtf * ridf_w\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if focus_acc is not None:\n                focus_acc += (qtf_raw * tf_row) / np.maximum(doc_lengths, 1.0)\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        if focus_acc is not None and focus_s > 0.0:\n            scores += focus_s * np.log1p(np.maximum(focus_acc / max(total_q, eps), 0.0))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3228), Performs well on beir_nfcorpus_recall@100 (0.2556), Performs well on beir_nfcorpus_index_time_ms (3589.6773), Performs well on beir_nfcorpus_query_time_ms (124.1974), Performs well on beir_scifact_ndcg@10 (0.6842), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4946.9265), Performs well on beir_scifact_query_time_ms (501.9903), Performs well on bright_pony_ndcg@10 (0.0825), Performs well on bright_pony_recall@100 (0.3143), Performs well on bright_pony_index_time_ms (1467.4110), Performs well on bright_pony_query_time_ms (700.0621), Performs well on beir_arguana_ndcg@10 (0.2777), Performs well on beir_arguana_recall@100 (0.9236), Performs well on beir_arguana_index_time_ms (5952.2930), Performs well on beir_arguana_query_time_ms (31319.8932), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0302), Performs well on bright_theoremqa_theorems_recall@100 (0.1513), Performs well on bright_theoremqa_theorems_index_time_ms (10567.6632), Performs well on bright_theoremqa_theorems_query_time_ms (1332.7658), Performs well on beir_scidocs_ndcg@10 (0.1461), Performs well on beir_scidocs_recall@100 (0.3495), Performs well on beir_scidocs_index_time_ms (19775.6025), Performs well on beir_scidocs_query_time_ms (4512.2918), Performs well on bright_economics_ndcg@10 (0.1514), Performs well on bright_economics_recall@100 (0.3881), Performs well on bright_economics_index_time_ms (14380.2947), Performs well on bright_economics_query_time_ms (3687.5617), Performs well on bright_biology_ndcg@10 (0.3044), Performs well on bright_biology_recall@100 (0.5854), Performs well on bright_biology_index_time_ms (15362.4323), Performs well on bright_biology_query_time_ms (3588.7763), Performs well on beir_fiqa_ndcg@10 (0.2241), Performs well on beir_fiqa_recall@100 (0.5142), Performs well on beir_fiqa_index_time_ms (32634.9129), Performs well on beir_fiqa_query_time_ms (7991.0925), Performs well on bright_earth_science_ndcg@10 (0.3263), Performs well on bright_earth_science_recall@100 (0.6682), Performs well on bright_earth_science_index_time_ms (31922.3339), Performs well on bright_earth_science_query_time_ms (5982.0736), Performs well on bright_stackoverflow_ndcg@10 (0.1937), Performs well on bright_stackoverflow_recall@100 (0.4949), Performs well on bright_stackoverflow_index_time_ms (99055.4851), Performs well on bright_stackoverflow_query_time_ms (13381.8923), Performs well on beir_trec-covid_ndcg@10 (0.6532), Performs well on beir_trec-covid_recall@100 (0.1109), Performs well on beir_trec-covid_index_time_ms (134138.2657), Performs well on beir_trec-covid_query_time_ms (2040.2857), Performs well on avg_ndcg@10 (0.2831), Performs well on avg_recall@100 (0.4734), Performs well on combined_score (0.4353), Performs well on total_index_time_ms (373793.2981), Performs well on total_query_time_ms (75162.8826), Performs well on total_time_ms (448956.1807), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4352)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n    doclen_prior_strength: float = 0.06\n\n    clarity_idf_clip: float = 6.0\n    clarity_gain: float = 0.8\n    miss_clarity_boost: float = 0.60\n    scope_clarity_boost: float = 0.60\n\n    # NEW: aboutness / topicality prior (query mass under doc LM).\n    # Helps nDCG by demoting boilerplate docs that \"mention\" a rare term once.\n    focus_strength: float = 0.08  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior: long docs tend to match many terms by chance.\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Prefer precomputed, renormalized collection LM when possible (faster + consistent).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # Query-scope prior in slow path, with the same clarity-adaptive scaling as rank().\n    scope_s0 = float(getattr(Config, \"scope_strength\", 0.0))\n    if corpus is not None and scope_s0 > 0.0:\n        idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n        gain = float(getattr(Config, \"clarity_gain\", 0.0))\n        scope_boost = float(getattr(Config, \"scope_clarity_boost\", 0.0))\n\n        # clarity in [0,1] from mean clipped IDF of query terms\n        tids = [corpus.get_term_id(t) for t in query_repr.terms]\n        tids = [t for t in tids if t is not None]\n        if tids and hasattr(corpus, \"_idf\"):\n            q_idf = [min(idf_clip, float(corpus._idf[t])) for t in tids]\n            clarity = (sum(q_idf) / float(len(q_idf))) / max(idf_clip, eps)\n        else:\n            clarity = 0.0\n\n        cfac = clarity ** max(gain, 0.0) if gain > 0.0 else clarity\n        scope_s = scope_s0 * (1.0 + scope_boost * cfac)\n\n        if scope_s > 0.0:\n            scope_num = 0.0\n            scope_den = 0.0\n            for term in query_repr.terms:\n                tid = corpus.get_term_id(term)\n                if tid is None:\n                    continue\n                info = math.log1p(float(corpus._ridf_qweight[tid]))\n                scope_den += info\n                if float(doc_tf.get(term, 0.0)) > 0.0:\n                    scope_num += info\n            if scope_den > 0.0:\n                frac = max(scope_num / scope_den, eps)\n                score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Store IDF for query-clarity modulation (cheap + query-time only).\n        if self.N > 0:\n            self._idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n        else:\n            self._idf = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: clarity-adaptive completeness.\n        # clarity in [0,1] from mean clipped IDF of query terms.\n        scope_s0 = float(getattr(Config, \"scope_strength\", 0.0))\n        miss_boost = float(getattr(Config, \"miss_clarity_boost\", 0.0))\n        scope_boost = float(getattr(Config, \"scope_clarity_boost\", 0.0))\n        idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n        gain = float(getattr(Config, \"clarity_gain\", 0.0))\n\n        if len(query_term_ids) > 0 and hasattr(self.corpus, \"_idf\"):\n            q_idf = np.minimum(self.corpus._idf[np.array(query_term_ids, dtype=np.int64)], idf_clip)\n            clarity = float(np.mean(q_idf)) / max(idf_clip, eps)\n        else:\n            clarity = 0.0\n\n        cfac = clarity ** max(gain, 0.0) if gain > 0.0 else clarity\n        miss_penalty_eff = Config.miss_penalty * (1.0 + miss_boost * cfac)\n\n        scope_s = scope_s0 * (1.0 + scope_boost * cfac)\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n        focus_acc = np.zeros(len(candidate_docs), dtype=np.float64) if focus_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(miss_penalty_eff * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            # Focus/aboutness: fraction of doc LM mass assigned to query terms.\n            # Uses raw tf (not smoothed) on purpose: penalizes \"one hit in huge doc\".\n            if focus_acc is not None:\n                focus_acc += (qtf_raw * tf_row) / np.maximum(doc_lengths, 1.0)\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0 and scope_s > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        # Focus prior: log of query-mass under doc LM (aboutness).\n        if focus_acc is not None and focus_s > 0.0:\n            # normalize by query length/mass to keep scale stable across queries\n            scores += focus_s * np.log(np.maximum(focus_acc / max(total_q, eps), eps))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4351)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1700.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    miss_penalty: float = 0.085\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n\n    collection_df_alpha: float = 0.10\n    doclen_prior_strength: float = 0.06\n\n    # NEW: anti-EDR downweighting for \u201cboilerplate spread\u201d terms where p_doc >> p_col.\n    # Many forum/web corpora have tokens that appear in *many* docs but with low tf each.\n    # These should not gate evidence upward; instead, damp them to improve nDCG@10.\n    anti_edr_strength: float = 0.12  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with soft completeness,\n    plus a mild MDL-ish document-length prior.\n\n    Why these changes can help fitness (esp. mixed BEIR + BRIGHT):\n      - Robust background LM (handled in Corpus) reduces \u201cbursty token\u201d artifacts (StackOverflow/forums).\n      - A doc-length prior reduces verbosity bias, improving nDCG@10 without crushing recall@100.\n      - Keep the surplus-evidence backbone + gentle soft-AND (miss + scope + coord).\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior (applied once).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw**alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Use robust, precomputed collection probability when available.\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid]) * float(\n                getattr(corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[tid]\n            )\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak a tiny amount of negative evidence (optional).\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        # Surplus-evidence backbone.\n        pos = max(per_term, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w) = df/N (stabilizes background under bursty tf distributions).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n\n        if self.N > 0:\n            ratio_full = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio_clip = np.clip(ratio_full, -clipc, clipc)\n        else:\n            ratio_full = np.zeros(self.vocab_size, dtype=np.float64)\n            ratio_clip = ratio_full\n\n        # Positive EDR gate (can boost discriminative terms where doc-spread > token-LM predicts).\n        if lam > 0.0 and self.N > 0:\n            self._edr_gate = 1.0 + lam * ratio_clip\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Anti-EDR gate: penalize only the positive side (p_doc >> p_col), bounded and monotone.\n        if anti > 0.0 and self.N > 0:\n            self._anti_edr_gate = 1.0 / (1.0 + anti * np.maximum(ratio_full, 0.0))\n        else:\n            self._anti_edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.maximum(ratio_full, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Leak a tiny amount of negative evidence (optional).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            # Surplus-evidence backbone.\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id]) * float(\n                getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id]\n            )\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4351)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n\n    # NEW: robust background for rare/bursty tokens:\n    # mix token-LM with a df-based \"presence LM\" (stabilizes p(w|C) for spiky corpora like StackOverflow).\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # NEW: explicit document prior to counter long-doc verbosity and improve early precision.\n    # prior(d) = -strength * log(1 + L_d/avgdl), i.e., mild MDL-ish penalty.\n    doclen_prior_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with gentle completeness,\n    plus a mild document-length prior and (optional) tiny negative-evidence leak.\n\n    Key idea: keep the strong surplus-evidence LM backbone for recall, but stabilize:\n      (1) background LM via df/token mixing (handled in Corpus._collection_prob)\n      (2) ranking of verbose/boilerplate docs via an explicit doc-length prior\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish prior against verbosity (helps early precision on web/forum corpora).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Use precomputed robust background when available (may include df/token mixing + renorm).\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n        else:\n            tid = None\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        # df-based \"document spread\" LM (used for diagnostics; distinct from token LM)\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: query-scope (information-mass) coverage accumulators.\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Optional small leak of negative evidence (prevents \"mention once everywhere\" docs).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            # NOTE: per_term already contains leaked negatives (if enabled).\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            # Query-scope prior: reward covering the query's \"information mass\".\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4353)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07  # 0 disables\n    neg_strength: float = 0.06  # 0 disables\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n    # Penalize \"over-spread\" terms where p_doc >> p_col (often boilerplate markers).\n    anti_edr_strength: float = 0.12  # 0 disables; keep small\n\n    # Aboutness prior: reward docs that allocate more *raw tf mass* to the query.\n    # Helps nDCG by demoting very long docs with a single mention.\n    focus_strength: float = 0.08  # 0 disables\n\n    # NEW: \"entropy prior\" on documents (query-independent).\n    # Intuition: verbose/boilerplate docs have high lexical entropy (many unique-ish tokens),\n    # while focused topical docs are more \"peaked\" (lower entropy). A tiny prior helps nDCG@10.\n    # We use normalized Shannon entropy H(p_d)/log|d| to be comparable across lengths.\n    entropy_prior_strength: float = 0.07  # 0 disables; keep small for recall safety\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization,\n    plus (optional) query-scope info-mass prior and tiny leaked negative evidence.\n\n    Why this helps:\n      - Positive-only surplus evidence is recall-friendly, but can over-rank \"one rare hit\" docs.\n      - A query-scope prior rewards covering more of the query's *information mass* (gentle soft-AND).\n      - Leaking a small fraction of negative LLR demotes boilerplate/weak matches without crushing recall.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish prior against verbosity (helps early precision on web/forum corpora).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Query-scope info-mass accumulators (slow path; score() isn't the bottleneck)\n    scope_num = 0.0\n    scope_den = 0.0\n\n    # Aboutness accumulator: fraction of doc mass assigned to query terms (raw tf / L).\n    focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n    focus_acc = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Prefer precomputed, renormalized collection LM when possible (faster + consistent).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Leak tiny negative evidence (optional): keep mostly surplus-evidence behavior.\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        if corpus is not None and tid is not None and hasattr(corpus, \"_anti_edr_gate\"):\n            gate *= float(corpus._anti_edr_gate[tid])\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n        if focus_s > 0.0 and doc_length > 0.0 and tf > 0.0:\n            focus_acc += qtf_raw * (tf / doc_length)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    # Aboutness prior: concave transform to keep scale stable across queries.\n    if focus_s > 0.0 and total_qw > 0.0:\n        score += focus_s * math.log1p(max(focus_acc / max(total_qw, eps), 0.0))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n\n        if self.N > 0:\n            ratio_full = np.log(\n                np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon)\n            )\n            ratio_clip = np.clip(ratio_full, -clipc, clipc)\n        else:\n            ratio_full = np.zeros(self.vocab_size, dtype=np.float64)\n            ratio_clip = ratio_full\n\n        if lam > 0.0 and self.N > 0:\n            self._edr_gate = 1.0 + lam * ratio_clip\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Anti-EDR: softly downweight only the positive side (p_doc >> p_col), bounded and monotone.\n        if anti > 0.0 and self.N > 0:\n            self._anti_edr_gate = 1.0 / (1.0 + anti * np.maximum(ratio_full, 0.0))\n        else:\n            self._anti_edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.maximum(ratio_full, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # --- Query-independent doc prior: normalized lexical entropy (focus vs boilerplate) ---\n        es = float(getattr(Config, \"entropy_prior_strength\", 0.0))\n        if es > 0.0 and self.N > 0:\n            ent = np.zeros(self.N, dtype=np.float64)\n            for i, tfc in enumerate(self._doc_tf_dicts):\n                L = float(self.doc_lengths[i])\n                if L <= 1.0:\n                    ent[i] = 0.0\n                    continue\n                # H(p) = -sum p log p, with p=tf/L\n                H = 0.0\n                invL = 1.0 / L\n                for c in tfc.values():\n                    p = float(c) * invL\n                    H -= p * math.log(max(p, Config.epsilon))\n                # Normalize by log(L) so the scale is roughly [0,1] across lengths.\n                ent[i] = H / max(math.log(L), Config.epsilon)\n            # Lower entropy => more focused; so add -es * entropy.\n            self._entropy_prior = -es * ent\n        else:\n            self._entropy_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Coordination + scope accumulators\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        focus_s = float(getattr(Config, \"focus_strength\", 0.0))\n        focus_acc = np.zeros(len(candidate_docs), dtype=np.float64) if focus_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Leak tiny negative evidence (optional)\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id]) * float(\n                getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id]\n            )\n\n            pos = np.maximum(per_term, 0.0)\n            w = qtf * ridf_w\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if focus_acc is not None:\n                focus_acc += (qtf_raw * tf_row) / np.maximum(doc_lengths, 1.0)\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        if focus_acc is not None and focus_s > 0.0:\n            scores += focus_s * np.log1p(np.maximum(focus_acc / max(total_q, eps), 0.0))\n\n        # Add query-independent doc entropy prior (computed in Corpus).\n        if hasattr(self.corpus, \"_entropy_prior\"):\n            scores += self.corpus._entropy_prior[candidate_docs]\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4336, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n    and_strength: float = 0.14  # 0 disables\n    and_saturation: float = 3.0\n\n    # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n    missing_strength: float = 0.07  # keep small to protect recall@100\n\n    # TF burstiness normalization (per-term exponent) to reduce domination by very common terms.\n    burstiness_strength: float = 0.30  # 0 disables; keep modest for recall\n\n    # Leak a small fraction of negative evidence for weak hits.\n    neg_strength: float = 0.12  # 0 disables\n\n    # Mix token-LM background with a df-based \"presence LM\" p_df(w)=df/N.\n    # Intuition: token LM can be dominated by a few extremely long/bursty docs (forums/code),\n    # while df/N is stabler. Mixing improves robustness without learning.\n    collection_df_alpha: float = 0.10  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        base = math.log(numerator / denominator + eps)\n        base = max(base, 0.0)\n\n        # Fallback term diagnostic: use a capped surprisal of the collection LM.\n        # (True residual-IDF uses df/N; we do that in vectorized rank()).\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n\n        # Approx residual-IDF in slow path: reuse spec as a proxy.\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM: tempered token-LM, optionally mixed with a df-based \"presence LM\",\n        # plus an optional tiny uniform mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM p_df(w)=df/N (stable under bursty tf distributions).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n        # Common terms saturate more: tf -> tf^beta(w).\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior: mild MDL-ish anti-verbosity prior\n        # prior(d) = -s * log(1 + L/avgdl). This is cheaper than a log-normal and tends to be\n        # more robust across corpora with extreme length variance (web/forum/code).\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            self._length_prior = -s * np.log(1.0 + self.doc_lengths / max(float(self.avgdl), 1.0) + Config.epsilon)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            # TF burstiness saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Apply EDR gate (query-independent)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            # Leak a small fraction of negative evidence for weak hits (optional).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            # Evidence added to score: prefer \"surplus evidence\" (positive LLR) for stability.\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += contrib\n\n            # Missing-term anti-evidence (scaled tf=0 Dirichlet contribution) within candidates.\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += miss_s * w * miss * base0\n\n            # Soft-AND: saturating coverage reward (encourages matching more query terms)\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4256, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n    burstiness_strength: float = 0.35\n\n    # Two-stage background (collection + uniform over vocab).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND query aggregation.\n    and_strength: float = 0.15  # 0 disables\n    and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n\n    # Dirichlet \"anti-evidence\" for missing query terms (tf=0).\n    # Keep slightly smaller by default: too much anti-evidence can hurt recall@100.\n    missing_strength: float = 0.06\n\n    # Query-scope prior: reward explaining a larger fraction of the query's information mass.\n    query_scope_strength: float = 0.07  # 0 disables\n\n    # Query-clarity modulation (dataset-robust AND-ness control).\n    clarity_idf_clip: float = 6.0\n    clarity_missing_gain: float = 0.6  # missing_strength *= (1 + gain * clarity)\n    clarity_and_gain: float = 0.4      # and_strength   *= (1 + gain * clarity)\n\n    # NEW: Query-conditioned prior that rewards documents whose \"lexical specificity\"\n    # matches the query's specificity. This addresses a common QL failure mode:\n    # very specific queries should prefer focused docs; vague queries shouldn't over-favor them.\n    # Implemented as a small quadratic penalty on (avg_surprisal_d - avg_surprisal_q).\n    spec_agreement_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n    scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        if burst_s > 0.0:\n            idf01_proxy = 1.0 / (1.0 + max(0.0, math.log(1.0 / max(p_collection, eps))))\n            beta = 1.0 - burst_s * (1.0 - idf01_proxy)\n            term_count = term_count ** beta if beta != 1.0 else term_count\n\n        base = math.log((1.0 + term_count / (mu * p_collection + eps)) / ((doc_length + mu) / mu) + eps)\n        base = max(base, 0.0)\n\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n        if scope_s > 0.0:\n            info = math.log1p(1.0 + ridf_s * (spec / max(clipc, eps)))\n            scope_den += info\n            if term_count > 0.0:\n                scope_num += info\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) for Dirichlet smoothing\n        # plus a small uniform background mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V)\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob[:] = (1.0 - gamma) * p_col + gamma * p_uni\n            self._collection_prob[:] = np.maximum(self._collection_prob, Config.epsilon)\n        else:\n            self._collection_prob[:] = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # IDF (kept) + IDF-in-[0,1] for burstiness exponent beta(w).\n        # We also keep raw idf to compute query-clarity at rank()-time (cheap + robust).\n        if self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n        else:\n            idf = np.zeros(self.vocab_size, dtype=np.float64)\n            idf01 = np.zeros(self.vocab_size, dtype=np.float64)\n\n        self._idf = idf\n\n        if burst_s > 0.0:\n            # beta in [1-burst_s, 1]; common terms (low idf01) -> smaller beta -> more saturation\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        # Precompute per-document \"specificity\" proxy:\n        # avg_surprisal_d = E_{w~p(w|d)}[-log p_col(w)].\n        # This is a query-independent doc statistic; later we compare it to the query's avg surprisal.\n        self._doc_avg_surprisal = np.zeros(self.N, dtype=np.float64)\n        log_pcol = np.log(np.maximum(self._collection_prob, Config.epsilon))\n        for di, tfc in enumerate(self._doc_tf_dicts):\n            L = float(self.doc_lengths[di])\n            if L <= 0:\n                self._doc_avg_surprisal[di] = 0.0\n                continue\n            acc = 0.0\n            for term, c in tfc.items():\n                tid = self._vocab.get(term)\n                if tid is None:\n                    continue\n                acc += float(c) * (-float(log_pcol[tid]))\n            self._doc_avg_surprisal[di] = acc / max(L, 1.0)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n        spec_agree_s = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # --- Query clarity (mean capped IDF) to modulate how AND-like we behave ---\n        idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n        q_idf = np.array([min(idf_clip, float(self.corpus._idf[tid])) for tid in query_term_ids], dtype=np.float64)\n        q_clarity = float(np.mean(q_idf)) / max(idf_clip, eps) if len(q_idf) > 0 else 0.0  # in [0,1]\n\n        miss_gain = float(getattr(Config, \"clarity_missing_gain\", 0.0))\n        and_gain = float(getattr(Config, \"clarity_and_gain\", 0.0))\n        miss_s_eff = miss_s * (1.0 + miss_gain * q_clarity)\n        and_strength_eff = and_strength * (1.0 + and_gain * q_clarity)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength_eff > 0.0 else None\n\n        # Query-scope accumulators: per-doc matched specificity mass vs total query specificity mass.\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Query avg surprisal proxy: avg over query terms of (-log p_col(w)).\n        # This is computed on-the-fly and compared to per-doc avg surprisal (precomputed in Corpus).\n        if spec_agree_s > 0.0 and len(query_term_ids) > 0:\n            q_avg_surprisal = float(np.mean(-np.log(np.maximum(self.corpus._collection_prob[query_term_ids], eps))))\n        else:\n            q_avg_surprisal = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += contrib\n\n            # Missing-term anti-evidence, clarity-modulated (generic queries => gentler).\n            if miss_s_eff > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                    scores += miss_s_eff * w * miss * base0\n\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n            if scope_num is not None:\n                info = float(np.log1p(self.corpus._ridf_qweight[term_id]))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength_eff * (and_acc / float(len(query_term_ids)))\n\n        if scope_num is not None:\n            denom = max(scope_den, eps)\n            scope_frac = np.maximum(scope_num / denom, eps)\n            scores += scope_s * np.log(scope_frac)\n\n        # Query-conditioned specificity\u2013agreement prior (mild, applied to candidates only).\n        if spec_agree_s > 0.0 and len(query_term_ids) > 0:\n            d_avg = self.corpus._doc_avg_surprisal[candidate_docs]\n            # Quadratic penalty centered at query specificity; keeps sign consistent and stable.\n            scores += -spec_agree_s * np.square(d_avg - q_avg_surprisal)\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4249, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n    burstiness_strength: float = 0.35\n\n    # Two-stage background (collection + uniform over vocab).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND query aggregation.\n    and_strength: float = 0.15  # 0 disables\n    and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n\n    # Dirichlet \"anti-evidence\" for missing query terms (tf=0).\n    # Keep slightly smaller by default: too much anti-evidence can hurt recall@100.\n    missing_strength: float = 0.06\n\n    # Query-scope prior: reward explaining a larger fraction of the query's information mass.\n    query_scope_strength: float = 0.07  # 0 disables\n\n    # Risk-sensitive aggregation (R\u00e9nyi / power mean) over per-term evidence.\n    risk_strength: float = 0.12  # 0 disables\n    risk_power: float = 0.55\n    risk_saturation: float = 6.0\n\n    # NEW: Query IDF-clarity regularizer.\n    # Idea: treat the query as a \"description\" of a latent topic; queries with many generic terms\n    # should behave more OR-like (to preserve recall), while highly specific queries can be more AND-like.\n    # We approximate query \"clarity\" by mean capped IDF over query terms, and use it to modulate:\n    #  - missing-term anti-evidence\n    #  - soft-AND reward\n    # This makes the model less brittle across datasets with different query styles.\n    clarity_idf_clip: float = 6.0\n    clarity_missing_gain: float = 0.55  # multiplies missing_strength by (1 + gain * clarity)\n    clarity_and_gain: float = 0.40      # multiplies and_strength by (1 + gain * clarity)\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n    scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        if burst_s > 0.0:\n            idf01_proxy = 1.0 / (1.0 + max(0.0, math.log(1.0 / max(p_collection, eps))))\n            beta = 1.0 - burst_s * (1.0 - idf01_proxy)\n            term_count = term_count ** beta if beta != 1.0 else term_count\n\n        base = math.log((1.0 + term_count / (mu * p_collection + eps)) / ((doc_length + mu) / mu) + eps)\n        base = max(base, 0.0)\n\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n        if scope_s > 0.0:\n            info = math.log1p(1.0 + ridf_s * (spec / max(clipc, eps)))\n            scope_den += info\n            if term_count > 0.0:\n                scope_num += info\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) for Dirichlet smoothing\n        # plus a small uniform background mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V)\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob[:] = (1.0 - gamma) * p_col + gamma * p_uni\n            self._collection_prob[:] = np.maximum(self._collection_prob, Config.epsilon)\n        else:\n            self._collection_prob[:] = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: IDF-in-[0,1] and burstiness exponent beta(w) for tf saturation (common terms saturate more).\n        # idf = log((N+1)/(df+1)) is classic; we normalize by max idf to get [0,1] without tuning per dataset.\n        if self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n        else:\n            idf01 = np.zeros(self.vocab_size, dtype=np.float64)\n            idf = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep IDF for query-clarity modulation and for any future priors.\n        self._idf = idf\n\n        if burst_s > 0.0:\n            # beta in [1-burst_s, 1]; common terms (low idf01) -> smaller beta -> more saturation\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query-scope accumulators: per-doc matched specificity mass vs total query specificity mass.\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Query clarity (mean capped IDF over query terms). Used to modulate \"AND-ness\" safely:\n        # generic queries -> less missing-term penalty / less AND reward; specific queries -> more.\n        idf_clip = float(getattr(Config, \"clarity_idf_clip\", 6.0))\n        q_idf = np.array(\n            [min(idf_clip, float(self.corpus._idf[tid])) for tid in query_term_ids],\n            dtype=np.float64,\n        )\n        q_clarity = float(np.mean(q_idf)) / max(idf_clip, eps) if len(q_idf) > 0 else 0.0  # in [0,1]\n\n        miss_gain = float(getattr(Config, \"clarity_missing_gain\", 0.0))\n        and_gain = float(getattr(Config, \"clarity_and_gain\", 0.0))\n        miss_s_eff = miss_s * (1.0 + miss_gain * q_clarity)\n        and_strength_eff = and_strength * (1.0 + and_gain * q_clarity)\n\n        # Optional risk-sensitive aggregator accumulator\n        risk_s = float(getattr(Config, \"risk_strength\", 0.0))\n        risk_p = float(getattr(Config, \"risk_power\", 0.55))\n        risk_sat = float(getattr(Config, \"risk_saturation\", 6.0))\n        risk_acc = np.zeros(len(candidate_docs), dtype=np.float64) if risk_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += contrib\n\n            if miss_s_eff > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                    scores += miss_s_eff * w * miss * base0\n\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n            if scope_num is not None:\n                # Stable per-term \"information mass\" proxy derived from residual IDF weights.\n                info = float(np.log1p(self.corpus._ridf_qweight[term_id]))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if risk_acc is not None:\n                # Concave utility of evidence; then power-mean across terms.\n                # u(x)=log(1+x/sat) is information-like (diminishing returns).\n                u = np.log1p(contrib / max(risk_sat, eps))\n                risk_acc += np.power(np.maximum(u, 0.0), risk_p)\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength_eff * (and_acc / float(len(query_term_ids)))\n\n        if scope_num is not None:\n            denom = max(scope_den, eps)\n            scope_frac = np.maximum(scope_num / denom, eps)\n            scores += scope_s * np.log(scope_frac)\n\n        if risk_acc is not None and len(query_term_ids) > 0:\n            # Turn accumulator into a normalized power mean (scale-stable across query length).\n            pm = np.power(risk_acc / float(len(query_term_ids)), 1.0 / max(risk_p, eps))\n            scores += risk_s * pm\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4273, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n    and_strength: float = 0.14  # 0 disables\n    and_saturation: float = 3.0\n\n    # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n    missing_strength: float = 0.07  # keep small to protect recall@100\n\n    # NEW: TF burstiness normalization (per-term exponent) to reduce domination by very common terms.\n    # Probabilistic story: approximate a sublinear TF model (mixture of \"aboutness\" + repetition noise).\n    # Common terms (low idf) get stronger saturation.\n    burstiness_strength: float = 0.30  # 0 disables; keep modest for recall\n\n    # NEW: Add back a tiny amount of negative evidence for \"weak hits\":\n    # when a query term appears but below its expectation under the doc+prior mixture,\n    # we keep a small fraction of the negative log-likelihood ratio.\n    # This reduces ranking of boilerplate/long docs that mention many terms once.\n    neg_strength: float = 0.12  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        base = math.log(numerator / denominator + eps)\n        base = max(base, 0.0)\n\n        # Fallback term diagnostic: use a capped surprisal of the collection LM.\n        # (True residual-IDF uses df/N; we do that in vectorized rank()).\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n\n        # Approx residual-IDF in slow path: reuse spec as a proxy.\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with an optional uniform mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n        # Common terms saturate more: tf -> tf^beta(w).\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            # TF burstiness saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Apply EDR gate (query-independent)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            # Keep most of the classic \"only reward positive LLR\", but leak a small fraction\n            # of negative evidence for weak hits to improve early precision.\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += w * per_term  # includes leaked negatives if enabled\n\n            # Missing-term anti-evidence (scaled tf=0 Dirichlet contribution) within candidates.\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += miss_s * w * miss * base0\n\n            # Soft-AND: saturating coverage reward (encourages matching more query terms)\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n    doclen_prior_strength: float = 0.06\n\n    # Term TF saturation (kept, but computed from both surprisal and df-stability; see Corpus)\n    tf_saturation_strength: float = 0.22\n    min_tf_beta: float = 0.65\n\n    # NEW: anti-EDR gate (penalize corpus-wide boilerplate markers where p_doc >> p_col)\n    anti_edr_strength: float = 0.12  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior: long docs tend to match many terms by chance.\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Prefer precomputed, renormalized collection LM when possible (faster + consistent).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        # Apply the same per-term TF saturation as the vectorized rank() path.\n        if corpus is not None and tid is not None and hasattr(corpus, \"_tf_beta\"):\n            beta = float(corpus._tf_beta[tid])\n            tf_eff = tf ** beta if beta != 1.0 else tf\n        else:\n            tf_eff = tf\n        numerator = 1.0 + tf_eff / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None and tid is not None:\n            gate = float(corpus._edr_gate[tid]) * float(getattr(corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            gate, ridf_w = 1.0, 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if self.N > 0:\n            ratio_full = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio_clip = np.clip(ratio_full, -clipc, clipc)\n        else:\n            ratio_full = np.zeros(self.vocab_size, dtype=np.float64)\n            ratio_clip = ratio_full\n\n        if lam > 0.0 and self.N > 0:\n            self._edr_gate = 1.0 + lam * ratio_clip\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Anti-EDR: if p_doc >> p_col, term is \"too spread\" (often boilerplate); downweight gently.\n        anti = float(getattr(Config, \"anti_edr_strength\", 0.0))\n        if anti > 0.0 and self.N > 0:\n            self._anti_edr_gate = 1.0 / (1.0 + anti * np.maximum(ratio_full, 0.0))\n        else:\n            self._anti_edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.maximum(ratio_full, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Per-term TF saturation: interpolate between surprisal-based and df-stability-based signals.\n        # Intuition: in bursty corpora, p_col may be distorted by a few long docs; df/N is stabler.\n        sat_s = float(getattr(Config, \"tf_saturation_strength\", 0.0))\n        minb = float(getattr(Config, \"min_tf_beta\", 0.60))\n        if sat_s > 0.0:\n            surpr = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n            surpr01 = surpr / max(float(np.max(surpr)), Config.epsilon)\n            # df-based idf01 proxy\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            mix = 0.5  # fixed: half surprisal, half df-stability (keeps code size small)\n            sat01 = mix * surpr01 + (1.0 - mix) * idf01\n            beta = 1.0 - sat_s * (1.0 - sat01)\n            self._tf_beta = np.clip(beta, minb, 1.0)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: query-scope (information-mass) coverage accumulators.\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            # Apply per-term TF saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = float(getattr(self.corpus, \"_tf_beta\", np.ones(1, dtype=np.float64))[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n            numerator = 1.0 + tf_eff / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Optional small leak of negative evidence (prevents \"mention once everywhere\" docs).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + (EDR * anti-EDR) gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id]) * float(getattr(self.corpus, \"_anti_edr_gate\", np.ones(1, dtype=np.float64))[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            # NOTE: per_term already contains leaked negatives (if enabled).\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            # Query-scope prior: reward covering the query's \"information mass\".\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Below are **targeted** changes aimed at recovering the fitness drop and improving robustness/precision without exploding code size.\n\nMain idea: the recent decline likely comes from the new **TF saturation** being too aggressive/unreliable across datasets (it changes the core evidence term). We\u2019ll (1) **disable TF saturation by default** (easy win to revert), and (2) add a **query-conditioned specificity\u2013agreement prior** that improves early precision (nDCG@10) for specific queries without harming recall@100 much, because it\u2019s small, bounded, and only depends on precomputed doc stats.\n\n---\n\n### Change 1 \u2014 Simplify / revert the risky TF-saturation default\nThis should recover the previous strong baseline behavior and reduce overfitting to bursty corpora.\n\n<<<<<<< SEARCH\n    # Term TF saturation (kept, but computed from both surprisal and df-stability; see Corpus)\n    tf_saturation_strength: float = 0.22\n    min_tf_beta: float = 0.65\n=======\n    # Term TF saturation.\n    # This can help on bursty/forum corpora, but it can also hurt recall on scientific/news corpora.\n    # Default to 0 (off) for robustness; enable during evolution if beneficial.\n    tf_saturation_strength: float = 0.0\n    min_tf_beta: float = 0.65\n>>>>>>> REPLACE\n\n---\n\n### Change 2 \u2014 Add a query-conditioned \u201cspecificity agreement\u201d prior (novel, bounded)\n**Motivation (probabilistic/story):**\nA query has an implicit *lexical specificity* (average surprisal under the collection LM). Documents also have a specificity proxy (expected surprisal of their tokens). For very specific queries, we want to prefer documents with similarly specific vocabulary (focused/topical), and avoid generic boilerplate that happens to mention a rare term once.\n\nThis is not just \u201cIDF boost\u201d: it\u2019s a **prior over documents** conditioned on the query\u2019s information content, implemented as a small quadratic penalty:\n\\[\n- \\lambda (s(d) - s(q))^2\n\\]\nThis tends to help **nDCG@10** especially on web/forum mixtures, while being gentle enough to preserve recall.\n\n#### 2a) Add config parameter\n<<<<<<< SEARCH\n    # NEW: anti-EDR gate (penalize corpus-wide boilerplate markers where p_doc >> p_col)\n    anti_edr_strength: float = 0.12  # 0 disables\n=======\n    # NEW: anti-EDR gate (penalize corpus-wide boilerplate markers where p_doc >> p_col)\n    anti_edr_strength: float = 0.12  # 0 disables\n\n    # Query-conditioned specificity agreement prior.\n    # Penalize docs whose average token surprisal under p_col differs too much from the query's.\n    # Helps early precision for specific queries; keep small to avoid recall loss.\n    spec_agreement_strength: float = 0.05  # 0 disables\n>>>>>>> REPLACE\n\n#### 2b) Precompute per-document average surprisal in Corpus (cheap, query-independent)\nAdd after `_tf_beta` computation block (right before posting lists are finalized).\n\n<<<<<<< SEARCH\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n=======\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # --- Query-independent doc specificity proxy ---\n        # avg_surprisal_d = E_{w~doc}[-log p_col(w)] (computed on raw tf distribution).\n        # Used for a small query-conditioned prior that prefers specificity-matched docs.\n        log_pcol = np.log(np.maximum(self._collection_prob, Config.epsilon))\n        self._doc_avg_surprisal = np.zeros(self.N, dtype=np.float64)\n        for di, tfc in enumerate(self._doc_tf_dicts):\n            L = float(self.doc_lengths[di])\n            if L <= 0.0:\n                continue\n            acc = 0.0\n            for term, c in tfc.items():\n                tid = self._vocab.get(term)\n                if tid is None:\n                    continue\n                acc += float(c) * (-float(log_pcol[tid]))\n            self._doc_avg_surprisal[di] = acc / max(L, 1.0)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n>>>>>>> REPLACE\n\n#### 2c) Apply the prior in vectorized ranking (fast path)\nWe compute query avg surprisal from the same `p_col` and add the quadratic penalty to candidate scores.\n\n<<<<<<< SEARCH\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n=======\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        # Query-conditioned specificity agreement prior (bounded, candidate-only).\n        s_ag = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n        if s_ag > 0.0 and len(query_term_ids) > 0 and hasattr(self.corpus, \"_doc_avg_surprisal\"):\n            q_avg = float(\n                np.mean(-np.log(np.maximum(self.corpus._collection_prob[np.array(query_term_ids, dtype=np.int64)], eps)))\n            )\n            d_avg = self.corpus._doc_avg_surprisal[candidate_docs]\n            scores += -s_ag * np.square(d_avg - q_avg)\n\n        return scores\n>>>>>>> REPLACE\n\n#### 2d) Apply the same prior in retrieval_score() (slow path consistency)\nThis keeps `QL.score()` aligned with `rank()`.\n\n<<<<<<< SEARCH\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    return score\n=======\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    # Query-conditioned specificity agreement prior (slow path).\n    s_ag = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n    if s_ag > 0.0 and corpus is not None and hasattr(corpus, \"_doc_avg_surprisal\"):\n        tids = [corpus.get_term_id(t) for t in query_repr.terms]\n        tids = [t for t in tids if t is not None]\n        if tids:\n            q_avg = float(np.mean(-np.log(np.maximum(corpus._collection_prob[np.array(tids, dtype=np.int64)], eps))))\n            score += -s_ag * (float(corpus._doc_avg_surprisal[doc_tf is None and 0 or 0]) if False else 0.0)\n            # (the above line is intentionally dead-code-free; compute per-doc below)\n            # doc specificity:\n            # We don't have doc_idx here; approximate from doc_tf + p_col:\n            acc = 0.0\n            for term, c in doc_tf.items():\n                tid = corpus.get_term_id(term)\n                if tid is None:\n                    continue\n                acc += float(c) * (-math.log(max(float(corpus._collection_prob[tid]), eps)))\n            d_avg = acc / max(doc_length, 1.0)\n            score += -s_ag * (d_avg - q_avg) ** 2\n\n    return score\n>>>>>>> REPLACE\n\nNote: the dead-code line is ugly; let\u2019s avoid that by doing a cleaner replace (below) that exactly matches without introducing any weirdness. Use this cleaner diff instead (replaces the messy insertion entirely):\n\n**Use this instead of 2d above:**\n\n<<<<<<< SEARCH\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    return score\n=======\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    # Query-conditioned specificity agreement prior (slow path; approximate doc specificity from doc_tf).\n    s_ag = float(getattr(Config, \"spec_agreement_strength\", 0.0))\n    if s_ag > 0.0 and corpus is not None:\n        tids = [corpus.get_term_id(t) for t in query_repr.terms]\n        tids = [t for t in tids if t is not None]\n        if tids:\n            q_avg = float(np.mean(-np.log(np.maximum(corpus._collection_prob[np.array(tids, dtype=np.int64)], eps))))\n            acc = 0.0\n            for t, c in doc_tf.items():\n                tid = corpus.get_term_id(t)\n                if tid is None:\n                    continue\n                acc += float(c) * (-math.log(max(float(corpus._collection_prob[tid]), eps)))\n            d_avg = acc / max(doc_length, 1.0)\n            score += -s_ag * (d_avg - q_avg) ** 2\n\n    return score\n>>>>>>> REPLACE\n\n---\n\n## Why these should help fitness\n\n- **Turning off TF saturation by default** likely restores the previous best-performing regime (your best programs didn\u2019t rely on tf^beta in the core term), boosting **recall@100 stability**.\n- The **specificity-agreement prior** is a *different axis* than your current EDR/ridf/miss penalties: it acts as a **query-conditioned document prior**, improving **nDCG@10** by demoting generic/boilerplate candidates that match by chance, especially on web/forum/math corpora.\n\nIf you want one more micro-tweak after this: slightly reduce `neg_strength` (e.g., 0.04) to protect recall on datasets where negative leaks can suppress relevant docs; but I\u2019d test the two changes above first."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}