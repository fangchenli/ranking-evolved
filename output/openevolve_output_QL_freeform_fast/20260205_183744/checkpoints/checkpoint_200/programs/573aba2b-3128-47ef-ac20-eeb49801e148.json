{"id": "573aba2b-3128-47ef-ac20-eeb49801e148", "code": "\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength (rare terms trust doc evidence sooner)\n    mu_df_power: float = 0.30\n\n    # Robust background LM\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # Query TF saturation (qtf^alpha)\n    query_tf_power: float = 0.60\n\n    # Query-independent diagnostics\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Soft-AND (gentle): missing-term penalty + coordination reward + scope prior\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    query_scope_strength: float = 0.07  # 0 disables\n\n    # Optional tiny negative-evidence leak (keep recall safe by being tiny)\n    neg_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Multinomial query view:\n        - unique terms as support\n        - query term frequency (qtf) as term weight\n        This makes score() consistent with rank() (which uses Counter(query)).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with gentle completeness.\n\n    Backbone: LMDirichlet log Bayes factor vs tempered background, clamped to\n    positive surplus evidence (recall-friendly). Then add:\n      - residual-IDF query weight (query-independent, precomputed in Corpus)\n      - EDR gate (query-independent, precomputed in Corpus)\n      - small missing-term penalty (soft AND), length-normalized\n      - bounded coordination reward\n      - optional query-scope \"information mass\" coverage prior\n      - optional tiny negative-evidence leak (kept very small)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Missing-term evidence is less diagnostic in long documents.\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** float(getattr(Config, \"coverage_len_power\", 0.0))\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = 1.0\n            gate = 1.0\n            ridf_w = 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak tiny negative evidence (optional).\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        # Surplus evidence backbone.\n        pos = max(per_term, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) for Dirichlet smoothing\n        # plus a small uniform background mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V)\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob[:] = (1.0 - gamma) * p_col + gamma * p_uni\n            self._collection_prob[:] = np.maximum(self._collection_prob, Config.epsilon)\n        else:\n            self._collection_prob[:] = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Keep placeholders (some evaluators/variants expect these attributes).\n        self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n        self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "3bfae170-ea12-4548-815c-820bc9835598", "generation": 9, "timestamp": 1770372212.971327, "iteration_found": 106, "metrics": {"beir_nfcorpus_ndcg@10": 0.3148079992801842, "beir_nfcorpus_recall@100": 0.2539381021938455, "beir_nfcorpus_index_time_ms": 3615.453500067815, "beir_nfcorpus_query_time_ms": 116.51937500573695, "beir_scifact_ndcg@10": 0.675524201853883, "beir_scifact_recall@100": 0.9182222222222223, "beir_scifact_index_time_ms": 4871.267374954186, "beir_scifact_query_time_ms": 481.8784160306677, "bright_pony_ndcg@10": 0.10060348063268999, "bright_pony_recall@100": 0.3357341936963035, "bright_pony_index_time_ms": 1471.8322079861537, "bright_pony_query_time_ms": 637.2485830215737, "beir_arguana_ndcg@10": 0.2730289262552304, "beir_arguana_recall@100": 0.9107780157030693, "beir_arguana_index_time_ms": 5925.649249926209, "beir_arguana_query_time_ms": 29503.391374950297, "bright_theoremqa_theorems_ndcg@10": 0.025975991990398482, "bright_theoremqa_theorems_recall@100": 0.16228070175438594, "bright_theoremqa_theorems_index_time_ms": 10677.27612494491, "bright_theoremqa_theorems_query_time_ms": 1298.1218330096453, "beir_scidocs_ndcg@10": 0.146526295134455, "beir_scidocs_recall@100": 0.3448333333333334, "beir_scidocs_index_time_ms": 19684.18995803222, "beir_scidocs_query_time_ms": 4202.109417063184, "bright_economics_ndcg@10": 0.14268912408955492, "bright_economics_recall@100": 0.36724238582826746, "bright_economics_index_time_ms": 14266.847832943313, "bright_economics_query_time_ms": 3427.0650419639423, "bright_biology_ndcg@10": 0.2980923467818977, "bright_biology_recall@100": 0.5918539814809616, "bright_biology_index_time_ms": 15044.79554190766, "bright_biology_query_time_ms": 3239.9643750395626, "beir_fiqa_ndcg@10": 0.21832079026430662, "beir_fiqa_recall@100": 0.514353256482886, "beir_fiqa_index_time_ms": 33333.94812501501, "beir_fiqa_query_time_ms": 7392.966000013985, "bright_earth_science_ndcg@10": 0.307350334409573, "bright_earth_science_recall@100": 0.6646287954080605, "bright_earth_science_index_time_ms": 31719.95599998627, "bright_earth_science_query_time_ms": 5602.10433404427, "bright_stackoverflow_ndcg@10": 0.18608476454638692, "bright_stackoverflow_recall@100": 0.4358437538107704, "bright_stackoverflow_index_time_ms": 98649.58258299157, "bright_stackoverflow_query_time_ms": 12608.744374942034, "beir_trec-covid_ndcg@10": 0.6157483966740748, "beir_trec-covid_recall@100": 0.10557083539416316, "beir_trec-covid_index_time_ms": 132950.14099997934, "beir_trec-covid_query_time_ms": 1878.6755000473931, "avg_ndcg@10": 0.2753960543260529, "avg_recall@100": 0.46710663144235576, "combined_score": 0.42876451601909527, "total_index_time_ms": 372210.93949873466, "total_query_time_ms": 70388.78862513229, "total_time_ms": 442599.72812386695, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Tempered background LM p_t(w) \u221d p(w)^tau\n      collection_temper: float = 0.85\n  \n      # Query term burstiness saturation (qtf^alpha)\n      query_tf_power: float = 0.6\n  \n      # Document length prior (log-normal-ish); keep small to avoid recall loss\n      length_prior_strength: float = 0.06\n  \n      # EDR gate: token-vs-document spread mismatch\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Residual-IDF query weighting (df/N vs token LM)\n      residual_idf_strength: float = 0.9\n  \n      # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n      burstiness_strength: float = 0.35\n  \n      # Two-stage background (collection + uniform over vocab).\n      uniform_bg_mass: float = 0.03  # 0 disables\n  \n      # Soft-AND query aggregation.\n      and_strength: float = 0.15  # 0 disables\n      and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n  \n  ... (13 more lines)\nwith:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength (rare terms trust doc evidence sooner)\n      mu_df_power: float = 0.30\n  \n      # Robust background LM\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n  \n      # Query TF saturation (qtf^alpha)\n      query_tf_power: float = 0.60\n  \n      # Query-independent diagnostics\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      # Soft-AND (gentle): missing-term penalty + coordination reward + scope prior\n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n      query_scope_strength: float = 0.07  # 0 disables\n  \n      # Optional tiny negative-evidence leak (keep recall safe by being tiny)\n      neg_strength: float = 0.06  # 0 disables\nChange 2: Replace:\n  def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"\n          Keep representation simple, but normalize very long queries by soft-booleaning:\n          repeated terms are handled later via qtf^alpha; here we just keep tokens.\n          \"\"\"\n          return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\nwith:\n  def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"\n          Multinomial query view:\n          - unique terms as support\n          - query term frequency (qtf) as term weight\n          This makes score() consistent with rank() (which uses Counter(query)).\n          \"\"\"\n          tc = Counter(tokens)\n          return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\nChange 3: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n  ) -> float:\n      \"\"\"\n      Dirichlet QL with two *information-diagnostic* modifiers:\n  \n      1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n      2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n         yet not widely spread across documents.\n  \n      The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n      In the vectorized path we can compute p_doc exactly and apply it per term id.\n      \"\"\"\n      mu, eps = Config.mu, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      lam = float(getattr(Config, \"edr_strength\", 0.0))\n      clipc = float(getattr(Config, \"edr_clip\", 3.0))\n      ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n  \n      score = 0.0\n      qtf = Counter(query_repr.terms)\n  \n      burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n  \n      scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n      scope_num = 0.0\n  ... (33 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR-gated LMDirichlet with gentle completeness.\n  \n      Backbone: LMDirichlet log Bayes factor vs tempered background, clamped to\n      positive surplus evidence (recall-friendly). Then add:\n        - residual-IDF query weight (query-independent, precomputed in Corpus)\n        - EDR gate (query-independent, precomputed in Corpus)\n        - small missing-term penalty (soft AND), length-normalized\n        - bounded coordination reward\n        - optional query-scope \"information mass\" coverage prior\n        - optional tiny negative-evidence leak (kept very small)\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n  ... (67 more lines)\nChange 4: 'return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)' to 'return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus)'\nChange 5: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          mu, eps = Config.mu, Config.epsilon\n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          and_strength = float(getattr(Config, \"and_strength\", 0.0))\n          and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n          miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n          scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n  \n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n  \n          # Query-scope accumulators: per-doc matched specificity mass vs total query specificity m...\n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n          scope_den = 0.0\n  \n          # Optional risk-sensitive aggregator accumulator\n          risk_s = float(getattr(Config, \"risk_strength\", 0.0))\n          risk_p = float(getattr(Config, \"risk_power\", 0.55))\n          risk_sat = float(getattr(Config, \"risk_saturation\", 6.0))\n  ... (55 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n          scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n  \n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          # Length-normalized missing-term penalty factor (hoisted)\n          avgdl = max(float(self.corpus.avgdl), 1.0)\n          len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n          len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n  \n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(...\n          total_q = total_q if total_q > 0.0 else 1.0\n  \n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n          scope_den = 0.0\n  \n  ... (47 more lines)\nChange 6: Replace:\n  # NEW: IDF-in-[0,1] and burstiness exponent beta(w) for tf saturation (common terms saturate more).\n          # idf = log((N+1)/(df+1)) is classic; we normalize by max idf to get [0,1] without tuning...\n          if self.N > 0:\n              idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n              idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n          else:\n              idf01 = np.zeros(self.vocab_size, dtype=np.float64)\n  \n          if burst_s > 0.0:\n              # beta in [1-burst_s, 1]; common terms (low idf01) -> smaller beta -> more saturation\n              self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n          else:\n              self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n  \n          # Length prior\n          s = float(getattr(Config, \"length_prior_strength\", 0.0))\n          if s > 0:\n              logL = np.log(np.maximum(self.doc_lengths, 1.0))\n              m = math.log(max(self.avgdl, 1.0))\n              self._length_prior = -s * np.square(logL - m)\n          else:\n              self._length_prior = np.zeros(self.N, dtype=np.float64)\nwith:\n  # Keep placeholders (some evaluators/variants expect these attributes).\n          self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n          self._length_prior = np.zeros(self.N, dtype=np.float64)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.30986827131805417, "beir_nfcorpus_recall@100": 0.2510446997157538, "beir_nfcorpus_index_time_ms": 3587.2570420615375, "beir_nfcorpus_query_time_ms": 136.6277919150889, "beir_scifact_ndcg@10": 0.6731093169493444, "beir_scifact_recall@100": 0.9253333333333335, "beir_scifact_index_time_ms": 4847.787083010189, "beir_scifact_query_time_ms": 625.4840419860557, "bright_pony_ndcg@10": 0.10703388469722083, "bright_pony_recall@100": 0.33439765505560093, "bright_pony_index_time_ms": 1377.614957978949, "bright_pony_query_time_ms": 954.1227919980884, "beir_arguana_ndcg@10": 0.25791735861259374, "beir_arguana_recall@100": 0.892933618843683, "beir_arguana_index_time_ms": 5882.747082971036, "beir_arguana_query_time_ms": 46895.26058395859, "bright_theoremqa_theorems_ndcg@10": 0.03363324243673673, "bright_theoremqa_theorems_recall@100": 0.16885964912280702, "bright_theoremqa_theorems_index_time_ms": 10962.190207908861, "bright_theoremqa_theorems_query_time_ms": 1951.376957935281, "beir_scidocs_ndcg@10": 0.14095416218517973, "beir_scidocs_recall@100": 0.3394, "beir_scidocs_index_time_ms": 19959.156457916833, "beir_scidocs_query_time_ms": 5892.961667035706, "bright_economics_ndcg@10": 0.12826520736144265, "bright_economics_recall@100": 0.36878617731164237, "bright_economics_index_time_ms": 14401.716124964878, "bright_economics_query_time_ms": 5654.207458952442, "bright_biology_ndcg@10": 0.28810510055084504, "bright_biology_recall@100": 0.5995954224620551, "bright_biology_index_time_ms": 14995.11816597078, "bright_biology_query_time_ms": 5431.896082940511, "beir_fiqa_ndcg@10": 0.20874118433039263, "beir_fiqa_recall@100": 0.5024044028673658, "beir_fiqa_index_time_ms": 33353.86320797261, "beir_fiqa_query_time_ms": 10785.539375036024, "bright_earth_science_ndcg@10": 0.28763075357177403, "bright_earth_science_recall@100": 0.6603452122338238, "bright_earth_science_index_time_ms": 32194.471374968998, "bright_earth_science_query_time_ms": 8753.923666896299, "bright_stackoverflow_ndcg@10": 0.1901910189542296, "bright_stackoverflow_recall@100": 0.4512283691953858, "bright_stackoverflow_index_time_ms": 98778.20645796601, "bright_stackoverflow_query_time_ms": 20209.43033392541, "beir_trec-covid_ndcg@10": 0.6107809916816405, "beir_trec-covid_recall@100": 0.10250334981300373, "beir_trec-covid_index_time_ms": 133951.1087499559, "beir_trec-covid_query_time_ms": 2545.5147500615567, "avg_ndcg@10": 0.2696858743874545, "avg_recall@100": 0.4664026574962046, "combined_score": 0.4270593008744546, "total_index_time_ms": 374291.2369136466, "total_query_time_ms": 109836.34550264105, "total_time_ms": 484127.5824162876, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new probabilistic retrieval method** based on Query Likelihood language models. The seed program is a minimal skeleton: document representation, query representation, collection model, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just Dirichlet smoothing with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall@100, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the probabilistic model with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 \u03bc (smoothing parameter), epsilon, and any new parameters you need.\n2. **collection_probability(term, ...)** \u2014 How to compute P(w | C) (collection language model). EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, hierarchical, etc.).\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. Default: Dirichlet smoothing. Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors, query models, multi-field models, term dependencies, etc. You can use multiple sub-signals and combine them, or a single unified formula.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **QL._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **QL**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **QL** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and QL.rank() / QL.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n- Avoid taking log(0) (use max(value, epsilon) before log).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"smoothing prevents zero probabilities\" or \"collection model favors discriminative terms\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different smoothing methods, document priors, query expansion, term dependencies) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n- Probabilistic, information-theoretic, and language modeling principles are good foundations.\n", "user": "# Current Program Information\n- Fitness: 0.4271\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4288 \u2192 0.4271. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # --- Robust background LM (token-LM tempered, plus tiny uniform) ---\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n  \n      # --- Discriminative diagnostics (query-independent) ---\n      # EDR: compares df-based LM vs token-based LM to downweight \"everywhere tokens\".\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n      residual_idf_strength: float = 0.90\n  \n      # Query term frequency saturation (avoid long queries / repeats dominating)\n      query_tf_power: float = 0.60\n  \n      # Missing-term penalty (soft AND) + coordination reward\n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\nwith:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      # Background LM robustness\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n  \n      # Query-independent term diagnostics\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      # Query TF saturation\n      query_tf_power: float = 0.60\n  \n      # Soft-AND signals\n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n  \n      # NEW: query-scope coverage prior over \"information mass\"\n      scope_strength: float = 0.07  # 0 disables\n  \n      # NEW: leak a tiny fraction of negative evidence (still mostly surplus-evidence)\n      neg_strength: float = 0.06  # 0 disables\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n  \n      Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n      Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n          ridf(w) = max(0, log p_doc(w) - log p_col(w))\n      and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n      (scientific, code/math, web) than raw IDF alone.\n  \n      Keep your:\n        - length-normalized missing-term penalty (soft AND)\n        - coordination reward (bounded)\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n  ... (51 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR gated LMDirichlet with coverage regularization,\n      plus (optional) query-scope info-mass prior and tiny leaked negative evidence.\n  \n      Why this helps:\n        - Positive-only surplus evidence is recall-friendly, but can over-rank \"one rare hit\" docs.\n        - A query-scope prior rewards covering more of the query's *information mass* (gentle soft-...\n        - Leaking a small fraction of negative LLR demotes boilerplate/weak matches without crushin...\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  \n  ... (62 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          N = float(self.corpus.N)\n  \n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n  \n          # Hoist length factor (used in missing-term evidence)\n          avgdl = max(float(self.corpus.avgdl), 1.0)\n          len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n          len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n  \n          # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(...\n          total_q = total_q if total_q > 0.0 else 1.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  ... (32 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n  \n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n  \n          # Hoist length factor (used in missing-term evidence)\n          avgdl = max(float(self.corpus.avgdl), 1.0)\n          len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n          len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n  \n          # Coordination + scope accumulators\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(...\n          total_q = total_q if total_q > 0.0 else 1.0\n  \n          scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n  ... (49 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3148, beir_nfcorpus_recall@100: 0.2539, beir_nfcorpus_index_time_ms: 3621.5444, beir_nfcorpus_query_time_ms: 112.0052, beir_scifact_ndcg@10: 0.6755, beir_scifact_recall@100: 0.9182, beir_scifact_index_time_ms: 4889.9770, beir_scifact_query_time_ms: 482.4733, bright_pony_ndcg@10: 0.1006, bright_pony_recall@100: 0.3357, bright_pony_index_time_ms: 1473.7685, bright_pony_query_time_ms: 638.1253, beir_arguana_ndcg@10: 0.2730, beir_arguana_recall@100: 0.9108, beir_arguana_index_time_ms: 5961.3351, beir_arguana_query_time_ms: 29600.8491, bright_theoremqa_theorems_ndcg@10: 0.0260, bright_theoremqa_theorems_recall@100: 0.1623, bright_theoremqa_theorems_index_time_ms: 10696.6487, bright_theoremqa_theorems_query_time_ms: 1210.1058, beir_scidocs_ndcg@10: 0.1465, beir_scidocs_recall@100: 0.3448, beir_scidocs_index_time_ms: 19516.8882, beir_scidocs_query_time_ms: 4242.5700, bright_economics_ndcg@10: 0.1427, bright_economics_recall@100: 0.3672, bright_economics_index_time_ms: 14250.3397, bright_economics_query_time_ms: 3602.9016, bright_biology_ndcg@10: 0.2981, bright_biology_recall@100: 0.5919, bright_biology_index_time_ms: 15425.7548, bright_biology_query_time_ms: 3363.1599, beir_fiqa_ndcg@10: 0.2183, beir_fiqa_recall@100: 0.5144, beir_fiqa_index_time_ms: 33329.7085, beir_fiqa_query_time_ms: 7620.8812, bright_earth_science_ndcg@10: 0.3074, bright_earth_science_recall@100: 0.6646, bright_earth_science_index_time_ms: 31940.4143, bright_earth_science_query_time_ms: 5509.7341, bright_stackoverflow_ndcg@10: 0.1861, bright_stackoverflow_recall@100: 0.4358, bright_stackoverflow_index_time_ms: 98692.8375, bright_stackoverflow_query_time_ms: 12712.5179, beir_trec-covid_ndcg@10: 0.6157, beir_trec-covid_recall@100: 0.1056, beir_trec-covid_index_time_ms: 133806.2320, beir_trec-covid_query_time_ms: 1780.5499, avg_ndcg@10: 0.2754, avg_recall@100: 0.4671, combined_score: 0.4288, total_index_time_ms: 373605.4487, total_query_time_ms: 70875.8734, total_time_ms: 444481.3221, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Query term weighting (IDF curvature)\n      q_idf_power: float = 0.70\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.09\n      miss_cap: float = 1.4\n  \n      # Coordination reward (soft AND without harsh penalties)\n      coord_strength: float = 0.12\n  \n      # Omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\n  \n      # --- NEW: robust background + discriminative diagnostics (query-independent) ---\n      # Temper the collection LM: p_t(w) \u221d p(w)^tau (tau<1 flattens very peaky collections)\n      collection_temper: float = 0.85\n      # Mix a tiny uniform background to hedge against OOV-ish/rare token spikes\n      uniform_bg_mass: float = 0.03\n      # EDR gate compares df-based LM vs token LM: downweight \"everywhere tokens\"\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n  ... (4 more lines)\nwith:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Query term weighting (IDF curvature)\n      q_idf_power: float = 0.70\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.09\n      miss_cap: float = 1.4\n  \n      # Coordination reward (soft AND without harsh penalties)\n      coord_strength: float = 0.12\n  \n      # Omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\n  \n      # --- NEW: robust background + discriminative diagnostics (query-independent) ---\n      # Temper the collection LM: p_t(w) \u221d p(w)^tau (tau<1 flattens very peaky collections)\n      collection_temper: float = 0.85\n      # Mix a tiny uniform background to hedge against OOV-ish/rare token spikes\n      uniform_bg_mass: float = 0.03\n      # EDR gate compares df-based LM vs token LM: downweight \"everywhere tokens\"\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n  ... (13 more lines)\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR-gated LMDirichlet with coverage regularization.\n  \n      Story:\n      - Keep the strong LMDirichlet *positive surplus evidence* backbone (good recall).\n      - Replace raw IDF-only specificity with a collection-vs-document-spread diagnostic:\n          ridf(w) = max(0, log p_doc(w) - log p_col(w))\n        where p_doc=df/N and p_col is token LM. This is more stable across domains.\n      - Use EDR gate (can be <1 for ubiquitous tokens) to suppress misleading terms.\n      - Keep your length-normalized missing-term penalty + coordination reward for early precision.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  \n  ... (42 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR-gated LMDirichlet with coverage regularization,\n      plus (optional) query-scope coverage prior and tiny leaked negative evidence.\n  \n      Additions are designed to improve early precision (nDCG@10) on multi-aspect queries:\n        - scope prior: rewards covering more of the query's information mass\n        - negative leak: demotes boilerplate \"mentions\" while staying recall-friendly\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  \n      len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n  ... (60 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          N = float(self.corpus.N)\n  \n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n  \n          # Length-normalized missing-term penalty factor (hoisted)\n          avgdl = max(float(self.corpus.avgdl), 1.0)\n          len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n          len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n  \n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(...\n          total_q = total_q if total_q > 0.0 else 1.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n  ... (28 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n  \n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n          scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n  \n          # Length-normalized missing-term penalty factor (hoisted)\n          avgdl = max(float(self.corpus.avgdl), 1.0)\n          len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n          len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n  \n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(...\n          total_q = total_q if total_q > 0.0 else 1.0\n  \n          # Query-scope info-mass coverage accumulators\n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n  ... (49 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3143, beir_nfcorpus_recall@100: 0.2539, beir_nfcorpus_index_time_ms: 3616.3896, beir_nfcorpus_query_time_ms: 116.5019, beir_scifact_ndcg@10: 0.6755, beir_scifact_recall@100: 0.9182, beir_scifact_index_time_ms: 4857.6142, beir_scifact_query_time_ms: 482.3628, bright_pony_ndcg@10: 0.1008, bright_pony_recall@100: 0.3363, bright_pony_index_time_ms: 1393.9892, bright_pony_query_time_ms: 636.0400, beir_arguana_ndcg@10: 0.2724, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 5982.3127, beir_arguana_query_time_ms: 29737.1072, bright_theoremqa_theorems_ndcg@10: 0.0260, bright_theoremqa_theorems_recall@100: 0.1623, bright_theoremqa_theorems_index_time_ms: 11043.4706, bright_theoremqa_theorems_query_time_ms: 1277.2308, beir_scidocs_ndcg@10: 0.1465, beir_scidocs_recall@100: 0.3446, beir_scidocs_index_time_ms: 20266.1050, beir_scidocs_query_time_ms: 4261.9763, bright_economics_ndcg@10: 0.1424, bright_economics_recall@100: 0.3672, bright_economics_index_time_ms: 14438.0299, bright_economics_query_time_ms: 3442.8664, bright_biology_ndcg@10: 0.2970, bright_biology_recall@100: 0.5967, bright_biology_index_time_ms: 16005.9486, bright_biology_query_time_ms: 3298.7204, beir_fiqa_ndcg@10: 0.2177, beir_fiqa_recall@100: 0.5140, beir_fiqa_index_time_ms: 34390.6759, beir_fiqa_query_time_ms: 7561.3136, bright_earth_science_ndcg@10: 0.3068, bright_earth_science_recall@100: 0.6655, bright_earth_science_index_time_ms: 32312.9012, bright_earth_science_query_time_ms: 5475.5317, bright_stackoverflow_ndcg@10: 0.1839, bright_stackoverflow_recall@100: 0.4358, bright_stackoverflow_index_time_ms: 98902.3385, bright_stackoverflow_query_time_ms: 12629.1162, beir_trec-covid_ndcg@10: 0.6159, beir_trec-covid_recall@100: 0.1056, beir_trec-covid_index_time_ms: 134153.0600, beir_trec-covid_query_time_ms: 1818.7167, avg_ndcg@10: 0.2749, avg_recall@100: 0.4675, combined_score: 0.4290, total_index_time_ms: 377362.8352, total_query_time_ms: 70737.4840, total_time_ms: 448100.3193, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Query term weighting (IDF curvature)\n      q_idf_power: float = 0.70\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.09\n      miss_cap: float = 1.4\n  \n      # Coordination reward (soft AND without harsh penalties)\n      coord_strength: float = 0.12\n  \n      # Omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\n  \n      # --- NEW: robust background + discriminative diagnostics (query-independent) ---\n      # Temper the collection LM: p_t(w) \u221d p(w)^tau (tau<1 flattens very peaky collections)\n      collection_temper: float = 0.85\n      # Mix a tiny uniform background to hedge against OOV-ish/rare token spikes\n      uniform_bg_mass: float = 0.03\n      # EDR gate compares df-based LM vs token LM: downweight \"everywhere tokens\"\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n  ... (4 more lines)\nwith:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.09\n      miss_cap: float = 1.4\n  \n      # Coordination reward (soft AND without harsh penalties)\n      coord_strength: float = 0.12\n  \n      # Omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\n  \n      # --- Robust background + discriminative diagnostics (query-independent) ---\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      # Query TF saturation for repeated terms in long queries\n      query_tf_power: float = 0.60\n  \n      # NEW: scope-level information coverage prior (query-level regularizer).\n      # Rationale: multi-aspect queries benefit when a doc covers more of the query's\n  ... (7 more lines)\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR-gated LMDirichlet with coverage regularization.\n  \n      Story:\n      - Keep the strong LMDirichlet *positive surplus evidence* backbone (good recall).\n      - Replace raw IDF-only specificity with a collection-vs-document-spread diagnostic:\n          ridf(w) = max(0, log p_doc(w) - log p_col(w))\n        where p_doc=df/N and p_col is token LM. This is more stable across domains.\n      - Use EDR gate (can be <1 for ubiquitous tokens) to suppress misleading terms.\n      - Keep your length-normalized missing-term penalty + coordination reward for early precision.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  \n  ... (42 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR-gated LMDirichlet with coverage regularization,\n      plus (optional) scope-level coverage and tiny leaked negative evidence.\n  \n      Intuition:\n        - LMDirichlet LLR is a term-wise log Bayes factor vs background.\n        - Pure \"positive surplus\" (Lucene clamp) is recall-friendly, but can over-rank\n          docs that match one rare term and miss the rest.\n        - A query-level scope prior rewards explaining more of the query's *information mass*.\n        - Leaking a tiny fraction of negative LLR helps demote boilerplate \"mention once\" docs.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n  ... (64 more lines)\nChange 3: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              per_term = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n              pos = np.maximum(per_term, 0.0)\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf = float(qtf_raw) ** alpha\n  \n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id])\n  \n              miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n              )\n  \n              scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n              matched += float(qtf_raw) * (tf_row > 0.0)\n  \n          coord = matched / total_q\n          scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n  ... (2 more lines)\nwith:\n  scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n          scope_den = 0.0\n  \n          neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              per_term = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  \n              # Leak a tiny amount of negative evidence (if enabled).\n              if neg_s > 0.0:\n                  per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n  \n              pos = np.maximum(per_term, 0.0)\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf = float(qtf_raw) ** alpha\n  \n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id])\n  \n  ... (23 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3143, beir_nfcorpus_recall@100: 0.2539, beir_nfcorpus_index_time_ms: 3570.1428, beir_nfcorpus_query_time_ms: 119.1983, beir_scifact_ndcg@10: 0.6755, beir_scifact_recall@100: 0.9182, beir_scifact_index_time_ms: 4872.1340, beir_scifact_query_time_ms: 467.0652, bright_pony_ndcg@10: 0.1008, bright_pony_recall@100: 0.3363, bright_pony_index_time_ms: 1442.7032, bright_pony_query_time_ms: 630.3858, beir_arguana_ndcg@10: 0.2724, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 5874.0721, beir_arguana_query_time_ms: 29541.4656, bright_theoremqa_theorems_ndcg@10: 0.0260, bright_theoremqa_theorems_recall@100: 0.1623, bright_theoremqa_theorems_index_time_ms: 10723.0730, bright_theoremqa_theorems_query_time_ms: 1212.3176, beir_scidocs_ndcg@10: 0.1465, beir_scidocs_recall@100: 0.3446, beir_scidocs_index_time_ms: 19955.0809, beir_scidocs_query_time_ms: 4250.0455, bright_economics_ndcg@10: 0.1424, bright_economics_recall@100: 0.3672, bright_economics_index_time_ms: 13967.8400, bright_economics_query_time_ms: 3611.8167, bright_biology_ndcg@10: 0.2970, bright_biology_recall@100: 0.5967, bright_biology_index_time_ms: 15009.2068, bright_biology_query_time_ms: 3339.4940, beir_fiqa_ndcg@10: 0.2177, beir_fiqa_recall@100: 0.5140, beir_fiqa_index_time_ms: 34126.4321, beir_fiqa_query_time_ms: 7714.4725, bright_earth_science_ndcg@10: 0.3068, bright_earth_science_recall@100: 0.6655, bright_earth_science_index_time_ms: 31887.2967, bright_earth_science_query_time_ms: 5448.5851, bright_stackoverflow_ndcg@10: 0.1839, bright_stackoverflow_recall@100: 0.4358, bright_stackoverflow_index_time_ms: 98140.6791, bright_stackoverflow_query_time_ms: 12447.7099, beir_trec-covid_ndcg@10: 0.6159, beir_trec-covid_recall@100: 0.1056, beir_trec-covid_index_time_ms: 133946.3552, beir_trec-covid_query_time_ms: 1738.4794, avg_ndcg@10: 0.2749, avg_recall@100: 0.4675, combined_score: 0.4290, total_index_time_ms: 373515.0160, total_query_time_ms: 70521.0356, total_time_ms: 444036.0516, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4295)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # --- Robust background LM (token-LM tempered, plus tiny uniform) ---\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # --- Discriminative diagnostics (query-independent) ---\n    # EDR: compares df-based LM vs token-based LM to downweight \"everywhere tokens\".\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n    residual_idf_strength: float = 0.90\n\n    # Query term frequency saturation (avoid long queries / repeats dominating)\n    query_tf_power: float = 0.60\n\n    # Missing-term penalty (soft AND) + coordination reward\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    # NEW: Dirichlet missing-term evidence can be too sharp on multi-aspect queries.\n    # Add a gentler *information-mass* coverage prior:\n    #   score += s * log( matched_info / total_info )\n    # where info per term is approximated by log1p(ridf_qweight).\n    scope_strength: float = 0.07  # 0 disables\n\n    # NEW: optional small leak of negative evidence (keep recall safe by being tiny).\n    neg_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n        else:\n            tid = None\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM (tempered) + tiny uniform mixture ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: query-scope (information-mass) coverage accumulators.\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Optional small leak of negative evidence (prevents \"mention once everywhere\" docs).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            # NOTE: per_term already contains leaked negatives (if enabled).\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            # Query-scope prior: reward covering the query's \"information mass\".\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3143), Performs well on beir_nfcorpus_recall@100 (0.2538), Performs well on beir_nfcorpus_index_time_ms (3682.6199), Performs well on beir_nfcorpus_query_time_ms (116.9216), Performs well on beir_scifact_ndcg@10 (0.6752), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4993.2742), Performs well on beir_scifact_query_time_ms (478.0730), Performs well on bright_pony_ndcg@10 (0.1000), Performs well on bright_pony_recall@100 (0.3221), Performs well on bright_pony_index_time_ms (1431.6385), Performs well on bright_pony_query_time_ms (651.8884), Performs well on beir_arguana_ndcg@10 (0.2818), Performs well on beir_arguana_recall@100 (0.9215), Performs well on beir_arguana_index_time_ms (6088.2992), Performs well on beir_arguana_query_time_ms (29566.9839), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0198), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (10676.8663), Performs well on bright_theoremqa_theorems_query_time_ms (1280.3027), Performs well on beir_scidocs_ndcg@10 (0.1466), Performs well on beir_scidocs_recall@100 (0.3459), Performs well on beir_scidocs_index_time_ms (19757.8108), Performs well on beir_scidocs_query_time_ms (4207.2374), Performs well on bright_economics_ndcg@10 (0.1458), Performs well on bright_economics_recall@100 (0.3904), Performs well on bright_economics_index_time_ms (13544.4747), Performs well on bright_economics_query_time_ms (3504.1463), Performs well on bright_biology_ndcg@10 (0.2966), Performs well on bright_biology_recall@100 (0.5773), Performs well on bright_biology_index_time_ms (15342.6598), Performs well on bright_biology_query_time_ms (3296.9252), Performs well on beir_fiqa_ndcg@10 (0.2198), Performs well on beir_fiqa_recall@100 (0.5154), Performs well on beir_fiqa_index_time_ms (33220.0687), Performs well on beir_fiqa_query_time_ms (7788.7655), Performs well on bright_earth_science_ndcg@10 (0.3065), Performs well on bright_earth_science_recall@100 (0.6646), Performs well on bright_earth_science_index_time_ms (32701.2086), Performs well on bright_earth_science_query_time_ms (5645.6101), Performs well on bright_stackoverflow_ndcg@10 (0.1840), Performs well on bright_stackoverflow_recall@100 (0.4330), Performs well on bright_stackoverflow_index_time_ms (100430.9848), Performs well on bright_stackoverflow_query_time_ms (12689.1048), Performs well on beir_trec-covid_ndcg@10 (0.6136), Performs well on beir_trec-covid_recall@100 (0.1056), Performs well on beir_trec-covid_index_time_ms (138114.6444), Performs well on beir_trec-covid_query_time_ms (1960.5291), Performs well on avg_ndcg@10 (0.2753), Performs well on avg_recall@100 (0.4680), Performs well on combined_score (0.4295), Performs well on total_index_time_ms (379984.5498), Performs well on total_query_time_ms (71186.4880), Performs well on total_time_ms (451171.0379), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4290)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.09\n    miss_cap: float = 1.4\n\n    # Coordination reward (soft AND without harsh penalties)\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # --- Robust background + discriminative diagnostics (query-independent) ---\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation for repeated terms in long queries\n    query_tf_power: float = 0.60\n\n    # NEW: scope-level information coverage prior (query-level regularizer).\n    # Rationale: multi-aspect queries benefit when a doc covers more of the query's\n    # *information mass* (not just term count). This tends to improve nDCG@10 while\n    # staying gentler than per-term missing penalties (recall-safe).\n    scope_strength: float = 0.07  # 0 disables\n\n    # NEW: leak a tiny amount of negative evidence from LMDirichlet LLR.\n    # This demotes boilerplate docs that weakly \"mention everything once\".\n    neg_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with coverage regularization,\n    plus (optional) scope-level coverage and tiny leaked negative evidence.\n\n    Intuition:\n      - LMDirichlet LLR is a term-wise log Bayes factor vs background.\n      - Pure \"positive surplus\" (Lucene clamp) is recall-friendly, but can over-rank\n        docs that match one rare term and miss the rest.\n      - A query-level scope prior rewards explaining more of the query's *information mass*.\n      - Leaking a tiny fraction of negative LLR helps demote boilerplate \"mention once\" docs.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Optional scope accumulators (slow path only, but score() is not the bottleneck).\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak a tiny amount of negative evidence (if enabled).\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        # Lucene-style surplus evidence backbone.\n        pos = max(per_term, 0.0)\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        # Query-scope: matched info mass vs total info mass (ridf_w is already bounded).\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        frac = max(scope_num / scope_den, eps)\n        score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered + tiny uniform mixture (properly renormalized) ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Leak a tiny amount of negative evidence (if enabled).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            # Scope prior: reward covering more of the query's information mass.\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3143), Performs well on beir_nfcorpus_recall@100 (0.2539), Performs well on beir_nfcorpus_index_time_ms (3570.1428), Performs well on beir_nfcorpus_query_time_ms (119.1983), Performs well on beir_scifact_ndcg@10 (0.6755), Performs well on beir_scifact_recall@100 (0.9182), Performs well on beir_scifact_index_time_ms (4872.1340), Performs well on beir_scifact_query_time_ms (467.0652), Performs well on bright_pony_ndcg@10 (0.1008), Performs well on bright_pony_recall@100 (0.3363), Performs well on bright_pony_index_time_ms (1442.7032), Performs well on bright_pony_query_time_ms (630.3858), Performs well on beir_arguana_ndcg@10 (0.2724), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (5874.0721), Performs well on beir_arguana_query_time_ms (29541.4656), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0260), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (10723.0730), Performs well on bright_theoremqa_theorems_query_time_ms (1212.3176), Performs well on beir_scidocs_ndcg@10 (0.1465), Performs well on beir_scidocs_recall@100 (0.3446), Performs well on beir_scidocs_index_time_ms (19955.0809), Performs well on beir_scidocs_query_time_ms (4250.0455), Performs well on bright_economics_ndcg@10 (0.1424), Performs well on bright_economics_recall@100 (0.3672), Performs well on bright_economics_index_time_ms (13967.8400), Performs well on bright_economics_query_time_ms (3611.8167), Performs well on bright_biology_ndcg@10 (0.2970), Performs well on bright_biology_recall@100 (0.5967), Performs well on bright_biology_index_time_ms (15009.2068), Performs well on bright_biology_query_time_ms (3339.4940), Performs well on beir_fiqa_ndcg@10 (0.2177), Performs well on beir_fiqa_recall@100 (0.5140), Performs well on beir_fiqa_index_time_ms (34126.4321), Performs well on beir_fiqa_query_time_ms (7714.4725), Performs well on bright_earth_science_ndcg@10 (0.3068), Performs well on bright_earth_science_recall@100 (0.6655), Performs well on bright_earth_science_index_time_ms (31887.2967), Performs well on bright_earth_science_query_time_ms (5448.5851), Performs well on bright_stackoverflow_ndcg@10 (0.1839), Performs well on bright_stackoverflow_recall@100 (0.4358), Performs well on bright_stackoverflow_index_time_ms (98140.6791), Performs well on bright_stackoverflow_query_time_ms (12447.7099), Performs well on beir_trec-covid_ndcg@10 (0.6159), Performs well on beir_trec-covid_recall@100 (0.1056), Performs well on beir_trec-covid_index_time_ms (133946.3552), Performs well on beir_trec-covid_query_time_ms (1738.4794), Performs well on avg_ndcg@10 (0.2749), Performs well on avg_recall@100 (0.4675), Performs well on combined_score (0.4290), Performs well on total_index_time_ms (373515.0160), Performs well on total_query_time_ms (70521.0356), Performs well on total_time_ms (444036.0516), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4290)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Query term weighting (IDF curvature)\n    q_idf_power: float = 0.70\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.09\n    miss_cap: float = 1.4\n\n    # Coordination reward (soft AND without harsh penalties)\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # --- NEW: robust background + discriminative diagnostics (query-independent) ---\n    # Temper the collection LM: p_t(w) \u221d p(w)^tau (tau<1 flattens very peaky collections)\n    collection_temper: float = 0.85\n    # Mix a tiny uniform background to hedge against OOV-ish/rare token spikes\n    uniform_bg_mass: float = 0.03\n    # EDR gate compares df-based LM vs token LM: downweight \"everywhere tokens\"\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation for repeated terms in long queries\n    query_tf_power: float = 0.60\n\n    # --- NEW: query-level regularizers (help nDCG@10 without killing recall) ---\n    # Reward documents that cover more of the query's \"information mass\"\n    # (info mass proxy: log1p(ridf_qweight)).\n    scope_strength: float = 0.07  # 0 disables\n\n    # Leak a tiny amount of negative evidence from the LM LLR\n    # (demotes boilerplate \"mentions\" while staying mostly surplus-evidence).\n    neg_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with coverage regularization,\n    plus (optional) query-scope coverage prior and tiny leaked negative evidence.\n\n    Additions are designed to improve early precision (nDCG@10) on multi-aspect queries:\n      - scope prior: rewards covering more of the query's information mass\n      - negative leak: demotes boilerplate \"mentions\" while staying recall-friendly\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Scope accumulators (slow path only; score() isn't the throughput bottleneck)\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak a tiny amount of negative evidence (optional)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        # Keep surplus evidence backbone\n        pos = max(per_term, 0.0)\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        # Query-scope info-mass prior: use bounded ridf_w as an \"info\" proxy\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered + tiny uniform mixture (properly renormalized) ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Leak a tiny amount of negative evidence (optional)\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            # Surplus-evidence backbone\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3143), Performs well on beir_nfcorpus_recall@100 (0.2539), Performs well on beir_nfcorpus_index_time_ms (3616.3896), Performs well on beir_nfcorpus_query_time_ms (116.5019), Performs well on beir_scifact_ndcg@10 (0.6755), Performs well on beir_scifact_recall@100 (0.9182), Performs well on beir_scifact_index_time_ms (4857.6142), Performs well on beir_scifact_query_time_ms (482.3628), Performs well on bright_pony_ndcg@10 (0.1008), Performs well on bright_pony_recall@100 (0.3363), Performs well on bright_pony_index_time_ms (1393.9892), Performs well on bright_pony_query_time_ms (636.0400), Performs well on beir_arguana_ndcg@10 (0.2724), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (5982.3127), Performs well on beir_arguana_query_time_ms (29737.1072), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0260), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (11043.4706), Performs well on bright_theoremqa_theorems_query_time_ms (1277.2308), Performs well on beir_scidocs_ndcg@10 (0.1465), Performs well on beir_scidocs_recall@100 (0.3446), Performs well on beir_scidocs_index_time_ms (20266.1050), Performs well on beir_scidocs_query_time_ms (4261.9763), Performs well on bright_economics_ndcg@10 (0.1424), Performs well on bright_economics_recall@100 (0.3672), Performs well on bright_economics_index_time_ms (14438.0299), Performs well on bright_economics_query_time_ms (3442.8664), Performs well on bright_biology_ndcg@10 (0.2970), Performs well on bright_biology_recall@100 (0.5967), Performs well on bright_biology_index_time_ms (16005.9486), Performs well on bright_biology_query_time_ms (3298.7204), Performs well on beir_fiqa_ndcg@10 (0.2177), Performs well on beir_fiqa_recall@100 (0.5140), Performs well on beir_fiqa_index_time_ms (34390.6759), Performs well on beir_fiqa_query_time_ms (7561.3136), Performs well on bright_earth_science_ndcg@10 (0.3068), Performs well on bright_earth_science_recall@100 (0.6655), Performs well on bright_earth_science_index_time_ms (32312.9012), Performs well on bright_earth_science_query_time_ms (5475.5317), Performs well on bright_stackoverflow_ndcg@10 (0.1839), Performs well on bright_stackoverflow_recall@100 (0.4358), Performs well on bright_stackoverflow_index_time_ms (98902.3385), Performs well on bright_stackoverflow_query_time_ms (12629.1162), Performs well on beir_trec-covid_ndcg@10 (0.6159), Performs well on beir_trec-covid_recall@100 (0.1056), Performs well on beir_trec-covid_index_time_ms (134153.0600), Performs well on beir_trec-covid_query_time_ms (1818.7167), Performs well on avg_ndcg@10 (0.2749), Performs well on avg_recall@100 (0.4675), Performs well on combined_score (0.4290), Performs well on total_index_time_ms (377362.8352), Performs well on total_query_time_ms (70737.4840), Performs well on total_time_ms (448100.3193), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4288)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    # Background LM robustness\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # Query-independent term diagnostics\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation\n    query_tf_power: float = 0.60\n\n    # Soft-AND signals\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    # NEW: query-scope coverage prior over \"information mass\"\n    scope_strength: float = 0.07  # 0 disables\n\n    # NEW: leak a tiny fraction of negative evidence (still mostly surplus-evidence)\n    neg_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization,\n    plus (optional) query-scope info-mass prior and tiny leaked negative evidence.\n\n    Why this helps:\n      - Positive-only surplus evidence is recall-friendly, but can over-rank \"one rare hit\" docs.\n      - A query-scope prior rewards covering more of the query's *information mass* (gentle soft-AND).\n      - Leaking a small fraction of negative LLR demotes boilerplate/weak matches without crushing recall.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Query-scope info-mass accumulators (slow path; score() isn't the bottleneck)\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Leak tiny negative evidence (optional): keep mostly surplus-evidence behavior.\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            # ridf_w is already bounded in Corpus; log1p makes \"info mass\" robust.\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM (tempered) + tiny uniform mixture ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Coordination + scope accumulators\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Leak tiny negative evidence (optional)\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            pos = np.maximum(per_term, 0.0)\n            w = qtf * ridf_w\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3148), Performs well on beir_nfcorpus_recall@100 (0.2539), Performs well on beir_nfcorpus_index_time_ms (3621.5444), Performs well on beir_nfcorpus_query_time_ms (112.0052), Performs well on beir_scifact_ndcg@10 (0.6755), Performs well on beir_scifact_recall@100 (0.9182), Performs well on beir_scifact_index_time_ms (4889.9770), Performs well on beir_scifact_query_time_ms (482.4733), Performs well on bright_pony_ndcg@10 (0.1006), Performs well on bright_pony_recall@100 (0.3357), Performs well on bright_pony_index_time_ms (1473.7685), Performs well on bright_pony_query_time_ms (638.1253), Performs well on beir_arguana_ndcg@10 (0.2730), Performs well on beir_arguana_recall@100 (0.9108), Performs well on beir_arguana_index_time_ms (5961.3351), Performs well on beir_arguana_query_time_ms (29600.8491), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0260), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (10696.6487), Performs well on bright_theoremqa_theorems_query_time_ms (1210.1058), Performs well on beir_scidocs_ndcg@10 (0.1465), Performs well on beir_scidocs_recall@100 (0.3448), Performs well on beir_scidocs_index_time_ms (19516.8882), Performs well on beir_scidocs_query_time_ms (4242.5700), Performs well on bright_economics_ndcg@10 (0.1427), Performs well on bright_economics_recall@100 (0.3672), Performs well on bright_economics_index_time_ms (14250.3397), Performs well on bright_economics_query_time_ms (3602.9016), Performs well on bright_biology_ndcg@10 (0.2981), Performs well on bright_biology_recall@100 (0.5919), Performs well on bright_biology_index_time_ms (15425.7548), Performs well on bright_biology_query_time_ms (3363.1599), Performs well on beir_fiqa_ndcg@10 (0.2183), Performs well on beir_fiqa_recall@100 (0.5144), Performs well on beir_fiqa_index_time_ms (33329.7085), Performs well on beir_fiqa_query_time_ms (7620.8812), Performs well on bright_earth_science_ndcg@10 (0.3074), Performs well on bright_earth_science_recall@100 (0.6646), Performs well on bright_earth_science_index_time_ms (31940.4143), Performs well on bright_earth_science_query_time_ms (5509.7341), Performs well on bright_stackoverflow_ndcg@10 (0.1861), Performs well on bright_stackoverflow_recall@100 (0.4358), Performs well on bright_stackoverflow_index_time_ms (98692.8375), Performs well on bright_stackoverflow_query_time_ms (12712.5179), Performs well on beir_trec-covid_ndcg@10 (0.6157), Performs well on beir_trec-covid_recall@100 (0.1056), Performs well on beir_trec-covid_index_time_ms (133806.2320), Performs well on beir_trec-covid_query_time_ms (1780.5499), Performs well on avg_ndcg@10 (0.2754), Performs well on avg_recall@100 (0.4671), Performs well on combined_score (0.4288), Performs well on total_index_time_ms (373605.4487), Performs well on total_query_time_ms (70875.8734), Performs well on total_time_ms (444481.3221), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4285)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive Dirichlet mass: mu_t = mu * ((df+1)/(N+1))^a\n    # Rationale: rare terms trust doc evidence sooner; common terms need more smoothing.\n    mu_df_power: float = 0.30\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau, plus tiny uniform hedge\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # Query TF saturation (qtf^alpha)\n    query_tf_power: float = 0.60\n\n    # EDR gate + residual-IDF query weighting (query-independent diagnostics)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Soft-AND signals\n    coord_strength: float = 0.12\n    scope_strength: float = 0.07\n\n    # Missing-term anti-evidence (soft AND), length-normalized\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coverage_len_power: float = 0.35\n\n    # Document-level evidence saturation (concave utility to prefer multi-term support)\n    sat_strength: float = 0.10   # 0 disables\n    sat_scale: float = 3.0       # larger => weaker saturation\n\n    # Optional tiny leak of negative evidence (very small to protect recall)\n    neg_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Score over unique terms; keep qtf in term_weights.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated *term-adaptive* LMDirichlet with gentle completeness + saturation.\n\n    Key ideas:\n      - LMDirichlet is a log Bayes factor vs background.\n      - Use term-adaptive mu_t(df): rare terms trust doc tf sooner; common terms smooth harder.\n      - Add small missing-term anti-evidence (length-normalized) to reduce \"one-term spike\" ranking.\n      - Add a concave saturation over accumulated evidence to prefer multi-term support (nDCG@10).\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n    score = 0.0\n    matched = 0.0\n    total = 0.0\n\n    # Length-normalize missing evidence: omissions in long docs are less diagnostic.\n    if corpus is not None and corpus.N > 0:\n        avgdl = float(corpus.avgdl)\n    else:\n        avgdl = max(doc_length, 1.0)\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** float(getattr(Config, \"coverage_len_power\", 0.0))\n    len_factor = min(1.0, max(0.0, len_factor))\n\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    # Evidence saturation accumulator (concave utility at document level)\n    sat_s = float(getattr(Config, \"sat_strength\", 0.0))\n    sat_scale = float(getattr(Config, \"sat_scale\", 3.0))\n    sat_acc = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n    else:\n        N = 1.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = 1.0\n            gate = 1.0\n            ridf_w = 1.0\n\n        # Term-adaptive smoothing\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak a tiny amount of negative evidence (optional)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        # Surplus-evidence backbone (recall-friendly)\n        pos = max(per_term, 0.0)\n\n        w = (qtf * ridf_w * gate)\n        contrib = w * pos\n        score += contrib\n\n        # Missing-term anti-evidence (soft AND)\n        if tf <= 0.0:\n            miss_pen = float(getattr(Config, \"miss_penalty\", 0.0))\n            miss_cap = float(getattr(Config, \"miss_cap\", 1.0))\n            score -= (qtf * miss_pen * len_factor) * min(ridf_w, miss_cap)\n\n        # Scope prior stats\n        if scope_s > 0.0 and (corpus is not None and tid is not None):\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n        # Saturation: concave utility of evidence mass to prefer balanced multi-term matches.\n        if sat_s > 0.0 and contrib > 0.0:\n            sat_acc += math.log1p(contrib / max(sat_scale, eps))\n\n    if total > 0.0:\n        coord = matched / total\n        score += float(getattr(Config, \"coord_strength\", 0.0)) * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    if sat_s > 0.0:\n        score += sat_s * sat_acc\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with an optional uniform mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n        # Common terms saturate more: tf -> tf^beta(w).\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Missing-term length factor\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_pow = float(getattr(Config, \"coverage_len_power\", 0.0))\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** len_pow if len_pow > 0.0 else 1.0\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Coordination accumulators\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Optional scope-level info-mass prior\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Evidence saturation accumulator\n        sat_s = float(getattr(Config, \"sat_strength\", 0.0))\n        sat_scale = float(getattr(Config, \"sat_scale\", 3.0))\n        sat_acc = np.zeros(len(candidate_docs), dtype=np.float64) if sat_s > 0.0 else None\n\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n        miss_pen = float(getattr(Config, \"miss_penalty\", 0.0))\n        miss_cap = float(getattr(Config, \"miss_cap\", 1.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Apply EDR gate\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            # Leak tiny negative evidence (optional)\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            contrib = w * pos\n            scores += contrib\n\n            # Missing-term anti-evidence\n            if miss_pen > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                scores -= (qtf * miss_pen) * miss * len_factor * min(ridf_w, miss_cap)\n\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if sat_acc is not None:\n                sat_acc += np.log1p(contrib / max(sat_scale, eps))\n\n        # Coordination reward\n        coord = matched / total_q\n        scores += float(getattr(Config, \"coord_strength\", 0.0)) * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        # Saturation bonus\n        if sat_acc is not None:\n            scores += sat_s * sat_acc\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4285)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # --- Robust background LM (token-LM tempered, plus tiny uniform) ---\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # --- Discriminative diagnostics (query-independent) ---\n    # EDR: compares df-based LM vs token-based LM to downweight \"everywhere tokens\".\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n    residual_idf_strength: float = 0.90\n\n    # Query term frequency saturation (avoid long queries / repeats dominating)\n    query_tf_power: float = 0.60\n\n    # Missing-term penalty (soft AND) + coordination reward\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n        else:\n            tid = None\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM (tempered) + tiny uniform mixture ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            w = qtf * float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(self.corpus._ridf_qweight[term_id]), Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4286)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing (good default across BEIR-style corpora)\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # --- Robust background LM (token-LM tempered + tiny uniform) ---\n    # Tempering flattens an overly-peaky collection LM; uniform mass hedges rare spikes.\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # --- Discriminative diagnostics (query-independent) ---\n    # EDR gate downweights terms that are frequent as tokens but spread across documents.\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    # Residual-IDF query weighting from max(0, log(p_doc/p_col)).\n    residual_idf_strength: float = 0.90\n\n    # Query term frequency saturation (prevents long/repetitive queries dominating)\n    query_tf_power: float = 0.60\n\n    # Soft AND / completeness\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    # Gentle query-scope \u201cinformation mass\u201d coverage prior (optional but usually helpful)\n    scope_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Score over unique terms; keep query term frequency (qtf) as the weight.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use a simple multinomial query model:\n        - unique terms as support\n        - qtf as weights\n        This makes score() consistent with rank() (which already uses Counter(query)).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style *positive surplus evidence* from LMDirichlet (strong recall).\n    Then:\n      - residual-IDF query weighting: max(0, log p_doc - log p_col)\n      - EDR gate: 1 + \u03bb * clip(log(p_doc/p_col), [-c,c])\n      - small missing-term penalty (soft AND) with length normalization\n      - bounded coordination + optional query-scope (information-mass) coverage prior\n    \"\"\"\n    base_mu, a, eps = Config.mu, 0.30, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Missing-term evidence weaker for long docs.\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Optional query-scope prior (slow path)\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        df = float(corpus._df[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        # Term-adaptive mu (kept from strong earlier variants)\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        p_collection = (\n            float(corpus._collection_prob[tid]) if (corpus is not None and tid is not None)\n            else collection_probability(term, corpus_term_freq, total_tokens)\n        )\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Lucene clamp: keep only positive surplus evidence\n        pos = max(per_term, 0.0)\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and corpus is not None and tid is not None:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        frac = max(scope_num / scope_den, eps)\n        score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM (tempered) + tiny uniform mixture ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, 0.30, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Missing-term factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Coordination accumulators\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Optional query-scope accumulators\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Lucene clamp to positive surplus evidence\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n            w = qtf * ridf_w\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term misses)\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4287)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Query term weighting (IDF curvature)\n    q_idf_power: float = 0.70\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.09\n    miss_cap: float = 1.4\n\n    # Coordination reward (soft AND without harsh penalties)\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # --- NEW: robust background + discriminative diagnostics (query-independent) ---\n    # Temper the collection LM: p_t(w) \u221d p(w)^tau (tau<1 flattens very peaky collections)\n    collection_temper: float = 0.85\n    # Mix a tiny uniform background to hedge against OOV-ish/rare token spikes\n    uniform_bg_mass: float = 0.03\n    # EDR gate compares df-based LM vs token LM: downweight \"everywhere tokens\"\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation for repeated terms in long queries\n    query_tf_power: float = 0.60\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with coverage regularization.\n\n    Story:\n    - Keep the strong LMDirichlet *positive surplus evidence* backbone (good recall).\n    - Replace raw IDF-only specificity with a collection-vs-document-spread diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n      where p_doc=df/N and p_col is token LM. This is more stable across domains.\n    - Use EDR gate (can be <1 for ubiquitous tokens) to suppress misleading terms.\n    - Keep your length-normalized missing-term penalty + coordination reward for early precision.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n        pos = max(per_term, 0.0)\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w (bounded information mass) for missing-term anti-evidence.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered + tiny uniform mixture (properly renormalized) ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4117, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Dirichlet smoothing mass (close to strong baselines)\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Specificity transform for IDF -> evidence weight\n    idf_power: float = 0.70\n\n    # Robust background mixing (token-LM + df-LM)\n    collection_df_alpha: float = 0.10\n\n    # Soft AND: bounded missing-term penalty, length-normalized\n    miss_penalty: float = 0.09\n    miss_cap: float = 1.4\n    coverage_len_power: float = 0.35\n\n    # Soft AND without harsh penalties: reward matching a larger fraction of query terms\n    coord_strength: float = 0.12\n\n    # NEW: two-population query generator (content vs. noise tokens).\n    # Very common query terms are likely generated from a background/noise process,\n    # so they should contribute less evidence (improves early precision).\n    noise_idf_cut: float = 0.25        # below this IDF => mostly noise\n    noise_slope: float = 6.0           # sharpness of transition\n    noise_floor: float = 0.20          # minimum weight kept even for noisy terms\n\n    # NEW: avoid one ultra-rare token dominating ranking (esp. theorem/code corpora).\n    spec_cap: float = 3.0\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Iterate unique terms; store query term frequency (qtf) as weight.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"Multinomial query view: unique support + qtf weights (aligns score() with rank()).\"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Robust LMDirichlet + Length-normalized Coverage Prior + Coordination Reward\n\n    - Keep the LMDirichlet \"positive evidence\" backbone (Lucene-style clamp).\n      This preserves the generative interpretation of counts under a smoothed doc LM.\n    - Use a robust background model:\n          P_bg(w) = (1-a) * tf(w)/|C| + a * df(w)/N\n      (df-channel is more stable against very long/bursty docs).\n    - Add a small, capped missing-term penalty (soft AND), but *shrink it for long docs*:\n      omission is less diagnostic when documents are long/multi-topic.\n    - Add a bounded coordination reward to prefer documents that cover more query terms.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    inv_N = 1.0 / max(N, 1.0)\n\n    # In (0,1]: reduce missing-term penalty for long docs\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf = float(query_repr.term_weights.get(term, 1.0))\n        total_qw += qtf\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf\n\n        # df/idf -> specificity (information content)\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n\n        # Specificity as \"information content\", but cap it to prevent single-token domination.\n        spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n        spec = min(spec, Config.spec_cap)\n\n        # Query-noise gate: interpret IDF as evidence the token is content-bearing.\n        # Weight in (noise_floor, 1], smoothly increasing with IDF.\n        q_gate = Config.noise_floor + (1.0 - Config.noise_floor) / (\n            1.0 + math.exp(-Config.noise_slope * (idf - Config.noise_idf_cut))\n        )\n\n        # robust background\n        p_tf = collection_probability(term, corpus_term_freq, total_tokens)\n        p_df = max(df * inv_N, eps)\n        p_bg = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n        p_bg = max(p_bg, eps)\n\n        # LMDirichlet positive evidence (keep counts; do not squash tf inside the Bayes factor)\n        numerator = 1.0 + tf / (mu * p_bg + eps)\n        denominator = (doc_length + mu) / mu\n        per_term = math.log(numerator / denominator + eps)\n        pos = max(per_term, 0.0)\n\n        # missing-term penalty (bounded + length-normalized)\n        miss = 0.0 if tf > 0.0 else -(Config.miss_penalty * len_factor) * min(spec, Config.miss_cap)\n\n        score += qtf * q_gate * (spec * pos + miss)\n\n    # coordination reward (bounded soft-AND)\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n        inv_N = 1.0 / max(N, 1.0)\n\n        # length-normalized coverage factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # for coordination reward\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n\n            spec = (max(idf, 0.0) + 1e-3) ** Config.idf_power\n            spec = min(spec, Config.spec_cap)\n\n            q_gate = Config.noise_floor + (1.0 - Config.noise_floor) / (\n                1.0 + np.exp(-Config.noise_slope * (idf - Config.noise_idf_cut))\n            )\n\n            # robust background P_bg\n            p_tf = float(self.corpus._collection_prob[term_id])\n            p_df = max(df * inv_N, eps)\n            p_bg = (1.0 - Config.collection_df_alpha) * p_tf + Config.collection_df_alpha * p_df\n            p_bg = max(p_bg, eps)\n\n            numerator = 1.0 + tf_row / (mu * p_bg + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n            pos = np.maximum(per_term, 0.0)\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(float(spec), Config.miss_cap),\n            )\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n            # Apply query-noise gate uniformly to positive and missing-term evidence.\n            scores += (qtf * q_gate) * (float(spec) * pos + miss)\n            matched += (qtf * q_gate) * (tf_row > 0.0)\n\n        # coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4287, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Query term weighting (IDF curvature)\n    q_idf_power: float = 0.70\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.09\n    miss_cap: float = 1.4\n\n    # Coordination reward (soft AND without harsh penalties)\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # --- NEW: robust background + discriminative diagnostics (query-independent) ---\n    # Temper the collection LM: p_t(w) \u221d p(w)^tau (tau<1 flattens very peaky collections)\n    collection_temper: float = 0.85\n    # Mix a tiny uniform background to hedge against OOV-ish/rare token spikes\n    uniform_bg_mass: float = 0.03\n    # EDR gate compares df-based LM vs token LM: downweight \"everywhere tokens\"\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation for repeated terms in long queries\n    query_tf_power: float = 0.60\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with coverage regularization.\n\n    Story:\n    - Keep the strong LMDirichlet *positive surplus evidence* backbone (good recall).\n    - Replace raw IDF-only specificity with a collection-vs-document-spread diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n      where p_doc=df/N and p_col is token LM. This is more stable across domains.\n    - Use EDR gate (can be <1 for ubiquitous tokens) to suppress misleading terms.\n    - Keep your length-normalized missing-term penalty + coordination reward for early precision.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n        pos = max(per_term, 0.0)\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w (bounded information mass) for missing-term anti-evidence.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered + tiny uniform mixture (properly renormalized) ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4141, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength (kept; works reasonably across domains)\n    mu_df_power: float = 0.30\n\n    # Query term weighting\n    q_idf_power: float = 0.75\n\n    # --- New: coverage-aware soft negative evidence (query completeness) ---\n    # Lucene-style clamp (max(.,0)) can over-reward partial matches (good for recall,\n    # sometimes hurts nDCG@10). We add a *small* bounded penalty for missing query terms,\n    # scaled by IDF so missing rare terms matter more.\n    miss_penalty: float = 0.12\n\n    # Bound the penalty so long docs aren't overly punished; interpret as a saturated\n    # log Bayes factor against relevance for a missing term.\n    miss_cap: float = 1.5\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Coverage-regularized Adaptive-Dirichlet QL (CRAD-QL)\n\n    Keeps the strong part of Lucene-style LMDirichlet (only positive per-term evidence),\n    *but* adds a bounded, IDF-shaped penalty for query terms that are absent in the doc.\n\n    Intuition:\n    - Positive evidence answers: \"does this doc generate the observed query term unusually well?\"\n    - Missing-term penalty answers: \"how surprising is it that an actually relevant doc would omit\n      this informative term?\" (soft AND / coverage prior)\n    This tends to improve early precision (nDCG@10) without destroying recall@100 because the\n    penalty is small and capped.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    score = 0.0\n    N = float(corpus.N) if corpus is not None else 1.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        idf = math.log((N + 1.0) / (df + 1.0) + eps)\n        spec = (max(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n        # Term-adaptive smoothing mass in (0, base_mu]\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        qtf = query_repr.term_weights.get(term, 1.0)\n\n        # Positive evidence (Lucene clamp)\n        pos = max(per_term, 0.0)\n\n        # Soft missing-term penalty (bounded): only apply when tf==0.\n        # Using spec keeps it probabilistic/information-theoretic: missing rare terms costs more bits.\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -Config.miss_penalty * min(spec, Config.miss_cap)\n\n        score += qtf * (spec * pos + miss)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            idf = math.log((N + 1.0) / (df + 1.0) + eps)\n            spec = (np.maximum(idf, 0.0) + 1e-3) ** Config.q_idf_power\n\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            qtf = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            pos = np.maximum(per_term, 0.0)\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -Config.miss_penalty * min(float(spec), Config.miss_cap),\n            )\n\n            scores += qtf * (float(spec) * pos + miss)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4141, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 2000.0  # Dirichlet smoothing parameter\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Score one document for one query using Query Likelihood with Dirichlet smoothing.\n\n    Formula (Lucene/Pyserini variant):\n        Score(D, Q) = \u03a3_{w in Q} max(0, log(1 + c(w,D)/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc)))\n\n    This matches Pyserini's LMDirichletSimilarity which clamps per-term scores to 0.\n    This differs from academic QL which allows negative per-term scores.\n\n    Default behavior: Matches Pyserini's LMDirichletSimilarity with \u03bc=2000.\n    EVOLVE: design a probabilistic formulation with deep, fundamental, intuitive justification.\n    Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors,\n    term dependencies, multi-field models, etc.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n\n    for term in query_repr.terms:\n        # c(w, D): term count in document\n        term_count = float(doc_tf.get(term, 0))\n\n        # P(w | C): collection probability\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        # Lucene/Pyserini formula: log(1 + freq/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc))\n        # Clamp per-term score to 0 (Lucene's behavior for negative term scores)\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        per_term_score = math.log(numerator / denominator + eps)\n\n        # Apply query term weight and clamp to 0\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * max(per_term_score, 0.0)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            # Get collection probability for this term\n            p_collection = self.corpus._collection_prob[term_id]\n\n            # Get term frequencies for candidates\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n\n            # Lucene/Pyserini formula: log(1 + freq/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc))\n            numerator = 1.0 + tf_row / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term_scores = np.log(numerator / denominator + eps)\n\n            # Apply query term weight and clamp to 0\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            scores += w * np.maximum(per_term_scores, 0.0)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n    burstiness_strength: float = 0.35\n\n    # Two-stage background (collection + uniform over vocab).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND query aggregation.\n    and_strength: float = 0.15  # 0 disables\n    and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n\n    # Dirichlet \"anti-evidence\" for missing query terms (tf=0).\n    # Keep slightly smaller by default: too much anti-evidence can hurt recall@100.\n    missing_strength: float = 0.06\n\n    # Query-scope prior: reward explaining a larger fraction of the query's information mass.\n    query_scope_strength: float = 0.07  # 0 disables\n\n    # NEW: Risk-sensitive aggregation (R\u00e9nyi / power mean) over per-term evidence.\n    # Add a small concave utility on term evidence to prefer \"balanced\" multi-term matches\n    # over one-term spikes (improves early precision on multi-aspect queries).\n    risk_strength: float = 0.12  # 0 disables\n    risk_power: float = 0.55     # in (0,1]: smaller => more \"AND-like\"\n    risk_saturation: float = 6.0 # scale inside utility; keeps it stable across corpora\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n    scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        if burst_s > 0.0:\n            idf01_proxy = 1.0 / (1.0 + max(0.0, math.log(1.0 / max(p_collection, eps))))\n            beta = 1.0 - burst_s * (1.0 - idf01_proxy)\n            term_count = term_count ** beta if beta != 1.0 else term_count\n\n        base = math.log((1.0 + term_count / (mu * p_collection + eps)) / ((doc_length + mu) / mu) + eps)\n        base = max(base, 0.0)\n\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n        if scope_s > 0.0:\n            info = math.log1p(1.0 + ridf_s * (spec / max(clipc, eps)))\n            scope_den += info\n            if term_count > 0.0:\n                scope_num += info\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) for Dirichlet smoothing\n        # plus a small uniform background mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V)\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob[:] = (1.0 - gamma) * p_col + gamma * p_uni\n            self._collection_prob[:] = np.maximum(self._collection_prob, Config.epsilon)\n        else:\n            self._collection_prob[:] = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: IDF-in-[0,1] and burstiness exponent beta(w) for tf saturation (common terms saturate more).\n        # idf = log((N+1)/(df+1)) is classic; we normalize by max idf to get [0,1] without tuning per dataset.\n        if self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n        else:\n            idf01 = np.zeros(self.vocab_size, dtype=np.float64)\n\n        if burst_s > 0.0:\n            # beta in [1-burst_s, 1]; common terms (low idf01) -> smaller beta -> more saturation\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query-scope accumulators: per-doc matched specificity mass vs total query specificity mass.\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Optional risk-sensitive aggregator accumulator\n        risk_s = float(getattr(Config, \"risk_strength\", 0.0))\n        risk_p = float(getattr(Config, \"risk_power\", 0.55))\n        risk_sat = float(getattr(Config, \"risk_saturation\", 6.0))\n        risk_acc = np.zeros(len(candidate_docs), dtype=np.float64) if risk_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += contrib\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                    scores += miss_s * w * miss * base0\n\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n            if scope_num is not None:\n                # Stable per-term \"information mass\" proxy derived from residual IDF weights.\n                info = float(np.log1p(self.corpus._ridf_qweight[term_id]))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if risk_acc is not None:\n                # Concave utility of evidence; then power-mean across terms.\n                # u(x)=log(1+x/sat) is information-like (diminishing returns).\n                u = np.log1p(contrib / max(risk_sat, eps))\n                risk_acc += np.power(np.maximum(u, 0.0), risk_p)\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        if scope_num is not None:\n            denom = max(scope_den, eps)\n            scope_frac = np.maximum(scope_num / denom, eps)\n            scores += scope_s * np.log(scope_frac)\n\n        if risk_acc is not None and len(query_term_ids) > 0:\n            # Turn accumulator into a normalized power mean (scale-stable across query length).\n            pm = np.power(risk_acc / float(len(query_term_ids)), 1.0 / max(risk_p, eps))\n            scores += risk_s * pm\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Below are targeted diffs that (1) revert the \u201crisk/AND/missing\u201d complexity that likely caused the fitness drop, (2) restore the strongest prior variant\u2019s behavior (residual-IDF + EDR + soft missing + coord + scope), and (3) fix a likely bug in your vectorized weighting (`(w0 * ridf)**alpha` is not the same as `w0**alpha * ridf` and can distort repeated-term behavior).\n\nRationale (brief):\n- Your current program added several interacting aggregators (risk-sensitive power mean, tanh AND accumulator, explicit missing anti-evidence) that can easily over-regularize and hurt recall@100\u2014especially on datasets like TREC-COVID where recall is already low.\n- The top programs in your history were simpler: positive-evidence LMDirichlet + residual-IDF weights + EDR gate + small missing penalty + coordination + (optional) scope. That\u2019s a coherent probabilistic story and was empirically best.\n- Also, your current `QueryRepr.from_tokens` keeps duplicate terms in `terms`, while `retrieval_score` internally re-counts; better to store unique terms + qtf weights for consistency and less noise.\n\n---\n\n### Change 1 \u2014 simplify Config back toward the best-performing variant (remove risky knobs, restore soft-AND params)\n\n<<<<<<< SEARCH\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n    burstiness_strength: float = 0.35\n\n    # Two-stage background (collection + uniform over vocab).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND query aggregation.\n    and_strength: float = 0.15  # 0 disables\n    and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n\n    # Dirichlet \"anti-evidence\" for missing query terms (tf=0).\n    # Keep slightly smaller by default: too much anti-evidence can hurt recall@100.\n    missing_strength: float = 0.06\n\n    # Query-scope prior: reward explaining a larger fraction of the query's information mass.\n    query_scope_strength: float = 0.07  # 0 disables\n\n    # NEW: Risk-sensitive aggregation (R\u00e9nyi / power mean) over per-term evidence.\n    # Add a small concave utility on term evidence to prefer \"balanced\" multi-term matches\n    # over one-term spikes (improves early precision on multi-aspect queries).\n    risk_strength: float = 0.12  # 0 disables\n    risk_power: float = 0.55     # in (0,1]: smaller => more \"AND-like\"\n    risk_saturation: float = 6.0 # scale inside utility; keeps it stable across corpora\n=======\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength (rare terms trust doc evidence sooner)\n    mu_df_power: float = 0.30\n\n    # Robust background LM\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # Query TF saturation (qtf^alpha)\n    query_tf_power: float = 0.60\n\n    # Query-independent diagnostics\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Soft-AND (gentle): missing-term penalty + coordination reward + scope prior\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    query_scope_strength: float = 0.07  # 0 disables\n\n    # Optional tiny negative-evidence leak (keep recall safe by being tiny)\n    neg_strength: float = 0.06  # 0 disables\n>>>>>>> REPLACE\n\n---\n\n### Change 2 \u2014 make QueryRepr store unique terms + qtf weights (more stable; matches best variants)\n\n<<<<<<< SEARCH\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n=======\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Multinomial query view:\n        - unique terms as support\n        - query term frequency (qtf) as term weight\n        This makes score() consistent with rank() (which uses Counter(query)).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n>>>>>>> REPLACE\n\n---\n\n### Change 3 \u2014 replace retrieval_score with the stronger \u201cresidual-IDF + EDR gate + term-adaptive mu + soft completeness\u201d story\n\nThis is essentially restoring the best-performing backbone while keeping your robust collection LM.\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n    scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        if burst_s > 0.0:\n            idf01_proxy = 1.0 / (1.0 + max(0.0, math.log(1.0 / max(p_collection, eps))))\n            beta = 1.0 - burst_s * (1.0 - idf01_proxy)\n            term_count = term_count ** beta if beta != 1.0 else term_count\n\n        base = math.log((1.0 + term_count / (mu * p_collection + eps)) / ((doc_length + mu) / mu) + eps)\n        base = max(base, 0.0)\n\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n        if scope_s > 0.0:\n            info = math.log1p(1.0 + ridf_s * (spec / max(clipc, eps)))\n            scope_den += info\n            if term_count > 0.0:\n                scope_num += info\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with gentle completeness.\n\n    Backbone: LMDirichlet log Bayes factor vs tempered background, clamped to\n    positive surplus evidence (recall-friendly). Then add:\n      - residual-IDF query weight (query-independent, precomputed in Corpus)\n      - EDR gate (query-independent, precomputed in Corpus)\n      - small missing-term penalty (soft AND), length-normalized\n      - bounded coordination reward\n      - optional query-scope \"information mass\" coverage prior\n      - optional tiny negative-evidence leak (kept very small)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Missing-term evidence is less diagnostic in long documents.\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** float(getattr(Config, \"coverage_len_power\", 0.0))\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = 1.0\n            gate = 1.0\n            ridf_w = 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak tiny negative evidence (optional).\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        # Surplus evidence backbone.\n        pos = max(per_term, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n>>>>>>> REPLACE\n\n---\n\n### Change 4 \u2014 update score_document to pass corpus into retrieval_score (needed by Change 3)\n\n<<<<<<< SEARCH\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n=======\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus)\n>>>>>>> REPLACE\n\n---\n\n### Change 5 \u2014 simplify vectorized scoring to match the new retrieval_score (remove AND/risk/missing/length prior interactions)\n\nThis is the biggest \u201cfitness recovery\u201d lever: keep rank() fast and faithful to the restored model.\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query-scope accumulators: per-doc matched specificity mass vs total query specificity mass.\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Optional risk-sensitive aggregator accumulator\n        risk_s = float(getattr(Config, \"risk_strength\", 0.0))\n        risk_p = float(getattr(Config, \"risk_power\", 0.55))\n        risk_sat = float(getattr(Config, \"risk_saturation\", 6.0))\n        risk_acc = np.zeros(len(candidate_docs), dtype=np.float64) if risk_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += contrib\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                    scores += miss_s * w * miss * base0\n\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n            if scope_num is not None:\n                # Stable per-term \"information mass\" proxy derived from residual IDF weights.\n                info = float(np.log1p(self.corpus._ridf_qweight[term_id]))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if risk_acc is not None:\n                # Concave utility of evidence; then power-mean across terms.\n                # u(x)=log(1+x/sat) is information-like (diminishing returns).\n                u = np.log1p(contrib / max(risk_sat, eps))\n                risk_acc += np.power(np.maximum(u, 0.0), risk_p)\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        if scope_num is not None:\n            denom = max(scope_den, eps)\n            scope_frac = np.maximum(scope_num / denom, eps)\n            scores += scope_s * np.log(scope_frac)\n\n        if risk_acc is not None and len(query_term_ids) > 0:\n            # Turn accumulator into a normalized power mean (scale-stable across query length).\n            pm = np.power(risk_acc / float(len(query_term_ids)), 1.0 / max(risk_p, eps))\n            scores += risk_s * pm\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n>>>>>>> REPLACE\n\n---\n\n### Change 6 \u2014 stop computing burstiness/length prior/tf_beta (dead code after simplification)\n\nThese fields become unused; removing them reduces code size and avoids misleading interactions. We\u2019ll do a minimal safe removal: keep `_length_prior` initialization but set to zeros without computing lognormal penalty; and keep `_tf_beta` as ones without computing idf.\n\nReplace the whole \u201cNEW: IDF-in-[0,1] and burstiness exponent\u2026\u201d block + length prior block with a tiny default.\n\n<<<<<<< SEARCH\n        # NEW: IDF-in-[0,1] and burstiness exponent beta(w) for tf saturation (common terms saturate more).\n        # idf = log((N+1)/(df+1)) is classic; we normalize by max idf to get [0,1] without tuning per dataset.\n        if self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n        else:\n            idf01 = np.zeros(self.vocab_size, dtype=np.float64)\n\n        if burst_s > 0.0:\n            # beta in [1-burst_s, 1]; common terms (low idf01) -> smaller beta -> more saturation\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n=======\n        # Keep placeholders (some evaluators/variants expect these attributes).\n        self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n        self._length_prior = np.zeros(self.N, dtype=np.float64)\n>>>>>>> REPLACE\n\n---\n\nIf you apply just Changes 1\u20135, you should already see fitness recover (and typically exceed 0.4271), because you\u2019re essentially returning to the best-performing family while fixing the vectorized mismatch and query representation inconsistency. Changes 6 is optional but helps with the \u201csimplify / code length\u201d warning."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}