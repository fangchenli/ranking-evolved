{"id": "a44c14e6-b840-47fe-b42d-9f7d9f1af9a5", "code": "\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n    neg_strength: float = 0.06\n    doclen_prior_strength: float = 0.06\n    scope_strength: float = 0.07\n\n    # NEW: two-channel evidence (multinomial tf + Bernoulli presence).\n    # Rationale: in code/math/biomed, many rare tokens are \"one-hit\" indicators; tf beyond 1 adds little.\n    presence_mix: float = 0.12  # 0 disables; interpolate LLRs\n\n    # NEW: coverage-adaptive smoothing: if a doc covers little of the query, be more uncertain about its LM.\n    # This reduces brittle over-scoring of one-term spikes while keeping recall (surplus evidence remains).\n    mu_coverage_strength: float = 0.25  # 0 disables\n    mu_coverage_cap: float = 1.5\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Surplus-evidence LMDirichlet + diagnostics, with two structural upgrades:\n\n      (A) Presence/multinomial fusion: interpolate tf-sensitive LLR with a Bernoulli \"present vs expected\"\n          LLR. This better matches rare/symbolic terms where tf beyond 1 is less meaningful.\n      (B) Coverage-adaptive \u03bc: if the doc covers little of the query, treat its LM as more uncertain\n          (extra smoothing), which reduces one-term spike dominance without harsh penalties.\n\n    Keep existing stabilizers: robust background, tiny negative leak, doclen prior, scope prior.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n    mu_cov_s = float(getattr(Config, \"mu_coverage_strength\", 0.0))\n    mu_cov_cap = float(getattr(Config, \"mu_coverage_cap\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    # Coverage estimate (cheap) to adapt \u03bc in a principled \"uncertainty\" way.\n    if mu_cov_s > 0.0:\n        mq, tq = 0.0, 0.0\n        for term in query_repr.terms:\n            qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n            tq += qtf_raw\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                mq += qtf_raw\n        cov = (mq / tq) if tq > 0.0 else 0.0\n        mu_mult = 1.0 + mu_cov_s * (1.0 - cov)\n        mu_mult = min(mu_mult, mu_cov_cap)\n    else:\n        mu_mult = 1.0\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        present = 1.0 if tf > 0.0 else 0.0\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        # Multinomial (Dirichlet) log-likelihood ratio vs background.\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Bernoulli presence channel: expected presence under background ~ 1-exp(-L pC).\n        if pres_mix > 0.0:\n            p_pres_bg = 1.0 - math.exp(-max(doc_length, 0.0) * max(p_collection, eps))\n            p_pres_bg = min(max(p_pres_bg, eps), 1.0 - eps)\n            llr_pres = math.log((present + eps) / p_pres_bg)\n            llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        pos = max(llr, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (more stable when tf is dominated by a few long/bursty docs).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Mild anti-verbosity prior (applied once; vectorized)\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n        mu_cov_s = float(getattr(Config, \"mu_coverage_strength\", 0.0))\n        mu_cov_cap = float(getattr(Config, \"mu_coverage_cap\", 1.0))\n\n        # Coverage-adaptive \u03bc multiplier (per candidate doc, query-dependent).\n        # Use current matched mass (starts at 0) -> encourages more smoothing early in accumulation.\n        if mu_cov_s > 0.0:\n            cov0 = matched / total_q\n            mu_mult = 1.0 + mu_cov_s * (1.0 - cov0)\n            mu_mult = np.minimum(mu_mult, mu_cov_cap)\n        else:\n            mu_mult = 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            df = float(self.corpus._df[term_id])\n            mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = np.maximum(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Presence channel (Bernoulli): expected presence under bg \u2248 1-exp(-L pC)\n            if pres_mix > 0.0:\n                p_pres_bg = 1.0 - np.exp(-np.maximum(doc_lengths, 0.0) * max(p_collection, eps))\n                p_pres_bg = np.minimum(np.maximum(p_pres_bg, eps), 1.0 - eps)\n                llr_pres = np.log((present + eps) / p_pres_bg)\n                llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            pos = np.maximum(llr, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * present\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += present * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "2d6563db-33be-4b56-ac66-b3180ba014d4", "generation": 6, "timestamp": 1770388250.137726, "iteration_found": 172, "metrics": {"beir_nfcorpus_ndcg@10": 0.32017049106602125, "beir_nfcorpus_recall@100": 0.25581634485543164, "beir_nfcorpus_index_time_ms": 3618.3952500578016, "beir_nfcorpus_query_time_ms": 131.08724995981902, "beir_scifact_ndcg@10": 0.682061179847854, "beir_scifact_recall@100": 0.9208888888888889, "beir_scifact_index_time_ms": 4871.183249983005, "beir_scifact_query_time_ms": 572.5939580006525, "bright_pony_ndcg@10": 0.0926122811055432, "bright_pony_recall@100": 0.3058401959447542, "bright_pony_index_time_ms": 1386.8502080440521, "bright_pony_query_time_ms": 840.3920830460265, "beir_arguana_ndcg@10": 0.30042707796079815, "beir_arguana_recall@100": 0.9379014989293362, "beir_arguana_index_time_ms": 5923.329291981645, "beir_arguana_query_time_ms": 40194.42733400501, "bright_theoremqa_theorems_ndcg@10": 0.028317705858202515, "bright_theoremqa_theorems_recall@100": 0.13815789473684212, "bright_theoremqa_theorems_index_time_ms": 10630.542125087231, "bright_theoremqa_theorems_query_time_ms": 1703.2978750066832, "beir_scidocs_ndcg@10": 0.1488308228167644, "beir_scidocs_recall@100": 0.34965, "beir_scidocs_index_time_ms": 19796.234874986112, "beir_scidocs_query_time_ms": 5162.781832972541, "bright_economics_ndcg@10": 0.147041814126408, "bright_economics_recall@100": 0.3969048742115979, "bright_economics_index_time_ms": 14270.118499989621, "bright_economics_query_time_ms": 4756.56833301764, "bright_biology_ndcg@10": 0.29182654250016354, "bright_biology_recall@100": 0.5625148569706567, "bright_biology_index_time_ms": 15224.77708302904, "bright_biology_query_time_ms": 4488.77212498337, "beir_fiqa_ndcg@10": 0.23869446016459067, "beir_fiqa_recall@100": 0.5412449133513948, "beir_fiqa_index_time_ms": 33679.468250018544, "beir_fiqa_query_time_ms": 9339.61112506222, "bright_earth_science_ndcg@10": 0.32975421995149523, "bright_earth_science_recall@100": 0.6652036635475838, "bright_earth_science_index_time_ms": 32201.635333010927, "bright_earth_science_query_time_ms": 7719.257334014401, "bright_stackoverflow_ndcg@10": 0.18252554952034664, "bright_stackoverflow_recall@100": 0.4846508469830137, "bright_stackoverflow_index_time_ms": 98596.43845900428, "bright_stackoverflow_query_time_ms": 17747.486709035, "beir_trec-covid_ndcg@10": 0.6496396926681167, "beir_trec-covid_recall@100": 0.11029974669735639, "beir_trec-covid_index_time_ms": 134054.47066703346, "beir_trec-covid_query_time_ms": 2215.783291030675, "avg_ndcg@10": 0.28432515313219203, "avg_recall@100": 0.4724228104264047, "combined_score": 0.4348032789675622, "total_index_time_ms": 374253.4432922257, "total_query_time_ms": 94872.05925013404, "total_time_ms": 469125.50254235975, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1750.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n  \n      # Coordination reward (soft AND without harsh penalties)\n      coord_strength: float = 0.12\n  \n      # Omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\n  \n      # Robust background (query-independent)\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n  \n      # NEW: mix token-LM with df/N \"presence LM\" to reduce burstiness domination by a few long docs\n      # (robust on StackOverflow/web/code-like corpora).\n      collection_df_alpha: float = 0.10  # 0 disables\n  \n      # Discriminative diagnostics (query-independent)\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  ... (12 more lines)\nwith:\n  class Config:\n      mu: float = 1750.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      collection_df_alpha: float = 0.10\n  \n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      query_tf_power: float = 0.60\n      neg_strength: float = 0.06\n      doclen_prior_strength: float = 0.06\n      scope_strength: float = 0.07\n  \n      # NEW: two-channel evidence (multinomial tf + Bernoulli presence).\n      # Rationale: in code/math/biomed, many rare tokens are \"one-hit\" indicators; tf beyond 1 adds...\n      presence_mix: float = 0.12  # 0 disables; interpolate LLRs\n  \n      # NEW: coverage-adaptive smoothing: if a doc covers little of the query, be more uncertain ab...\n      # This reduces brittle over-scoring of one-term spikes while keeping recall (surplus evidence...\n      mu_coverage_strength: float = 0.25  # 0 disables\n  ... (1 more lines)\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Surplus-evidence LMDirichlet (recall-friendly) + query-independent diagnostics, with 3 stabil...\n  \n        (1) robust background LM via Corpus._collection_prob (token-LM tempered + optional df-LM mix)\n        (2) tiny negative-evidence leak: demotes boilerplate \"weak mention\" docs without becoming B...\n        (3) mild anti-verbosity prior + query-scope info-mass coverage prior: improves early precision\n            on long docs and multi-aspect queries.\n  \n      The aim is to keep the QL probabilistic story while addressing two common failure modes:\n        - long docs winning by chance term coverage\n        - one-term spikes outranking balanced matches\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n  ... (70 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Surplus-evidence LMDirichlet + diagnostics, with two structural upgrades:\n  \n        (A) Presence/multinomial fusion: interpolate tf-sensitive LLR with a Bernoulli \"present vs ...\n            LLR. This better matches rare/symbolic terms where tf beyond 1 is less meaningful.\n        (B) Coverage-adaptive \u03bc: if the doc covers little of the query, treat its LM as more uncertain\n            (extra smoothing), which reduces one-term spike dominance without harsh penalties.\n  \n      Keep existing stabilizers: robust background, tiny negative leak, doclen prior, scope prior.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n      mu_cov_s = float(getattr(Config, \"mu_coverage_strength\", 0.0))\n      mu_cov_cap = float(getattr(Config, \"mu_coverage_cap\", 1.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n  ... (90 more lines)\nChange 3: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              llr = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  \n              # Tiny negative-evidence leak\n              if neg_s > 0.0:\n                  llr = np.where(llr >= 0.0, llr, neg_s * llr)\n  \n              pos = np.maximum(llr, 0.0)\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf = float(qtf_raw) ** alpha\n  \n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id])\n  \n              miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n              )\n  \n  ... (7 more lines)\nwith:\n  pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n          mu_cov_s = float(getattr(Config, \"mu_coverage_strength\", 0.0))\n          mu_cov_cap = float(getattr(Config, \"mu_coverage_cap\", 1.0))\n  \n          # Coverage-adaptive \u03bc multiplier (per candidate doc, query-dependent).\n          # Use current matched mass (starts at 0) -> encourages more smoothing early in accumulation.\n          if mu_cov_s > 0.0:\n              cov0 = matched / total_q\n              mu_mult = 1.0 + mu_cov_s * (1.0 - cov0)\n              mu_mult = np.minimum(mu_mult, mu_cov_cap)\n          else:\n              mu_mult = 1.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = np.maximum(mu_t, 1.0)\n  \n              llr = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  \n              # Presence channel (Bernoulli): expected presence under bg \u2248 1-exp(-L pC)\n              if pres_mix > 0.0:\n                  p_pres_bg = 1.0 - np.exp(-np.maximum(doc_lengths, 0.0) * max(p_collection, eps))\n                  p_pres_bg = np.minimum(np.maximum(p_pres_bg, eps), 1.0 - eps)\n  ... (27 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3225529126534571, "beir_nfcorpus_recall@100": 0.25573234730670596, "beir_nfcorpus_index_time_ms": 3678.3131669508293, "beir_nfcorpus_query_time_ms": 117.27254197467119, "beir_scifact_ndcg@10": 0.6850328202746198, "beir_scifact_recall@100": 0.9242222222222222, "beir_scifact_index_time_ms": 4881.124250008725, "beir_scifact_query_time_ms": 479.2848749784753, "bright_pony_ndcg@10": 0.08446145874291622, "bright_pony_recall@100": 0.3128219003275701, "bright_pony_index_time_ms": 1398.2523750746623, "bright_pony_query_time_ms": 640.5043330742046, "beir_arguana_ndcg@10": 0.28072315306840306, "beir_arguana_recall@100": 0.9250535331905781, "beir_arguana_index_time_ms": 5949.688625056297, "beir_arguana_query_time_ms": 29713.02058303263, "bright_theoremqa_theorems_ndcg@10": 0.030033558036453543, "bright_theoremqa_theorems_recall@100": 0.1513157894736842, "bright_theoremqa_theorems_index_time_ms": 10711.042708018795, "bright_theoremqa_theorems_query_time_ms": 1299.0819159895182, "beir_scidocs_ndcg@10": 0.14583950541717125, "beir_scidocs_recall@100": 0.3501, "beir_scidocs_index_time_ms": 19684.06645907089, "beir_scidocs_query_time_ms": 4272.434999933466, "bright_economics_ndcg@10": 0.15180783376636683, "bright_economics_recall@100": 0.3856708758910116, "bright_economics_index_time_ms": 14352.094208006747, "bright_economics_query_time_ms": 3579.003749997355, "bright_biology_ndcg@10": 0.3021437450291027, "bright_biology_recall@100": 0.5853814895715764, "bright_biology_index_time_ms": 15205.087042064406, "bright_biology_query_time_ms": 3296.374916099012, "beir_fiqa_ndcg@10": 0.22399970522094756, "beir_fiqa_recall@100": 0.5161584728714358, "beir_fiqa_index_time_ms": 33523.21383403614, "beir_fiqa_query_time_ms": 7684.913499979302, "bright_earth_science_ndcg@10": 0.3228693802842105, "bright_earth_science_recall@100": 0.668181719973916, "bright_earth_science_index_time_ms": 31866.822125040926, "bright_earth_science_query_time_ms": 5434.835124993697, "bright_stackoverflow_ndcg@10": 0.19044794389545325, "bright_stackoverflow_recall@100": 0.49063858178430786, "bright_stackoverflow_index_time_ms": 98987.03729198314, "bright_stackoverflow_query_time_ms": 12620.513999951072, "beir_trec-covid_ndcg@10": 0.6432182230516217, "beir_trec-covid_recall@100": 0.11071608867954748, "beir_trec-covid_index_time_ms": 134701.399417012, "beir_trec-covid_query_time_ms": 1824.4574160780758, "avg_ndcg@10": 0.2819275199533936, "avg_recall@100": 0.47299941844104626, "combined_score": 0.43478503874351576, "total_index_time_ms": 374938.14150232356, "total_query_time_ms": 70961.69795608148, "total_time_ms": 445899.83945840504, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new probabilistic retrieval method** based on Query Likelihood language models. The seed program is a minimal skeleton: document representation, query representation, collection model, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just Dirichlet smoothing with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall@100, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the probabilistic model with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 \u03bc (smoothing parameter), epsilon, and any new parameters you need.\n2. **collection_probability(term, ...)** \u2014 How to compute P(w | C) (collection language model). EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, hierarchical, etc.).\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. Default: Dirichlet smoothing. Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors, query models, multi-field models, term dependencies, etc. You can use multiple sub-signals and combine them, or a single unified formula.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **QL._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **QL**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **QL** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and QL.rank() / QL.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n- Avoid taking log(0) (use max(value, epsilon) before log).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"smoothing prevents zero probabilities\" or \"collection model favors discriminative terms\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different smoothing methods, document priors, query expansion, term dependencies) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n- Probabilistic, information-theoretic, and language modeling principles are good foundations.\n", "user": "# Current Program Information\n- Fitness: 0.4348\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4349 \u2192 0.4348. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1700.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.085\n      miss_cap: float = 1.4\n  \n      # Coordination reward (soft AND without harsh penalties)\n      coord_strength: float = 0.12\n  \n      # Omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\n  \n      # Robust background + discriminative diagnostics (query-independent)\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      # Query TF saturation\n      query_tf_power: float = 0.60\n  \n      # Query-level regularizers\n      scope_strength: float = 0.07\n  ... (9 more lines)\nwith:\n  class Config:\n      mu: float = 1700.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      miss_penalty: float = 0.085\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      query_tf_power: float = 0.60\n      scope_strength: float = 0.07\n      neg_strength: float = 0.06\n  \n      collection_df_alpha: float = 0.10\n      doclen_prior_strength: float = 0.06\n  \n      # NEW: \"coordination via information\" (rarer terms should count more for completeness).\n      # We reward covering the query's IDF-ish information mass rather than raw term count.\n      info_coord_strength: float = 0.10  # 0 disables\n  \n      # NEW: make missing-term penalty depend on term rarity (rarer term miss should hurt more),\n      # but keep it bounded to preserve recall.\n      miss_idf_power: float = 0.65  # 0 disables\nChange 2: Replace:\n  matched_qw = 0.0\n      total_qw = 0.0\n      scope_num = 0.0\n      scope_den = 0.0\n  \n      for term in query_repr.terms:\n          qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n          qtf = qtf_raw**alpha\n          total_qw += qtf_raw\n  \n          tf = float(doc_tf.get(term, 0.0))\n          if tf > 0.0:\n              matched_qw += qtf_raw\n  \n          # Use robust, precomputed collection probability when available.\n          tid = corpus.get_term_id(term) if corpus is not None else None\n          if corpus is not None and tid is not None:\n              p_collection = float(corpus._collection_prob[tid])\n              df = float(corpus._df[tid])\n              gate = float(corpus._edr_gate[tid])\n              ridf_w = float(corpus._ridf_qweight[tid])\n          else:\n              p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n              df, gate, ridf_w = 1.0, 1.0, 1.0\n  \n          mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n          mu_t = max(mu_t, 1.0)\n  \n          per_term = math.log(\n              (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n  ... (27 more lines)\nwith:\n  matched_qw = 0.0\n      total_qw = 0.0\n      matched_info = 0.0\n      total_info = 0.0\n      scope_num = 0.0\n      scope_den = 0.0\n  \n      for term in query_repr.terms:\n          qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n          qtf = qtf_raw**alpha\n          total_qw += qtf_raw\n  \n          tf = float(doc_tf.get(term, 0.0))\n  \n          # Use robust, precomputed collection probability when available.\n          tid = corpus.get_term_id(term) if corpus is not None else None\n          if corpus is not None and tid is not None:\n              p_collection = float(corpus._collection_prob[tid])\n              df = float(corpus._df[tid])\n              gate = float(corpus._edr_gate[tid])\n              ridf_w = float(corpus._ridf_qweight[tid])\n          else:\n              p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n              df, gate, ridf_w = 1.0, 1.0, 1.0\n  \n          # Info-mass proxy for coordination and miss shaping (bounded, stable across domains).\n          info = math.log1p(max(ridf_w, 0.0))\n          total_info += info\n  \n          if tf > 0.0:\n  ... (45 more lines)\nChange 3: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              per_term = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  \n              # Leak a tiny amount of negative evidence (optional).\n              if neg_s > 0.0:\n                  per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n  \n              # Surplus-evidence backbone.\n              pos = np.maximum(per_term, 0.0)\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf = float(qtf_raw) ** alpha\n  \n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id])\n  \n              miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n              )\n  ... (17 more lines)\nwith:\n  # Info-coordination accumulators (vectorized)\n          info_coord_s = float(getattr(Config, \"info_coord_strength\", 0.0))\n          matched_info = np.zeros(len(candidate_docs), dtype=np.float64) if info_coord_s > 0.0 else...\n          total_info = 0.0\n  \n          miss_idf_pow = float(getattr(Config, \"miss_idf_power\", 0.0))\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              per_term = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  \n              # Leak a tiny amount of negative evidence (optional).\n              if neg_s > 0.0:\n                  per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n  \n              pos = np.maximum(per_term, 0.0)\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf = float(qtf_raw) ** alpha\n  \n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id])\n  ... (38 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3212, beir_nfcorpus_recall@100: 0.2555, beir_nfcorpus_index_time_ms: 3576.0568, beir_nfcorpus_query_time_ms: 124.5653, beir_scifact_ndcg@10: 0.6837, beir_scifact_recall@100: 0.9242, beir_scifact_index_time_ms: 4903.6848, beir_scifact_query_time_ms: 499.5710, bright_pony_ndcg@10: 0.0932, bright_pony_recall@100: 0.3341, bright_pony_index_time_ms: 1411.9153, bright_pony_query_time_ms: 679.8782, beir_arguana_ndcg@10: 0.2672, beir_arguana_recall@100: 0.9065, beir_arguana_index_time_ms: 5937.6433, beir_arguana_query_time_ms: 31803.9639, bright_theoremqa_theorems_ndcg@10: 0.0302, bright_theoremqa_theorems_recall@100: 0.1382, bright_theoremqa_theorems_index_time_ms: 10626.6126, bright_theoremqa_theorems_query_time_ms: 1379.1850, beir_scidocs_ndcg@10: 0.1460, beir_scidocs_recall@100: 0.3496, beir_scidocs_index_time_ms: 19852.6161, beir_scidocs_query_time_ms: 4565.8672, bright_economics_ndcg@10: 0.1502, bright_economics_recall@100: 0.3837, bright_economics_index_time_ms: 14617.7882, bright_economics_query_time_ms: 3880.5417, bright_biology_ndcg@10: 0.3049, bright_biology_recall@100: 0.6085, bright_biology_index_time_ms: 15592.4461, bright_biology_query_time_ms: 3619.7672, beir_fiqa_ndcg@10: 0.2217, beir_fiqa_recall@100: 0.5125, beir_fiqa_index_time_ms: 33959.9462, beir_fiqa_query_time_ms: 7761.2329, bright_earth_science_ndcg@10: 0.3238, bright_earth_science_recall@100: 0.6704, bright_earth_science_index_time_ms: 32529.3161, bright_earth_science_query_time_ms: 5996.2939, bright_stackoverflow_ndcg@10: 0.1961, bright_stackoverflow_recall@100: 0.4823, bright_stackoverflow_index_time_ms: 99067.4364, bright_stackoverflow_query_time_ms: 13523.4101, beir_trec-covid_ndcg@10: 0.6514, beir_trec-covid_recall@100: 0.1110, beir_trec-covid_index_time_ms: 134580.0337, beir_trec-covid_query_time_ms: 1883.8108, avg_ndcg@10: 0.2825, avg_recall@100: 0.4730, combined_score: 0.4349, total_index_time_ms: 376655.4957, total_query_time_ms: 75718.0872, total_time_ms: 452373.5829, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # --- Robust background LM (token-LM tempered, plus tiny uniform) ---\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n  \n      # --- Discriminative diagnostics (query-independent) ---\n      # EDR: compares df-based LM vs token-based LM to downweight \"everywhere tokens\".\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      # Residual-IDF query weighting derived from max(0, log(p_doc/p_col))\n      residual_idf_strength: float = 0.90\n  \n      # Query term frequency saturation (avoid long queries / repeats dominating)\n      query_tf_power: float = 0.60\n  \n      # Missing-term penalty (soft AND) + coordination reward\n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n  \n      # NEW: Dirichlet missing-term evidence can be too sharp on multi-aspect queries.\n      # Add a gentler *information-mass* coverage prior:\n  ... (6 more lines)\nwith:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n  \n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      query_tf_power: float = 0.60\n  \n      miss_penalty: float = 0.08\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n  \n      scope_strength: float = 0.07\n      neg_strength: float = 0.06\n  \n      # NEW: robust background for rare/bursty tokens:\n      # mix token-LM with a df-based \"presence LM\" (stabilizes p(w|C) for spiky corpora like StackO...\n      collection_df_alpha: float = 0.10  # 0 disables\n  \n      # NEW: explicit document prior to counter long-doc verbosity and improve early precision.\n      # prior(d) = -strength * log(1 + L_d/avgdl), i.e., mild MDL-ish penalty.\n      doclen_prior_strength: float = 0.06\nChange 2: Replace:\n  # --- Collection LM (tempered) + tiny uniform mixture ---\n          tau = float(getattr(Config, \"collection_temper\", 1.0))\n          base_p = np.zeros(self.vocab_size, dtype=np.float64)\n          for term, tid in self._vocab.items():\n              base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens...\n  \n          if tau != 1.0:\n              tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n              z = float(np.sum(tmp))\n              p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n          else:\n              p_col = np.maximum(base_p, Config.epsilon)\n  \n          gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n          if gamma > 0.0 and self.vocab_size > 0:\n              p_uni = 1.0 / float(self.vocab_size)\n              self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsi...\n          else:\n              self._collection_prob = p_col\nwith:\n  # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n          tau = float(getattr(Config, \"collection_temper\", 1.0))\n          base_p = np.zeros(self.vocab_size, dtype=np.float64)\n          for term, tid in self._vocab.items():\n              base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens...\n  \n          if tau != 1.0:\n              tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n              z = float(np.sum(tmp))\n              p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n          else:\n              p_tf = np.maximum(base_p, Config.epsilon)\n  \n          # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n          if self.N > 0:\n              p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n          else:\n              p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n  \n          mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n          if mix > 0.0:\n              p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n              p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n          else:\n              p_col = p_tf\n  \n          gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n          if gamma > 0.0 and self.vocab_size > 0:\n              p_uni = 1.0 / float(self.vocab_size)\n              self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsi...\n  ... (2 more lines)\nChange 3: Replace:\n  if self.N > 0:\n              p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n          else:\n              p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\nwith:\n  # df-based \"document spread\" LM (used for diagnostics; distinct from token LM)\n          if self.N > 0:\n              p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n          else:\n              p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\nChange 4: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n  \n      Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n      Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n          ridf(w) = max(0, log p_doc(w) - log p_col(w))\n      and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n      (scientific, code/math, web) than raw IDF alone.\n  \n      Keep your:\n        - length-normalized missing-term penalty (soft AND)\n        - coordination reward (bounded)\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n  ... (7 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR-gated LMDirichlet with gentle completeness,\n      plus a mild document-length prior and (optional) tiny negative-evidence leak.\n  \n      Key idea: keep the strong surplus-evidence LM backbone for recall, but stabilize:\n        (1) background LM via df/token mixing (handled in Corpus._collection_prob)\n        (2) ranking of verbose/boilerplate docs via an explicit doc-length prior\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  \n      # Mild MDL-ish prior against verbosity (helps early precision on web/forum corpora).\n      prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n      if prior_s > 0.0:\n  ... (7 more lines)\nChange 5: Replace:\n  p_collection = collection_probability(term, corpus_term_freq, total_tokens)\nwith:\n  # Use precomputed robust background when available (may include df/token mixing + renorm).\n          if corpus is not None and tid is not None:\n              p_collection = float(corpus._collection_prob[tid])\n          else:\n              p_collection = collection_probability(term, corpus_term_freq, total_tokens)\nChange 6: Replace:\n  # NOTE: per_term already contains leaked negatives (if enabled).\n          scores += (w * gate) * per_term + (qtf * miss)\nwith:\n  # Use surplus-evidence backbone; if neg leak enabled, per_term carries small negative signal.\n          scores += (w * gate) * np.maximum(per_term, 0.0) + (qtf * miss)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3226, beir_nfcorpus_recall@100: 0.2559, beir_nfcorpus_index_time_ms: 3593.8085, beir_nfcorpus_query_time_ms: 116.1346, beir_scifact_ndcg@10: 0.6846, beir_scifact_recall@100: 0.9242, beir_scifact_index_time_ms: 4928.3526, beir_scifact_query_time_ms: 465.3982, bright_pony_ndcg@10: 0.0829, bright_pony_recall@100: 0.3017, bright_pony_index_time_ms: 1390.3778, bright_pony_query_time_ms: 647.0179, beir_arguana_ndcg@10: 0.2871, beir_arguana_recall@100: 0.9343, beir_arguana_index_time_ms: 5926.7369, beir_arguana_query_time_ms: 29693.8877, bright_theoremqa_theorems_ndcg@10: 0.0278, bright_theoremqa_theorems_recall@100: 0.1579, bright_theoremqa_theorems_index_time_ms: 10652.2979, bright_theoremqa_theorems_query_time_ms: 1223.5245, beir_scidocs_ndcg@10: 0.1466, beir_scidocs_recall@100: 0.3501, beir_scidocs_index_time_ms: 19799.4122, beir_scidocs_query_time_ms: 4250.6606, bright_economics_ndcg@10: 0.1540, bright_economics_recall@100: 0.3915, bright_economics_index_time_ms: 14302.2690, bright_economics_query_time_ms: 3509.0320, bright_biology_ndcg@10: 0.3012, bright_biology_recall@100: 0.5806, bright_biology_index_time_ms: 15182.6247, bright_biology_query_time_ms: 3282.9243, beir_fiqa_ndcg@10: 0.2244, beir_fiqa_recall@100: 0.5160, beir_fiqa_index_time_ms: 34049.0906, beir_fiqa_query_time_ms: 7822.3285, bright_earth_science_ndcg@10: 0.3277, bright_earth_science_recall@100: 0.6644, bright_earth_science_index_time_ms: 32680.8539, bright_earth_science_query_time_ms: 5648.6894, bright_stackoverflow_ndcg@10: 0.1849, bright_stackoverflow_recall@100: 0.4910, bright_stackoverflow_index_time_ms: 98910.9632, bright_stackoverflow_query_time_ms: 12753.9079, beir_trec-covid_ndcg@10: 0.6477, beir_trec-covid_recall@100: 0.1104, beir_trec-covid_index_time_ms: 133703.5586, beir_trec-covid_query_time_ms: 1891.7935, avg_ndcg@10: 0.2826, avg_recall@100: 0.4732, combined_score: 0.4351, total_index_time_ms: 375120.3460, total_query_time_ms: 71305.2990, total_time_ms: 446425.6450, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1700.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.085\n      miss_cap: float = 1.4\n  \n      # Coordination reward (soft AND without harsh penalties)\n      coord_strength: float = 0.12\n  \n      # Omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\n  \n      # Robust background + discriminative diagnostics (query-independent)\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      # Query TF saturation\n      query_tf_power: float = 0.60\n  \n      # Query-level regularizers\n      scope_strength: float = 0.07\n  ... (9 more lines)\nwith:\n  class Config:\n      mu: float = 1700.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      miss_penalty: float = 0.085\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      query_tf_power: float = 0.60\n      scope_strength: float = 0.07\n      neg_strength: float = 0.06\n  \n      collection_df_alpha: float = 0.10\n      doclen_prior_strength: float = 0.06\n  \n      # NEW: \"Scope-per-\u03bc\" smoothing.\n      # Multi-aspect queries often fail because missing rare terms are overly punished\n      # when \u03bc is small; but increasing \u03bc globally hurts precision.\n      # We instead add extra smoothing mass only when the query has high information mass.\n      scope_mu_strength: float = 0.35   # 0 disables; scales extra \u03bc by query scope\n      scope_mu_cap: float = 1.5         # max multiplier on \u03bc (safety)\n  \n  ... (4 more lines)\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR-gated LMDirichlet with soft completeness,\n      plus a mild MDL-ish document-length prior.\n  \n      Why these changes can help fitness (esp. mixed BEIR + BRIGHT):\n        - Robust background LM (handled in Corpus) reduces \u201cbursty token\u201d artifacts (StackOverflow/...\n        - A doc-length prior reduces verbosity bias, improving nDCG@10 without crushing recall@100.\n        - Keep the surplus-evidence backbone + gentle soft-AND (miss + scope + coord).\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  \n  ... (67 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      QL core: LMDirichlet surplus-evidence + soft-AND, but with two structural upgrades:\n  \n      (1) Scope-conditioned smoothing: add extra \u03bc only when the query has high information mass.\n          Intuition: multi-aspect queries should be evaluated under a \"more uncertain\" doc model,\n          preventing over-penalizing missing rare terms while keeping single-aspect precision.\n  \n      (2) Presence mixing: rare terms often act like binary \"did it appear at all?\" signals.\n          Mix a small Bernoulli-style evidence with the multinomial LM evidence.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n  ... (97 more lines)\nChange 3: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              per_term = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  \n              # Leak a tiny amount of negative evidence (optional).\n              if neg_s > 0.0:\n                  per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n  \n              # Surplus-evidence backbone.\n              pos = np.maximum(per_term, 0.0)\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf = float(qtf_raw) ** alpha\n  \n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id])\n  \n              miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n              )\n  ... (8 more lines)\nwith:\n  pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n          mu_boost_s = float(getattr(Config, \"scope_mu_strength\", 0.0))\n          mu_boost_cap = float(getattr(Config, \"scope_mu_cap\", 1.0))\n  \n          # Precompute scope_den once (query-only)\n          if scope_num is not None:\n              for term_id in query_term_ids:\n                  scope_den += float(np.log1p(float(self.corpus._ridf_qweight[term_id])))\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n  \n              df = float(self.corpus._df[term_id])\n  \n              # Scope-conditioned \u03bc (vectorized): use current scope match estimate if enabled\n              if mu_boost_s > 0.0 and scope_num is not None and scope_den > 0.0:\n                  scope_match = np.maximum(scope_num / max(scope_den, eps), 0.0)\n                  mu_mult = 1.0 + mu_boost_s * (1.0 - scope_match)\n                  mu_mult = np.minimum(mu_mult, mu_boost_cap)\n              else:\n                  mu_mult = 1.0\n  \n              mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = np.maximum(mu_t, 1.0)\n  \n              llr = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  ... (31 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3188, beir_nfcorpus_recall@100: 0.2556, beir_nfcorpus_index_time_ms: 3643.6038, beir_nfcorpus_query_time_ms: 137.7197, beir_scifact_ndcg@10: 0.6868, beir_scifact_recall@100: 0.9242, beir_scifact_index_time_ms: 4949.1120, beir_scifact_query_time_ms: 592.8508, bright_pony_ndcg@10: 0.0957, bright_pony_recall@100: 0.3074, bright_pony_index_time_ms: 1406.1663, bright_pony_query_time_ms: 897.2058, beir_arguana_ndcg@10: 0.3106, beir_arguana_recall@100: 0.9429, beir_arguana_index_time_ms: 5997.6241, beir_arguana_query_time_ms: 43585.0765, bright_theoremqa_theorems_ndcg@10: 0.0269, bright_theoremqa_theorems_recall@100: 0.1425, bright_theoremqa_theorems_index_time_ms: 10779.3784, bright_theoremqa_theorems_query_time_ms: 1825.0776, beir_scidocs_ndcg@10: 0.1495, beir_scidocs_recall@100: 0.3508, beir_scidocs_index_time_ms: 19848.1309, beir_scidocs_query_time_ms: 5461.6043, bright_economics_ndcg@10: 0.1459, bright_economics_recall@100: 0.4007, bright_economics_index_time_ms: 14431.3589, bright_economics_query_time_ms: 5158.5375, bright_biology_ndcg@10: 0.2762, bright_biology_recall@100: 0.5439, bright_biology_index_time_ms: 14936.7571, bright_biology_query_time_ms: 4817.4105, beir_fiqa_ndcg@10: 0.2414, beir_fiqa_recall@100: 0.5480, beir_fiqa_index_time_ms: 34676.1610, beir_fiqa_query_time_ms: 9902.3342, bright_earth_science_ndcg@10: 0.3337, bright_earth_science_recall@100: 0.6611, bright_earth_science_index_time_ms: 32522.8370, bright_earth_science_query_time_ms: 8238.9397, bright_stackoverflow_ndcg@10: 0.1828, bright_stackoverflow_recall@100: 0.4844, bright_stackoverflow_index_time_ms: 99583.0987, bright_stackoverflow_query_time_ms: 19038.3467, beir_trec-covid_ndcg@10: 0.6589, beir_trec-covid_recall@100: 0.1088, beir_trec-covid_index_time_ms: 135007.0583, beir_trec-covid_query_time_ms: 2323.1341, avg_ndcg@10: 0.2856, avg_recall@100: 0.4725, combined_score: 0.4351, total_index_time_ms: 377781.2865, total_query_time_ms: 101978.2373, total_time_ms: 479759.5238, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4352)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Robust background LM (token-LM tempered, plus tiny uniform)\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    # Mix token-LM with df/N \"presence LM\" to reduce burstiness domination by a few long docs.\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # Discriminative diagnostics (query-independent)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation\n    query_tf_power: float = 0.60\n\n    # Soft-AND signals\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07  # 0 disables\n\n    # Leak a tiny fraction of negative evidence (mostly surplus-evidence behavior).\n    neg_strength: float = 0.06  # 0 disables\n\n    # NEW: mild anti-verbosity prior (MDL-ish): long docs match more terms by chance.\n    # Helps nDCG@10 on web/forum/scidocs without directly penalizing missing terms (recall-safe).\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n    # NEW: Risk-sensitive aggregation over query terms.\n    # Treat each term's positive surplus evidence as a \"return\"; penalize spiky one-term wins.\n    # Utility \u2248 sum(pos) - \u03bb * Var(pos), where Var is over query terms.\n    risk_aversion: float = 0.10   # 0 disables; higher => more balanced coverage\n    risk_power: float = 1.0       # apply to pos^p before variance (p<1 dampens spikes)\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior: reduces the tendency of very long docs to win by \"topic coverage\".\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Risk-sensitive accumulators over per-term positive evidence.\n    risk_lam = float(getattr(Config, \"risk_aversion\", 0.0))\n    risk_p = float(getattr(Config, \"risk_power\", 1.0))\n    sum_pos = 0.0\n    sum_pos2 = 0.0\n    n_terms = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n        else:\n            tid = None\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if risk_lam > 0.0:\n            # Use term-weighted positive evidence as the per-term \"return\".\n            # This makes variance reflect uneven contribution across query terms.\n            r = (w * gate) * pos\n            if risk_p != 1.0 and r > 0.0:\n                r = r ** risk_p\n            sum_pos += r\n            sum_pos2 += r * r\n            n_terms += 1.0\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # Risk-sensitive utility: penalize spiky evidence distributions across query terms.\n    if risk_lam > 0.0 and n_terms > 1.0:\n        mean = sum_pos / n_terms\n        var = max(sum_pos2 / n_terms - mean * mean, 0.0)\n        score -= risk_lam * var\n\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (more stable when tf is dominated by a few long/bursty docs).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        # NOTE: optionally mixing token-LM and presence-LM in the collection background\n        # can improve robustness on bursty corpora (forums/code); see Config.collection_df_alpha.\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: query-scope (information-mass) coverage accumulators.\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Risk-sensitive accumulators (over query terms)\n        risk_lam = float(getattr(Config, \"risk_aversion\", 0.0))\n        risk_p = float(getattr(Config, \"risk_power\", 1.0))\n        sum_pos = np.zeros(len(candidate_docs), dtype=np.float64) if risk_lam > 0.0 else None\n        sum_pos2 = np.zeros(len(candidate_docs), dtype=np.float64) if risk_lam > 0.0 else None\n        n_terms = 0.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Optional small leak of negative evidence (prevents \"mention once everywhere\" docs).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            # NOTE: per_term already contains leaked negatives (if enabled).\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if sum_pos is not None:\n                r = (w * gate) * pos\n                if risk_p != 1.0:\n                    r = np.power(np.maximum(r, 0.0), risk_p)\n                sum_pos += r\n                sum_pos2 += r * r\n                n_terms += 1.0\n\n            # Query-scope prior: reward covering the query's \"information mass\".\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Risk-sensitive utility: penalize spiky evidence distributions across query terms.\n        if sum_pos is not None and n_terms > 1.0:\n            mean = sum_pos / n_terms\n            var = np.maximum(sum_pos2 / n_terms - mean * mean, 0.0)\n            scores -= risk_lam * var\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3177), Performs well on beir_nfcorpus_recall@100 (0.2557), Performs well on beir_nfcorpus_index_time_ms (3609.5968), Performs well on beir_nfcorpus_query_time_ms (122.1402), Performs well on beir_scifact_ndcg@10 (0.6868), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4891.5471), Performs well on beir_scifact_query_time_ms (495.1524), Performs well on bright_pony_ndcg@10 (0.0925), Performs well on bright_pony_recall@100 (0.3118), Performs well on bright_pony_index_time_ms (1433.5570), Performs well on bright_pony_query_time_ms (667.5287), Performs well on beir_arguana_ndcg@10 (0.2874), Performs well on beir_arguana_recall@100 (0.9336), Performs well on beir_arguana_index_time_ms (5945.0919), Performs well on beir_arguana_query_time_ms (31132.5275), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0283), Performs well on bright_theoremqa_theorems_recall@100 (0.1579), Performs well on bright_theoremqa_theorems_index_time_ms (10666.7967), Performs well on bright_theoremqa_theorems_query_time_ms (1326.2469), Performs well on beir_scidocs_ndcg@10 (0.1437), Performs well on beir_scidocs_recall@100 (0.3484), Performs well on beir_scidocs_index_time_ms (19666.2561), Performs well on beir_scidocs_query_time_ms (4465.1741), Performs well on bright_economics_ndcg@10 (0.1483), Performs well on bright_economics_recall@100 (0.3980), Performs well on bright_economics_index_time_ms (14363.0852), Performs well on bright_economics_query_time_ms (3599.0189), Performs well on bright_biology_ndcg@10 (0.2994), Performs well on bright_biology_recall@100 (0.5735), Performs well on bright_biology_index_time_ms (15397.5808), Performs well on bright_biology_query_time_ms (3522.1424), Performs well on beir_fiqa_ndcg@10 (0.2182), Performs well on beir_fiqa_recall@100 (0.5168), Performs well on beir_fiqa_index_time_ms (34022.4177), Performs well on beir_fiqa_query_time_ms (8119.7080), Performs well on bright_earth_science_ndcg@10 (0.3179), Performs well on bright_earth_science_recall@100 (0.6667), Performs well on bright_earth_science_index_time_ms (31731.4713), Performs well on bright_earth_science_query_time_ms (5721.0363), Performs well on bright_stackoverflow_ndcg@10 (0.1905), Performs well on bright_stackoverflow_recall@100 (0.4922), Performs well on bright_stackoverflow_index_time_ms (98850.4195), Performs well on bright_stackoverflow_query_time_ms (13407.3280), Performs well on beir_trec-covid_ndcg@10 (0.6342), Performs well on beir_trec-covid_recall@100 (0.1076), Performs well on beir_trec-covid_index_time_ms (134578.5357), Performs well on beir_trec-covid_query_time_ms (2004.5387), Performs well on avg_ndcg@10 (0.2804), Performs well on avg_recall@100 (0.4739), Performs well on combined_score (0.4352), Performs well on total_index_time_ms (375156.3558), Performs well on total_query_time_ms (74582.5421), Performs well on total_time_ms (449738.8979), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4351)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1700.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    miss_penalty: float = 0.085\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n\n    collection_df_alpha: float = 0.10\n    doclen_prior_strength: float = 0.06\n\n    # NEW: \"Scope-per-\u03bc\" smoothing.\n    # Multi-aspect queries often fail because missing rare terms are overly punished\n    # when \u03bc is small; but increasing \u03bc globally hurts precision.\n    # We instead add extra smoothing mass only when the query has high information mass.\n    scope_mu_strength: float = 0.35   # 0 disables; scales extra \u03bc by query scope\n    scope_mu_cap: float = 1.5         # max multiplier on \u03bc (safety)\n\n    # NEW: higher-order evidence for rare terms:\n    # a single occurrence of a rare term can be decisive; approximate this by\n    # mixing a \"presence LM\" component (Bernoulli) into the per-term evidence.\n    presence_mix: float = 0.18        # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    QL core: LMDirichlet surplus-evidence + soft-AND, but with two structural upgrades:\n\n    (1) Scope-conditioned smoothing: add extra \u03bc only when the query has high information mass.\n        Intuition: multi-aspect queries should be evaluated under a \"more uncertain\" doc model,\n        preventing over-penalizing missing rare terms while keeping single-aspect precision.\n\n    (2) Presence mixing: rare terms often act like binary \"did it appear at all?\" signals.\n        Mix a small Bernoulli-style evidence with the multinomial LM evidence.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    # Precompute query \"scope\" once (information mass proxy).\n    # This is reused for both the scope prior and scope-conditioned \u03bc.\n    for term in query_repr.terms:\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if tid is None:\n            continue\n        ridf_w = float(corpus._ridf_qweight[tid])\n        scope_den += math.log1p(ridf_w)\n\n    scope_frac = (scope_num / scope_den) if scope_den > 0.0 else 0.0  # scope_num computed in main loop\n    mu_boost_s = float(getattr(Config, \"scope_mu_strength\", 0.0))\n    mu_boost_cap = float(getattr(Config, \"scope_mu_cap\", 1.0))\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw**alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        present = 1.0 if tf > 0.0 else 0.0\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        # Update scope matched mass (for the actual scope prior, needs presence in doc)\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            if tf > 0.0:\n                scope_num += info\n\n        # Scope-conditioned \u03bc multiplier (cap for safety)\n        # Use *expected* scope: if doc matches more info mass, we can trust its LM a bit more.\n        # Approx: interpolate between 1 and (1 + s*(1-scope_match)).\n        if mu_boost_s > 0.0 and scope_den > 0.0:\n            scope_match = max(scope_num / scope_den, 0.0)\n            mu_mult = 1.0 + mu_boost_s * (1.0 - scope_match)\n            mu_mult = min(mu_mult, mu_boost_cap)\n        else:\n            mu_mult = 1.0\n\n        mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        # Presence evidence: compare \"present\" vs expected presence under background.\n        # Approximate expected presence with 1-exp(-L*pC), and document-side with present.\n        if pres_mix > 0.0:\n            p_pres_bg = 1.0 - math.exp(-max(doc_length, 0.0) * max(p_collection, eps))\n            p_pres_bg = min(max(p_pres_bg, eps), 1.0 - eps)\n            llr_pres = math.log((present + eps) / p_pres_bg)\n            llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n        pos = max(llr, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w) = df/N (stabilizes background under bursty tf distributions).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n        mu_boost_s = float(getattr(Config, \"scope_mu_strength\", 0.0))\n        mu_boost_cap = float(getattr(Config, \"scope_mu_cap\", 1.0))\n\n        # Precompute scope_den once (query-only)\n        if scope_num is not None:\n            for term_id in query_term_ids:\n                scope_den += float(np.log1p(float(self.corpus._ridf_qweight[term_id])))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            df = float(self.corpus._df[term_id])\n\n            # Scope-conditioned \u03bc (vectorized): use current scope match estimate if enabled\n            if mu_boost_s > 0.0 and scope_num is not None and scope_den > 0.0:\n                scope_match = np.maximum(scope_num / max(scope_den, eps), 0.0)\n                mu_mult = 1.0 + mu_boost_s * (1.0 - scope_match)\n                mu_mult = np.minimum(mu_mult, mu_boost_cap)\n            else:\n                mu_mult = 1.0\n\n            mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = np.maximum(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            # Presence mixing (vectorized)\n            if pres_mix > 0.0:\n                p_pres_bg = 1.0 - np.exp(-np.maximum(doc_lengths, 0.0) * max(p_collection, eps))\n                p_pres_bg = np.minimum(np.maximum(p_pres_bg, eps), 1.0 - eps)\n                llr_pres = np.log((present + eps) / p_pres_bg)\n                llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n            pos = np.maximum(llr, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_num += present * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3188), Performs well on beir_nfcorpus_recall@100 (0.2556), Performs well on beir_nfcorpus_index_time_ms (3643.6038), Performs well on beir_nfcorpus_query_time_ms (137.7197), Performs well on beir_scifact_ndcg@10 (0.6868), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4949.1120), Performs well on beir_scifact_query_time_ms (592.8508), Performs well on bright_pony_ndcg@10 (0.0957), Performs well on bright_pony_recall@100 (0.3074), Performs well on bright_pony_index_time_ms (1406.1663), Performs well on bright_pony_query_time_ms (897.2058), Performs well on beir_arguana_ndcg@10 (0.3106), Performs well on beir_arguana_recall@100 (0.9429), Performs well on beir_arguana_index_time_ms (5997.6241), Performs well on beir_arguana_query_time_ms (43585.0765), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0269), Performs well on bright_theoremqa_theorems_recall@100 (0.1425), Performs well on bright_theoremqa_theorems_index_time_ms (10779.3784), Performs well on bright_theoremqa_theorems_query_time_ms (1825.0776), Performs well on beir_scidocs_ndcg@10 (0.1495), Performs well on beir_scidocs_recall@100 (0.3508), Performs well on beir_scidocs_index_time_ms (19848.1309), Performs well on beir_scidocs_query_time_ms (5461.6043), Performs well on bright_economics_ndcg@10 (0.1459), Performs well on bright_economics_recall@100 (0.4007), Performs well on bright_economics_index_time_ms (14431.3589), Performs well on bright_economics_query_time_ms (5158.5375), Performs well on bright_biology_ndcg@10 (0.2762), Performs well on bright_biology_recall@100 (0.5439), Performs well on bright_biology_index_time_ms (14936.7571), Performs well on bright_biology_query_time_ms (4817.4105), Performs well on beir_fiqa_ndcg@10 (0.2414), Performs well on beir_fiqa_recall@100 (0.5480), Performs well on beir_fiqa_index_time_ms (34676.1610), Performs well on beir_fiqa_query_time_ms (9902.3342), Performs well on bright_earth_science_ndcg@10 (0.3337), Performs well on bright_earth_science_recall@100 (0.6611), Performs well on bright_earth_science_index_time_ms (32522.8370), Performs well on bright_earth_science_query_time_ms (8238.9397), Performs well on bright_stackoverflow_ndcg@10 (0.1828), Performs well on bright_stackoverflow_recall@100 (0.4844), Performs well on bright_stackoverflow_index_time_ms (99583.0987), Performs well on bright_stackoverflow_query_time_ms (19038.3467), Performs well on beir_trec-covid_ndcg@10 (0.6589), Performs well on beir_trec-covid_recall@100 (0.1088), Performs well on beir_trec-covid_index_time_ms (135007.0583), Performs well on beir_trec-covid_query_time_ms (2323.1341), Performs well on avg_ndcg@10 (0.2856), Performs well on avg_recall@100 (0.4725), Performs well on combined_score (0.4351), Performs well on total_index_time_ms (377781.2865), Performs well on total_query_time_ms (101978.2373), Performs well on total_time_ms (479759.5238), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4351)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n\n    # NEW: robust background for rare/bursty tokens:\n    # mix token-LM with a df-based \"presence LM\" (stabilizes p(w|C) for spiky corpora like StackOverflow).\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # NEW: explicit document prior to counter long-doc verbosity and improve early precision.\n    # prior(d) = -strength * log(1 + L_d/avgdl), i.e., mild MDL-ish penalty.\n    doclen_prior_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with gentle completeness,\n    plus a mild document-length prior and (optional) tiny negative-evidence leak.\n\n    Key idea: keep the strong surplus-evidence LM backbone for recall, but stabilize:\n      (1) background LM via df/token mixing (handled in Corpus._collection_prob)\n      (2) ranking of verbose/boilerplate docs via an explicit doc-length prior\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish prior against verbosity (helps early precision on web/forum corpora).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Use precomputed robust background when available (may include df/token mixing + renorm).\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n        else:\n            tid = None\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        # df-based \"document spread\" LM (used for diagnostics; distinct from token LM)\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: query-scope (information-mass) coverage accumulators.\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Optional small leak of negative evidence (prevents \"mention once everywhere\" docs).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            # NOTE: per_term already contains leaked negatives (if enabled).\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            # Query-scope prior: reward covering the query's \"information mass\".\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3226), Performs well on beir_nfcorpus_recall@100 (0.2559), Performs well on beir_nfcorpus_index_time_ms (3593.8085), Performs well on beir_nfcorpus_query_time_ms (116.1346), Performs well on beir_scifact_ndcg@10 (0.6846), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4928.3526), Performs well on beir_scifact_query_time_ms (465.3982), Performs well on bright_pony_ndcg@10 (0.0829), Performs well on bright_pony_recall@100 (0.3017), Performs well on bright_pony_index_time_ms (1390.3778), Performs well on bright_pony_query_time_ms (647.0179), Performs well on beir_arguana_ndcg@10 (0.2871), Performs well on beir_arguana_recall@100 (0.9343), Performs well on beir_arguana_index_time_ms (5926.7369), Performs well on beir_arguana_query_time_ms (29693.8877), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0278), Performs well on bright_theoremqa_theorems_recall@100 (0.1579), Performs well on bright_theoremqa_theorems_index_time_ms (10652.2979), Performs well on bright_theoremqa_theorems_query_time_ms (1223.5245), Performs well on beir_scidocs_ndcg@10 (0.1466), Performs well on beir_scidocs_recall@100 (0.3501), Performs well on beir_scidocs_index_time_ms (19799.4122), Performs well on beir_scidocs_query_time_ms (4250.6606), Performs well on bright_economics_ndcg@10 (0.1540), Performs well on bright_economics_recall@100 (0.3915), Performs well on bright_economics_index_time_ms (14302.2690), Performs well on bright_economics_query_time_ms (3509.0320), Performs well on bright_biology_ndcg@10 (0.3012), Performs well on bright_biology_recall@100 (0.5806), Performs well on bright_biology_index_time_ms (15182.6247), Performs well on bright_biology_query_time_ms (3282.9243), Performs well on beir_fiqa_ndcg@10 (0.2244), Performs well on beir_fiqa_recall@100 (0.5160), Performs well on beir_fiqa_index_time_ms (34049.0906), Performs well on beir_fiqa_query_time_ms (7822.3285), Performs well on bright_earth_science_ndcg@10 (0.3277), Performs well on bright_earth_science_recall@100 (0.6644), Performs well on bright_earth_science_index_time_ms (32680.8539), Performs well on bright_earth_science_query_time_ms (5648.6894), Performs well on bright_stackoverflow_ndcg@10 (0.1849), Performs well on bright_stackoverflow_recall@100 (0.4910), Performs well on bright_stackoverflow_index_time_ms (98910.9632), Performs well on bright_stackoverflow_query_time_ms (12753.9079), Performs well on beir_trec-covid_ndcg@10 (0.6477), Performs well on beir_trec-covid_recall@100 (0.1104), Performs well on beir_trec-covid_index_time_ms (133703.5586), Performs well on beir_trec-covid_query_time_ms (1891.7935), Performs well on avg_ndcg@10 (0.2826), Performs well on avg_recall@100 (0.4732), Performs well on combined_score (0.4351), Performs well on total_index_time_ms (375120.3460), Performs well on total_query_time_ms (71305.2990), Performs well on total_time_ms (446425.6450), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4349)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1700.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    miss_penalty: float = 0.085\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n\n    collection_df_alpha: float = 0.10\n    doclen_prior_strength: float = 0.06\n\n    # NEW: \"coordination via information\" (rarer terms should count more for completeness).\n    # We reward covering the query's IDF-ish information mass rather than raw term count.\n    info_coord_strength: float = 0.10  # 0 disables\n\n    # NEW: make missing-term penalty depend on term rarity (rarer term miss should hurt more),\n    # but keep it bounded to preserve recall.\n    miss_idf_power: float = 0.65  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with soft completeness,\n    plus a mild MDL-ish document-length prior.\n\n    Why these changes can help fitness (esp. mixed BEIR + BRIGHT):\n      - Robust background LM (handled in Corpus) reduces \u201cbursty token\u201d artifacts (StackOverflow/forums).\n      - A doc-length prior reduces verbosity bias, improving nDCG@10 without crushing recall@100.\n      - Keep the surplus-evidence backbone + gentle soft-AND (miss + scope + coord).\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior (applied once).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    matched_info = 0.0\n    total_info = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw**alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n\n        # Use robust, precomputed collection probability when available.\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        # Info-mass proxy for coordination and miss shaping (bounded, stable across domains).\n        info = math.log1p(max(ridf_w, 0.0))\n        total_info += info\n\n        if tf > 0.0:\n            matched_qw += qtf_raw\n            matched_info += info\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak a tiny amount of negative evidence (optional).\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        # Surplus-evidence backbone.\n        pos = max(per_term, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Rarity-shaped miss: rarer terms are more diagnostic to be missing.\n            pw = max(p_collection, eps)\n            rarity = (-math.log(pw)) ** float(getattr(Config, \"miss_idf_power\", 0.0)) if Config.miss_idf_power > 0.0 else 1.0\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap) * rarity\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    # Two coordination rewards:\n    #  (a) classic coord over raw qtf mass\n    #  (b) info-coordination: covering rarer terms matters more (helps nDCG@10 on multi-aspect queries).\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    info_coord_s = float(getattr(Config, \"info_coord_strength\", 0.0))\n    if info_coord_s > 0.0 and total_info > 0.0:\n        icoord = matched_info / total_info\n        score += info_coord_s * math.log(1.0 + 9.0 * icoord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w) = df/N (stabilizes background under bursty tf distributions).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        # Info-coordination accumulators (vectorized)\n        info_coord_s = float(getattr(Config, \"info_coord_strength\", 0.0))\n        matched_info = np.zeros(len(candidate_docs), dtype=np.float64) if info_coord_s > 0.0 else None\n        total_info = 0.0\n\n        miss_idf_pow = float(getattr(Config, \"miss_idf_power\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Leak a tiny amount of negative evidence (optional).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            # Rarity-shaped miss penalty (bounded by miss_cap via ridf_w)\n            if miss_idf_pow > 0.0:\n                rarity = (-math.log(max(p_collection, eps))) ** miss_idf_pow\n            else:\n                rarity = 1.0\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap) * rarity,\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if matched_info is not None:\n                info = float(np.log1p(ridf_w))\n                total_info += info\n                matched_info += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if matched_info is not None and total_info > 0.0:\n            icoord = matched_info / max(total_info, eps)\n            scores += info_coord_s * np.log(1.0 + 9.0 * icoord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3212), Performs well on beir_nfcorpus_recall@100 (0.2555), Performs well on beir_nfcorpus_index_time_ms (3576.0568), Performs well on beir_nfcorpus_query_time_ms (124.5653), Performs well on beir_scifact_ndcg@10 (0.6837), Performs well on beir_scifact_recall@100 (0.9242), Performs well on beir_scifact_index_time_ms (4903.6848), Performs well on beir_scifact_query_time_ms (499.5710), Performs well on bright_pony_ndcg@10 (0.0932), Performs well on bright_pony_recall@100 (0.3341), Performs well on bright_pony_index_time_ms (1411.9153), Performs well on bright_pony_query_time_ms (679.8782), Performs well on beir_arguana_ndcg@10 (0.2672), Performs well on beir_arguana_recall@100 (0.9065), Performs well on beir_arguana_index_time_ms (5937.6433), Performs well on beir_arguana_query_time_ms (31803.9639), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0302), Performs well on bright_theoremqa_theorems_recall@100 (0.1382), Performs well on bright_theoremqa_theorems_index_time_ms (10626.6126), Performs well on bright_theoremqa_theorems_query_time_ms (1379.1850), Performs well on beir_scidocs_ndcg@10 (0.1460), Performs well on beir_scidocs_recall@100 (0.3496), Performs well on beir_scidocs_index_time_ms (19852.6161), Performs well on beir_scidocs_query_time_ms (4565.8672), Performs well on bright_economics_ndcg@10 (0.1502), Performs well on bright_economics_recall@100 (0.3837), Performs well on bright_economics_index_time_ms (14617.7882), Performs well on bright_economics_query_time_ms (3880.5417), Performs well on bright_biology_ndcg@10 (0.3049), Performs well on bright_biology_recall@100 (0.6085), Performs well on bright_biology_index_time_ms (15592.4461), Performs well on bright_biology_query_time_ms (3619.7672), Performs well on beir_fiqa_ndcg@10 (0.2217), Performs well on beir_fiqa_recall@100 (0.5125), Performs well on beir_fiqa_index_time_ms (33959.9462), Performs well on beir_fiqa_query_time_ms (7761.2329), Performs well on bright_earth_science_ndcg@10 (0.3238), Performs well on bright_earth_science_recall@100 (0.6704), Performs well on bright_earth_science_index_time_ms (32529.3161), Performs well on bright_earth_science_query_time_ms (5996.2939), Performs well on bright_stackoverflow_ndcg@10 (0.1961), Performs well on bright_stackoverflow_recall@100 (0.4823), Performs well on bright_stackoverflow_index_time_ms (99067.4364), Performs well on bright_stackoverflow_query_time_ms (13523.4101), Performs well on beir_trec-covid_ndcg@10 (0.6514), Performs well on beir_trec-covid_recall@100 (0.1110), Performs well on beir_trec-covid_index_time_ms (134580.0337), Performs well on beir_trec-covid_query_time_ms (1883.8108), Performs well on avg_ndcg@10 (0.2825), Performs well on avg_recall@100 (0.4730), Performs well on combined_score (0.4349), Performs well on total_index_time_ms (376655.4957), Performs well on total_query_time_ms (75718.0872), Performs well on total_time_ms (452373.5829), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4348)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    # Background LM robustness\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    # Mix token-LM background with a df-based \"presence LM\" to reduce burstiness bias\n    # in corpora with very long / code-heavy docs (StackOverflow / web dumps).\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # Query-independent term diagnostics\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation\n    query_tf_power: float = 0.60\n\n    # Soft-AND signals\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    # Query-scope coverage prior over \"information mass\"\n    scope_strength: float = 0.07  # 0 disables\n\n    # Leak a tiny fraction of negative evidence (still mostly surplus-evidence)\n    neg_strength: float = 0.06  # 0 disables\n\n    # Mild anti-verbosity prior: long docs match many terms by chance; improves early precision.\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization,\n    plus (optional) query-scope info-mass prior and tiny leaked negative evidence.\n\n    Why this helps:\n      - Positive-only surplus evidence is recall-friendly, but can over-rank \"one rare hit\" docs.\n      - A query-scope prior rewards covering more of the query's *information mass* (gentle soft-AND).\n      - Leaking a small fraction of negative LLR demotes boilerplate/weak matches without crushing recall.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish prior against verbosity (helps early precision on web/forum corpora).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Query-scope info-mass accumulators (slow path; score() isn't the bottleneck)\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Prefer precomputed, renormalized collection LM when possible (faster + consistent).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Leak tiny negative evidence (optional): keep mostly surplus-evidence behavior.\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            # ridf_w is already bounded in Corpus; log1p makes \"info mass\" robust.\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Coordination + scope accumulators\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Leak tiny negative evidence (optional)\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            pos = np.maximum(per_term, 0.0)\n            w = qtf * ridf_w\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4348)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n\n    # Coordination reward (soft AND without harsh penalties)\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # Robust background (query-independent)\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # NEW: mix token-LM with df/N \"presence LM\" to reduce burstiness domination by a few long docs\n    # (robust on StackOverflow/web/code-like corpora).\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # Discriminative diagnostics (query-independent)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation for repeated terms in long queries\n    query_tf_power: float = 0.60\n\n    # NEW: tiny negative-evidence leak (still mostly surplus-evidence); helps early precision.\n    neg_strength: float = 0.06  # 0 disables\n\n    # NEW: mild anti-verbosity prior (MDL-ish); improves nDCG@10 without strongly hurting recall.\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n    # NEW: query-scope info-mass prior (gentle soft-AND over rare/diagnostic terms).\n    scope_strength: float = 0.07  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Surplus-evidence LMDirichlet (recall-friendly) + query-independent diagnostics, with 3 stabilizers:\n\n      (1) robust background LM via Corpus._collection_prob (token-LM tempered + optional df-LM mix)\n      (2) tiny negative-evidence leak: demotes boilerplate \"weak mention\" docs without becoming BM25-like\n      (3) mild anti-verbosity prior + query-scope info-mass coverage prior: improves early precision\n          on long docs and multi-aspect queries.\n\n    The aim is to keep the QL probabilistic story while addressing two common failure modes:\n      - long docs winning by chance term coverage\n      - one-term spikes outranking balanced matches\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish anti-verbosity prior (helps nDCG@10 on web/forum/scidocs-like corpora)\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Query-scope info-mass coverage (gentle soft-AND; uses bounded ridf weights)\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Mostly surplus evidence, but allow a tiny fraction of negative evidence.\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        pos = max(llr, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (more stable when tf is dominated by a few long/bursty docs).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Mild anti-verbosity prior (applied once; vectorized)\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Tiny negative-evidence leak\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            pos = np.maximum(llr, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4348)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1700.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.085\n    miss_cap: float = 1.4\n\n    # Coordination reward (soft AND without harsh penalties)\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # Robust background + discriminative diagnostics (query-independent)\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation\n    query_tf_power: float = 0.60\n\n    # Query-level regularizers\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n\n    # NEW: mix token-LM with a df-based \"presence LM\" for the collection background.\n    # Intuition: token LM can be dominated by a few long/bursty docs (forums/code);\n    # df/N is stabler. Mixing them improves robustness without learning.\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # NEW: mild document-length prior (MDL-ish anti-verbosity), helps early precision.\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with soft completeness,\n    plus a mild MDL-ish document-length prior.\n\n    Why these changes can help fitness (esp. mixed BEIR + BRIGHT):\n      - Robust background LM (handled in Corpus) reduces \u201cbursty token\u201d artifacts (StackOverflow/forums).\n      - A doc-length prior reduces verbosity bias, improving nDCG@10 without crushing recall@100.\n      - Keep the surplus-evidence backbone + gentle soft-AND (miss + scope + coord).\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior (applied once).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw**alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Use robust, precomputed collection probability when available.\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak a tiny amount of negative evidence (optional).\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        # Surplus-evidence backbone.\n        pos = max(per_term, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w) = df/N (stabilizes background under bursty tf distributions).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            per_term = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Leak a tiny amount of negative evidence (optional).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            # Surplus-evidence backbone.\n            pos = np.maximum(per_term, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4348)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    scope_strength: float = 0.07  # 0 disables\n    neg_strength: float = 0.06  # 0 disables\n\n    # NEW: stabilize p(w|C) on bursty corpora by mixing token LM with a df-based presence LM.\n    # Intuition: token LM can be dominated by a few very long/code-heavy docs; df/N is stabler.\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # NEW: mild anti-verbosity document prior (MDL-ish).\n    # Long docs match more query terms by chance; a small prior often improves nDCG@10.\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with:\n      - surplus-evidence backbone (recall-friendly)\n      - tiny negative-evidence leak (demote boilerplate/weak mentions)\n      - query-scope information-mass coverage prior (improves early precision on multi-aspect queries)\n      - mild anti-verbosity prior (improves nDCG@10 on long-doc corpora)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish prior against verbosity (small -> recall-safe; helps early precision).\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        # Use precomputed term diagnostics when available (gate + ridf weight + robust p(w|C)).\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n        else:\n            gate, ridf_w = 1.0, 1.0\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        per_term = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Mostly surplus evidence, but allow a tiny fraction of negative evidence.\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n        pos = max(per_term, 0.0)\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        # Query-scope info-mass coverage (ridf_w is already bounded; log1p stabilizes).\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stabilizes background when tf is dominated by bursty/long docs).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4352, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Robust background LM (token-LM tempered, plus tiny uniform)\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    # Mix token-LM with df/N \"presence LM\" to reduce burstiness domination by a few long docs.\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # Discriminative diagnostics (query-independent)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation\n    query_tf_power: float = 0.60\n\n    # Soft-AND signals\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07  # 0 disables\n\n    # Leak a tiny fraction of negative evidence (mostly surplus-evidence behavior).\n    neg_strength: float = 0.06  # 0 disables\n\n    # NEW: mild anti-verbosity prior (MDL-ish): long docs match more terms by chance.\n    # Helps nDCG@10 on web/forum/scidocs without directly penalizing missing terms (recall-safe).\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n    # NEW: Risk-sensitive aggregation over query terms.\n    # Treat each term's positive surplus evidence as a \"return\"; penalize spiky one-term wins.\n    # Utility \u2248 sum(pos) - \u03bb * Var(pos), where Var is over query terms.\n    risk_aversion: float = 0.10   # 0 disables; higher => more balanced coverage\n    risk_power: float = 1.0       # apply to pos^p before variance (p<1 dampens spikes)\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR gated LMDirichlet with coverage regularization.\n\n    Backbone: Lucene-style positive-evidence LMDirichlet (keeps recall strong).\n    Improvement: replace hand-shaped IDF \"spec\" with a discriminative diagnostic:\n        ridf(w) = max(0, log p_doc(w) - log p_col(w))\n    and an EDR gate based on log(p_doc/p_col). This is more stable across domains\n    (scientific, code/math, web) than raw IDF alone.\n\n    Keep your:\n      - length-normalized missing-term penalty (soft AND)\n      - coordination reward (bounded)\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild anti-verbosity prior: reduces the tendency of very long docs to win by \"topic coverage\".\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Risk-sensitive accumulators over per-term positive evidence.\n    risk_lam = float(getattr(Config, \"risk_aversion\", 0.0))\n    risk_p = float(getattr(Config, \"risk_power\", 1.0))\n    sum_pos = 0.0\n    sum_pos2 = 0.0\n    n_terms = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        df = float(corpus.get_df(term)) if corpus is not None else 1.0\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        numerator = 1.0 + tf / (mu_t * p_collection + eps)\n        denominator = (doc_length + mu_t) / mu_t\n        per_term = math.log(numerator / denominator + eps)\n\n        # Optional small leak of negative evidence to reduce boilerplate/weak-match ranking.\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term *= neg_s\n\n        pos = max(per_term, 0.0)\n\n        # Discriminative term weighting/gating (use precomputed signals when available)\n        if corpus is not None:\n            tid = corpus.get_term_id(term)\n        else:\n            tid = None\n\n        gate = float(corpus._edr_gate[tid]) if (corpus is not None and tid is not None) else 1.0\n        ridf_w = float(corpus._ridf_qweight[tid]) if (corpus is not None and tid is not None) else 1.0\n\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            # Use ridf_w as a bounded \"information mass\" proxy for the missing-term penalty.\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if risk_lam > 0.0:\n            # Use term-weighted positive evidence as the per-term \"return\".\n            # This makes variance reflect uneven contribution across query terms.\n            r = (w * gate) * pos\n            if risk_p != 1.0 and r > 0.0:\n                r = r ** risk_p\n            sum_pos += r\n            sum_pos2 += r * r\n            n_terms += 1.0\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    # Risk-sensitive utility: penalize spiky evidence distributions across query terms.\n    if risk_lam > 0.0 and n_terms > 1.0:\n        mean = sum_pos / n_terms\n        var = max(sum_pos2 / n_terms - mean * mean, 0.0)\n        score -= risk_lam * var\n\n    # NEW: query-scope prior in slow path (approximate using ridf weights when available).\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    if scope_s > 0.0 and corpus is not None:\n        scope_num = 0.0\n        scope_den = 0.0\n        for term in query_repr.terms:\n            tid = corpus.get_term_id(term)\n            if tid is None:\n                continue\n            info = math.log1p(float(corpus._ridf_qweight[tid]))\n            scope_den += info\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                scope_num += info\n        if scope_den > 0.0:\n            frac = max(scope_num / scope_den, eps)\n            score += scope_s * math.log(frac)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (more stable when tf is dominated by a few long/bursty docs).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        # NOTE: optionally mixing token-LM and presence-LM in the collection background\n        # can improve robustness on bursty corpora (forums/code); see Config.collection_df_alpha.\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Accumulate coordination match mass during scoring (avoid second tf_matrix scan)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # NEW: query-scope (information-mass) coverage accumulators.\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Risk-sensitive accumulators (over query terms)\n        risk_lam = float(getattr(Config, \"risk_aversion\", 0.0))\n        risk_p = float(getattr(Config, \"risk_power\", 1.0))\n        sum_pos = np.zeros(len(candidate_docs), dtype=np.float64) if risk_lam > 0.0 else None\n        sum_pos2 = np.zeros(len(candidate_docs), dtype=np.float64) if risk_lam > 0.0 else None\n        n_terms = 0.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            numerator = 1.0 + tf_row / (mu_t * p_collection + eps)\n            denominator = (doc_lengths + mu_t) / mu_t\n            per_term = np.log(numerator / denominator + eps)\n\n            # Optional small leak of negative evidence (prevents \"mention once everywhere\" docs).\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            pos = np.maximum(per_term, 0.0)\n\n            # Residual-IDF weighting + EDR gate (query-independent)\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            w = qtf * ridf_w\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            # NOTE: per_term already contains leaked negatives (if enabled).\n            scores += (w * gate) * per_term + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if sum_pos is not None:\n                r = (w * gate) * pos\n                if risk_p != 1.0:\n                    r = np.power(np.maximum(r, 0.0), risk_p)\n                sum_pos += r\n                sum_pos2 += r * r\n                n_terms += 1.0\n\n            # Query-scope prior: reward covering the query's \"information mass\".\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        # Risk-sensitive utility: penalize spiky evidence distributions across query terms.\n        if sum_pos is not None and n_terms > 1.0:\n            mean = sum_pos / n_terms\n            var = np.maximum(sum_pos2 / n_terms - mean * mean, 0.0)\n            scores -= risk_lam * var\n\n        # Coordination reward (bounded)\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        # Scope prior (gentler than per-term missing penalties; helps nDCG on multi-aspect queries).\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4342, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10  # mix token LM with df LM\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n    doclen_prior_strength: float = 0.06\n\n    # NEW: two-channel evidence for rare/symbolic terms:\n    # mix multinomial (tf-sensitive) with Bernoulli \"present vs expected present\".\n    presence_mix: float = 0.16  # 0 disables\n\n    # NEW: \"\u03bc by coverage\" \u2014 if doc covers little of query, treat its LM as more uncertain (more smoothing).\n    # This reduces brittle over-penalization and tends to help early precision while keeping recall stable.\n    mu_cov_strength: float = 0.30  # 0 disables\n    mu_cov_cap: float = 1.6\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Two-channel QL (multinomial + Bernoulli presence) with coverage-adaptive smoothing.\n\n    Intuition:\n      - Multinomial LMDirichlet is strong for recall but can be noisy for rare/symbolic terms.\n      - Presence is a better abstraction for \"one hit is decisive\" terms (math/code/biomed).\n      - If a doc covers little of the query, we should be more uncertain about its LM => larger \u03bc.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n    mu_cov_s = float(getattr(Config, \"mu_cov_strength\", 0.0))\n    mu_cov_cap = float(getattr(Config, \"mu_cov_cap\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    # Compute query coverage once to adapt \u03bc (cheap in slow path).\n    matched_qw = 0.0\n    total_qw = 0.0\n    if mu_cov_s > 0.0:\n        for term in query_repr.terms:\n            qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n            total_qw += qtf_raw\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                matched_qw += qtf_raw\n        cov = (matched_qw / total_qw) if total_qw > 0.0 else 0.0\n        mu_mult = 1.0 + mu_cov_s * (1.0 - cov)\n        mu_mult = min(mu_mult, mu_cov_cap)\n        matched_qw = 0.0\n        total_qw = 0.0\n    else:\n        mu_mult = 1.0\n\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        present = 1.0 if tf > 0.0 else 0.0\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n            gate, ridf_w = 1.0, 1.0\n\n        mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Bernoulli presence LLR (expected presence under background \u2248 1-exp(-L pC)).\n        if pres_mix > 0.0:\n            p_pres_bg = 1.0 - math.exp(-max(doc_length, 0.0) * max(p_collection, eps))\n            p_pres_bg = min(max(p_pres_bg, eps), 1.0 - eps)\n            llr_pres = math.log((present + eps) / p_pres_bg)\n            llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        pos = max(llr, 0.0)\n        w = qtf * ridf_w\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Coordination + scope accumulators\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n        mu_cov_s = float(getattr(Config, \"mu_cov_strength\", 0.0))\n        mu_cov_cap = float(getattr(Config, \"mu_cov_cap\", 1.0))\n\n        # Coverage-adaptive \u03bc multiplier (per candidate doc, query-dependent).\n        if mu_cov_s > 0.0:\n            cov = matched / total_q\n            mu_mult = 1.0 + mu_cov_s * (1.0 - cov)\n            mu_mult = np.minimum(mu_mult, mu_cov_cap)\n        else:\n            mu_mult = 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            df = float(self.corpus._df[term_id])\n            mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = np.maximum(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            if pres_mix > 0.0:\n                p_pres_bg = 1.0 - np.exp(-np.maximum(doc_lengths, 0.0) * max(p_collection, eps))\n                p_pres_bg = np.minimum(np.maximum(p_pres_bg, eps), 1.0 - eps)\n                llr_pres = np.log((present + eps) / p_pres_bg)\n                llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            pos = np.maximum(llr, 0.0)\n            w = qtf * ridf_w\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += present * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4287, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1700.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    # Replace hand-shaped IDF weighting with residual-IDF (computed from df vs collection LM).\n    # This tends to generalize better across BEIR + BRIGHT than raw IDF.\n    residual_idf_strength: float = 0.90\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Robustify background LM (flatten + tiny uniform hedge).\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # Query TF saturation (repeated terms shouldn't dominate).\n    query_tf_power: float = 0.60\n\n    miss_penalty: float = 0.085\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    # Gentle query-level completeness regularizer: reward covering query \"information mass\".\n    scope_strength: float = 0.07\n\n    # Tiny negative-evidence leak: demote boilerplate \"mentions\" without killing recall.\n    neg_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w|C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    Tempering flattens overly-peaky collections; a tiny uniform mixture hedges rare-token spikes.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Residual-IDF + EDR-gated LMDirichlet with soft completeness.\n\n    Core story:\n      - LMDirichlet LLR is a per-term log Bayes factor vs background.\n      - Use *surplus evidence* (clamp negatives) for recall, but keep:\n          (a) gentle missing-term penalty (length-normalized)\n          (b) bounded coordination reward\n          (c) query-scope \"info mass\" coverage prior (helps nDCG on multi-aspect queries)\n          (d) tiny negative leak to demote boilerplate \"mentions\"\n      - Replace raw IDF weighting with residual-IDF from p_doc(df) vs p_col(tf).\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Leak tiny negative evidence (optional): keep mostly surplus-evidence behavior.\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        pos = max(llr, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered + tiny uniform mixture (properly renormalized) ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            pos = np.maximum(llr, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4345, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    # Background LM robustness\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # Query-independent term diagnostics\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation\n    query_tf_power: float = 0.60\n\n    # Soft-AND signals\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n    scope_strength: float = 0.07  # 0 disables\n    neg_strength: float = 0.06  # 0 disables\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n    # NEW: two-stage generative view (multinomial tf + Bernoulli presence).\n    # Many rare/symbolic terms behave more like \"did it appear at all?\" than \"how many times?\".\n    presence_mix: float = 0.14  # 0 disables; mixes Bernoulli LLR into multinomial LLR\n\n    # NEW: coverage-adaptive smoothing mass.\n    # When a document covers little of the query, we should be uncertain about its LM and\n    # avoid overly sharp missing-term penalties; when coverage is high, trust the LM more.\n    mu_coverage_strength: float = 0.35  # 0 disables\n    mu_coverage_cap: float = 1.6        # safety cap on \u03bc multiplier\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback used outside Corpus precomputation:\n      p_t(w) \u221d p(w)^tau  (tau<1 flattens overly-peaky collections).\n    Exact renormalization is done in Corpus for speed/consistency.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Two-channel QL:\n\n    - Multinomial LMDirichlet (tf evidence) is great for recall but can over-score verbosity.\n    - Bernoulli \"presence\" evidence helps rare/symbolic terms (math/code/biomed) where 1 hit matters.\n    - Coverage-adaptive \u03bc: if the document matches little of the query, treat its LM as more uncertain\n      (extra smoothing), which reduces brittle missing-term effects and improves early precision.\n\n    We keep: residual-IDF weighting, EDR gate, soft-miss, coord, scope prior, and mild doclen prior.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n    mu_cov_s = float(getattr(Config, \"mu_coverage_strength\", 0.0))\n    mu_cov_cap = float(getattr(Config, \"mu_coverage_cap\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    scope_num = 0.0\n    scope_den = 0.0\n\n    # First pass: compute an inexpensive coverage estimate (over qtf mass).\n    # This is used only to adapt \u03bc (query-document interaction), not as a separate signal.\n    if mu_cov_s > 0.0:\n        for term in query_repr.terms:\n            qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n            total_qw += qtf_raw\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                matched_qw += qtf_raw\n        cov = (matched_qw / total_qw) if total_qw > 0.0 else 0.0\n        mu_mult = 1.0 + mu_cov_s * (1.0 - cov)\n        mu_mult = min(mu_mult, mu_cov_cap)\n        # reset accumulators for main loop (so coord stays exact)\n        matched_qw = 0.0\n        total_qw = 0.0\n    else:\n        mu_mult = 1.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        present = 1.0 if tf > 0.0 else 0.0\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df = float(corpus.get_df(term)) if corpus is not None else 1.0\n            gate, ridf_w = 1.0, 1.0\n\n        mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        # Multinomial LLR (Dirichlet)\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Bernoulli presence LLR: compare present vs expected presence under background.\n        if pres_mix > 0.0:\n            p_pres_bg = 1.0 - math.exp(-max(doc_length, 0.0) * max(p_collection, eps))\n            p_pres_bg = min(max(p_pres_bg, eps), 1.0 - eps)\n            llr_pres = math.log((present + eps) / p_pres_bg)\n            llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        pos = max(llr, 0.0)\n\n        w = qtf * ridf_w\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (stable when tf is dominated by a few long/bursty docs)\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent term diagnostics (p_doc vs p_col) ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        # Hoist length factor (used in missing-term evidence)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        # Coordination + scope accumulators\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n        mu_cov_s = float(getattr(Config, \"mu_coverage_strength\", 0.0))\n        mu_cov_cap = float(getattr(Config, \"mu_coverage_cap\", 1.0))\n\n        # Coverage-adaptive \u03bc: based on candidate doc's qtf coverage mass.\n        if mu_cov_s > 0.0:\n            cov = matched / total_q\n            mu_mult = 1.0 + mu_cov_s * (1.0 - cov)\n            mu_mult = np.minimum(mu_mult, mu_cov_cap)\n        else:\n            mu_mult = 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            df = float(self.corpus._df[term_id])\n            mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = np.maximum(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Presence mixing (Bernoulli channel)\n            if pres_mix > 0.0:\n                p_pres_bg = 1.0 - np.exp(-np.maximum(doc_lengths, 0.0) * max(p_collection, eps))\n                p_pres_bg = np.minimum(np.maximum(p_pres_bg, eps), 1.0 - eps)\n                llr_pres = np.log((present + eps) / p_pres_bg)\n                llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            pos = np.maximum(llr, 0.0)\n            w = qtf * ridf_w\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += present * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n\n    # Coordination reward (soft AND without harsh penalties)\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # Robust background (query-independent)\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # NEW: mix token-LM with df/N \"presence LM\" to reduce burstiness domination by a few long docs\n    # (robust on StackOverflow/web/code-like corpora).\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # Discriminative diagnostics (query-independent)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation for repeated terms in long queries\n    query_tf_power: float = 0.60\n\n    # NEW: tiny negative-evidence leak (still mostly surplus-evidence); helps early precision.\n    neg_strength: float = 0.06  # 0 disables\n\n    # NEW: mild anti-verbosity prior (MDL-ish); improves nDCG@10 without strongly hurting recall.\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n    # NEW: query-scope info-mass prior (gentle soft-AND over rare/diagnostic terms).\n    scope_strength: float = 0.07  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Surplus-evidence LMDirichlet (recall-friendly) + query-independent diagnostics, with 3 stabilizers:\n\n      (1) robust background LM via Corpus._collection_prob (token-LM tempered + optional df-LM mix)\n      (2) tiny negative-evidence leak: demotes boilerplate \"weak mention\" docs without becoming BM25-like\n      (3) mild anti-verbosity prior + query-scope info-mass coverage prior: improves early precision\n          on long docs and multi-aspect queries.\n\n    The aim is to keep the QL probabilistic story while addressing two common failure modes:\n      - long docs winning by chance term coverage\n      - one-term spikes outranking balanced matches\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish anti-verbosity prior (helps nDCG@10 on web/forum/scidocs-like corpora)\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Query-scope info-mass coverage (gentle soft-AND; uses bounded ridf weights)\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Mostly surplus evidence, but allow a tiny fraction of negative evidence.\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        pos = max(llr, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w)=df/N (more stable when tf is dominated by a few long/bursty docs).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Mild anti-verbosity prior (applied once; vectorized)\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Tiny negative-evidence leak\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            pos = np.maximum(llr, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # Base Dirichlet smoothing mass\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n\n    # Term-adaptive prior strength\n    mu_df_power: float = 0.30\n\n    # Missing-term penalty (soft AND)\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n\n    # Coordination reward (soft AND without harsh penalties)\n    coord_strength: float = 0.12\n\n    # Omissions in long docs are less diagnostic than in short docs.\n    coverage_len_power: float = 0.35\n\n    # Robust background (query-independent)\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    # NEW: mix token-LM with df/N \"presence LM\" to reduce burstiness domination by a few long docs\n    # (robust on StackOverflow/web/code-like corpora).\n    collection_df_alpha: float = 0.10  # 0 disables\n\n    # Discriminative diagnostics (query-independent)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    # Query TF saturation for repeated terms in long queries\n    query_tf_power: float = 0.60\n\n    # NEW: tiny negative-evidence leak (still mostly surplus-evidence); helps early precision.\n    neg_strength: float = 0.06  # 0 disables\n\n    # NEW: mild anti-verbosity prior (MDL-ish); improves nDCG@10 without strongly hurting recall.\n    doclen_prior_strength: float = 0.06  # 0 disables\n\n    # NEW: query-scope info-mass prior (gentle soft-AND over rare/diagnostic terms).\n    scope_strength: float = 0.07  # 0 disables\n=======\nclass Config:\n    mu: float = 1750.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    miss_penalty: float = 0.08\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    collection_df_alpha: float = 0.10\n\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n    neg_strength: float = 0.06\n    doclen_prior_strength: float = 0.06\n    scope_strength: float = 0.07\n\n    # NEW: two-channel evidence (multinomial tf + Bernoulli presence).\n    # Rationale: in code/math/biomed, many rare tokens are \"one-hit\" indicators; tf beyond 1 adds little.\n    presence_mix: float = 0.12  # 0 disables; interpolate LLRs\n\n    # NEW: coverage-adaptive smoothing: if a doc covers little of the query, be more uncertain about its LM.\n    # This reduces brittle over-scoring of one-term spikes while keeping recall (surplus evidence remains).\n    mu_coverage_strength: float = 0.25  # 0 disables\n    mu_coverage_cap: float = 1.5\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Surplus-evidence LMDirichlet (recall-friendly) + query-independent diagnostics, with 3 stabilizers:\n\n      (1) robust background LM via Corpus._collection_prob (token-LM tempered + optional df-LM mix)\n      (2) tiny negative-evidence leak: demotes boilerplate \"weak mention\" docs without becoming BM25-like\n      (3) mild anti-verbosity prior + query-scope info-mass coverage prior: improves early precision\n          on long docs and multi-aspect queries.\n\n    The aim is to keep the QL probabilistic story while addressing two common failure modes:\n      - long docs winning by chance term coverage\n      - one-term spikes outranking balanced matches\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    # Mild MDL-ish anti-verbosity prior (helps nDCG@10 on web/forum/scidocs-like corpora)\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n\n    # Query-scope info-mass coverage (gentle soft-AND; uses bounded ridf weights)\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Mostly surplus evidence, but allow a tiny fraction of negative evidence.\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        pos = max(llr, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    Surplus-evidence LMDirichlet + diagnostics, with two structural upgrades:\n\n      (A) Presence/multinomial fusion: interpolate tf-sensitive LLR with a Bernoulli \"present vs expected\"\n          LLR. This better matches rare/symbolic terms where tf beyond 1 is less meaningful.\n      (B) Coverage-adaptive \u03bc: if the doc covers little of the query, treat its LM as more uncertain\n          (extra smoothing), which reduces one-term spike dominance without harsh penalties.\n\n    Keep existing stabilizers: robust background, tiny negative leak, doclen prior, scope prior.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n    mu_cov_s = float(getattr(Config, \"mu_coverage_strength\", 0.0))\n    mu_cov_cap = float(getattr(Config, \"mu_coverage_cap\", 1.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    # Coverage estimate (cheap) to adapt \u03bc in a principled \"uncertainty\" way.\n    if mu_cov_s > 0.0:\n        mq, tq = 0.0, 0.0\n        for term in query_repr.terms:\n            qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n            tq += qtf_raw\n            if float(doc_tf.get(term, 0.0)) > 0.0:\n                mq += qtf_raw\n        cov = (mq / tq) if tq > 0.0 else 0.0\n        mu_mult = 1.0 + mu_cov_s * (1.0 - cov)\n        mu_mult = min(mu_mult, mu_cov_cap)\n    else:\n        mu_mult = 1.0\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw ** alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        present = 1.0 if tf > 0.0 else 0.0\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        # Multinomial (Dirichlet) log-likelihood ratio vs background.\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        # Bernoulli presence channel: expected presence under background ~ 1-exp(-L pC).\n        if pres_mix > 0.0:\n            p_pres_bg = 1.0 - math.exp(-max(doc_length, 0.0) * max(p_collection, eps))\n            p_pres_bg = min(max(p_pres_bg, eps), 1.0 - eps)\n            llr_pres = math.log((present + eps) / p_pres_bg)\n            llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        pos = max(llr, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            scope_den += info\n            if tf > 0.0:\n                scope_num += info\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            df = float(self.corpus._df[term_id])\n            mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = max(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Tiny negative-evidence leak\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            pos = np.maximum(llr, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n=======\n        pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n        mu_cov_s = float(getattr(Config, \"mu_coverage_strength\", 0.0))\n        mu_cov_cap = float(getattr(Config, \"mu_coverage_cap\", 1.0))\n\n        # Coverage-adaptive \u03bc multiplier (per candidate doc, query-dependent).\n        # Use current matched mass (starts at 0) -> encourages more smoothing early in accumulation.\n        if mu_cov_s > 0.0:\n            cov0 = matched / total_q\n            mu_mult = 1.0 + mu_cov_s * (1.0 - cov0)\n            mu_mult = np.minimum(mu_mult, mu_cov_cap)\n        else:\n            mu_mult = 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            df = float(self.corpus._df[term_id])\n            mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = np.maximum(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            # Presence channel (Bernoulli): expected presence under bg \u2248 1-exp(-L pC)\n            if pres_mix > 0.0:\n                p_pres_bg = 1.0 - np.exp(-np.maximum(doc_lengths, 0.0) * max(p_collection, eps))\n                p_pres_bg = np.minimum(np.maximum(p_pres_bg, eps), 1.0 - eps)\n                llr_pres = np.log((present + eps) / p_pres_bg)\n                llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            pos = np.maximum(llr, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * present\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_den += info\n                scope_num += present * info\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}