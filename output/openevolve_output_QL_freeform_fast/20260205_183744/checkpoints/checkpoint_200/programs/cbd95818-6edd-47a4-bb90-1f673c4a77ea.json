{"id": "cbd95818-6edd-47a4-bb90-1f673c4a77ea", "code": "\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1700.0\n    epsilon: float = 1e-9\n    mu_df_power: float = 0.30\n\n    miss_penalty: float = 0.085\n    miss_cap: float = 1.4\n    coord_strength: float = 0.12\n    coverage_len_power: float = 0.35\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n    residual_idf_strength: float = 0.90\n\n    query_tf_power: float = 0.60\n    scope_strength: float = 0.07\n    neg_strength: float = 0.06\n\n    collection_df_alpha: float = 0.10\n    doclen_prior_strength: float = 0.06\n\n    # NEW: \"Scope-per-\u03bc\" smoothing.\n    # Multi-aspect queries often fail because missing rare terms are overly punished\n    # when \u03bc is small; but increasing \u03bc globally hurts precision.\n    # We instead add extra smoothing mass only when the query has high information mass.\n    scope_mu_strength: float = 0.35   # 0 disables; scales extra \u03bc by query scope\n    scope_mu_cap: float = 1.5         # max multiplier on \u03bc (safety)\n\n    # NEW: higher-order evidence for rare terms:\n    # a single occurrence of a rare term can be decisive; approximate this by\n    # mixing a \"presence LM\" component (Bernoulli) into the per-term evidence.\n    presence_mix: float = 0.18        # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Fallback: tempered token LM p_t(w) \u221d p(w)^tau (exact renorm done in Corpus).\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    return max(p ** tau, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Keep order for scoring loop, but weights are keyed by unique term.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query model:\n        - use term counts (qtf) so repeated terms add evidence\n        - but score over unique terms to avoid double-iterating the same term in retrieval_score()\n          (rank() already passes qtf via query_term_weights; this improves score() consistency).\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    corpus: Corpus | None = None,\n) -> float:\n    \"\"\"\n    QL core: LMDirichlet surplus-evidence + soft-AND, but with two structural upgrades:\n\n    (1) Scope-conditioned smoothing: add extra \u03bc only when the query has high information mass.\n        Intuition: multi-aspect queries should be evaluated under a \"more uncertain\" doc model,\n        preventing over-penalizing missing rare terms while keeping single-aspect precision.\n\n    (2) Presence mixing: rare terms often act like binary \"did it appear at all?\" signals.\n        Mix a small Bernoulli-style evidence with the multinomial LM evidence.\n    \"\"\"\n    base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n    scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n    pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n    score = 0.0\n\n    if corpus is not None and corpus.N > 0:\n        N = float(corpus.N)\n        avgdl = float(corpus.avgdl)\n    else:\n        N = 1.0\n        avgdl = max(doc_length, 1.0)\n\n    prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n    if prior_s > 0.0:\n        score -= prior_s * math.log(1.0 + max(doc_length, 0.0) / max(avgdl, 1.0) + eps)\n\n    len_factor = (max(avgdl, 1.0) / max(doc_length, 1.0)) ** Config.coverage_len_power\n    len_factor = min(1.0, max(len_factor, 0.0))\n\n    matched_qw = 0.0\n    total_qw = 0.0\n    scope_num = 0.0\n    scope_den = 0.0\n\n    # Precompute query \"scope\" once (information mass proxy).\n    # This is reused for both the scope prior and scope-conditioned \u03bc.\n    for term in query_repr.terms:\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if tid is None:\n            continue\n        ridf_w = float(corpus._ridf_qweight[tid])\n        scope_den += math.log1p(ridf_w)\n\n    scope_frac = (scope_num / scope_den) if scope_den > 0.0 else 0.0  # scope_num computed in main loop\n    mu_boost_s = float(getattr(Config, \"scope_mu_strength\", 0.0))\n    mu_boost_cap = float(getattr(Config, \"scope_mu_cap\", 1.0))\n\n    for term in query_repr.terms:\n        qtf_raw = float(query_repr.term_weights.get(term, 1.0))\n        qtf = qtf_raw**alpha\n        total_qw += qtf_raw\n\n        tf = float(doc_tf.get(term, 0.0))\n        present = 1.0 if tf > 0.0 else 0.0\n        if tf > 0.0:\n            matched_qw += qtf_raw\n\n        tid = corpus.get_term_id(term) if corpus is not None else None\n        if corpus is not None and tid is not None:\n            p_collection = float(corpus._collection_prob[tid])\n            df = float(corpus._df[tid])\n            gate = float(corpus._edr_gate[tid])\n            ridf_w = float(corpus._ridf_qweight[tid])\n        else:\n            p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n            df, gate, ridf_w = 1.0, 1.0, 1.0\n\n        # Update scope matched mass (for the actual scope prior, needs presence in doc)\n        if scope_s > 0.0 and ridf_w > 0.0:\n            info = math.log1p(ridf_w)\n            if tf > 0.0:\n                scope_num += info\n\n        # Scope-conditioned \u03bc multiplier (cap for safety)\n        # Use *expected* scope: if doc matches more info mass, we can trust its LM a bit more.\n        # Approx: interpolate between 1 and (1 + s*(1-scope_match)).\n        if mu_boost_s > 0.0 and scope_den > 0.0:\n            scope_match = max(scope_num / scope_den, 0.0)\n            mu_mult = 1.0 + mu_boost_s * (1.0 - scope_match)\n            mu_mult = min(mu_mult, mu_boost_cap)\n        else:\n            mu_mult = 1.0\n\n        mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n        mu_t = max(mu_t, 1.0)\n\n        llr = math.log(\n            (1.0 + tf / (mu_t * p_collection + eps)) / ((doc_length + mu_t) / mu_t) + eps\n        )\n\n        if neg_s > 0.0 and llr < 0.0:\n            llr *= neg_s\n\n        # Presence evidence: compare \"present\" vs expected presence under background.\n        # Approximate expected presence with 1-exp(-L*pC), and document-side with present.\n        if pres_mix > 0.0:\n            p_pres_bg = 1.0 - math.exp(-max(doc_length, 0.0) * max(p_collection, eps))\n            p_pres_bg = min(max(p_pres_bg, eps), 1.0 - eps)\n            llr_pres = math.log((present + eps) / p_pres_bg)\n            llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n        pos = max(llr, 0.0)\n\n        miss = 0.0\n        if tf <= 0.0:\n            miss = -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap)\n\n        score += (qtf * ridf_w * gate) * pos + (qtf * miss)\n\n    if total_qw > 0.0:\n        coord = matched_qw / total_qw\n        score += Config.coord_strength * math.log(1.0 + 9.0 * coord + eps)\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, corpus=corpus\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # --- Collection LM: tempered token-LM, optionally mixed with df-LM, plus tiny uniform ---\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            base_p[tid] = float(self.corpus_term_freq.get(term, 0)) / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_tf = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_tf = np.maximum(base_p, Config.epsilon)\n\n        # Presence LM: p_df(w) = df/N (stabilizes background under bursty tf distributions).\n        if self.N > 0:\n            p_df = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_df = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        mix = float(getattr(Config, \"collection_df_alpha\", 0.0))\n        if mix > 0.0:\n            p_col = np.maximum((1.0 - mix) * p_tf + mix * p_df, Config.epsilon)\n            p_col = p_col / max(float(np.sum(p_col)), Config.epsilon)\n        else:\n            p_col = p_tf\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # --- Precompute query-independent diagnostics: p_doc vs p_col ---\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        N = float(self.corpus.N) if self.corpus.N > 0 else 1.0\n\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n        scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n\n        # Length-normalized missing-term penalty factor (hoisted)\n        avgdl = max(float(self.corpus.avgdl), 1.0)\n        len_factor = (avgdl / np.maximum(doc_lengths, 1.0)) ** Config.coverage_len_power\n        len_factor = np.minimum(1.0, np.maximum(len_factor, 0.0))\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        total_q = float(np.sum(query_term_weights)) if query_term_weights is not None else float(len(query_term_ids))\n        total_q = total_q if total_q > 0.0 else 1.0\n\n        # Query-scope info-mass coverage accumulators\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Mild anti-verbosity prior (vectorized; applied once).\n        prior_s = float(getattr(Config, \"doclen_prior_strength\", 0.0))\n        if prior_s > 0.0:\n            scores -= prior_s * np.log(1.0 + doc_lengths / max(float(self.corpus.avgdl), 1.0) + eps)\n\n        pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n        mu_boost_s = float(getattr(Config, \"scope_mu_strength\", 0.0))\n        mu_boost_cap = float(getattr(Config, \"scope_mu_cap\", 1.0))\n\n        # Precompute scope_den once (query-only)\n        if scope_num is not None:\n            for term_id in query_term_ids:\n                scope_den += float(np.log1p(float(self.corpus._ridf_qweight[term_id])))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            df = float(self.corpus._df[term_id])\n\n            # Scope-conditioned \u03bc (vectorized): use current scope match estimate if enabled\n            if mu_boost_s > 0.0 and scope_num is not None and scope_den > 0.0:\n                scope_match = np.maximum(scope_num / max(scope_den, eps), 0.0)\n                mu_mult = 1.0 + mu_boost_s * (1.0 - scope_match)\n                mu_mult = np.minimum(mu_mult, mu_boost_cap)\n            else:\n                mu_mult = 1.0\n\n            mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n            mu_t = np.maximum(mu_t, 1.0)\n\n            llr = np.log(\n                (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n            )\n\n            if neg_s > 0.0:\n                llr = np.where(llr >= 0.0, llr, neg_s * llr)\n\n            # Presence mixing (vectorized)\n            if pres_mix > 0.0:\n                p_pres_bg = 1.0 - np.exp(-np.maximum(doc_lengths, 0.0) * max(p_collection, eps))\n                p_pres_bg = np.minimum(np.maximum(p_pres_bg, eps), 1.0 - eps)\n                llr_pres = np.log((present + eps) / p_pres_bg)\n                llr = (1.0 - pres_mix) * llr + pres_mix * llr_pres\n\n            pos = np.maximum(llr, 0.0)\n\n            qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n            qtf = float(qtf_raw) ** alpha\n\n            ridf_w = float(self.corpus._ridf_qweight[term_id])\n            gate = float(self.corpus._edr_gate[term_id])\n\n            miss = np.where(\n                tf_row > 0.0,\n                0.0,\n                -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n            )\n\n            scores += (qtf * ridf_w * gate) * pos + (qtf * miss)\n            matched += float(qtf_raw) * (tf_row > 0.0)\n\n            if scope_num is not None:\n                info = float(np.log1p(ridf_w))\n                scope_num += present * info\n\n        coord = matched / total_q\n        scores += Config.coord_strength * np.log(1.0 + 9.0 * coord + eps)\n\n        if scope_num is not None and scope_den > 0.0:\n            frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += scope_s * np.log(frac)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "e66efbbc-31bc-4325-aabe-c399d82482e2", "generation": 7, "timestamp": 1770389766.0268228, "iteration_found": 0, "metrics": {"beir_nfcorpus_ndcg@10": 0.318765385176372, "beir_nfcorpus_recall@100": 0.2556348830982833, "beir_nfcorpus_index_time_ms": 3643.603750038892, "beir_nfcorpus_query_time_ms": 137.71970802918077, "beir_scifact_ndcg@10": 0.6867679776926799, "beir_scifact_recall@100": 0.9242222222222222, "beir_scifact_index_time_ms": 4949.112040922046, "beir_scifact_query_time_ms": 592.8507919888943, "bright_pony_ndcg@10": 0.09569140980926243, "bright_pony_recall@100": 0.3073779113100072, "bright_pony_index_time_ms": 1406.1662920285016, "bright_pony_query_time_ms": 897.2058329964057, "beir_arguana_ndcg@10": 0.31061649067819846, "beir_arguana_recall@100": 0.9428979300499644, "beir_arguana_index_time_ms": 5997.624124982394, "beir_arguana_query_time_ms": 43585.07645793725, "bright_theoremqa_theorems_ndcg@10": 0.0268690620951247, "bright_theoremqa_theorems_recall@100": 0.14254385964912278, "bright_theoremqa_theorems_index_time_ms": 10779.37837492209, "bright_theoremqa_theorems_query_time_ms": 1825.077582965605, "beir_scidocs_ndcg@10": 0.1495047958621745, "beir_scidocs_recall@100": 0.35085, "beir_scidocs_index_time_ms": 19848.130916943774, "beir_scidocs_query_time_ms": 5461.604250012897, "bright_economics_ndcg@10": 0.14585832914039454, "bright_economics_recall@100": 0.40072203636561105, "bright_economics_index_time_ms": 14431.358917034231, "bright_economics_query_time_ms": 5158.537458977662, "bright_biology_ndcg@10": 0.2762235874806316, "bright_biology_recall@100": 0.5439092815434154, "bright_biology_index_time_ms": 14936.757124960423, "bright_biology_query_time_ms": 4817.410542047583, "beir_fiqa_ndcg@10": 0.24140404347331615, "beir_fiqa_recall@100": 0.5479982937158863, "beir_fiqa_index_time_ms": 34676.1609580135, "beir_fiqa_query_time_ms": 9902.334207901731, "bright_earth_science_ndcg@10": 0.3336995573437931, "bright_earth_science_recall@100": 0.6610985732355968, "bright_earth_science_index_time_ms": 32522.83695805818, "bright_earth_science_query_time_ms": 8238.93966700416, "bright_stackoverflow_ndcg@10": 0.1828199091468222, "bright_stackoverflow_recall@100": 0.4843611178797253, "bright_stackoverflow_index_time_ms": 99583.0987499794, "bright_stackoverflow_query_time_ms": 19038.346707937308, "beir_trec-covid_ndcg@10": 0.6589365481716548, "beir_trec-covid_recall@100": 0.10879807720101789, "beir_trec-covid_index_time_ms": 135007.05829192884, "beir_trec-covid_query_time_ms": 2323.1341250939295, "avg_ndcg@10": 0.2855964246725354, "avg_recall@100": 0.47253451552257103, "combined_score": 0.4351468973525639, "total_index_time_ms": 377781.28649981227, "total_query_time_ms": 101978.2373328926, "total_time_ms": 479759.5238327049, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Base Dirichlet smoothing mass\n      mu: float = 1700.0\n      epsilon: float = 1e-9\n  \n      # Term-adaptive prior strength\n      mu_df_power: float = 0.30\n  \n      # Missing-term penalty (soft AND)\n      miss_penalty: float = 0.085\n      miss_cap: float = 1.4\n  \n      # Coordination reward (soft AND without harsh penalties)\n      coord_strength: float = 0.12\n  \n      # Omissions in long docs are less diagnostic than in short docs.\n      coverage_len_power: float = 0.35\n  \n      # Robust background + discriminative diagnostics (query-independent)\n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      # Query TF saturation\n      query_tf_power: float = 0.60\n  \n      # Query-level regularizers\n      scope_strength: float = 0.07\n  ... (9 more lines)\nwith:\n  class Config:\n      mu: float = 1700.0\n      epsilon: float = 1e-9\n      mu_df_power: float = 0.30\n  \n      miss_penalty: float = 0.085\n      miss_cap: float = 1.4\n      coord_strength: float = 0.12\n      coverage_len_power: float = 0.35\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n      residual_idf_strength: float = 0.90\n  \n      query_tf_power: float = 0.60\n      scope_strength: float = 0.07\n      neg_strength: float = 0.06\n  \n      collection_df_alpha: float = 0.10\n      doclen_prior_strength: float = 0.06\n  \n      # NEW: \"Scope-per-\u03bc\" smoothing.\n      # Multi-aspect queries often fail because missing rare terms are overly punished\n      # when \u03bc is small; but increasing \u03bc globally hurts precision.\n      # We instead add extra smoothing mass only when the query has high information mass.\n      scope_mu_strength: float = 0.35   # 0 disables; scales extra \u03bc by query scope\n      scope_mu_cap: float = 1.5         # max multiplier on \u03bc (safety)\n  \n  ... (4 more lines)\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      Residual-IDF + EDR-gated LMDirichlet with soft completeness,\n      plus a mild MDL-ish document-length prior.\n  \n      Why these changes can help fitness (esp. mixed BEIR + BRIGHT):\n        - Robust background LM (handled in Corpus) reduces \u201cbursty token\u201d artifacts (StackOverflow/...\n        - A doc-length prior reduces verbosity bias, improving nDCG@10 without crushing recall@100.\n        - Keep the surplus-evidence backbone + gentle soft-AND (miss + scope + coord).\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n          avgdl = max(doc_length, 1.0)\n  \n  ... (67 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n      corpus: Corpus | None = None,\n  ) -> float:\n      \"\"\"\n      QL core: LMDirichlet surplus-evidence + soft-AND, but with two structural upgrades:\n  \n      (1) Scope-conditioned smoothing: add extra \u03bc only when the query has high information mass.\n          Intuition: multi-aspect queries should be evaluated under a \"more uncertain\" doc model,\n          preventing over-penalizing missing rare terms while keeping single-aspect precision.\n  \n      (2) Presence mixing: rare terms often act like binary \"did it appear at all?\" signals.\n          Mix a small Bernoulli-style evidence with the multinomial LM evidence.\n      \"\"\"\n      base_mu, a, eps = Config.mu, Config.mu_df_power, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n      scope_s = float(getattr(Config, \"scope_strength\", 0.0))\n      pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n      score = 0.0\n  \n      if corpus is not None and corpus.N > 0:\n          N = float(corpus.N)\n          avgdl = float(corpus.avgdl)\n      else:\n          N = 1.0\n  ... (97 more lines)\nChange 3: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              df = float(self.corpus._df[term_id])\n              mu_t = base_mu * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = max(mu_t, 1.0)\n  \n              per_term = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  \n              # Leak a tiny amount of negative evidence (optional).\n              if neg_s > 0.0:\n                  per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n  \n              # Surplus-evidence backbone.\n              pos = np.maximum(per_term, 0.0)\n  \n              qtf_raw = query_term_weights[i] if query_term_weights is not None else 1.0\n              qtf = float(qtf_raw) ** alpha\n  \n              ridf_w = float(self.corpus._ridf_qweight[term_id])\n              gate = float(self.corpus._edr_gate[term_id])\n  \n              miss = np.where(\n                  tf_row > 0.0,\n                  0.0,\n                  -(Config.miss_penalty * len_factor) * min(ridf_w, Config.miss_cap),\n              )\n  ... (8 more lines)\nwith:\n  pres_mix = float(getattr(Config, \"presence_mix\", 0.0))\n          mu_boost_s = float(getattr(Config, \"scope_mu_strength\", 0.0))\n          mu_boost_cap = float(getattr(Config, \"scope_mu_cap\", 1.0))\n  \n          # Precompute scope_den once (query-only)\n          if scope_num is not None:\n              for term_id in query_term_ids:\n                  scope_den += float(np.log1p(float(self.corpus._ridf_qweight[term_id])))\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n  \n              df = float(self.corpus._df[term_id])\n  \n              # Scope-conditioned \u03bc (vectorized): use current scope match estimate if enabled\n              if mu_boost_s > 0.0 and scope_num is not None and scope_den > 0.0:\n                  scope_match = np.maximum(scope_num / max(scope_den, eps), 0.0)\n                  mu_mult = 1.0 + mu_boost_s * (1.0 - scope_match)\n                  mu_mult = np.minimum(mu_mult, mu_boost_cap)\n              else:\n                  mu_mult = 1.0\n  \n              mu_t = (base_mu * mu_mult) * ((df + 1.0) / (N + 1.0)) ** a\n              mu_t = np.maximum(mu_t, 1.0)\n  \n              llr = np.log(\n                  (1.0 + tf_row / (mu_t * p_collection + eps)) / ((doc_lengths + mu_t) / mu_t) + eps\n              )\n  ... (31 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3223160757916006, "beir_nfcorpus_recall@100": 0.2558994872125246, "beir_nfcorpus_index_time_ms": 3648.2996669365093, "beir_nfcorpus_query_time_ms": 119.78845798876137, "beir_scifact_ndcg@10": 0.6850328202746198, "beir_scifact_recall@100": 0.9242222222222222, "beir_scifact_index_time_ms": 4836.766167078167, "beir_scifact_query_time_ms": 477.3480420699343, "bright_pony_ndcg@10": 0.0865799063788735, "bright_pony_recall@100": 0.3129962200554613, "bright_pony_index_time_ms": 1411.3358330214396, "bright_pony_query_time_ms": 635.4923750041053, "beir_arguana_ndcg@10": 0.28117773280220126, "beir_arguana_recall@100": 0.9243397573162027, "beir_arguana_index_time_ms": 5933.166375034489, "beir_arguana_query_time_ms": 29583.313624956645, "bright_theoremqa_theorems_ndcg@10": 0.030033558036453543, "bright_theoremqa_theorems_recall@100": 0.1513157894736842, "bright_theoremqa_theorems_index_time_ms": 10707.037165993825, "bright_theoremqa_theorems_query_time_ms": 1252.8901670593768, "beir_scidocs_ndcg@10": 0.14560646978520433, "beir_scidocs_recall@100": 0.34970000000000007, "beir_scidocs_index_time_ms": 19694.079583045095, "beir_scidocs_query_time_ms": 4249.358458910137, "bright_economics_ndcg@10": 0.15152806133214827, "bright_economics_recall@100": 0.3856708758910116, "bright_economics_index_time_ms": 14377.252792008221, "bright_economics_query_time_ms": 3530.3572919219732, "bright_biology_ndcg@10": 0.3054748208559125, "bright_biology_recall@100": 0.5853814895715764, "bright_biology_index_time_ms": 15203.752415953204, "bright_biology_query_time_ms": 3227.726834011264, "beir_fiqa_ndcg@10": 0.2244242102722689, "beir_fiqa_recall@100": 0.5146152629948926, "beir_fiqa_index_time_ms": 33732.8983329935, "beir_fiqa_query_time_ms": 7398.21920893155, "bright_earth_science_ndcg@10": 0.32418818215572276, "bright_earth_science_recall@100": 0.668181719973916, "bright_earth_science_index_time_ms": 31913.138707983308, "bright_earth_science_query_time_ms": 5592.193624936044, "bright_stackoverflow_ndcg@10": 0.19183594871190637, "bright_stackoverflow_recall@100": 0.48998111958838414, "bright_stackoverflow_index_time_ms": 98327.67433393747, "bright_stackoverflow_query_time_ms": 12561.729416018352, "beir_trec-covid_ndcg@10": 0.648140048058496, "beir_trec-covid_recall@100": 0.11077274333751065, "beir_trec-covid_index_time_ms": 133769.7829999961, "beir_trec-covid_query_time_ms": 1801.1591249378398, "avg_ndcg@10": 0.283028152871284, "avg_recall@100": 0.47275639063644886, "combined_score": 0.43481074308341594, "total_index_time_ms": 373555.1843739813, "total_query_time_ms": 70429.57662674598, "total_time_ms": 443984.7610007273, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}