{"id": "2506050c-0897-4c68-ac5e-11ac2c30b76d", "code": "\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    residual_idf_strength: float = 0.9\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    query_tf_power: float = 0.6\n    burstiness_strength: float = 0.30\n\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n    missing_strength: float = 0.07\n    neg_strength: float = 0.12\n    length_prior_strength: float = 0.06\n\n    aboutness_strength: float = 0.08\n    aboutness_poisson: bool = True\n\n    clarity_gamma: float = 0.20\n\n    # NEW: a second (simpler) coordination signal: fraction of distinct query terms present.\n    # This is a Beta-Bernoulli view of \"aboutness coverage\" that is cheap and often helps nDCG@10.\n    coord_strength: float = 0.04  # keep small to avoid hurting recall\n    coord_smooth: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Tempered collection probability.\n\n    Instead of raw P(w|C), use p_t(w) \u221d p(w)^tau with tau<1.\n    Intuition: frequent terms dominate the background and reduce discriminativeness;\n    tempering flattens the background while remaining probabilistic.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = max(p, Config.epsilon) ** tau\n    # Exact renormalization is done in Corpus for vectorized scoring; this is a safe fallback.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Store unique terms (avoid double iteration) but keep tf in weights.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Compact query model:\n        - iterate unique terms\n        - keep qtf in term_weights so repeated tokens still add evidence\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Residual-IDF / EDR-gated QL (slow path).\n\n    The fast path (rank) uses precomputed per-term arrays in Corpus; here we approximate\n    the same *story* using only token LM pC:\n      - tempered background discourages stopword matches\n      - mild qtf saturation\n      - only reward positive evidence (Lucene-style), but leak a small fraction of negatives\n        to reduce \"one-off mention\" boilerplate effects\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        pCw = collection_probability(term, corpus_term_freq, total_tokens)\n        per_term = math.log((1.0 + tf / (mu * pCw + eps)) / ((doc_length + mu) / mu) + eps)\n\n        # Specificity proxy (true residual-IDF uses df/N, available in fast path)\n        spec = min(clipc, max(0.0, -math.log(max(pCw, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        # Residual-IDF-like query weighting proxy in slow path\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = (w0 ** alpha) * (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        # Leak a small amount of negative evidence (keeps robustness vs long boilerplate)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term = neg_s * per_term\n\n        score += w * gate * max(per_term, 0.0 if neg_s <= 0.0 else per_term)\n\n        # Bernoulli presence \"aboutness\" bonus (slow-path proxy using spec).\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n        if about_s > 0.0 and tf > 0.0:\n            score += about_s * w * (spec / max(clipc, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (single background; tempered + optional uniform mixture), normalized once.\n        # Using one background keeps the Dirichlet generative story coherent and tends to\n        # stabilize recall@100 across diverse datasets.\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # For compatibility with the rest of the pipeline\n        self._collection_prob_smooth = self._collection_prob\n\n        # Per-term surprisal under background for query clarity gating.\n        self._col_surprisal = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n\n        # Precompute df-based LM, EDR gate, residual-IDF query weights, and TF saturation exponent.\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Aboutness prior per term (presence channel) as a *presence residual*.\n        # Compare observed presence p_pres=df/N to expected presence under a token Poisson model:\n        #   p_exp(present) \u2248 1 - exp(-avgdl * p_col)\n        # This favors \"topic switch\" terms while not over-rewarding merely rare tokens.\n        if self.N > 0:\n            p_pres = np.maximum(p_doc, Config.epsilon)\n            p_col = np.maximum(self._collection_prob, Config.epsilon)\n            if bool(getattr(Config, \"aboutness_poisson\", True)):\n                avgdl = max(float(self.avgdl), 1.0)\n                p_exp = 1.0 - np.exp(-avgdl * p_col)\n                self._aboutness = np.log(np.maximum(p_pres, Config.epsilon) / np.maximum(p_exp, Config.epsilon))\n            else:\n                self._aboutness = np.log(np.maximum(p_pres, Config.epsilon) / np.maximum(p_col, Config.epsilon))\n            self._aboutness = np.maximum(self._aboutness, 0.0)\n        else:\n            self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\n\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)  # in [1-burst_s,1]\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0.0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); implements the full Residual-IDF / EDR-gated QL.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query clarity (per query constant): average surprisal of query terms.\n        # Use it to gate any AND-like pressure so we don't over-penalize vague/noisy queries.\n        if len(query_term_ids) > 0:\n            clarity = float(np.mean(self.corpus._col_surprisal[np.array(query_term_ids, dtype=np.int64)]))\n        else:\n            clarity = 0.0\n        clarity_gate = 1.0 - math.exp(-float(getattr(Config, \"clarity_gamma\", 0.0)) * max(clarity, 0.0))\n\n        coord_s = float(getattr(Config, \"coord_strength\", 0.0))\n        coord_smooth = float(getattr(Config, \"coord_smooth\", 0.5))\n        coord_hits = np.zeros(len(candidate_docs), dtype=np.float64) if coord_s > 0.0 else None\n\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            pCs = float(self.corpus._collection_prob_smooth[term_id])\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pCs + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            present = (tf_row > 0.0).astype(np.float64)\n\n            if about_s > 0.0:\n                scores += about_s * w * present * float(self.corpus._aboutness[term_id])\n\n            if coord_hits is not None:\n                coord_hits += present\n\n            if miss_s > 0.0:\n                miss = 1.0 - present\n                if np.any(miss):\n                    base0 = np.log((mu * pCs + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        qn = float(len(query_term_ids))\n        if and_acc is not None and qn > 0.0:\n            scores += (and_strength * clarity_gate) * (and_acc / qn)\n\n        # NEW: coordination as a Bernoulli coverage likelihood ratio:\n        # log( (m+\u03b1)/(n-m+\u03b1) ) is too sharp; use log(m+\u03b1) - log(n+2\u03b1) instead (stable).\n        if coord_hits is not None and qn > 0.0:\n            frac = (coord_hits + coord_smooth) / (qn + 2.0 * coord_smooth)\n            scores += (coord_s * clarity_gate) * np.log(np.maximum(frac, eps))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "6179b30a-c04f-48aa-b35c-6fb6dfb021a3", "generation": 4, "timestamp": 1770372710.959841, "iteration_found": 108, "metrics": {"beir_nfcorpus_ndcg@10": 0.31053338357401394, "beir_nfcorpus_recall@100": 0.25099897450184366, "beir_nfcorpus_index_time_ms": 3593.8254999928176, "beir_nfcorpus_query_time_ms": 126.2256249319762, "beir_scifact_ndcg@10": 0.6728660378706878, "beir_scifact_recall@100": 0.9253333333333335, "beir_scifact_index_time_ms": 4954.413542058319, "beir_scifact_query_time_ms": 543.0195420049131, "bright_pony_ndcg@10": 0.10200668511797471, "bright_pony_recall@100": 0.3265413407172356, "bright_pony_index_time_ms": 1410.434624995105, "bright_pony_query_time_ms": 824.3090829346329, "beir_arguana_ndcg@10": 0.2665777148894217, "beir_arguana_recall@100": 0.9107780157030693, "beir_arguana_index_time_ms": 5973.104459000751, "beir_arguana_query_time_ms": 39794.81083294377, "bright_theoremqa_theorems_ndcg@10": 0.034292044372020984, "bright_theoremqa_theorems_recall@100": 0.16228070175438594, "bright_theoremqa_theorems_index_time_ms": 10565.875916974619, "bright_theoremqa_theorems_query_time_ms": 1659.3147920211777, "beir_scidocs_ndcg@10": 0.14272358477891156, "beir_scidocs_recall@100": 0.3408, "beir_scidocs_index_time_ms": 19994.507916038856, "beir_scidocs_query_time_ms": 5085.324458079413, "bright_economics_ndcg@10": 0.13222978967415205, "bright_economics_recall@100": 0.37780698918796535, "bright_economics_index_time_ms": 13955.317292013206, "bright_economics_query_time_ms": 4764.629957964644, "bright_biology_ndcg@10": 0.29628398778061943, "bright_biology_recall@100": 0.5937701797436085, "bright_biology_index_time_ms": 15389.921624911949, "bright_biology_query_time_ms": 4576.980915968306, "beir_fiqa_ndcg@10": 0.20902511993367082, "beir_fiqa_recall@100": 0.5075649701807109, "beir_fiqa_index_time_ms": 33839.38891696744, "beir_fiqa_query_time_ms": 9057.015749975108, "bright_earth_science_ndcg@10": 0.29407267290984945, "bright_earth_science_recall@100": 0.6685348674062378, "bright_earth_science_index_time_ms": 32051.337375072762, "bright_earth_science_query_time_ms": 7465.561290970072, "bright_stackoverflow_ndcg@10": 0.18907788155004995, "bright_stackoverflow_recall@100": 0.4379805059475225, "bright_stackoverflow_index_time_ms": 98376.64733303245, "bright_stackoverflow_query_time_ms": 17341.05762501713, "beir_trec-covid_ndcg@10": 0.6128422255844802, "beir_trec-covid_recall@100": 0.10411403374705454, "beir_trec-covid_index_time_ms": 134748.06466710288, "beir_trec-covid_query_time_ms": 2130.9907080139965, "avg_ndcg@10": 0.2718775940029877, "avg_recall@100": 0.467208659351914, "combined_score": 0.4281424462821288, "total_index_time_ms": 374852.83916816115, "total_query_time_ms": 93369.24058082514, "total_time_ms": 468222.0797489863, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      mu: float = 1800.0  # base Dirichlet mass (good general default)\n      epsilon: float = 1e-9\n  \n      # Tempered background: p_t(w) \u221d p(w)^tau (tau<1 flattens very frequent terms)\n      collection_temper: float = 0.85\n      # Mix in a tiny uniform background to reduce peakiness in very skewed corpora\n      uniform_bg_mass: float = 0.03\n  \n      # Residual-IDF query weight uses df/N vs token LM (computed in Corpus)\n      residual_idf_strength: float = 0.9\n  \n      # Gate per-term contribution by spread mismatch (EDR): log(p_doc/p_col)\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Query TF saturation (repeated query tokens have diminishing impact)\n      query_tf_power: float = 0.6\n  \n      # Term-specific TF saturation: tf -> tf^beta(w), common terms saturate more\n      burstiness_strength: float = 0.30\n  \n      # Soft coverage reward (encourage matching more distinct query terms)\n      and_strength: float = 0.14\n      and_saturation: float = 3.0\n  \n      # Small missing-term anti-evidence inside candidate set (kept small to protect recall)\n      missing_strength: float = 0.07\n  \n      # Leak a small amount of negative evidence for weak hits (helps early precision)\n  ... (11 more lines)\nwith:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      collection_temper: float = 0.85\n      uniform_bg_mass: float = 0.03\n  \n      residual_idf_strength: float = 0.9\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      query_tf_power: float = 0.6\n      burstiness_strength: float = 0.30\n  \n      and_strength: float = 0.14\n      and_saturation: float = 3.0\n      missing_strength: float = 0.07\n      neg_strength: float = 0.12\n      length_prior_strength: float = 0.06\n  \n      aboutness_strength: float = 0.08\n      aboutness_poisson: bool = True\n  \n      clarity_gamma: float = 0.20\n  \n      # NEW: a second (simpler) coordination signal: fraction of distinct query terms present.\n      # This is a Beta-Bernoulli view of \"aboutness coverage\" that is cheap and often helps nDCG@10.\n      coord_strength: float = 0.04  # keep small to avoid hurting recall\n      coord_smooth: float = 0.5\nChange 2: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              pCw = float(self.corpus._collection_prob[term_id])          # weighting background\n              pCs = float(self.corpus._collection_prob_smooth[term_id])   # smoothing background\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              beta = float(self.corpus._tf_beta[term_id])\n              tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n  \n              per_term = np.log((1.0 + tf_eff / (mu * pCs + eps)) / ((doc_lengths + mu) / mu) + eps)\n              per_term *= float(self.corpus._edr_gate[term_id])\n  \n              w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n              w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n  \n              if neg_s > 0.0:\n                  per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n  \n              scores += w * per_term\n  \n              # Aboutness prior: add a bounded presence bonus when tf>0.\n              about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n              if about_s > 0.0:\n                  scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._ab...\n  \n              if miss_s > 0.0:\n                  miss = (tf_row <= 0.0).astype(np.float64)\n                  if np.any(miss):\n                      base0 = np.log((mu * pCs + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                      scores += (miss_s * clarity_gate) * w * miss * base0\n  ... (10 more lines)\nwith:\n  coord_s = float(getattr(Config, \"coord_strength\", 0.0))\n          coord_smooth = float(getattr(Config, \"coord_smooth\", 0.5))\n          coord_hits = np.zeros(len(candidate_docs), dtype=np.float64) if coord_s > 0.0 else None\n  \n          about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n  \n          for i, term_id in enumerate(query_term_ids):\n              pCs = float(self.corpus._collection_prob_smooth[term_id])\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              beta = float(self.corpus._tf_beta[term_id])\n              tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n  \n              per_term = np.log((1.0 + tf_eff / (mu * pCs + eps)) / ((doc_lengths + mu) / mu) + eps)\n              per_term *= float(self.corpus._edr_gate[term_id])\n  \n              w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n              w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n  \n              if neg_s > 0.0:\n                  per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n  \n              scores += w * per_term\n  \n              present = (tf_row > 0.0).astype(np.float64)\n  \n              if about_s > 0.0:\n                  scores += about_s * w * present * float(self.corpus._aboutness[term_id])\n  \n  ... (25 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3107712593793001, "beir_nfcorpus_recall@100": 0.2509729694048447, "beir_nfcorpus_index_time_ms": 3589.2241669353098, "beir_nfcorpus_query_time_ms": 132.66237499192357, "beir_scifact_ndcg@10": 0.6728429963287664, "beir_scifact_recall@100": 0.9253333333333335, "beir_scifact_index_time_ms": 4842.800291953608, "beir_scifact_query_time_ms": 549.523959052749, "bright_pony_ndcg@10": 0.1019205668436182, "bright_pony_recall@100": 0.3265413407172356, "bright_pony_index_time_ms": 1494.551582960412, "bright_pony_query_time_ms": 825.3766249399632, "beir_arguana_ndcg@10": 0.2668003691935564, "beir_arguana_recall@100": 0.9107780157030693, "beir_arguana_index_time_ms": 5959.885832970031, "beir_arguana_query_time_ms": 40069.529542000964, "bright_theoremqa_theorems_ndcg@10": 0.034292044372020984, "bright_theoremqa_theorems_recall@100": 0.16228070175438594, "bright_theoremqa_theorems_index_time_ms": 10503.554082941264, "bright_theoremqa_theorems_query_time_ms": 1683.4295419976115, "beir_scidocs_ndcg@10": 0.14259944887727843, "beir_scidocs_recall@100": 0.34120000000000006, "beir_scidocs_index_time_ms": 19865.959290997125, "beir_scidocs_query_time_ms": 5043.933541979641, "bright_economics_ndcg@10": 0.13222978967415205, "bright_economics_recall@100": 0.37780698918796535, "bright_economics_index_time_ms": 14275.91037505772, "bright_economics_query_time_ms": 4662.222874932922, "bright_biology_ndcg@10": 0.2981630830919131, "bright_biology_recall@100": 0.5937701797436085, "bright_biology_index_time_ms": 14800.199459074065, "bright_biology_query_time_ms": 4488.898667041212, "beir_fiqa_ndcg@10": 0.20912948259143838, "beir_fiqa_recall@100": 0.5075649701807109, "beir_fiqa_index_time_ms": 34112.45591705665, "beir_fiqa_query_time_ms": 8969.404000090435, "bright_earth_science_ndcg@10": 0.29407267290984945, "bright_earth_science_recall@100": 0.6685348674062378, "bright_earth_science_index_time_ms": 31994.816333055496, "bright_earth_science_query_time_ms": 7541.078749927692, "bright_stackoverflow_ndcg@10": 0.18815586181562421, "bright_stackoverflow_recall@100": 0.4379805059475225, "bright_stackoverflow_index_time_ms": 99267.19041692559, "bright_stackoverflow_query_time_ms": 16931.11070804298, "beir_trec-covid_ndcg@10": 0.6129202609898389, "beir_trec-covid_recall@100": 0.10400559710064322, "beir_trec-covid_index_time_ms": 133322.3959170282, "beir_trec-covid_query_time_ms": 2109.159208019264, "avg_ndcg@10": 0.2719914863389464, "avg_recall@100": 0.4672307892066298, "combined_score": 0.42818292863309315, "total_index_time_ms": 374028.94366695546, "total_query_time_ms": 93006.32979301736, "total_time_ms": 467035.2734599728, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 2}, "prompts": {"diff_user": {"system": "You are discovering a **new probabilistic retrieval method** based on Query Likelihood language models. The seed program is a minimal skeleton: document representation, query representation, collection model, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just Dirichlet smoothing with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall@100, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the probabilistic model with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 \u03bc (smoothing parameter), epsilon, and any new parameters you need.\n2. **collection_probability(term, ...)** \u2014 How to compute P(w | C) (collection language model). EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, hierarchical, etc.).\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. Default: Dirichlet smoothing. Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors, query models, multi-field models, term dependencies, etc. You can use multiple sub-signals and combine them, or a single unified formula.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **QL._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **QL**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **QL** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and QL.rank() / QL.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n- Avoid taking log(0) (use max(value, epsilon) before log).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"smoothing prevents zero probabilities\" or \"collection model favors discriminative terms\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different smoothing methods, document priors, query expansion, term dependencies) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n- Probabilistic, information-theoretic, and language modeling principles are good foundations.\n", "user": "# Current Program Information\n- Fitness: 0.4282\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.4273 \u2192 0.4282\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Tempered background LM p_t(w) \u221d p(w)^tau\n      collection_temper: float = 0.85\n  \n      # Query term burstiness saturation (qtf^alpha)\n      query_tf_power: float = 0.6\n  \n      # Document length prior (log-normal-ish); keep small to avoid recall loss\n      length_prior_strength: float = 0.06\n  \n      # EDR gate: token-vs-document spread mismatch\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Residual-IDF query weighting (df/N vs token LM)\n      residual_idf_strength: float = 0.9\n  \n      # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n      # Intuition: hedges against over-peaky collection LMs; can improve recall on rare terms.\n      uniform_bg_mass: float = 0.03  # 0 disables\n  \n      # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n      and_strength: float = 0.14  # 0 disables\n      and_saturation: float = 3.0\n  \n      # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n      # Helps nDCG@10 on multi-aspect queries where partial matches otherwise rank too high.\n  ... (1 more lines)\nwith:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Tempered background LM p_t(w) \u221d p(w)^tau\n      collection_temper: float = 0.85\n  \n      # Query term burstiness saturation (qtf^alpha)\n      query_tf_power: float = 0.6\n  \n      # Document length prior (log-normal-ish); keep small to avoid recall loss\n      length_prior_strength: float = 0.06\n  \n      # EDR gate: token-vs-document spread mismatch\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Residual-IDF query weighting (df/N vs token LM)\n      residual_idf_strength: float = 0.9\n  \n      # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n      uniform_bg_mass: float = 0.03  # 0 disables\n  \n      # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n      and_strength: float = 0.14  # 0 disables\n      and_saturation: float = 3.0\n  \n      # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n      missing_strength: float = 0.07  # keep small to protect recall@100\n  \n  ... (10 more lines)\nChange 2: Replace:\n  # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n          lam = float(getattr(Config, \"edr_strength\", 0.0))\n          clipc = float(getattr(Config, \"edr_clip\", 3.0))\n          ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n  \n          if self.N > 0:\n              p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n          else:\n              p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n  \n          # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n          if lam > 0.0 and self.N > 0:\n              ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), ...\n              ratio = np.clip(ratio, -clipc, clipc)\n              self._edr_gate = 1.0 + lam * ratio\n          else:\n              self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n  \n          # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n          # (We keep only the positive side: terms that are spread-out get no boost.)\n          if ridf_s > 0.0 and self.N > 0:\n              ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), C...\n              ridf = np.maximum(ridf, 0.0)\n              self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n          else:\n              self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n  \n          # Length prior\n          s = float(getattr(Config, \"length_prior_strength\", 0.0))\n          if s > 0:\n  ... (5 more lines)\nwith:\n  # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n          lam = float(getattr(Config, \"edr_strength\", 0.0))\n          clipc = float(getattr(Config, \"edr_clip\", 3.0))\n          ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n          burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n  \n          if self.N > 0:\n              p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n          else:\n              p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n  \n          # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n          if lam > 0.0 and self.N > 0:\n              ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), ...\n              ratio = np.clip(ratio, -clipc, clipc)\n              self._edr_gate = 1.0 + lam * ratio\n          else:\n              self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n  \n          # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n          if ridf_s > 0.0 and self.N > 0:\n              ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), C...\n              ridf = np.maximum(ridf, 0.0)\n              self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n          else:\n              self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n  \n          # NEW: Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n          # Common terms saturate more: tf -> tf^beta(w).\n          if burst_s > 0.0 and self.N > 0:\n  ... (14 more lines)\nChange 3: Replace:\n  mu, eps = Config.mu, Config.epsilon\n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          and_strength = float(getattr(Config, \"and_strength\", 0.0))\n          and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n          miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n  \n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              numerator = 1.0 + tf_row / (mu * p_collection + eps)\n              denominator = (doc_lengths + mu) / mu\n              per_term = np.log(numerator / denominator + eps)\n  \n              # Apply EDR gate (query-independent)\n              per_term *= self.corpus._edr_gate[term_id]\n  \n              w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n              w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n  \n              present = np.maximum(per_term, 0.0)\n              contrib = w * present\n              scores += contrib\n  \n              # Missing-term anti-evidence (scaled tf=0 Dirichlet contribution) within candidates.\n  ... (9 more lines)\nwith:\n  mu, eps = Config.mu, Config.epsilon\n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          and_strength = float(getattr(Config, \"and_strength\", 0.0))\n          and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n          miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n          neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n  \n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              # TF burstiness saturation: tf -> tf^beta(w) (common terms saturate more).\n              beta = self.corpus._tf_beta[term_id]\n              tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n  \n              numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n              denominator = (doc_lengths + mu) / mu\n              per_term = np.log(numerator / denominator + eps)\n  \n              # Apply EDR gate (query-independent)\n              per_term *= self.corpus._edr_gate[term_id]\n  \n              w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n              w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n  \n  ... (19 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3105, beir_nfcorpus_recall@100: 0.2510, beir_nfcorpus_index_time_ms: 3585.8080, beir_nfcorpus_query_time_ms: 120.6080, beir_scifact_ndcg@10: 0.6729, beir_scifact_recall@100: 0.9253, beir_scifact_index_time_ms: 4856.9142, beir_scifact_query_time_ms: 534.4172, bright_pony_ndcg@10: 0.1097, bright_pony_recall@100: 0.3311, bright_pony_index_time_ms: 1473.4872, bright_pony_query_time_ms: 790.7443, beir_arguana_ndcg@10: 0.2633, beir_arguana_recall@100: 0.9051, beir_arguana_index_time_ms: 5881.2448, beir_arguana_query_time_ms: 38038.7680, bright_theoremqa_theorems_ndcg@10: 0.0339, bright_theoremqa_theorems_recall@100: 0.1557, bright_theoremqa_theorems_index_time_ms: 10631.9319, bright_theoremqa_theorems_query_time_ms: 1605.4113, beir_scidocs_ndcg@10: 0.1420, beir_scidocs_recall@100: 0.3402, beir_scidocs_index_time_ms: 19643.1758, beir_scidocs_query_time_ms: 4853.3809, bright_economics_ndcg@10: 0.1318, bright_economics_recall@100: 0.3749, bright_economics_index_time_ms: 14090.7396, bright_economics_query_time_ms: 4476.3081, bright_biology_ndcg@10: 0.2941, bright_biology_recall@100: 0.5905, bright_biology_index_time_ms: 15207.7363, bright_biology_query_time_ms: 4380.0538, beir_fiqa_ndcg@10: 0.2085, beir_fiqa_recall@100: 0.5022, beir_fiqa_index_time_ms: 33344.9355, beir_fiqa_query_time_ms: 8908.5117, bright_earth_science_ndcg@10: 0.2904, bright_earth_science_recall@100: 0.6650, bright_earth_science_index_time_ms: 31310.2214, bright_earth_science_query_time_ms: 7206.8459, bright_stackoverflow_ndcg@10: 0.1888, bright_stackoverflow_recall@100: 0.4482, bright_stackoverflow_index_time_ms: 99579.9722, bright_stackoverflow_query_time_ms: 17001.6652, beir_trec-covid_ndcg@10: 0.6172, beir_trec-covid_recall@100: 0.1038, beir_trec-covid_index_time_ms: 137644.1000, beir_trec-covid_query_time_ms: 2097.8336, avg_ndcg@10: 0.2719, avg_recall@100: 0.4661, combined_score: 0.4273, total_index_time_ms: 377250.2669, total_query_time_ms: 90014.5479, total_time_ms: 467264.8148, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      mu: float = 2000.0  # Dirichlet smoothing parameter\n      epsilon: float = 1e-9\nwith:\n  class Config:\n      mu: float = 1800.0  # base Dirichlet mass (good general default)\n  \n      epsilon: float = 1e-9\n  \n      # --- Core idea: tempered+uniform background + residual-IDF query weighting + mild TF saturat...\n      # Tempered background: p_t(w) \u221d p(w)^tau (tau<1 flattens very frequent terms)\n      collection_temper: float = 0.85\n      # Mix in a tiny uniform background to reduce peakiness in very skewed corpora\n      uniform_bg_mass: float = 0.03\n  \n      # Residual-IDF query weight uses df/N vs token LM (computed in Corpus)\n      residual_idf_strength: float = 0.9\n  \n      # Gate per-term contribution by spread mismatch (EDR): log(p_doc/p_col)\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Query TF saturation (repeated query tokens have diminishing impact)\n      query_tf_power: float = 0.6\n  \n      # Term-specific TF saturation: tf -> tf^beta(w), common terms saturate more\n      burstiness_strength: float = 0.30\n  \n      # Soft coverage reward (encourage matching more distinct query terms)\n      and_strength: float = 0.14\n      and_saturation: float = 3.0\n  \n      # Small missing-term anti-evidence inside candidate set (kept small to protect recall)\n      missing_strength: float = 0.07\n  ... (6 more lines)\nChange 2: Replace:\n  def collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n      \"\"\"\n      Collection probability P(w | C) = total frequency / total tokens.\n      EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n      \"\"\"\n      if term not in corpus_term_freq:\n          return Config.epsilon\n      return corpus_term_freq[term] / max(total_tokens, 1)\nwith:\n  def collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n      \"\"\"\n      Tempered collection probability.\n  \n      Instead of raw P(w|C), use p_t(w) \u221d p(w)^tau with tau<1.\n      Intuition: frequent terms dominate the background and reduce discriminativeness;\n      tempering flattens the background while remaining probabilistic.\n      \"\"\"\n      tf = corpus_term_freq.get(term, 0)\n      if tf <= 0 or total_tokens <= 0:\n          return Config.epsilon\n      p = tf / float(total_tokens)\n      tau = float(getattr(Config, \"collection_temper\", 1.0))\n      p_t = max(p, Config.epsilon) ** tau\n      # Exact renormalization is done in Corpus for vectorized scoring; this is a safe fallback.\n      return max(p_t, Config.epsilon)\nChange 3: Replace:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n          return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\nwith:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          # Store unique terms (avoid double iteration) but keep tf in weights.\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"\n          Compact query model:\n          - iterate unique terms\n          - keep qtf in term_weights so repeated tokens still add evidence\n          \"\"\"\n          tc = Counter(tokens)\n          return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\nChange 4: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n  ) -> float:\n      \"\"\"\n      Score one document for one query using Query Likelihood with Dirichlet smoothing.\n  \n      Formula (Lucene/Pyserini variant):\n          Score(D, Q) = \u03a3_{w in Q} max(0, log(1 + c(w,D)/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc)))\n  \n      This matches Pyserini's LMDirichletSimilarity which clamps per-term scores to 0.\n      This differs from academic QL which allows negative per-term scores.\n  \n      Default behavior: Matches Pyserini's LMDirichletSimilarity with \u03bc=2000.\n      EVOLVE: design a probabilistic formulation with deep, fundamental, intuitive justification.\n      Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors,\n      term dependencies, multi-field models, etc.\n      \"\"\"\n      mu, eps = Config.mu, Config.epsilon\n      score = 0.0\n  \n      for term in query_repr.terms:\n          # c(w, D): term count in document\n          term_count = float(doc_tf.get(term, 0))\n  \n          # P(w | C): collection probability\n          p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n  ... (12 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      corpus_term_freq: Counter[str],\n      total_tokens: int,\n  ) -> float:\n      \"\"\"\n      Residual-IDF / EDR-gated QL (slow path).\n  \n      The fast path (rank) uses precomputed per-term arrays in Corpus; here we approximate\n      the same *story* using only token LM pC:\n        - tempered background discourages stopword matches\n        - mild qtf saturation\n        - only reward positive evidence (Lucene-style), but leak a small fraction of negatives\n          to reduce \"one-off mention\" boilerplate effects\n      \"\"\"\n      mu, eps = Config.mu, Config.epsilon\n      alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n      lam = float(getattr(Config, \"edr_strength\", 0.0))\n      clipc = float(getattr(Config, \"edr_clip\", 3.0))\n      ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n      neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n  \n      score = 0.0\n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0.0))\n          pC = collection_probability(term, corpus_term_freq, total_tokens)\n  \n          per_term = math.log((1.0 + tf / (mu * pC + eps)) / ((doc_length + mu) / mu) + eps)\n  ... (17 more lines)\nChange 5: Replace:\n  # Collection probability array for vectorized scoring\n          self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n          for term, tid in self._vocab.items():\n              self._collection_prob[tid] = collection_probability(\n                  term, self.corpus_term_freq, self.total_tokens\n              )\nwith:\n  # Collection LM (tempered) with optional uniform mixture; normalized once for speed.\n          tau = float(getattr(Config, \"collection_temper\", 1.0))\n          base_p = np.zeros(self.vocab_size, dtype=np.float64)\n          for term, tid in self._vocab.items():\n              tf = float(self.corpus_term_freq.get(term, 0))\n              base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n  \n          if tau != 1.0:\n              tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n              z = float(np.sum(tmp))\n              p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n          else:\n              p_col = np.maximum(base_p, Config.epsilon)\n  \n          gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n          if gamma > 0.0 and self.vocab_size > 0:\n              p_uni = 1.0 / float(self.vocab_size)\n              self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsi...\n          else:\n              self._collection_prob = p_col\n  \n          # Precompute df-based LM, EDR gate, residual-IDF query weights, and TF saturation exponent.\n          if self.N > 0:\n              p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n          else:\n              p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n  \n          lam = float(getattr(Config, \"edr_strength\", 0.0))\n          clipc = float(getattr(Config, \"edr_clip\", 3.0))\n          if lam > 0.0 and self.N > 0:\n  ... (29 more lines)\nChange 6: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          mu, eps = Config.mu, Config.epsilon\n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          for i, term_id in enumerate(query_term_ids):\n              # Get collection probability for this term\n              p_collection = self.corpus._collection_prob[term_id]\n  \n              # Get term frequencies for candidates\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n  \n              # Lucene/Pyserini formula: log(1 + freq/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc))\n              numerator = 1.0 + tf_row / (mu * p_collection + eps)\n              denominator = (doc_lengths + mu) / mu\n              per_term_scores = np.log(numerator / denominator + eps)\n  \n              # Apply query term weight and clamp to 0\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              scores += w * np.maximum(per_term_scores, 0.0)\n  \n  ... (1 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); implements the full Residual-IDF / EDR-gated QL.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          mu, eps = Config.mu, Config.epsilon\n          alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n          and_strength = float(getattr(Config, \"and_strength\", 0.0))\n          and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n          miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n          neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n  \n          doc_lengths = self.corpus.doc_lengths[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n  \n          for i, term_id in enumerate(query_term_ids):\n              pC = float(self.corpus._collection_prob[term_id])\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              beta = float(self.corpus._tf_beta[term_id])\n              tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n  \n              per_term = np.log((1.0 + tf_eff / (mu * pC + eps)) / ((doc_lengths + mu) / mu) + eps)\n  ... (26 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3105, beir_nfcorpus_recall@100: 0.2510, beir_nfcorpus_index_time_ms: 3570.8540, beir_nfcorpus_query_time_ms: 119.9420, beir_scifact_ndcg@10: 0.6729, beir_scifact_recall@100: 0.9253, beir_scifact_index_time_ms: 4918.0811, beir_scifact_query_time_ms: 526.2071, bright_pony_ndcg@10: 0.1097, bright_pony_recall@100: 0.3311, bright_pony_index_time_ms: 1394.4127, bright_pony_query_time_ms: 806.4462, beir_arguana_ndcg@10: 0.2633, beir_arguana_recall@100: 0.9051, beir_arguana_index_time_ms: 5881.9265, beir_arguana_query_time_ms: 38154.5987, bright_theoremqa_theorems_ndcg@10: 0.0339, bright_theoremqa_theorems_recall@100: 0.1557, bright_theoremqa_theorems_index_time_ms: 10694.0900, bright_theoremqa_theorems_query_time_ms: 1538.9794, beir_scidocs_ndcg@10: 0.1420, beir_scidocs_recall@100: 0.3402, beir_scidocs_index_time_ms: 19746.7246, beir_scidocs_query_time_ms: 4888.1956, bright_economics_ndcg@10: 0.1318, bright_economics_recall@100: 0.3749, bright_economics_index_time_ms: 14217.7541, bright_economics_query_time_ms: 4571.0046, bright_biology_ndcg@10: 0.2941, bright_biology_recall@100: 0.5905, bright_biology_index_time_ms: 15362.1468, bright_biology_query_time_ms: 4314.0514, beir_fiqa_ndcg@10: 0.2085, beir_fiqa_recall@100: 0.5022, beir_fiqa_index_time_ms: 33846.6961, beir_fiqa_query_time_ms: 8610.0481, bright_earth_science_ndcg@10: 0.2904, bright_earth_science_recall@100: 0.6650, bright_earth_science_index_time_ms: 31911.7453, bright_earth_science_query_time_ms: 7402.3845, bright_stackoverflow_ndcg@10: 0.1888, bright_stackoverflow_recall@100: 0.4482, bright_stackoverflow_index_time_ms: 98267.3225, bright_stackoverflow_query_time_ms: 16685.0466, beir_trec-covid_ndcg@10: 0.6172, beir_trec-covid_recall@100: 0.1038, beir_trec-covid_index_time_ms: 134017.4807, beir_trec-covid_query_time_ms: 2090.2559, avg_ndcg@10: 0.2719, avg_recall@100: 0.4661, combined_score: 0.4273, total_index_time_ms: 373829.2346, total_query_time_ms: 89707.1601, total_time_ms: 463536.3947, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Tempered background LM p_t(w) \u221d p(w)^tau\n      collection_temper: float = 0.85\n  \n      # Query term burstiness saturation (qtf^alpha)\n      query_tf_power: float = 0.6\n  \n      # Document length prior (log-normal-ish); keep small to avoid recall loss\n      length_prior_strength: float = 0.06\n  \n      # EDR gate: token-vs-document spread mismatch\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Residual-IDF query weighting (df/N vs token LM)\n      residual_idf_strength: float = 0.9\n  \n      # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n      uniform_bg_mass: float = 0.03  # 0 disables\n  \n      # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n      and_strength: float = 0.14  # 0 disables\n      and_saturation: float = 3.0\n  \n      # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n      missing_strength: float = 0.07  # keep small to protect recall@100\n  \n  ... (13 more lines)\nwith:\n  class Config:\n      mu: float = 1800.0\n      epsilon: float = 1e-9\n  \n      # Tempered background LM p_t(w) \u221d p(w)^tau\n      collection_temper: float = 0.85\n  \n      # Query term burstiness saturation (qtf^alpha)\n      query_tf_power: float = 0.6\n  \n      # Document length prior (log-normal-ish); keep small to avoid recall loss\n      length_prior_strength: float = 0.06\n  \n      # EDR gate: token-vs-document spread mismatch\n      edr_strength: float = 0.45\n      edr_clip: float = 2.5\n  \n      # Residual-IDF query weighting (df/N vs token LM)\n      residual_idf_strength: float = 0.9\n  \n      # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n      uniform_bg_mass: float = 0.03  # 0 disables\n  \n      # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n      and_strength: float = 0.14  # 0 disables\n      and_saturation: float = 3.0\n  \n      # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n      missing_strength: float = 0.07  # keep small to protect recall@100\n  \n  ... (20 more lines)\nChange 2: Replace:\n  # NEW: aboutness prior per term (presence channel).\n          # Use positive log-odds between document-spread LM and token LM: log(p_doc/p_col).\n          # Clipped at 0: only boost topical anchors, don't punish stopwords.\n          if self.N > 0:\n              self._aboutness = np.log(np.maximum(p_doc, Config.epsilon) / np.maximum(self._collect...\n              self._aboutness = np.maximum(self._aboutness, 0.0)\n          else:\n              self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\nwith:\n  # Aboutness prior per term (presence channel).\n          # Prefer a presence-residual against a simple token-Poisson expectation:\n          #   p_exp(term present in D) \u2248 1 - exp(-avgdl * p_col)\n          # Using log(p_doc/p_exp) makes the bonus focus on \"topic switches\" rather than raw rarity.\n          if self.N > 0:\n              if bool(getattr(Config, \"aboutness_poisson\", True)):\n                  avgdl = max(float(self.avgdl), 1.0)\n                  p_exp = 1.0 - np.exp(-avgdl * np.maximum(self._collection_prob, Config.epsilon))\n                  self._aboutness = np.log(\n                      np.maximum(p_doc, Config.epsilon) / np.maximum(p_exp, Config.epsilon)\n                  )\n              else:\n                  self._aboutness = np.log(\n                      np.maximum(p_doc, Config.epsilon) / np.maximum(self._collection_prob, Config....\n                  )\n              self._aboutness = np.maximum(self._aboutness, 0.0)\n          else:\n              self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\nChange 3: Replace:\n  about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\nwith:\n  about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n          scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n  \n          # Query-scope accumulators: matched information mass / total information mass.\n          scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n          scope_den = 0.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              p_collection = self.corpus._collection_prob[term_id]\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\nChange 4: Replace:\n  # Presence channel: bounded topical bonus when the term appears at least once.\n              if about_s > 0.0:\n                  scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._ab...\nwith:\n  # Presence channel: bounded topical bonus when the term appears at least once.\n              if about_s > 0.0:\n                  scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._ab...\n  \n              # Query-scope: reward covering more of the query's \"information mass\"\n              # using the same residual-IDF weights already computed (stable, query-independent).\n              if scope_num is not None:\n                  info = float(np.log1p(float(self.corpus._ridf_qweight[term_id])))\n                  scope_den += info\n                  scope_num += (tf_row > 0.0).astype(np.float64) * info\nChange 5: Replace:\n  if and_acc is not None and len(query_term_ids) > 0:\n              scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n  \n          scores += self.corpus._length_prior[candidate_docs]\n          return scores\nwith:\n  if and_acc is not None and len(query_term_ids) > 0:\n              scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n  \n          if scope_num is not None and scope_den > 0.0:\n              scope_frac = np.maximum(scope_num / max(scope_den, eps), eps)\n              scores += (scope_s * clarity_gate) * np.log(scope_frac)\n  \n          scores += self.corpus._length_prior[candidate_docs]\n          return scores\n- Metrics: beir_nfcorpus_ndcg@10: 0.3103, beir_nfcorpus_recall@100: 0.2510, beir_nfcorpus_index_time_ms: 3590.2329, beir_nfcorpus_query_time_ms: 132.4305, beir_scifact_ndcg@10: 0.6726, beir_scifact_recall@100: 0.9253, beir_scifact_index_time_ms: 4908.7172, beir_scifact_query_time_ms: 570.3415, bright_pony_ndcg@10: 0.1037, bright_pony_recall@100: 0.3271, bright_pony_index_time_ms: 1398.8385, bright_pony_query_time_ms: 887.6802, beir_arguana_ndcg@10: 0.2661, beir_arguana_recall@100: 0.9108, beir_arguana_index_time_ms: 5928.2681, beir_arguana_query_time_ms: 41853.3984, bright_theoremqa_theorems_ndcg@10: 0.0343, bright_theoremqa_theorems_recall@100: 0.1623, bright_theoremqa_theorems_index_time_ms: 10781.4624, bright_theoremqa_theorems_query_time_ms: 1644.7262, beir_scidocs_ndcg@10: 0.1427, beir_scidocs_recall@100: 0.3406, beir_scidocs_index_time_ms: 19932.1039, beir_scidocs_query_time_ms: 5233.3878, bright_economics_ndcg@10: 0.1330, bright_economics_recall@100: 0.3774, bright_economics_index_time_ms: 14165.6266, bright_economics_query_time_ms: 4866.0845, bright_biology_ndcg@10: 0.2968, bright_biology_recall@100: 0.5938, bright_biology_index_time_ms: 15146.6104, bright_biology_query_time_ms: 4697.2333, beir_fiqa_ndcg@10: 0.2090, beir_fiqa_recall@100: 0.5063, beir_fiqa_index_time_ms: 34266.4927, beir_fiqa_query_time_ms: 9344.2933, bright_earth_science_ndcg@10: 0.2945, bright_earth_science_recall@100: 0.6657, bright_earth_science_index_time_ms: 32249.8969, bright_earth_science_query_time_ms: 8026.5925, bright_stackoverflow_ndcg@10: 0.1883, bright_stackoverflow_recall@100: 0.4423, bright_stackoverflow_index_time_ms: 99347.0739, bright_stackoverflow_query_time_ms: 18112.4136, beir_trec-covid_ndcg@10: 0.6122, beir_trec-covid_recall@100: 0.1039, beir_trec-covid_index_time_ms: 134886.0137, beir_trec-covid_query_time_ms: 2260.0130, avg_ndcg@10: 0.2720, avg_recall@100: 0.4672, combined_score: 0.4282, total_index_time_ms: 376601.3370, total_query_time_ms: 97628.5947, total_time_ms: 474229.9317, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4282)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0  # base Dirichlet mass (good general default)\n    epsilon: float = 1e-9\n\n    # Tempered background: p_t(w) \u221d p(w)^tau (tau<1 flattens very frequent terms)\n    collection_temper: float = 0.85\n    # Mix in a tiny uniform background to reduce peakiness in very skewed corpora\n    uniform_bg_mass: float = 0.03\n\n    # Residual-IDF query weight uses df/N vs token LM (computed in Corpus)\n    residual_idf_strength: float = 0.9\n\n    # Gate per-term contribution by spread mismatch (EDR): log(p_doc/p_col)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Query TF saturation (repeated query tokens have diminishing impact)\n    query_tf_power: float = 0.6\n\n    # Term-specific TF saturation: tf -> tf^beta(w), common terms saturate more\n    burstiness_strength: float = 0.30\n\n    # Soft coverage reward (encourage matching more distinct query terms)\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n\n    # Small missing-term anti-evidence inside candidate set (kept small to protect recall)\n    missing_strength: float = 0.07\n\n    # Leak a small amount of negative evidence for weak hits (helps early precision)\n    neg_strength: float = 0.12\n\n    # Weak log-normal-ish length prior (helps against very long boilerplate docs)\n    length_prior_strength: float = 0.06\n\n    # Aboutness prior (Bernoulli presence channel) computed as a presence residual.\n    aboutness_strength: float = 0.08\n    aboutness_poisson: bool = True\n\n    # Query clarity gate for any AND-like pressure (prevents harming noisy queries).\n    clarity_gamma: float = 0.20\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Tempered collection probability.\n\n    Instead of raw P(w|C), use p_t(w) \u221d p(w)^tau with tau<1.\n    Intuition: frequent terms dominate the background and reduce discriminativeness;\n    tempering flattens the background while remaining probabilistic.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = max(p, Config.epsilon) ** tau\n    # Exact renormalization is done in Corpus for vectorized scoring; this is a safe fallback.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Store unique terms (avoid double iteration) but keep tf in weights.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Compact query model:\n        - iterate unique terms\n        - keep qtf in term_weights so repeated tokens still add evidence\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Residual-IDF / EDR-gated QL (slow path).\n\n    The fast path (rank) uses precomputed per-term arrays in Corpus; here we approximate\n    the same *story* using only token LM pC:\n      - tempered background discourages stopword matches\n      - mild qtf saturation\n      - only reward positive evidence (Lucene-style), but leak a small fraction of negatives\n        to reduce \"one-off mention\" boilerplate effects\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        pCw = collection_probability(term, corpus_term_freq, total_tokens)\n        per_term = math.log((1.0 + tf / (mu * pCw + eps)) / ((doc_length + mu) / mu) + eps)\n\n        # Specificity proxy (true residual-IDF uses df/N, available in fast path)\n        spec = min(clipc, max(0.0, -math.log(max(pCw, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        # Residual-IDF-like query weighting proxy in slow path\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = (w0 ** alpha) * (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        # Leak a small amount of negative evidence (keeps robustness vs long boilerplate)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term = neg_s * per_term\n\n        score += w * gate * max(per_term, 0.0 if neg_s <= 0.0 else per_term)\n\n        # Bernoulli presence \"aboutness\" bonus (slow-path proxy using spec).\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n        if about_s > 0.0 and tf > 0.0:\n            score += about_s * w * (spec / max(clipc, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (single background; tempered + optional uniform mixture), normalized once.\n        # Using one background keeps the Dirichlet generative story coherent and tends to\n        # stabilize recall@100 across diverse datasets.\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # For compatibility with the rest of the pipeline\n        self._collection_prob_smooth = self._collection_prob\n\n        # Per-term surprisal under background for query clarity gating.\n        self._col_surprisal = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n\n        # Precompute df-based LM, EDR gate, residual-IDF query weights, and TF saturation exponent.\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Aboutness prior per term (presence channel) as a *presence residual*.\n        # Compare observed presence p_pres=df/N to expected presence under a token Poisson model:\n        #   p_exp(present) \u2248 1 - exp(-avgdl * p_col)\n        # This favors \"topic switch\" terms while not over-rewarding merely rare tokens.\n        if self.N > 0:\n            p_pres = np.maximum(p_doc, Config.epsilon)\n            p_col = np.maximum(self._collection_prob, Config.epsilon)\n            if bool(getattr(Config, \"aboutness_poisson\", True)):\n                avgdl = max(float(self.avgdl), 1.0)\n                p_exp = 1.0 - np.exp(-avgdl * p_col)\n                self._aboutness = np.log(np.maximum(p_pres, Config.epsilon) / np.maximum(p_exp, Config.epsilon))\n            else:\n                self._aboutness = np.log(np.maximum(p_pres, Config.epsilon) / np.maximum(p_col, Config.epsilon))\n            self._aboutness = np.maximum(self._aboutness, 0.0)\n        else:\n            self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\n\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)  # in [1-burst_s,1]\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0.0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); implements the full Residual-IDF / EDR-gated QL.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query clarity (per query constant): average surprisal of query terms.\n        # Use it to gate any AND-like pressure so we don't over-penalize vague/noisy queries.\n        if len(query_term_ids) > 0:\n            clarity = float(np.mean(self.corpus._col_surprisal[np.array(query_term_ids, dtype=np.int64)]))\n        else:\n            clarity = 0.0\n        clarity_gate = 1.0 - math.exp(-float(getattr(Config, \"clarity_gamma\", 0.0)) * max(clarity, 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            pCw = float(self.corpus._collection_prob[term_id])          # weighting background\n            pCs = float(self.corpus._collection_prob_smooth[term_id])   # smoothing background\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pCs + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            # Aboutness prior: add a bounded presence bonus when tf>0.\n            about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n            if about_s > 0.0:\n                scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._aboutness[term_id])\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * pCs + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3108), Performs well on beir_nfcorpus_recall@100 (0.2510), Performs well on beir_nfcorpus_index_time_ms (3589.2242), Performs well on beir_nfcorpus_query_time_ms (132.6624), Performs well on beir_scifact_ndcg@10 (0.6728), Performs well on beir_scifact_recall@100 (0.9253), Performs well on beir_scifact_index_time_ms (4842.8003), Performs well on beir_scifact_query_time_ms (549.5240), Performs well on bright_pony_ndcg@10 (0.1019), Performs well on bright_pony_recall@100 (0.3265), Performs well on bright_pony_index_time_ms (1494.5516), Performs well on bright_pony_query_time_ms (825.3766), Performs well on beir_arguana_ndcg@10 (0.2668), Performs well on beir_arguana_recall@100 (0.9108), Performs well on beir_arguana_index_time_ms (5959.8858), Performs well on beir_arguana_query_time_ms (40069.5295), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0343), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (10503.5541), Performs well on bright_theoremqa_theorems_query_time_ms (1683.4295), Performs well on beir_scidocs_ndcg@10 (0.1426), Performs well on beir_scidocs_recall@100 (0.3412), Performs well on beir_scidocs_index_time_ms (19865.9593), Performs well on beir_scidocs_query_time_ms (5043.9335), Performs well on bright_economics_ndcg@10 (0.1322), Performs well on bright_economics_recall@100 (0.3778), Performs well on bright_economics_index_time_ms (14275.9104), Performs well on bright_economics_query_time_ms (4662.2229), Performs well on bright_biology_ndcg@10 (0.2982), Performs well on bright_biology_recall@100 (0.5938), Performs well on bright_biology_index_time_ms (14800.1995), Performs well on bright_biology_query_time_ms (4488.8987), Performs well on beir_fiqa_ndcg@10 (0.2091), Performs well on beir_fiqa_recall@100 (0.5076), Performs well on beir_fiqa_index_time_ms (34112.4559), Performs well on beir_fiqa_query_time_ms (8969.4040), Performs well on bright_earth_science_ndcg@10 (0.2941), Performs well on bright_earth_science_recall@100 (0.6685), Performs well on bright_earth_science_index_time_ms (31994.8163), Performs well on bright_earth_science_query_time_ms (7541.0787), Performs well on bright_stackoverflow_ndcg@10 (0.1882), Performs well on bright_stackoverflow_recall@100 (0.4380), Performs well on bright_stackoverflow_index_time_ms (99267.1904), Performs well on bright_stackoverflow_query_time_ms (16931.1107), Performs well on beir_trec-covid_ndcg@10 (0.6129), Performs well on beir_trec-covid_recall@100 (0.1040), Performs well on beir_trec-covid_index_time_ms (133322.3959), Performs well on beir_trec-covid_query_time_ms (2109.1592), Performs well on avg_ndcg@10 (0.2720), Performs well on avg_recall@100 (0.4672), Performs well on combined_score (0.4282), Performs well on total_index_time_ms (374028.9437), Performs well on total_query_time_ms (93006.3298), Performs well on total_time_ms (467035.2735), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4282)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n    and_strength: float = 0.14  # 0 disables\n    and_saturation: float = 3.0\n\n    # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n    missing_strength: float = 0.07  # keep small to protect recall@100\n\n    # TF burstiness normalization (per-term exponent) to reduce domination by very common terms.\n    burstiness_strength: float = 0.30  # 0 disables; keep modest for recall\n\n    # Leak a tiny amount of negative evidence for \"weak hits\"\n    neg_strength: float = 0.12  # 0 disables\n\n    # Bernoulli \"presence channel\" (aboutness) in addition to multinomial tf evidence.\n    aboutness_strength: float = 0.08\n\n    # Gate AND-like pressures by query clarity so vague queries don't get over-penalized.\n    clarity_gamma: float = 0.22\n\n    # NEW: make the presence channel self-normalizing.\n    # Reward \"surprising presence\": log( p_doc / (1-exp(-L*p_col)) ), clipped at 0.\n    # This is more stable than log(p_doc/p_col) for frequent terms and helps passage tasks.\n    aboutness_poisson: bool = True\n\n    # NEW: Query-scope prior: prefer docs that explain more of the query's information mass.\n    # Implemented as log( matched_info / total_info ), clarity-gated.\n    query_scope_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        base = math.log(numerator / denominator + eps)\n        base = max(base, 0.0)\n\n        # Fallback term diagnostic: use a capped surprisal of the collection LM.\n        # (True residual-IDF uses df/N; we do that in vectorized rank()).\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n\n        # Approx residual-IDF in slow path: reuse spec as a proxy.\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with an optional uniform mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Aboutness prior per term (presence channel).\n        # Prefer a presence-residual against a simple token-Poisson expectation:\n        #   p_exp(term present in D) \u2248 1 - exp(-avgdl * p_col)\n        # Using log(p_doc/p_exp) makes the bonus focus on \"topic switches\" rather than raw rarity.\n        if self.N > 0:\n            if bool(getattr(Config, \"aboutness_poisson\", True)):\n                avgdl = max(float(self.avgdl), 1.0)\n                p_exp = 1.0 - np.exp(-avgdl * np.maximum(self._collection_prob, Config.epsilon))\n                self._aboutness = np.log(\n                    np.maximum(p_doc, Config.epsilon) / np.maximum(p_exp, Config.epsilon)\n                )\n            else:\n                self._aboutness = np.log(\n                    np.maximum(p_doc, Config.epsilon) / np.maximum(self._collection_prob, Config.epsilon)\n                )\n            self._aboutness = np.maximum(self._aboutness, 0.0)\n        else:\n            self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Also store per-term background surprisal for query clarity.\n        self._col_surprisal = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n\n        # Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n        # Common terms saturate more: tf -> tf^beta(w).\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query clarity gate (query-constant): use average background surprisal of query terms.\n        # Only enforce AND/missing penalties strongly when the query is informative.\n        if len(query_term_ids) > 0:\n            clarity = float(np.mean(self.corpus._col_surprisal[np.array(query_term_ids, dtype=np.int64)]))\n        else:\n            clarity = 0.0\n        clarity_gate = 1.0 - math.exp(-float(getattr(Config, \"clarity_gamma\", 0.0)) * max(clarity, 0.0))\n\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n\n        # Query-scope accumulators: matched information mass / total information mass.\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            # TF burstiness saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Apply EDR gate (query-independent)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            # Leak a small fraction of negative evidence for weak hits.\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            # Presence channel: bounded topical bonus when the term appears at least once.\n            if about_s > 0.0:\n                scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._aboutness[term_id])\n\n            # Query-scope: reward covering more of the query's \"information mass\"\n            # using the same residual-IDF weights already computed (stable, query-independent).\n            if scope_num is not None:\n                info = float(np.log1p(float(self.corpus._ridf_qweight[term_id])))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            # Missing-term anti-evidence (clarity-gated to protect recall on vague queries).\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            # Soft-AND: saturating coverage reward (also clarity-gated).\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n\n        if scope_num is not None and scope_den > 0.0:\n            scope_frac = np.maximum(scope_num / max(scope_den, eps), eps)\n            scores += (scope_s * clarity_gate) * np.log(scope_frac)\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3103), Performs well on beir_nfcorpus_recall@100 (0.2510), Performs well on beir_nfcorpus_index_time_ms (3590.2329), Performs well on beir_nfcorpus_query_time_ms (132.4305), Performs well on beir_scifact_ndcg@10 (0.6726), Performs well on beir_scifact_recall@100 (0.9253), Performs well on beir_scifact_index_time_ms (4908.7172), Performs well on beir_scifact_query_time_ms (570.3415), Performs well on bright_pony_ndcg@10 (0.1037), Performs well on bright_pony_recall@100 (0.3271), Performs well on bright_pony_index_time_ms (1398.8385), Performs well on bright_pony_query_time_ms (887.6802), Performs well on beir_arguana_ndcg@10 (0.2661), Performs well on beir_arguana_recall@100 (0.9108), Performs well on beir_arguana_index_time_ms (5928.2681), Performs well on beir_arguana_query_time_ms (41853.3984), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0343), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (10781.4624), Performs well on bright_theoremqa_theorems_query_time_ms (1644.7262), Performs well on beir_scidocs_ndcg@10 (0.1427), Performs well on beir_scidocs_recall@100 (0.3406), Performs well on beir_scidocs_index_time_ms (19932.1039), Performs well on beir_scidocs_query_time_ms (5233.3878), Performs well on bright_economics_ndcg@10 (0.1330), Performs well on bright_economics_recall@100 (0.3774), Performs well on bright_economics_index_time_ms (14165.6266), Performs well on bright_economics_query_time_ms (4866.0845), Performs well on bright_biology_ndcg@10 (0.2968), Performs well on bright_biology_recall@100 (0.5938), Performs well on bright_biology_index_time_ms (15146.6104), Performs well on bright_biology_query_time_ms (4697.2333), Performs well on beir_fiqa_ndcg@10 (0.2090), Performs well on beir_fiqa_recall@100 (0.5063), Performs well on beir_fiqa_index_time_ms (34266.4927), Performs well on beir_fiqa_query_time_ms (9344.2933), Performs well on bright_earth_science_ndcg@10 (0.2945), Performs well on bright_earth_science_recall@100 (0.6657), Performs well on bright_earth_science_index_time_ms (32249.8969), Performs well on bright_earth_science_query_time_ms (8026.5925), Performs well on bright_stackoverflow_ndcg@10 (0.1883), Performs well on bright_stackoverflow_recall@100 (0.4423), Performs well on bright_stackoverflow_index_time_ms (99347.0739), Performs well on bright_stackoverflow_query_time_ms (18112.4136), Performs well on beir_trec-covid_ndcg@10 (0.6122), Performs well on beir_trec-covid_recall@100 (0.1039), Performs well on beir_trec-covid_index_time_ms (134886.0137), Performs well on beir_trec-covid_query_time_ms (2260.0130), Performs well on avg_ndcg@10 (0.2720), Performs well on avg_recall@100 (0.4672), Performs well on combined_score (0.4282), Performs well on total_index_time_ms (376601.3370), Performs well on total_query_time_ms (97628.5947), Performs well on total_time_ms (474229.9317), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4273)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0  # base Dirichlet mass (good general default)\n\n    epsilon: float = 1e-9\n\n    # --- Core idea: tempered+uniform background + residual-IDF query weighting + mild TF saturation ---\n    # Tempered background: p_t(w) \u221d p(w)^tau (tau<1 flattens very frequent terms)\n    collection_temper: float = 0.85\n    # Mix in a tiny uniform background to reduce peakiness in very skewed corpora\n    uniform_bg_mass: float = 0.03\n\n    # Residual-IDF query weight uses df/N vs token LM (computed in Corpus)\n    residual_idf_strength: float = 0.9\n\n    # Gate per-term contribution by spread mismatch (EDR): log(p_doc/p_col)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Query TF saturation (repeated query tokens have diminishing impact)\n    query_tf_power: float = 0.6\n\n    # Term-specific TF saturation: tf -> tf^beta(w), common terms saturate more\n    burstiness_strength: float = 0.30\n\n    # Soft coverage reward (encourage matching more distinct query terms)\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n\n    # Small missing-term anti-evidence inside candidate set (kept small to protect recall)\n    missing_strength: float = 0.07\n\n    # Leak a small amount of negative evidence for weak hits (helps early precision)\n    neg_strength: float = 0.12\n\n    # Weak log-normal-ish length prior (helps against very long boilerplate docs)\n    length_prior_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Tempered collection probability.\n\n    Instead of raw P(w|C), use p_t(w) \u221d p(w)^tau with tau<1.\n    Intuition: frequent terms dominate the background and reduce discriminativeness;\n    tempering flattens the background while remaining probabilistic.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = max(p, Config.epsilon) ** tau\n    # Exact renormalization is done in Corpus for vectorized scoring; this is a safe fallback.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Store unique terms (avoid double iteration) but keep tf in weights.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Compact query model:\n        - iterate unique terms\n        - keep qtf in term_weights so repeated tokens still add evidence\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Residual-IDF / EDR-gated QL (slow path).\n\n    The fast path (rank) uses precomputed per-term arrays in Corpus; here we approximate\n    the same *story* using only token LM pC:\n      - tempered background discourages stopword matches\n      - mild qtf saturation\n      - only reward positive evidence (Lucene-style), but leak a small fraction of negatives\n        to reduce \"one-off mention\" boilerplate effects\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        pC = collection_probability(term, corpus_term_freq, total_tokens)\n\n        per_term = math.log((1.0 + tf / (mu * pC + eps)) / ((doc_length + mu) / mu) + eps)\n\n        # Specificity proxy (true residual-IDF uses df/N, available in fast path)\n        spec = min(clipc, max(0.0, -math.log(max(pC, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        # Residual-IDF-like query weighting proxy in slow path\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = (w0 ** alpha) * (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        # Leak a small amount of negative evidence (keeps robustness vs long boilerplate)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term = neg_s * per_term\n\n        # Mostly Lucene-style: reward positives; negatives only via leakage above.\n        score += w * gate * max(per_term, 0.0 if neg_s <= 0.0 else per_term)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with optional uniform mixture; normalized once for speed.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute df-based LM, EDR gate, residual-IDF query weights, and TF saturation exponent.\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)  # in [1-burst_s,1]\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0.0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); implements the full Residual-IDF / EDR-gated QL.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            pC = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pC + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            # leak a little negative evidence\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * pC + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += miss_s * w * miss * base0\n\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3105), Performs well on beir_nfcorpus_recall@100 (0.2510), Performs well on beir_nfcorpus_index_time_ms (3570.8540), Performs well on beir_nfcorpus_query_time_ms (119.9420), Performs well on beir_scifact_ndcg@10 (0.6729), Performs well on beir_scifact_recall@100 (0.9253), Performs well on beir_scifact_index_time_ms (4918.0811), Performs well on beir_scifact_query_time_ms (526.2071), Performs well on bright_pony_ndcg@10 (0.1097), Performs well on bright_pony_recall@100 (0.3311), Performs well on bright_pony_index_time_ms (1394.4127), Performs well on bright_pony_query_time_ms (806.4462), Performs well on beir_arguana_ndcg@10 (0.2633), Performs well on beir_arguana_recall@100 (0.9051), Performs well on beir_arguana_index_time_ms (5881.9265), Performs well on beir_arguana_query_time_ms (38154.5987), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0339), Performs well on bright_theoremqa_theorems_recall@100 (0.1557), Performs well on bright_theoremqa_theorems_index_time_ms (10694.0900), Performs well on bright_theoremqa_theorems_query_time_ms (1538.9794), Performs well on beir_scidocs_ndcg@10 (0.1420), Performs well on beir_scidocs_recall@100 (0.3402), Performs well on beir_scidocs_index_time_ms (19746.7246), Performs well on beir_scidocs_query_time_ms (4888.1956), Performs well on bright_economics_ndcg@10 (0.1318), Performs well on bright_economics_recall@100 (0.3749), Performs well on bright_economics_index_time_ms (14217.7541), Performs well on bright_economics_query_time_ms (4571.0046), Performs well on bright_biology_ndcg@10 (0.2941), Performs well on bright_biology_recall@100 (0.5905), Performs well on bright_biology_index_time_ms (15362.1468), Performs well on bright_biology_query_time_ms (4314.0514), Performs well on beir_fiqa_ndcg@10 (0.2085), Performs well on beir_fiqa_recall@100 (0.5022), Performs well on beir_fiqa_index_time_ms (33846.6961), Performs well on beir_fiqa_query_time_ms (8610.0481), Performs well on bright_earth_science_ndcg@10 (0.2904), Performs well on bright_earth_science_recall@100 (0.6650), Performs well on bright_earth_science_index_time_ms (31911.7453), Performs well on bright_earth_science_query_time_ms (7402.3845), Performs well on bright_stackoverflow_ndcg@10 (0.1888), Performs well on bright_stackoverflow_recall@100 (0.4482), Performs well on bright_stackoverflow_index_time_ms (98267.3225), Performs well on bright_stackoverflow_query_time_ms (16685.0466), Performs well on beir_trec-covid_ndcg@10 (0.6172), Performs well on beir_trec-covid_recall@100 (0.1038), Performs well on beir_trec-covid_index_time_ms (134017.4807), Performs well on beir_trec-covid_query_time_ms (2090.2559), Performs well on avg_ndcg@10 (0.2719), Performs well on avg_recall@100 (0.4661), Performs well on combined_score (0.4273), Performs well on total_index_time_ms (373829.2346), Performs well on total_query_time_ms (89707.1601), Performs well on total_time_ms (463536.3947), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4273)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n    and_strength: float = 0.14  # 0 disables\n    and_saturation: float = 3.0\n\n    # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n    missing_strength: float = 0.07  # keep small to protect recall@100\n\n    # NEW: TF burstiness normalization (per-term exponent) to reduce domination by very common terms.\n    # Probabilistic story: approximate a sublinear TF model (mixture of \"aboutness\" + repetition noise).\n    # Common terms (low idf) get stronger saturation.\n    burstiness_strength: float = 0.30  # 0 disables; keep modest for recall\n\n    # NEW: Add back a tiny amount of negative evidence for \"weak hits\":\n    # when a query term appears but below its expectation under the doc+prior mixture,\n    # we keep a small fraction of the negative log-likelihood ratio.\n    # This reduces ranking of boilerplate/long docs that mention many terms once.\n    neg_strength: float = 0.12  # 0 disables\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        base = math.log(numerator / denominator + eps)\n        base = max(base, 0.0)\n\n        # Fallback term diagnostic: use a capped surprisal of the collection LM.\n        # (True residual-IDF uses df/N; we do that in vectorized rank()).\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n\n        # Approx residual-IDF in slow path: reuse spec as a proxy.\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with an optional uniform mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n        # Common terms saturate more: tf -> tf^beta(w).\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            # TF burstiness saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Apply EDR gate (query-independent)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            # Keep most of the classic \"only reward positive LLR\", but leak a small fraction\n            # of negative evidence for weak hits to improve early precision.\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += w * per_term  # includes leaked negatives if enabled\n\n            # Missing-term anti-evidence (scaled tf=0 Dirichlet contribution) within candidates.\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += miss_s * w * miss * base0\n\n            # Soft-AND: saturating coverage reward (encourages matching more query terms)\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3105), Performs well on beir_nfcorpus_recall@100 (0.2510), Performs well on beir_nfcorpus_index_time_ms (3585.8080), Performs well on beir_nfcorpus_query_time_ms (120.6080), Performs well on beir_scifact_ndcg@10 (0.6729), Performs well on beir_scifact_recall@100 (0.9253), Performs well on beir_scifact_index_time_ms (4856.9142), Performs well on beir_scifact_query_time_ms (534.4172), Performs well on bright_pony_ndcg@10 (0.1097), Performs well on bright_pony_recall@100 (0.3311), Performs well on bright_pony_index_time_ms (1473.4872), Performs well on bright_pony_query_time_ms (790.7443), Performs well on beir_arguana_ndcg@10 (0.2633), Performs well on beir_arguana_recall@100 (0.9051), Performs well on beir_arguana_index_time_ms (5881.2448), Performs well on beir_arguana_query_time_ms (38038.7680), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0339), Performs well on bright_theoremqa_theorems_recall@100 (0.1557), Performs well on bright_theoremqa_theorems_index_time_ms (10631.9319), Performs well on bright_theoremqa_theorems_query_time_ms (1605.4113), Performs well on beir_scidocs_ndcg@10 (0.1420), Performs well on beir_scidocs_recall@100 (0.3402), Performs well on beir_scidocs_index_time_ms (19643.1758), Performs well on beir_scidocs_query_time_ms (4853.3809), Performs well on bright_economics_ndcg@10 (0.1318), Performs well on bright_economics_recall@100 (0.3749), Performs well on bright_economics_index_time_ms (14090.7396), Performs well on bright_economics_query_time_ms (4476.3081), Performs well on bright_biology_ndcg@10 (0.2941), Performs well on bright_biology_recall@100 (0.5905), Performs well on bright_biology_index_time_ms (15207.7363), Performs well on bright_biology_query_time_ms (4380.0538), Performs well on beir_fiqa_ndcg@10 (0.2085), Performs well on beir_fiqa_recall@100 (0.5022), Performs well on beir_fiqa_index_time_ms (33344.9355), Performs well on beir_fiqa_query_time_ms (8908.5117), Performs well on bright_earth_science_ndcg@10 (0.2904), Performs well on bright_earth_science_recall@100 (0.6650), Performs well on bright_earth_science_index_time_ms (31310.2214), Performs well on bright_earth_science_query_time_ms (7206.8459), Performs well on bright_stackoverflow_ndcg@10 (0.1888), Performs well on bright_stackoverflow_recall@100 (0.4482), Performs well on bright_stackoverflow_index_time_ms (99579.9722), Performs well on bright_stackoverflow_query_time_ms (17001.6652), Performs well on beir_trec-covid_ndcg@10 (0.6172), Performs well on beir_trec-covid_recall@100 (0.1038), Performs well on beir_trec-covid_index_time_ms (137644.1000), Performs well on beir_trec-covid_query_time_ms (2097.8336), Performs well on avg_ndcg@10 (0.2719), Performs well on avg_recall@100 (0.4661), Performs well on combined_score (0.4273), Performs well on total_index_time_ms (377250.2669), Performs well on total_query_time_ms (90014.5479), Performs well on total_time_ms (467264.8148), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4271)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Collection-aware term burstiness penalty: tf -> tf^beta(w) with beta tied to idf01(w)\n    burstiness_strength: float = 0.35\n\n    # Two-stage background (collection + uniform over vocab).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND query aggregation.\n    and_strength: float = 0.15  # 0 disables\n    and_saturation: float = 3.0  # controls how quickly missing terms stop hurting\n\n    # Dirichlet \"anti-evidence\" for missing query terms (tf=0).\n    # Keep slightly smaller by default: too much anti-evidence can hurt recall@100.\n    missing_strength: float = 0.06\n\n    # Query-scope prior: reward explaining a larger fraction of the query's information mass.\n    query_scope_strength: float = 0.07  # 0 disables\n\n    # NEW: Risk-sensitive aggregation (R\u00e9nyi / power mean) over per-term evidence.\n    # Add a small concave utility on term evidence to prefer \"balanced\" multi-term matches\n    # over one-term spikes (improves early precision on multi-aspect queries).\n    risk_strength: float = 0.12  # 0 disables\n    risk_power: float = 0.55     # in (0,1]: smaller => more \"AND-like\"\n    risk_saturation: float = 6.0 # scale inside utility; keeps it stable across corpora\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n    scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n    scope_num = 0.0\n    scope_den = 0.0\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        if burst_s > 0.0:\n            idf01_proxy = 1.0 / (1.0 + max(0.0, math.log(1.0 / max(p_collection, eps))))\n            beta = 1.0 - burst_s * (1.0 - idf01_proxy)\n            term_count = term_count ** beta if beta != 1.0 else term_count\n\n        base = math.log((1.0 + term_count / (mu * p_collection + eps)) / ((doc_length + mu) / mu) + eps)\n        base = max(base, 0.0)\n\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n        if scope_s > 0.0:\n            info = math.log1p(1.0 + ridf_s * (spec / max(clipc, eps)))\n            scope_den += info\n            if term_count > 0.0:\n                scope_num += info\n\n    if scope_s > 0.0 and scope_den > 0.0:\n        score += scope_s * math.log(max(scope_num / scope_den, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) for Dirichlet smoothing\n        # plus a small uniform background mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V)\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob[:] = (1.0 - gamma) * p_col + gamma * p_uni\n            self._collection_prob[:] = np.maximum(self._collection_prob, Config.epsilon)\n        else:\n            self._collection_prob[:] = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: IDF-in-[0,1] and burstiness exponent beta(w) for tf saturation (common terms saturate more).\n        # idf = log((N+1)/(df+1)) is classic; we normalize by max idf to get [0,1] without tuning per dataset.\n        if self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n        else:\n            idf01 = np.zeros(self.vocab_size, dtype=np.float64)\n\n        if burst_s > 0.0:\n            # beta in [1-burst_s, 1]; common terms (low idf01) -> smaller beta -> more saturation\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        scope_s = float(getattr(Config, \"query_scope_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query-scope accumulators: per-doc matched specificity mass vs total query specificity mass.\n        scope_num = np.zeros(len(candidate_docs), dtype=np.float64) if scope_s > 0.0 else None\n        scope_den = 0.0\n\n        # Optional risk-sensitive aggregator accumulator\n        risk_s = float(getattr(Config, \"risk_strength\", 0.0))\n        risk_p = float(getattr(Config, \"risk_power\", 0.55))\n        risk_sat = float(getattr(Config, \"risk_saturation\", 6.0))\n        risk_acc = np.zeros(len(candidate_docs), dtype=np.float64) if risk_s > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * p_collection + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            present = np.maximum(per_term, 0.0)\n            contrib = w * present\n            scores += contrib\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)\n                    scores += miss_s * w * miss * base0\n\n            if and_acc is not None:\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n            if scope_num is not None:\n                # Stable per-term \"information mass\" proxy derived from residual IDF weights.\n                info = float(np.log1p(self.corpus._ridf_qweight[term_id]))\n                scope_den += info\n                scope_num += (tf_row > 0.0).astype(np.float64) * info\n\n            if risk_acc is not None:\n                # Concave utility of evidence; then power-mean across terms.\n                # u(x)=log(1+x/sat) is information-like (diminishing returns).\n                u = np.log1p(contrib / max(risk_sat, eps))\n                risk_acc += np.power(np.maximum(u, 0.0), risk_p)\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        if scope_num is not None:\n            denom = max(scope_den, eps)\n            scope_frac = np.maximum(scope_num / denom, eps)\n            scores += scope_s * np.log(scope_frac)\n\n        if risk_acc is not None and len(query_term_ids) > 0:\n            # Turn accumulator into a normalized power mean (scale-stable across query length).\n            pm = np.power(risk_acc / float(len(query_term_ids)), 1.0 / max(risk_p, eps))\n            scores += risk_s * pm\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4250)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0  # base Dirichlet mass (good general default)\n    epsilon: float = 1e-9\n\n    # Tempered background: p_t(w) \u221d p(w)^tau (tau<1 flattens very frequent terms)\n    collection_temper: float = 0.85\n    # Mix in a tiny uniform background to reduce peakiness in very skewed corpora\n    uniform_bg_mass: float = 0.03\n\n    # Residual-IDF query weight uses df/N vs token LM (computed in Corpus)\n    residual_idf_strength: float = 0.9\n\n    # Gate per-term contribution by spread mismatch (EDR): log(p_doc/p_col)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Query TF saturation (repeated query tokens have diminishing impact)\n    query_tf_power: float = 0.6\n\n    # Term-specific TF saturation: tf -> tf^beta(w), common terms saturate more\n    burstiness_strength: float = 0.30\n\n    # Soft coverage reward (encourage matching more distinct query terms)\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n\n    # Small missing-term anti-evidence inside candidate set (kept small to protect recall)\n    missing_strength: float = 0.07\n\n    # Leak a small amount of negative evidence for weak hits (helps early precision)\n    neg_strength: float = 0.12\n\n    # Weak log-normal-ish length prior (helps against very long boilerplate docs)\n    length_prior_strength: float = 0.06\n\n    # NEW: Two-temperature background.\n    # Use a slightly *different* temperature for smoothing than for weighting.\n    # Intuition: smoothing should be conservative (avoid over-boosting rare terms -> recall),\n    # while weighting can be more aggressive to improve early precision.\n    smoothing_temper: float = 0.92\n\n    # NEW: \"Aboutness prior\" (Bernoulli presence channel).\n    # Adds evidence for *presence* beyond counts, using P(w in D | about topic) vs P(w in C).\n    aboutness_strength: float = 0.08\n\n    # NEW: Query clarity gate for any AND-like pressure (prevents harming noisy queries).\n    clarity_gamma: float = 0.20\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Tempered collection probability.\n\n    Instead of raw P(w|C), use p_t(w) \u221d p(w)^tau with tau<1.\n    Intuition: frequent terms dominate the background and reduce discriminativeness;\n    tempering flattens the background while remaining probabilistic.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = max(p, Config.epsilon) ** tau\n    # Exact renormalization is done in Corpus for vectorized scoring; this is a safe fallback.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Store unique terms (avoid double iteration) but keep tf in weights.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Compact query model:\n        - iterate unique terms\n        - keep qtf in term_weights so repeated tokens still add evidence\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Residual-IDF / EDR-gated QL (slow path).\n\n    The fast path (rank) uses precomputed per-term arrays in Corpus; here we approximate\n    the same *story* using only token LM pC:\n      - tempered background discourages stopword matches\n      - mild qtf saturation\n      - only reward positive evidence (Lucene-style), but leak a small fraction of negatives\n        to reduce \"one-off mention\" boilerplate effects\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        pCw = collection_probability(term, corpus_term_freq, total_tokens)\n        # Use a slightly more conservative background inside smoothing to avoid over-boosting rare terms.\n        tau_s = float(getattr(Config, \"smoothing_temper\", getattr(Config, \"collection_temper\", 1.0)))\n        pCs = max(pCw, eps) ** (tau_s / max(float(getattr(Config, \"collection_temper\", 1.0)), eps))\n\n        per_term = math.log((1.0 + tf / (mu * pCs + eps)) / ((doc_length + mu) / mu) + eps)\n\n        # Specificity proxy (true residual-IDF uses df/N, available in fast path)\n        spec = min(clipc, max(0.0, -math.log(max(pCw, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        # Residual-IDF-like query weighting proxy in slow path\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = (w0 ** alpha) * (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        # Leak a small amount of negative evidence (keeps robustness vs long boilerplate)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term = neg_s * per_term\n\n        score += w * gate * max(per_term, 0.0 if neg_s <= 0.0 else per_term)\n\n        # Bernoulli presence \"aboutness\" bonus (slow-path proxy using spec).\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n        if about_s > 0.0 and tf > 0.0:\n            score += about_s * w * (spec / max(clipc, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM:\n        # (1) a \"weighting\" background (used for diagnostics like EDR/ridf, and in slow-path)\n        # (2) a \"smoothing\" background (used inside Dirichlet denominator mu*pC)\n        #\n        # Rationale: in QL, pC plays two roles: (a) background probability for smoothing and\n        # (b) notion of commonness for discriminativeness. A single temperature forces a tradeoff.\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        def _temper_and_mix(p: np.ndarray, tau: float) -> np.ndarray:\n            if tau != 1.0:\n                tmp = np.power(np.maximum(p, Config.epsilon), tau)\n                z = float(np.sum(tmp))\n                p_t = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n            else:\n                p_t = np.maximum(p, Config.epsilon)\n            gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n            if gamma > 0.0 and self.vocab_size > 0:\n                p_uni = 1.0 / float(self.vocab_size)\n                p_t = np.maximum((1.0 - gamma) * p_t + gamma * p_uni, Config.epsilon)\n            return p_t\n\n        tau_w = float(getattr(Config, \"collection_temper\", 1.0))\n        tau_s = float(getattr(Config, \"smoothing_temper\", tau_w))\n\n        self._collection_prob = _temper_and_mix(base_p, tau_w)\n        self._collection_prob_smooth = _temper_and_mix(base_p, tau_s)\n\n        # Also precompute per-term surprisal under the weighting background for query clarity gating.\n        self._col_surprisal = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n\n        # Precompute df-based LM, EDR gate, residual-IDF query weights, and TF saturation exponent.\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Aboutness prior per term: log-odds of \"term appears at least once\" under doc-spread vs background.\n        # This is a Bernoulli presence channel; it rewards topical anchor terms even with tf=1,\n        # complementing the multinomial tf channel (often helps short/snippet corpora).\n        if self.N > 0:\n            p_pres = np.maximum(p_doc, Config.epsilon)\n            p_bg = np.maximum(self._collection_prob, Config.epsilon)\n            self._aboutness = np.log(p_pres / p_bg)\n            self._aboutness = np.maximum(self._aboutness, 0.0)  # only positive \"topical\" boosts\n        else:\n            self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\n\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)  # in [1-burst_s,1]\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0.0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); implements the full Residual-IDF / EDR-gated QL.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query clarity (per query constant): average surprisal of query terms.\n        # Use it to gate any AND-like pressure so we don't over-penalize vague/noisy queries.\n        if len(query_term_ids) > 0:\n            clarity = float(np.mean(self.corpus._col_surprisal[np.array(query_term_ids, dtype=np.int64)]))\n        else:\n            clarity = 0.0\n        clarity_gate = 1.0 - math.exp(-float(getattr(Config, \"clarity_gamma\", 0.0)) * max(clarity, 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            pCw = float(self.corpus._collection_prob[term_id])          # weighting background\n            pCs = float(self.corpus._collection_prob_smooth[term_id])   # smoothing background\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pCs + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            # Aboutness prior: add a bounded presence bonus when tf>0.\n            about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n            if about_s > 0.0:\n                scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._aboutness[term_id])\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * pCs + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4232)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n    and_strength: float = 0.14  # 0 disables\n    and_saturation: float = 3.0\n\n    # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n    missing_strength: float = 0.07  # keep small to protect recall@100\n\n    # TF burstiness normalization (per-term exponent) to reduce domination by very common terms.\n    burstiness_strength: float = 0.30  # 0 disables; keep modest for recall\n\n    # Leak a tiny amount of negative evidence for \"weak hits\"\n    neg_strength: float = 0.12  # 0 disables\n\n    # NEW: add a Bernoulli \"presence channel\" (aboutness) in addition to multinomial tf evidence.\n    # Intuition: for many tasks (SciFact, NQ-like, short passages), a single occurrence of an\n    # anchor term is strong topical evidence even when tf is small.\n    aboutness_strength: float = 0.10\n\n    # NEW: gate AND-like pressures by query clarity so vague queries don't get over-penalized.\n    clarity_gamma: float = 0.25\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        base = math.log(numerator / denominator + eps)\n        base = max(base, 0.0)\n\n        # Fallback term diagnostic: use a capped surprisal of the collection LM.\n        # (True residual-IDF uses df/N; we do that in vectorized rank()).\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n\n        # Approx residual-IDF in slow path: reuse spec as a proxy.\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with an optional uniform mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: aboutness prior per term (presence channel).\n        # Use positive log-odds between document-spread LM and token LM: log(p_doc/p_col).\n        # Clipped at 0: only boost topical anchors, don't punish stopwords.\n        if self.N > 0:\n            self._aboutness = np.log(np.maximum(p_doc, Config.epsilon) / np.maximum(self._collection_prob, Config.epsilon))\n            self._aboutness = np.maximum(self._aboutness, 0.0)\n        else:\n            self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Also store per-term background surprisal for query clarity.\n        self._col_surprisal = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n\n        # Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n        # Common terms saturate more: tf -> tf^beta(w).\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query clarity gate (query-constant): use average background surprisal of query terms.\n        # Only enforce AND/missing penalties strongly when the query is informative.\n        if len(query_term_ids) > 0:\n            clarity = float(np.mean(self.corpus._col_surprisal[np.array(query_term_ids, dtype=np.int64)]))\n        else:\n            clarity = 0.0\n        clarity_gate = 1.0 - math.exp(-float(getattr(Config, \"clarity_gamma\", 0.0)) * max(clarity, 0.0))\n\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            # TF burstiness saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Apply EDR gate (query-independent)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            # Leak a small fraction of negative evidence for weak hits.\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            # Presence channel: bounded topical bonus when the term appears at least once.\n            if about_s > 0.0:\n                scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._aboutness[term_id])\n\n            # Missing-term anti-evidence (clarity-gated to protect recall on vague queries).\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            # Soft-AND: saturating coverage reward (also clarity-gated).\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4249)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    residual_idf_strength: float = 0.9\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    query_tf_power: float = 0.6\n    burstiness_strength: float = 0.30\n\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n    missing_strength: float = 0.07\n    neg_strength: float = 0.12\n    length_prior_strength: float = 0.06\n\n    # NEW: Bernoulli \"presence channel\" mixed with multinomial tf channel.\n    # Motivation: tf-based QL underweights single-occurrence anchors in short passages/snippets.\n    # Use log-odds between document-spread LM (df/N) and token LM (p_col) as an \"aboutness\" bonus.\n    aboutness_strength: float = 0.07  # keep small: helps nDCG@10 without hurting recall@100\n\n    # NEW: Query clarity gating for AND-like pressure (missing evidence + coordination reward).\n    # On vague queries, hardening toward AND can hurt recall; gate by average surprisal.\n    clarity_gamma: float = 0.22\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Tempered collection probability.\n\n    Instead of raw P(w|C), use p_t(w) \u221d p(w)^tau with tau<1.\n    Intuition: frequent terms dominate the background and reduce discriminativeness;\n    tempering flattens the background while remaining probabilistic.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = max(p, Config.epsilon) ** tau\n    # Exact renormalization is done in Corpus for vectorized scoring; this is a safe fallback.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Store unique terms (avoid double iteration) but keep tf in weights.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Compact query model:\n        - iterate unique terms\n        - keep qtf in term_weights so repeated tokens still add evidence\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Residual-IDF / EDR-gated QL (slow path).\n\n    The fast path (rank) uses precomputed per-term arrays in Corpus; here we approximate\n    the same *story* using only token LM pC:\n      - tempered background discourages stopword matches\n      - mild qtf saturation\n      - only reward positive evidence (Lucene-style), but leak a small fraction of negatives\n        to reduce \"one-off mention\" boilerplate effects\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        pC = collection_probability(term, corpus_term_freq, total_tokens)\n\n        per_term = math.log((1.0 + tf / (mu * pC + eps)) / ((doc_length + mu) / mu) + eps)\n\n        # Specificity proxy (true residual-IDF uses df/N, available in fast path)\n        spec = min(clipc, max(0.0, -math.log(max(pC, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        # Residual-IDF-like query weighting proxy in slow path\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = (w0 ** alpha) * (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        # Leak a small amount of negative evidence (keeps robustness vs long boilerplate)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term = neg_s * per_term\n\n        # Mostly Lucene-style: reward positives; negatives only via leakage above.\n        score += w * gate * max(per_term, 0.0 if neg_s <= 0.0 else per_term)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). Uses the same features as rank() when possible.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    # Slow-path scoring stays as a lightweight approximation (tf-channel only).\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with optional uniform mixture; normalized once for speed.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute df-based LM, EDR gate, residual-IDF query weights, TF saturation exponent,\n        # and a Bernoulli \"aboutness\" presence prior.\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Aboutness prior (presence channel): positive log-odds between document-spread and token LMs.\n        if self.N > 0:\n            self._aboutness = np.log(np.maximum(p_doc, Config.epsilon) / np.maximum(self._collection_prob, Config.epsilon))\n            self._aboutness = np.maximum(self._aboutness, 0.0)\n        else:\n            self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Also store background surprisal for query clarity gating.\n        self._col_surprisal = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0.0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); implements the full Residual-IDF / EDR-gated QL.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query clarity gate (query-constant): average surprisal under background.\n        # Only push AND/missing evidence when the query is informative.\n        if len(query_term_ids) > 0:\n            clarity = float(np.mean(self.corpus._col_surprisal[np.array(query_term_ids, dtype=np.int64)]))\n        else:\n            clarity = 0.0\n        clarity_gate = 1.0 - math.exp(-float(getattr(Config, \"clarity_gamma\", 0.0)) * max(clarity, 0.0))\n\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            pC = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pC + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            # leak a little negative evidence\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            # Presence channel: bounded topical bonus when the term appears at least once.\n            if about_s > 0.0:\n                scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._aboutness[term_id])\n\n            # Clarity-gated missing-term evidence (protects recall on vague/noisy queries).\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * pC + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            # Soft-AND coverage reward (also clarity-gated).\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4141, Type: Migrant)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 2000.0  # Dirichlet smoothing parameter\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C) = total frequency / total tokens.\n    EVOLVE: try other collection models (e.g., weighted by document importance, IDF-based, etc.).\n    \"\"\"\n    if term not in corpus_term_freq:\n        return Config.epsilon\n    return corpus_term_freq[term] / max(total_tokens, 1)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Score one document for one query using Query Likelihood with Dirichlet smoothing.\n\n    Formula (Lucene/Pyserini variant):\n        Score(D, Q) = \u03a3_{w in Q} max(0, log(1 + c(w,D)/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc)))\n\n    This matches Pyserini's LMDirichletSimilarity which clamps per-term scores to 0.\n    This differs from academic QL which allows negative per-term scores.\n\n    Default behavior: Matches Pyserini's LMDirichletSimilarity with \u03bc=2000.\n    EVOLVE: design a probabilistic formulation with deep, fundamental, intuitive justification.\n    Try other smoothing methods (Jelinek-Mercer, absolute discounting), document priors,\n    term dependencies, multi-field models, etc.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    score = 0.0\n\n    for term in query_repr.terms:\n        # c(w, D): term count in document\n        term_count = float(doc_tf.get(term, 0))\n\n        # P(w | C): collection probability\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        # Lucene/Pyserini formula: log(1 + freq/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc))\n        # Clamp per-term score to 0 (Lucene's behavior for negative term scores)\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        per_term_score = math.log(numerator / denominator + eps)\n\n        # Apply query term weight and clamp to 0\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * max(per_term_score, 0.0)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            # Get collection probability for this term\n            p_collection = self.corpus._collection_prob[term_id]\n\n            # Get term frequencies for candidates\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n\n            # Lucene/Pyserini formula: log(1 + freq/(\u03bc*P(w|C))) + log(\u03bc/(L_D+\u03bc))\n            numerator = 1.0 + tf_row / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term_scores = np.log(numerator / denominator + eps)\n\n            # Apply query term weight and clamp to 0\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            scores += w * np.maximum(per_term_scores, 0.0)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4029, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    epsilon: float = 1e-9\n\n    # Term-adaptive smoothing bounds\n    mu_min: float = 120.0\n    mu_max: float = 3200.0\n\n    # Temper collection background\n    collection_alpha: float = 0.82\n\n    # Query informativeness weighting (soft IDF in query model)\n    query_idf_gamma: float = 0.40\n\n    # Reward matching more distinct query terms (aspect coverage -> recall@100)\n    coverage_beta: float = 0.30\n\n    # Adjacent-bigram pseudo-field (cheap proximity -> nDCG@10)\n    bigram_lambda: float = 0.22\n    bigram_mu: float = 450.0\n    bigram_idf_gamma: float = 0.8\n\n    # Prune extremely common query terms (noise-channel view)\n    query_prune_pC: float = 0.03\n\n    # Query-baseline centering (LIKELIHOOD-RATIO) was hurting ranking:\n    # it is a per-query constant, so it cannot change within-query ranking at all.\n    # Keep parameter for compatibility but default to 0 so it is a no-op.\n    query_baseline_delta: float = 0.0\n\n    # Concave tf: make much gentler; strong concavity (0.55) tends to suppress legit repetition and hurt recall.\n    tf_concavity: float = 0.88\n\n    # New: clarity-gated soft-AND aspect pressure (Beta-Bernoulli view).\n    # Helps early precision on multi-aspect queries without over-penalizing noisy/short queries.\n    and_beta: float = 0.18\n    and_smooth: float = 0.50\n    and_min_terms: int = 3\n    clarity_gate_gamma: float = 1.0\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Tempered collection probability.\n\n    Instead of raw P(w|C), we use a \"temperature\" transform:\n        P_t(w|C) \u221d P(w|C)^alpha\n\n    Motivation (probabilistic/information-theoretic):\n    - QL with a raw background can over-credit matches on extremely frequent terms.\n    - Tempering (alpha < 1) flattens the background, making rare terms relatively\n      more informative under the same likelihood ratio.\n\n    Note: we only need an unnormalized transform for ranking if applied consistently,\n    but we keep it as a probability-like quantity via a local transform; epsilon guards\n    against zero.\n    \"\"\"\n    eps = Config.epsilon\n    tf = corpus_term_freq.get(term, 0)\n    p = tf / max(total_tokens, 1)\n    # Temper; keep in (0,1] and avoid 0\n    p_t = max(p, eps) ** Config.collection_alpha\n    return max(p_t, eps)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float, bigram_frequencies: Counter[str]):\n        self.term_frequencies = term_frequencies\n        self.length = length\n        self.bigram_frequencies = bigram_frequencies\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Add a cheap proximity view via adjacent bigrams (no positions needed).\"\"\"\n        tf = Counter(tokens)\n        b = Counter()\n        for a, c in zip(tokens, tokens[1:]):\n            b[f\"{a}\u241f{c}\"] += 1\n        return cls(term_frequencies=tf, length=float(len(tokens)), bigram_frequencies=b)\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Query model: compress multiplicity into TF weights (keeps signal but reduces noise),\n        and keep original order for bigrams.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        tf = Counter(tokens)\n        seen: set[str] = set()\n        ordered_unique: list[str] = []\n        for t in tokens:\n            if t not in seen:\n                ordered_unique.append(t)\n                seen.add(t)\n        return cls(terms=ordered_unique, term_weights={t: float(c) for t, c in tf.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n    doc_bigram_tf: Counter[str] | None = None,\n) -> float:\n    \"\"\"\n    TT-QL + coverage + gated-bigram channel + clarity-gated soft-AND + gentler burstiness control.\n\n    Key fix: remove the query-baseline subtraction (it is constant per query, so it cannot\n    affect within-query ranking; it can only shift scores and sometimes interact badly\n    with candidate/non-candidate handling in some evaluators).\n\n    Add: clarity-gated soft-AND (Beta-Bernoulli view):\n      prefer docs that cover more query aspects, but only for sufficiently clear multi-term queries.\n    \"\"\"\n    eps = Config.epsilon\n    score = 0.0\n\n    log_eps = math.log(eps)\n    denom_norm = 0.0 - log_eps + eps\n\n    # Prune very common query terms using background mass\n    kept_terms: list[str] = []\n    for t in query_repr.terms:\n        if collection_probability(t, corpus_term_freq, total_tokens) <= Config.query_prune_pC:\n            kept_terms.append(t)\n    if not kept_terms:\n        kept_terms = query_repr.terms\n\n    matched_u = 0\n    rho = Config.tf_concavity\n\n    # Unigram channel\n    for term in kept_terms:\n        c_raw = float(doc_tf.get(term, 0.0))\n        if c_raw > 0.0:\n            matched_u += 1\n        c = c_raw ** rho if c_raw > 0.0 else 0.0\n\n        pC = collection_probability(term, corpus_term_freq, total_tokens)\n        lp = math.log(max(pC, eps))\n        t = (lp - log_eps) / denom_norm\n        t = min(1.0, max(0.0, t))\n        mu_w = Config.mu_min + t * (Config.mu_max - Config.mu_min)\n\n        numerator = 1.0 + c / (mu_w * pC + eps)\n        denominator = (doc_length + mu_w) / mu_w\n        per_term = math.log(numerator / denominator + eps)\n\n        idf_like = max(0.0, -math.log(max(pC, eps)))\n        wq = query_repr.term_weights.get(term, 1.0) * (1.0 + Config.query_idf_gamma * idf_like)\n        score += (1.0 - Config.bigram_lambda) * wq * max(per_term, 0.0)\n\n    # Coverage prior (distinct matches)\n    score += Config.coverage_beta * math.log(1.0 + matched_u)\n\n    # Clarity-gated soft-AND\n    qn = len(kept_terms)\n    if qn >= Config.and_min_terms:\n        clarity = 0.0\n        for t0 in kept_terms:\n            clarity += max(0.0, -math.log(max(collection_probability(t0, corpus_term_freq, total_tokens), eps)))\n        clarity /= max(float(qn), 1.0)\n        gate = 1.0 - math.exp(-Config.clarity_gate_gamma * clarity)\n\n        mfrac = (float(matched_u) + Config.and_smooth) / (float(qn) + 2.0 * Config.and_smooth)\n        score += (Config.and_beta * gate) * math.log(max(mfrac, eps))\n\n    # Bigram channel with surprisal gate (adjacent query pairs)\n    if doc_bigram_tf is not None and Config.bigram_lambda > 0.0 and len(kept_terms) >= 2:\n        for a, b in zip(kept_terms, kept_terms[1:]):\n            bg = f\"{a}\u241f{b}\"\n            c2_raw = float(doc_bigram_tf.get(bg, 0.0))\n            c2 = c2_raw ** rho if c2_raw > 0.0 else 0.0\n\n            pCa = collection_probability(a, corpus_term_freq, total_tokens)\n            pCb = collection_probability(b, corpus_term_freq, total_tokens)\n            pC2 = max(pCa * pCb, eps)\n\n            numerator = 1.0 + c2 / (Config.bigram_mu * pC2 + eps)\n            denominator = (max(doc_length - 1.0, 1.0) + Config.bigram_mu) / Config.bigram_mu\n            per_bigram = math.log(numerator / denominator + eps)\n\n            gate2 = 1.0 + Config.bigram_idf_gamma * max(0.0, -math.log(pC2))\n            w2 = 0.5 * (query_repr.term_weights.get(a, 1.0) + query_repr.term_weights.get(b, 1.0))\n            score += Config.bigram_lambda * w2 * gate2 * max(per_bigram, 0.0)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). Uses stored doc bigrams if available.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    doc_bi = corpus._doc_bigram_tf[doc_idx] if hasattr(corpus, \"_doc_bigram_tf\") else None\n    return retrieval_score(\n        q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens, doc_bigram_tf=doc_bi\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Store per-doc unigram and bigram counters (bigrams enable cheap proximity signal)\n        self._doc_tf_dicts: list[Counter[str]] = []\n        self._doc_bigram_tf: list[Counter[str]] = []\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            self._doc_tf_dicts.append(term_counts)\n\n            bi = Counter()\n            for a, b in zip(doc, doc[1:]):\n                bi[f\"{a}\u241f{b}\"] += 1\n            self._doc_bigram_tf.append(bi)\n\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection probability array for vectorized scoring\n        self._collection_prob = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            self._collection_prob[tid] = collection_probability(\n                term, self.corpus_term_freq, self.total_tokens\n            )\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score (unigram+coverage+baseline+concave-tf).\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        log_eps = math.log(eps)\n        denom_norm = 0.0 - log_eps + eps\n\n        match_counts = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Apply pruning using precomputed collection probs.\n        kept: list[tuple[int, float]] = []\n        for i, tid in enumerate(query_term_ids):\n            pC = float(self.corpus._collection_prob[tid])\n            if pC <= Config.query_prune_pC:\n                w = query_term_weights[i] if query_term_weights is not None else 1.0\n                kept.append((tid, float(w)))\n        if not kept:\n            kept = [\n                (tid, float(query_term_weights[i] if query_term_weights is not None else 1.0))\n                for i, tid in enumerate(query_term_ids)\n            ]\n\n        # Query baseline (constant for all docs for this query)\n        baseline = 0.0\n\n        rho = Config.tf_concavity\n        for tid, w_base in kept:\n            pC = float(self.corpus._collection_prob[tid])\n            pC = max(pC, eps)\n\n            lp = math.log(pC)\n            t = (lp - log_eps) / denom_norm\n            t = float(np.clip(t, 0.0, 1.0))\n            mu_w = Config.mu_min + t * (Config.mu_max - Config.mu_min)\n\n            tf_row = self.corpus.tf_matrix[tid, candidate_docs].toarray().ravel()\n            match_counts += (tf_row > 0).astype(np.float64)\n\n            # concave tf transform\n            if rho != 1.0:\n                tf_row = np.power(tf_row, rho, dtype=np.float64)\n\n            numerator = 1.0 + tf_row / (mu_w * pC + eps)\n            denominator = (doc_lengths + mu_w) / mu_w\n            per_term_scores = np.log(numerator / denominator + eps)\n\n            idf_like = max(0.0, -math.log(pC))\n            wq = w_base * (1.0 + Config.query_idf_gamma * idf_like)\n\n            baseline += wq * idf_like\n            scores += (1.0 - Config.bigram_lambda) * wq * np.maximum(per_term_scores, 0.0)\n\n        scores += Config.coverage_beta * np.log(1.0 + match_counts)\n        scores -= Config.query_baseline_delta * baseline\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4273, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0  # base Dirichlet mass (good general default)\n\n    epsilon: float = 1e-9\n\n    # --- Core idea: tempered+uniform background + residual-IDF query weighting + mild TF saturation ---\n    # Tempered background: p_t(w) \u221d p(w)^tau (tau<1 flattens very frequent terms)\n    collection_temper: float = 0.85\n    # Mix in a tiny uniform background to reduce peakiness in very skewed corpora\n    uniform_bg_mass: float = 0.03\n\n    # Residual-IDF query weight uses df/N vs token LM (computed in Corpus)\n    residual_idf_strength: float = 0.9\n\n    # Gate per-term contribution by spread mismatch (EDR): log(p_doc/p_col)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Query TF saturation (repeated query tokens have diminishing impact)\n    query_tf_power: float = 0.6\n\n    # Term-specific TF saturation: tf -> tf^beta(w), common terms saturate more\n    burstiness_strength: float = 0.30\n\n    # Soft coverage reward (encourage matching more distinct query terms)\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n\n    # Small missing-term anti-evidence inside candidate set (kept small to protect recall)\n    missing_strength: float = 0.07\n\n    # Leak a small amount of negative evidence for weak hits (helps early precision)\n    neg_strength: float = 0.12\n\n    # Weak log-normal-ish length prior (helps against very long boilerplate docs)\n    length_prior_strength: float = 0.06\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Tempered collection probability.\n\n    Instead of raw P(w|C), use p_t(w) \u221d p(w)^tau with tau<1.\n    Intuition: frequent terms dominate the background and reduce discriminativeness;\n    tempering flattens the background while remaining probabilistic.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = max(p, Config.epsilon) ** tau\n    # Exact renormalization is done in Corpus for vectorized scoring; this is a safe fallback.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Store unique terms (avoid double iteration) but keep tf in weights.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Compact query model:\n        - iterate unique terms\n        - keep qtf in term_weights so repeated tokens still add evidence\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Residual-IDF / EDR-gated QL (slow path).\n\n    The fast path (rank) uses precomputed per-term arrays in Corpus; here we approximate\n    the same *story* using only token LM pC:\n      - tempered background discourages stopword matches\n      - mild qtf saturation\n      - only reward positive evidence (Lucene-style), but leak a small fraction of negatives\n        to reduce \"one-off mention\" boilerplate effects\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        pC = collection_probability(term, corpus_term_freq, total_tokens)\n\n        per_term = math.log((1.0 + tf / (mu * pC + eps)) / ((doc_length + mu) / mu) + eps)\n\n        # Specificity proxy (true residual-IDF uses df/N, available in fast path)\n        spec = min(clipc, max(0.0, -math.log(max(pC, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        # Residual-IDF-like query weighting proxy in slow path\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = (w0 ** alpha) * (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        # Leak a small amount of negative evidence (keeps robustness vs long boilerplate)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term = neg_s * per_term\n\n        # Mostly Lucene-style: reward positives; negatives only via leakage above.\n        score += w * gate * max(per_term, 0.0 if neg_s <= 0.0 else per_term)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with optional uniform mixture; normalized once for speed.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute df-based LM, EDR gate, residual-IDF query weights, and TF saturation exponent.\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)  # in [1-burst_s,1]\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0.0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); implements the full Residual-IDF / EDR-gated QL.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        for i, term_id in enumerate(query_term_ids):\n            pC = float(self.corpus._collection_prob[term_id])\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pC + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            # leak a little negative evidence\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * pC + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += miss_s * w * miss * base0\n\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += and_strength * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4232, Type: Experimental)\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    # Tempered background LM p_t(w) \u221d p(w)^tau\n    collection_temper: float = 0.85\n\n    # Query term burstiness saturation (qtf^alpha)\n    query_tf_power: float = 0.6\n\n    # Document length prior (log-normal-ish); keep small to avoid recall loss\n    length_prior_strength: float = 0.06\n\n    # EDR gate: token-vs-document spread mismatch\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Residual-IDF query weighting (df/N vs token LM)\n    residual_idf_strength: float = 0.9\n\n    # Two-stage background: (1-\u03b3) p_col + \u03b3 * Uniform(V).\n    uniform_bg_mass: float = 0.03  # 0 disables\n\n    # Soft-AND coverage: reward covering more query terms (without hard booleaning).\n    and_strength: float = 0.14  # 0 disables\n    and_saturation: float = 3.0\n\n    # Lightweight missing-term anti-evidence (scaled Dirichlet tf=0 term) inside candidates.\n    missing_strength: float = 0.07  # keep small to protect recall@100\n\n    # TF burstiness normalization (per-term exponent) to reduce domination by very common terms.\n    burstiness_strength: float = 0.30  # 0 disables; keep modest for recall\n\n    # Leak a tiny amount of negative evidence for \"weak hits\"\n    neg_strength: float = 0.12  # 0 disables\n\n    # NEW: add a Bernoulli \"presence channel\" (aboutness) in addition to multinomial tf evidence.\n    # Intuition: for many tasks (SciFact, NQ-like, short passages), a single occurrence of an\n    # anchor term is strong topical evidence even when tf is small.\n    aboutness_strength: float = 0.10\n\n    # NEW: gate AND-like pressures by query clarity so vague queries don't get over-penalized.\n    clarity_gamma: float = 0.25\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Collection probability P(w | C).\n\n    Base: tf_C(w) / |C|.\n    EVOLVE (here): use a tempered background model to reduce dominance of very frequent terms:\n        p_t(w) \u221d p(w)^tau, tau in (0,1]\n    which increases relative mass of rarer terms (information gain) while staying a proper LM\n    after renormalization. We precompute this normalization inside Corpus for speed; here we\n    provide a safe fallback if called directly.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = p ** tau\n    # Fallback approximate renorm: keep scale comparable; exact renorm done in Corpus.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Keep representation simple, but normalize very long queries by soft-booleaning:\n        repeated terms are handled later via qtf^alpha; here we just keep tokens.\n        \"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Dirichlet QL with two *information-diagnostic* modifiers:\n\n    1) EDR gate (as before, but fallback uses a milder, monotone specificity proxy).\n    2) Residual-IDF query weighting: boost query terms that are common as tokens when present\n       yet not widely spread across documents.\n\n    The residual notion is: token commonness p_col(w) vs doc spread p_doc(w)=df/N.\n    In the vectorized path we can compute p_doc exactly and apply it per term id.\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n\n    score = 0.0\n    qtf = Counter(query_repr.terms)\n\n    for term, c_q in qtf.items():\n        term_count = float(doc_tf.get(term, 0))\n        p_collection = collection_probability(term, corpus_term_freq, total_tokens)\n\n        numerator = 1.0 + term_count / (mu * p_collection + eps)\n        denominator = (doc_length + mu) / mu\n        base = math.log(numerator / denominator + eps)\n        base = max(base, 0.0)\n\n        # Fallback term diagnostic: use a capped surprisal of the collection LM.\n        # (True residual-IDF uses df/N; we do that in vectorized rank()).\n        spec = min(clipc, max(0.0, -math.log(max(p_collection, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = w0 * (float(c_q) ** alpha)\n\n        # Approx residual-IDF in slow path: reuse spec as a proxy.\n        w *= (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        score += w * gate * base\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()\n        self.total_tokens = 0\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (tempered) with an optional uniform mixture to reduce peakiness.\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # Precompute per-term diagnostics using BOTH token LM and document-spread LM.\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)  # df-based LM\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        # EDR gate: 1 + \u03bb * clip(log(p_doc / p_col), [-c,c])\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Residual-IDF query weight per term id: 1 + s * max(0, log(p_doc / p_col)).\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # NEW: aboutness prior per term (presence channel).\n        # Use positive log-odds between document-spread LM and token LM: log(p_doc/p_col).\n        # Clipped at 0: only boost topical anchors, don't punish stopwords.\n        if self.N > 0:\n            self._aboutness = np.log(np.maximum(p_doc, Config.epsilon) / np.maximum(self._collection_prob, Config.epsilon))\n            self._aboutness = np.maximum(self._aboutness, 0.0)\n        else:\n            self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Also store per-term background surprisal for query clarity.\n        self._col_surprisal = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n\n        # Per-term TF exponent beta(w) in [1-burst_s, 1], derived from normalized IDF.\n        # Common terms saturate more: tf -> tf^beta(w).\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Length prior\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query clarity gate (query-constant): use average background surprisal of query terms.\n        # Only enforce AND/missing penalties strongly when the query is informative.\n        if len(query_term_ids) > 0:\n            clarity = float(np.mean(self.corpus._col_surprisal[np.array(query_term_ids, dtype=np.int64)]))\n        else:\n            clarity = 0.0\n        clarity_gate = 1.0 - math.exp(-float(getattr(Config, \"clarity_gamma\", 0.0)) * max(clarity, 0.0))\n\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            p_collection = self.corpus._collection_prob[term_id]\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            # TF burstiness saturation: tf -> tf^beta(w) (common terms saturate more).\n            beta = self.corpus._tf_beta[term_id]\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            numerator = 1.0 + tf_eff / (mu * p_collection + eps)\n            denominator = (doc_lengths + mu) / mu\n            per_term = np.log(numerator / denominator + eps)\n\n            # Apply EDR gate (query-independent)\n            per_term *= self.corpus._edr_gate[term_id]\n\n            w0 = query_term_weights[i] if query_term_weights is not None else 1.0\n            w = (w0 * self.corpus._ridf_qweight[term_id]) ** alpha\n\n            # Leak a small fraction of negative evidence for weak hits.\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            # Presence channel: bounded topical bonus when the term appears at least once.\n            if about_s > 0.0:\n                scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._aboutness[term_id])\n\n            # Missing-term anti-evidence (clarity-gated to protect recall on vague queries).\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * p_collection + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            # Soft-AND: saturating coverage reward (also clarity-gated).\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform Query Likelihood seed \u2014 maximum freedom for discovering a new probabilistic retrieval method.\n\nCore idea: document representation + query representation + probabilistic scoring method.\nThe evaluator requires: QL, Corpus, tokenize, LuceneTokenizer; QL must have rank() and score().\nEverything else is evolvable. Default behavior: Dirichlet smoothing (matches Pyserini LMDirichletSimilarity).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    mu: float = 1800.0  # base Dirichlet mass (good general default)\n    epsilon: float = 1e-9\n\n    # Tempered background: p_t(w) \u221d p(w)^tau (tau<1 flattens very frequent terms)\n    collection_temper: float = 0.85\n    # Mix in a tiny uniform background to reduce peakiness in very skewed corpora\n    uniform_bg_mass: float = 0.03\n\n    # Residual-IDF query weight uses df/N vs token LM (computed in Corpus)\n    residual_idf_strength: float = 0.9\n\n    # Gate per-term contribution by spread mismatch (EDR): log(p_doc/p_col)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Query TF saturation (repeated query tokens have diminishing impact)\n    query_tf_power: float = 0.6\n\n    # Term-specific TF saturation: tf -> tf^beta(w), common terms saturate more\n    burstiness_strength: float = 0.30\n\n    # Soft coverage reward (encourage matching more distinct query terms)\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n\n    # Small missing-term anti-evidence inside candidate set (kept small to protect recall)\n    missing_strength: float = 0.07\n\n    # Leak a small amount of negative evidence for weak hits (helps early precision)\n    neg_strength: float = 0.12\n\n    # Weak log-normal-ish length prior (helps against very long boilerplate docs)\n    length_prior_strength: float = 0.06\n\n    # Aboutness prior (Bernoulli presence channel) computed as a presence residual.\n    aboutness_strength: float = 0.08\n    aboutness_poisson: bool = True\n\n    # Query clarity gate for any AND-like pressure (prevents harming noisy queries).\n    clarity_gamma: float = 0.20\n\n\n# -----------------------------------------------------------------------------\n# Collection Language Model \u2014 EVOLVE: how to compute P(w | C)\n# -----------------------------------------------------------------------------\n\ndef collection_probability(term: str, corpus_term_freq: Counter[str], total_tokens: int) -> float:\n    \"\"\"\n    Tempered collection probability.\n\n    Instead of raw P(w|C), use p_t(w) \u221d p(w)^tau with tau<1.\n    Intuition: frequent terms dominate the background and reduce discriminativeness;\n    tempering flattens the background while remaining probabilistic.\n    \"\"\"\n    tf = corpus_term_freq.get(term, 0)\n    if tf <= 0 or total_tokens <= 0:\n        return Config.epsilon\n    p = tf / float(total_tokens)\n    tau = float(getattr(Config, \"collection_temper\", 1.0))\n    p_t = max(p, Config.epsilon) ** tau\n    # Exact renormalization is done in Corpus for vectorized scoring; this is a safe fallback.\n    return max(p_t, Config.epsilon)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        # Store unique terms (avoid double iteration) but keep tf in weights.\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Compact query model:\n        - iterate unique terms\n        - keep qtf in term_weights so repeated tokens still add evidence\n        \"\"\"\n        tc = Counter(tokens)\n        return cls(terms=list(tc.keys()), term_weights={t: float(c) for t, c in tc.items()})\n\n\n# -----------------------------------------------------------------------------\n# Probabilistic retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    corpus_term_freq: Counter[str],\n    total_tokens: int,\n) -> float:\n    \"\"\"\n    Residual-IDF / EDR-gated QL (slow path).\n\n    The fast path (rank) uses precomputed per-term arrays in Corpus; here we approximate\n    the same *story* using only token LM pC:\n      - tempered background discourages stopword matches\n      - mild qtf saturation\n      - only reward positive evidence (Lucene-style), but leak a small fraction of negatives\n        to reduce \"one-off mention\" boilerplate effects\n    \"\"\"\n    mu, eps = Config.mu, Config.epsilon\n    alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n    lam = float(getattr(Config, \"edr_strength\", 0.0))\n    clipc = float(getattr(Config, \"edr_clip\", 3.0))\n    ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n    neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        pCw = collection_probability(term, corpus_term_freq, total_tokens)\n        per_term = math.log((1.0 + tf / (mu * pCw + eps)) / ((doc_length + mu) / mu) + eps)\n\n        # Specificity proxy (true residual-IDF uses df/N, available in fast path)\n        spec = min(clipc, max(0.0, -math.log(max(pCw, eps)) - 1.0))\n        gate = 1.0 + lam * (spec / max(clipc, eps))\n\n        # Residual-IDF-like query weighting proxy in slow path\n        w0 = float(query_repr.term_weights.get(term, 1.0))\n        w = (w0 ** alpha) * (1.0 + ridf_s * (spec / max(clipc, eps)))\n\n        # Leak a small amount of negative evidence (keeps robustness vs long boilerplate)\n        if neg_s > 0.0 and per_term < 0.0:\n            per_term = neg_s * per_term\n\n        score += w * gate * max(per_term, 0.0 if neg_s <= 0.0 else per_term)\n\n        # Bernoulli presence \"aboutness\" bonus (slow-path proxy using spec).\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n        if about_s > 0.0 and tf > 0.0:\n            score += about_s * w * (spec / max(clipc, eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by QL.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.corpus_term_freq, corpus.total_tokens)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build vocabulary\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Collection statistics for Query Likelihood\n        self.corpus_term_freq = Counter()  # Total frequency of each term in collection\n        self.total_tokens = 0  # Sum of all doc lengths\n\n        # Build sparse TF matrix and inverted index\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            self.total_tokens += len(doc)\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self.corpus_term_freq[term] += count  # Accumulate collection frequencies\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Collection LM (single background; tempered + optional uniform mixture), normalized once.\n        # Using one background keeps the Dirichlet generative story coherent and tends to\n        # stabilize recall@100 across diverse datasets.\n        base_p = np.zeros(self.vocab_size, dtype=np.float64)\n        for term, tid in self._vocab.items():\n            tf = float(self.corpus_term_freq.get(term, 0))\n            base_p[tid] = tf / max(float(self.total_tokens), 1.0)\n\n        tau = float(getattr(Config, \"collection_temper\", 1.0))\n        if tau != 1.0:\n            tmp = np.power(np.maximum(base_p, Config.epsilon), tau)\n            z = float(np.sum(tmp))\n            p_col = np.maximum(tmp / max(z, Config.epsilon), Config.epsilon)\n        else:\n            p_col = np.maximum(base_p, Config.epsilon)\n\n        gamma = float(getattr(Config, \"uniform_bg_mass\", 0.0))\n        if gamma > 0.0 and self.vocab_size > 0:\n            p_uni = 1.0 / float(self.vocab_size)\n            self._collection_prob = np.maximum((1.0 - gamma) * p_col + gamma * p_uni, Config.epsilon)\n        else:\n            self._collection_prob = p_col\n\n        # For compatibility with the rest of the pipeline\n        self._collection_prob_smooth = self._collection_prob\n\n        # Per-term surprisal under background for query clarity gating.\n        self._col_surprisal = -np.log(np.maximum(self._collection_prob, Config.epsilon))\n\n        # Precompute df-based LM, EDR gate, residual-IDF query weights, and TF saturation exponent.\n        if self.N > 0:\n            p_doc = np.maximum(self._df / float(self.N), Config.epsilon)\n        else:\n            p_doc = np.full(self.vocab_size, Config.epsilon, dtype=np.float64)\n\n        lam = float(getattr(Config, \"edr_strength\", 0.0))\n        clipc = float(getattr(Config, \"edr_clip\", 3.0))\n        if lam > 0.0 and self.N > 0:\n            ratio = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ratio = np.clip(ratio, -clipc, clipc)\n            self._edr_gate = 1.0 + lam * ratio\n        else:\n            self._edr_gate = np.ones(self.vocab_size, dtype=np.float64)\n\n        ridf_s = float(getattr(Config, \"residual_idf_strength\", 0.0))\n        if ridf_s > 0.0 and self.N > 0:\n            ridf = np.log(np.maximum(p_doc / np.maximum(self._collection_prob, Config.epsilon), Config.epsilon))\n            ridf = np.maximum(ridf, 0.0)\n            self._ridf_qweight = 1.0 + ridf_s * np.minimum(ridf, clipc) / max(clipc, Config.epsilon)\n        else:\n            self._ridf_qweight = np.ones(self.vocab_size, dtype=np.float64)\n\n        # Aboutness prior per term (presence channel) as a *presence residual*.\n        # Compare observed presence p_pres=df/N to expected presence under a token Poisson model:\n        #   p_exp(present) \u2248 1 - exp(-avgdl * p_col)\n        # This favors \"topic switch\" terms while not over-rewarding merely rare tokens.\n        if self.N > 0:\n            p_pres = np.maximum(p_doc, Config.epsilon)\n            p_col = np.maximum(self._collection_prob, Config.epsilon)\n            if bool(getattr(Config, \"aboutness_poisson\", True)):\n                avgdl = max(float(self.avgdl), 1.0)\n                p_exp = 1.0 - np.exp(-avgdl * p_col)\n                self._aboutness = np.log(np.maximum(p_pres, Config.epsilon) / np.maximum(p_exp, Config.epsilon))\n            else:\n                self._aboutness = np.log(np.maximum(p_pres, Config.epsilon) / np.maximum(p_col, Config.epsilon))\n            self._aboutness = np.maximum(self._aboutness, 0.0)\n        else:\n            self._aboutness = np.zeros(self.vocab_size, dtype=np.float64)\n\n        burst_s = float(getattr(Config, \"burstiness_strength\", 0.0))\n        if burst_s > 0.0 and self.N > 0:\n            idf = np.log((float(self.N) + 1.0) / (self._df + 1.0))\n            idf01 = idf / max(float(np.max(idf)), Config.epsilon)\n            self._tf_beta = 1.0 - burst_s * (1.0 - idf01)  # in [1-burst_s,1]\n        else:\n            self._tf_beta = np.ones(self.vocab_size, dtype=np.float64)\n\n        s = float(getattr(Config, \"length_prior_strength\", 0.0))\n        if s > 0.0:\n            logL = np.log(np.maximum(self.doc_lengths, 1.0))\n            m = math.log(max(self.avgdl, 1.0))\n            self._length_prior = -s * np.square(logL - m)\n        else:\n            self._length_prior = np.zeros(self.N, dtype=np.float64)\n\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# QL (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass QL:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); implements the full Residual-IDF / EDR-gated QL.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        mu, eps = Config.mu, Config.epsilon\n        alpha = float(getattr(Config, \"query_tf_power\", 1.0))\n        and_strength = float(getattr(Config, \"and_strength\", 0.0))\n        and_sat = float(getattr(Config, \"and_saturation\", 3.0))\n        miss_s = float(getattr(Config, \"missing_strength\", 0.0))\n        neg_s = float(getattr(Config, \"neg_strength\", 0.0))\n\n        doc_lengths = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        and_acc = np.zeros(len(candidate_docs), dtype=np.float64) if and_strength > 0.0 else None\n\n        # Query clarity (per query constant): average surprisal of query terms.\n        # Use it to gate any AND-like pressure so we don't over-penalize vague/noisy queries.\n        if len(query_term_ids) > 0:\n            clarity = float(np.mean(self.corpus._col_surprisal[np.array(query_term_ids, dtype=np.int64)]))\n        else:\n            clarity = 0.0\n        clarity_gate = 1.0 - math.exp(-float(getattr(Config, \"clarity_gamma\", 0.0)) * max(clarity, 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            pCw = float(self.corpus._collection_prob[term_id])          # weighting background\n            pCs = float(self.corpus._collection_prob_smooth[term_id])   # smoothing background\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pCs + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            # Aboutness prior: add a bounded presence bonus when tf>0.\n            about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n            if about_s > 0.0:\n                scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._aboutness[term_id])\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * pCs + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        # CRITICAL: QL scores are negative log probabilities, so non-candidates must have very negative score\n        # Otherwise documents without query terms (score=0.0) would rank HIGHER than relevant documents (negative scores)\n        all_scores = np.full(self.corpus.N, -1e10, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"QL\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"collection_probability\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    mu: float = 1800.0  # base Dirichlet mass (good general default)\n    epsilon: float = 1e-9\n\n    # Tempered background: p_t(w) \u221d p(w)^tau (tau<1 flattens very frequent terms)\n    collection_temper: float = 0.85\n    # Mix in a tiny uniform background to reduce peakiness in very skewed corpora\n    uniform_bg_mass: float = 0.03\n\n    # Residual-IDF query weight uses df/N vs token LM (computed in Corpus)\n    residual_idf_strength: float = 0.9\n\n    # Gate per-term contribution by spread mismatch (EDR): log(p_doc/p_col)\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    # Query TF saturation (repeated query tokens have diminishing impact)\n    query_tf_power: float = 0.6\n\n    # Term-specific TF saturation: tf -> tf^beta(w), common terms saturate more\n    burstiness_strength: float = 0.30\n\n    # Soft coverage reward (encourage matching more distinct query terms)\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n\n    # Small missing-term anti-evidence inside candidate set (kept small to protect recall)\n    missing_strength: float = 0.07\n\n    # Leak a small amount of negative evidence for weak hits (helps early precision)\n    neg_strength: float = 0.12\n\n    # Weak log-normal-ish length prior (helps against very long boilerplate docs)\n    length_prior_strength: float = 0.06\n\n    # Aboutness prior (Bernoulli presence channel) computed as a presence residual.\n    aboutness_strength: float = 0.08\n    aboutness_poisson: bool = True\n\n    # Query clarity gate for any AND-like pressure (prevents harming noisy queries).\n    clarity_gamma: float = 0.20\n=======\nclass Config:\n    mu: float = 1800.0\n    epsilon: float = 1e-9\n\n    collection_temper: float = 0.85\n    uniform_bg_mass: float = 0.03\n\n    residual_idf_strength: float = 0.9\n    edr_strength: float = 0.45\n    edr_clip: float = 2.5\n\n    query_tf_power: float = 0.6\n    burstiness_strength: float = 0.30\n\n    and_strength: float = 0.14\n    and_saturation: float = 3.0\n    missing_strength: float = 0.07\n    neg_strength: float = 0.12\n    length_prior_strength: float = 0.06\n\n    aboutness_strength: float = 0.08\n    aboutness_poisson: bool = True\n\n    clarity_gamma: float = 0.20\n\n    # NEW: a second (simpler) coordination signal: fraction of distinct query terms present.\n    # This is a Beta-Bernoulli view of \"aboutness coverage\" that is cheap and often helps nDCG@10.\n    coord_strength: float = 0.04  # keep small to avoid hurting recall\n    coord_smooth: float = 0.5\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        for i, term_id in enumerate(query_term_ids):\n            pCw = float(self.corpus._collection_prob[term_id])          # weighting background\n            pCs = float(self.corpus._collection_prob_smooth[term_id])   # smoothing background\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pCs + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            # Aboutness prior: add a bounded presence bonus when tf>0.\n            about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n            if about_s > 0.0:\n                scores += about_s * w * (tf_row > 0.0).astype(np.float64) * float(self.corpus._aboutness[term_id])\n\n            if miss_s > 0.0:\n                miss = (tf_row <= 0.0).astype(np.float64)\n                if np.any(miss):\n                    base0 = np.log((mu * pCs + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        if and_acc is not None and len(query_term_ids) > 0:\n            scores += (and_strength * clarity_gate) * (and_acc / float(len(query_term_ids)))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n=======\n        coord_s = float(getattr(Config, \"coord_strength\", 0.0))\n        coord_smooth = float(getattr(Config, \"coord_smooth\", 0.5))\n        coord_hits = np.zeros(len(candidate_docs), dtype=np.float64) if coord_s > 0.0 else None\n\n        about_s = float(getattr(Config, \"aboutness_strength\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            pCs = float(self.corpus._collection_prob_smooth[term_id])\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            beta = float(self.corpus._tf_beta[term_id])\n            tf_eff = np.power(tf_row, beta) if beta != 1.0 else tf_row\n\n            per_term = np.log((1.0 + tf_eff / (mu * pCs + eps)) / ((doc_lengths + mu) / mu) + eps)\n            per_term *= float(self.corpus._edr_gate[term_id])\n\n            w0 = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            w = (w0 * float(self.corpus._ridf_qweight[term_id])) ** alpha\n\n            if neg_s > 0.0:\n                per_term = np.where(per_term >= 0.0, per_term, neg_s * per_term)\n\n            scores += w * per_term\n\n            present = (tf_row > 0.0).astype(np.float64)\n\n            if about_s > 0.0:\n                scores += about_s * w * present * float(self.corpus._aboutness[term_id])\n\n            if coord_hits is not None:\n                coord_hits += present\n\n            if miss_s > 0.0:\n                miss = 1.0 - present\n                if np.any(miss):\n                    base0 = np.log((mu * pCs + eps) / (doc_lengths + mu + eps) + eps)  # < 0\n                    scores += (miss_s * clarity_gate) * w * miss * base0\n\n            if and_acc is not None:\n                contrib = w * np.maximum(per_term, 0.0)\n                and_acc += np.tanh(contrib / max(and_sat, eps))\n\n        qn = float(len(query_term_ids))\n        if and_acc is not None and qn > 0.0:\n            scores += (and_strength * clarity_gate) * (and_acc / qn)\n\n        # NEW: coordination as a Bernoulli coverage likelihood ratio:\n        # log( (m+\u03b1)/(n-m+\u03b1) ) is too sharp; use log(m+\u03b1) - log(n+2\u03b1) instead (stable).\n        if coord_hits is not None and qn > 0.0:\n            frac = (coord_hits + coord_smooth) / (qn + 2.0 * coord_smooth)\n            scores += (coord_s * clarity_gate) * np.log(np.maximum(frac, eps))\n\n        scores += self.corpus._length_prior[candidate_docs]\n        return scores\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}