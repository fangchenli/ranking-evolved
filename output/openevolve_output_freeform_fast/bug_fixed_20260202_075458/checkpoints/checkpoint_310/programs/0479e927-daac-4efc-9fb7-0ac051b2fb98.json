{"id": "0479e927-daac-4efc-9fb7-0ac051b2fb98", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + informative-coverage pressure.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (robustness for morphology/identifiers; cheaper/cleaner than char-ngrams).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (cheap lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Residual-IDF reliability gate: downweight ultra-common query terms smoothly (recall-safe).\n    residual_idf_tau: float = 1.25\n\n    # Rare-term anchor (single best rare hit; bounded, recall-safe).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Bounded specificity/aboutness prior (positive-PMI density lift).\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    # Cohesion prior via concentration over matched query evidence.\n    gini_alpha: float = 0.08\n\n    # Compatibility (Corpus still builds norm_array).\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + clarity gate + IDF-mass coverage,\n    plus bounded priors that mainly affect early precision:\n\n    (1) Rare-term *anchor* (single best rare hit; recall-safe, robust to verbose queries).\n    (2) Specificity/aboutness gain via positive PMI (bounded).\n    (3) Cohesion prior via Gini concentration of matched query evidence m_t = wt*tf.\n        Higher concentration => doc likely focuses on a decisive subset of query constraints.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Rare anchor: strongest hinge among matched high-idf terms.\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    # Gini concentration accumulators over matched query terms.\n    use_gini = float(getattr(Config, \"gini_alpha\", 0.0)) != 0.0\n    m_sum = 0.0\n    m_sq_sum = 0.0\n    k_match = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)  # in (0,1)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_spec:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n        if use_gini:\n            mt = wt * tf\n            if mt > 0.0:\n                k_match += 1.0\n                m_sum += mt\n                m_sq_sum += mt * mt\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_gini and k_match >= 2.0 and m_sum > 0.0:\n        # Proxy Gini via Herfindahl: H = sum (p_i^2) where p_i = m_i/sum m.\n        # Map to [0,1]: conc = (H - 1/K) / (1 - 1/K). High when concentrated.\n        H = m_sq_sum / ((m_sum * m_sum) + eps)\n        invk = 1.0 / (k_match + eps)\n        conc = (H - invk) / (1.0 - invk + eps)\n        conc = max(0.0, min(1.0, conc))\n        score *= 1.0 + float(Config.gini_alpha) * conc\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram channel (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq,\n                corpus.bigram_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    # Prefix channel (morphology/identifier robustness).\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq,\n                corpus.prefix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view (morphology/identifier robustness).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 5)))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Bigram view (adjacent tokens) as cheap phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n        self._id_to_term = {i: t for t, i in self._vocab.items()}\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Base doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        # Rare anchor (max hinge) per doc.\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        # Gini proxy via Herfindahl concentration over m_t = wt*tf.\n        use_gini = float(getattr(Config, \"gini_alpha\", 0.0)) != 0.0\n        m_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        m_sq_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        k_match = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity_pow = Config.q_clarity_power\n            # Approximate tag-aware clarity: bigram pseudo-terms are tagged \"B:\" in vocab.\n            if hasattr(self.corpus, \"_vocab\"):\n                # best-effort: infer via stored term strings if present (safe if absent)\n                pass\n            if hasattr(self.corpus, \"_id_to_term\"):\n                t = self.corpus._id_to_term.get(term_id)\n                if t is not None and t.startswith(\"B:\"):\n                    clarity_pow = float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n            clarity = rarity ** clarity_pow\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n            if use_gini:\n                mt = wt * tf_row\n                mt_pos = np.maximum(mt, 0.0)\n                k_match += (mt_pos > 0.0).astype(np.float64)\n                m_sum += mt_pos\n                m_sq_sum += mt_pos * mt_pos\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_gini:\n            mask = (k_match >= 2.0) & (m_sum > 0.0)\n            if np.any(mask):\n                H = m_sq_sum / ((m_sum * m_sum) + eps)\n                invk = 1.0 / (k_match + eps)\n                conc = (H - invk) / (1.0 - invk + eps)\n                conc = np.clip(conc, 0.0, 1.0)\n                scores *= 1.0 + float(Config.gini_alpha) * (conc * mask.astype(np.float64))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query over tokens + (optional) prefixes + (optional) bigrams as pseudo-terms.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "7da1e82b-747b-475b-852b-05734a9641bf", "generation": 5, "timestamp": 1770319976.816403, "iteration_found": 285, "metrics": {"beir_nfcorpus_ndcg@10": 0.333434112393122, "beir_nfcorpus_recall@100": 0.26105267970764506, "beir_nfcorpus_index_time_ms": 6421.072416938841, "beir_nfcorpus_query_time_ms": 193.3812500210479, "bright_pony_ndcg@10": 0.08887080259120217, "bright_pony_recall@100": 0.2835770133797982, "bright_pony_index_time_ms": 2275.8241669507697, "bright_pony_query_time_ms": 1146.8476250302047, "beir_scifact_ndcg@10": 0.6771770376694076, "beir_scifact_recall@100": 0.93, "beir_scifact_index_time_ms": 8560.90708298143, "beir_scifact_query_time_ms": 852.5918329833075, "beir_arguana_ndcg@10": 0.31195989183483264, "beir_arguana_recall@100": 0.947180585296217, "beir_arguana_index_time_ms": 10234.433209057897, "beir_arguana_query_time_ms": 81177.19083407428, "bright_theoremqa_theorems_ndcg@10": 0.02723129420357544, "bright_theoremqa_theorems_recall@100": 0.1732456140350877, "bright_theoremqa_theorems_index_time_ms": 17392.795333988033, "bright_theoremqa_theorems_query_time_ms": 2336.488874978386, "beir_scidocs_ndcg@10": 0.15422367655793817, "beir_scidocs_recall@100": 0.3599, "beir_scidocs_index_time_ms": 35235.77954096254, "beir_scidocs_query_time_ms": 8466.896332916804, "bright_economics_ndcg@10": 0.1488359015185019, "bright_economics_recall@100": 0.39770797961459203, "bright_economics_index_time_ms": 23061.96075002663, "bright_economics_query_time_ms": 8503.23683291208, "bright_biology_ndcg@10": 0.23527826124522477, "bright_biology_recall@100": 0.5289759125221567, "bright_biology_index_time_ms": 24823.5885000322, "bright_biology_query_time_ms": 7828.485457925126, "bright_earth_science_ndcg@10": 0.3385155059498696, "bright_earth_science_recall@100": 0.6649461460600299, "bright_earth_science_index_time_ms": 53408.17666694056, "bright_earth_science_query_time_ms": 13053.955707931891, "beir_fiqa_ndcg@10": 0.24161007959826766, "beir_fiqa_recall@100": 0.5452394056560723, "beir_fiqa_index_time_ms": 62592.41729101632, "beir_fiqa_query_time_ms": 13135.504125035368, "bright_stackoverflow_ndcg@10": 0.2004113531344532, "bright_stackoverflow_recall@100": 0.5143810346376736, "bright_stackoverflow_index_time_ms": 150176.87787499744, "bright_stackoverflow_query_time_ms": 32557.63425002806, "beir_trec-covid_ndcg@10": 0.683400238724724, "beir_trec-covid_recall@100": 0.12002087101925987, "beir_trec-covid_index_time_ms": 261533.4609580459, "beir_trec-covid_query_time_ms": 3579.3062089942396, "avg_ndcg@10": 0.2867456796184266, "avg_recall@100": 0.4771856034940443, "combined_score": 0.43909761871892083, "total_index_time_ms": 655717.2937919386, "total_query_time_ms": 172831.5193328308, "total_time_ms": 828548.8131247694, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Concave evidence + informative-coverage pressure.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Rare-term anchor (single best rare hit; bounded, recall-safe).\n      # More stable than summing many \"somewhat rare\" terms on verbose queries.\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Bounded specificity/aboutness prior (positive-PMI density lift).\n      spec_beta: float = 0.10\n      spec_cap: float = 3.0\n      spec_len_floor: float = 25.0\n  \n      # NEW: cohesion prior via concentration (Gini) over matched query evidence.\n      # Let m_t = wt*tf over matched query terms in the doc.\n      # Gini in [0,1] is higher when evidence concentrates on a few decisive terms.\n      # Gentle multiplier to lift early precision.\n      gini_alpha: float = 0.08\n  \n      # Tiny micro-token channel (char n-grams) to reduce tokenization brittleness.\n      micro_len: int = 3\n      micro_min_token_len: int = 2\n      micro_weight: float = 0.12\n  \n  ... (3 more lines)\nwith:\n  class Config:\n      # Concave evidence + informative-coverage pressure.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (robustness for morphology/identifiers; cheaper/cleaner than char-ngrams).\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # Bigram channel (cheap lexical proximity / phrase specificity).\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90\n  \n      # Residual-IDF reliability gate: downweight ultra-common query terms smoothly (recall-safe).\n      residual_idf_tau: float = 1.25\n  \n      # Rare-term anchor (single best rare hit; bounded, recall-safe).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Bounded specificity/aboutness prior (positive-PMI density lift).\n      spec_beta: float = 0.10\n      spec_cap: float = 3.0\n      spec_len_floor: float = 25.0\n  \n      # Cohesion prior via concentration over matched query evidence.\n  ... (5 more lines)\nChange 2: Replace:\n  eps = Config.epsilon\n      base = Config.tf_log_base\nwith:\n  eps = Config.epsilon\n      base = Config.tf_log_base\n      tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\nChange 3: Replace:\n  rarity = tidf / (tidf + 1.0)\n          clarity = rarity ** Config.q_clarity_power\nwith:\n  rarity = tidf / (tidf + 1.0)\n          if term.startswith(\"B:\"):\n              clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_po...\n          else:\n              clarity = rarity ** Config.q_clarity_power\n          residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\nChange 4: 'wt = wq * tidf * clarity' to 'wt = wq * tidf * clarity * residual'\nChange 5: Replace:\n  def score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n      if not query:\n          return 0.0\n      q = QueryRepr.from_tokens(query)\n      if not q.terms:\n          return 0.0\n  \n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      doc_length = float(corpus.doc_lengths[doc_idx])\n  \n      s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n  \n      # Micro channel: char n-grams over query tokens (lexical, more forgiving for symbols/identifi...\n      if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n          m = max(2, int(Config.micro_len))\n          min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n          mtoks: list[str] = []\n          for t in query:\n              if len(t) < min_tok:\n                  continue\n              if len(t) <= m:\n                  mtoks.append(\"M:\" + t)\n              else:\n                  for i in range(0, len(t) - m + 1):\n                      mtoks.append(\"M:\" + t[i : i + m])\n          if mtoks:\n              mq = QueryRepr.from_tokens(mtoks)\n              s += float(Config.micro_weight) * retrieval_score(\n                  mq,\n  ... (7 more lines)\nwith:\n  def score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Entry point used by BM25.score().\"\"\"\n      if not query:\n          return 0.0\n      q = QueryRepr.from_tokens(query)\n      if not q.terms:\n          return 0.0\n  \n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      doc_length = float(corpus.doc_lengths[doc_idx])\n  \n      s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n  \n      # Bigram channel (phrase/proximity specificity; purely lexical).\n      if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n          qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n          if qb:\n              bq = QueryRepr.from_tokens(qb)\n              s += float(Config.bigram_weight) * retrieval_score(\n                  bq,\n                  corpus.bigram_doc_tf_dicts[doc_idx],\n                  doc_length,\n                  corpus.N,\n                  corpus.avgdl,\n                  corpus.document_frequency,\n              )\n  \n      # Prefix channel (morphology/identifier robustness).\n      if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n          pfx = max(1, int(Config.prefix_len))\n  ... (13 more lines)\nChange 6: Replace:\n  class Corpus:\n      def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n          self.documents = documents\n          self.ids = ids or [str(i) for i in range(len(documents))]\n          self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n          self.N = len(documents)\n          self.document_count = self.N\n          self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n          self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n          self.average_document_length = self.avgdl\n  \n          # Micro-token field: character n-grams prefixed with \"M:\" to keep disjoint vocab.\n          m = max(2, int(getattr(Config, \"micro_len\", 3)))\n          min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n          micro_docs: list[list[str]] = []\n          for doc in documents:\n              grams: list[str] = []\n              for t in doc:\n                  if len(t) < min_tok:\n                      continue\n                  if len(t) <= m:\n                      grams.append(\"M:\" + t)\n                  else:\n                      for i in range(0, len(t) - m + 1):\n                          grams.append(\"M:\" + t[i : i + m])\n              micro_docs.append(grams)\n          self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n  \n          # Joint vocabulary over base tokens and micro tokens.\n          self._vocab: dict[str, int] = {}\n  ... (52 more lines)\nwith:\n  class Corpus:\n      def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n          self.documents = documents\n          self.ids = ids or [str(i) for i in range(len(documents))]\n          self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n          self.N = len(documents)\n          self.document_count = self.N\n          self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n          self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n          self.average_document_length = self.avgdl\n  \n          # Prefix view (morphology/identifier robustness).\n          pfx = max(1, int(getattr(Config, \"prefix_len\", 5)))\n          docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n          self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n  \n          # Bigram view (adjacent tokens) as cheap phrase/proximity evidence.\n          docs_bigram: list[list[str]] = []\n          for doc in documents:\n              if len(doc) < 2:\n                  docs_bigram.append([])\n              else:\n                  docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n          self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n  \n          # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n          self._vocab: dict[str, int] = {}\n          for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n              for term in doc:\n                  if term not in self._vocab:\n  ... (65 more lines)\nChange 7: Replace:\n  # Combined query over base tokens + micro (char n-gram) tokens.\n          term_counts = Counter(query)\n          query_term_ids: list[int] = []\n          query_term_weights: list[float] = []\n  \n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n  \n          if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n              m = max(2, int(Config.micro_len))\n              min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n              grams: list[str] = []\n              for t in query:\n                  if len(t) < min_tok:\n                      continue\n                  if len(t) <= m:\n                      grams.append(\"M:\" + t)\n                  else:\n                      for i in range(0, len(t) - m + 1):\n                          grams.append(\"M:\" + t[i : i + m])\n              if grams:\n                  gcounts = Counter(grams)\n                  for g, c in gcounts.items():\n                      tid = self.corpus.get_term_id(g)\n                      if tid is not None:\n                          query_term_ids.append(tid)\n                          query_term_weights.append(float(Config.micro_weight) * (float(c) ** Confi...\nwith:\n  # Combined query over tokens + (optional) prefixes + (optional) bigrams as pseudo-terms.\n          term_counts = Counter(query)\n          query_term_ids: list[int] = []\n          query_term_weights: list[float] = []\n  \n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n  \n          if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n              pfx = max(1, int(Config.prefix_len))\n              pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n              for p, c in pcounts.items():\n                  tid = self.corpus.get_term_id(\"P:\" + p)\n                  if tid is not None:\n                      query_term_ids.append(tid)\n                      query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.q...\n  \n          if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n              bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n              for bg, c in bcounts.items():\n                  tid = self.corpus.get_term_id(\"B:\" + bg)\n                  if tid is not None:\n                      query_term_ids.append(tid)\n                      query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.q...\nChange 8: Replace:\n  eps = Config.epsilon\n          base = Config.tf_log_base\nwith:\n  eps = Config.epsilon\n          base = Config.tf_log_base\n          tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\nChange 9: Replace:\n  rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = wq * idf_val * clarity\nwith:\n  rarity = idf_val / (idf_val + 1.0)\n              clarity_pow = Config.q_clarity_power\n              # Approximate tag-aware clarity: bigram pseudo-terms are tagged \"B:\" in vocab.\n              if hasattr(self.corpus, \"_vocab\"):\n                  # best-effort: infer via stored term strings if present (safe if absent)\n                  pass\n              if hasattr(self.corpus, \"_id_to_term\"):\n                  t = self.corpus._id_to_term.get(term_id)\n                  if t is not None and t.startswith(\"B:\"):\n                      clarity_pow = float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_...\n              clarity = rarity ** clarity_pow\n              residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n  \n              wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = wq * idf_val * clarity * residual\nChange 10: Replace:\n  self.vocab_size = len(self._vocab)\n  \n          tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\nwith:\n  self.vocab_size = len(self._vocab)\n          self._id_to_term = {i: t for t, i in self._vocab.items()}\n  \n          tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3339813678415768, "beir_nfcorpus_recall@100": 0.2617413849856317, "beir_nfcorpus_index_time_ms": 6154.257582966238, "beir_nfcorpus_query_time_ms": 674.6087500359863, "bright_pony_ndcg@10": 0.0942636046914432, "bright_pony_recall@100": 0.30209972472619573, "bright_pony_index_time_ms": 2338.875208981335, "bright_pony_query_time_ms": 2405.237084021792, "beir_scifact_ndcg@10": 0.681673513379849, "beir_scifact_recall@100": 0.9416666666666667, "beir_scifact_index_time_ms": 8726.03799996432, "beir_scifact_query_time_ms": 2463.692917022854, "beir_arguana_ndcg@10": 0.2924281716586133, "beir_arguana_recall@100": 0.9214846538187009, "beir_arguana_index_time_ms": 10372.75254202541, "beir_arguana_query_time_ms": 119706.12608396914, "bright_theoremqa_theorems_ndcg@10": 0.027958904765377136, "bright_theoremqa_theorems_recall@100": 0.19298245614035087, "bright_theoremqa_theorems_index_time_ms": 15901.00537496619, "bright_theoremqa_theorems_query_time_ms": 5250.72920799721, "beir_scidocs_ndcg@10": 0.15193052566735182, "beir_scidocs_recall@100": 0.3573, "beir_scidocs_index_time_ms": 35704.63608298451, "beir_scidocs_query_time_ms": 28315.987665904686, "bright_economics_ndcg@10": 0.14715167734319934, "bright_economics_recall@100": 0.3893849458241561, "bright_economics_index_time_ms": 25341.72458294779, "bright_economics_query_time_ms": 23075.833457987756, "bright_biology_ndcg@10": 0.24779400889579178, "bright_biology_recall@100": 0.551331558919704, "bright_biology_index_time_ms": 31036.84879199136, "bright_biology_query_time_ms": 21151.743708061986, "beir_fiqa_ndcg@10": 0.23425807354350411, "beir_fiqa_recall@100": 0.5337056547704696, "beir_fiqa_index_time_ms": 68240.73650001083, "beir_fiqa_query_time_ms": 33665.49008397851, "bright_earth_science_ndcg@10": 0.3227233772297725, "bright_earth_science_recall@100": 0.6620072745866756, "bright_earth_science_index_time_ms": 67662.92300005443, "bright_earth_science_query_time_ms": 48566.60370901227, "bright_stackoverflow_ndcg@10": 0.2076488751257668, "bright_stackoverflow_recall@100": 0.5158577863534518, "bright_stackoverflow_index_time_ms": 188437.65837489627, "bright_stackoverflow_query_time_ms": 72886.38800010085, "beir_trec-covid_ndcg@10": 0.7037782061103751, "beir_trec-covid_recall@100": 0.12151604902985691, "beir_trec-covid_index_time_ms": 335217.9776659468, "beir_trec-covid_query_time_ms": 9334.914332954213, "avg_ndcg@10": 0.2871325255210518, "avg_recall@100": 0.479256512985155, "combined_score": 0.4408317154923344, "total_index_time_ms": 795135.4337077355, "total_query_time_ms": 367497.35500104725, "total_time_ms": 1162632.7887087828, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 2}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4408\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4431 \u2192 0.4408. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # NEW: bigram channel (purely lexical proximity / phrase specificity).\n      # Bigrams often lift early precision on abstract-heavy corpora (SciDocs, FiQA, ArguAna)\n      # without needing positional indexes. Keep weight small to stay recall-safe.\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90  # suppress very common bigrams a bit more than unigrams\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Compatibility leftovers.\n      k1: float = 0.9\n      b: float = 0.4\nwith:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # Bigram channel (purely lexical proximity / phrase specificity).\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # NEW: residual-IDF gate (smoothly downweight ultra-common query tokens).\n      # residual = idf/(idf+tau) in (0,1). Keeps recall (no pruning) but reduces generic-term noise.\n      residual_idf_tau: float = 1.25\n  \n      # Compatibility leftovers.\n  ... (2 more lines)\nChange 2: Replace:\n  eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      matched = 0.0\n      uq = 0.0\n  \n      anchor = 0.0\n      pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n  \n      for term in query_repr.terms:\n          uq += 1.0\n          df = float(corpus_df.get(term, 1.0))\n          tidf = float(idf(df, N))\n          if tidf <= 0.0:\n              continue\n  \n          rarity = tidf / (tidf + 1.0)\n          # Tag-aware clarity: pseudo-terms (prefixes/bigrams) are easier to match and can be noisier,\n          # so we optionally suppress them a bit more than full tokens.\n          if term.startswith(\"B:\"):\n              clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_po...\n          else:\n              clarity = rarity ** Config.q_clarity_power\n  \n          wq = float(query_repr.term_weights.get(term, 1.0))\n          wt = wq * tidf * clarity\n          cov_den += wt\nwith:\n  eps = Config.epsilon\n      base = Config.tf_log_base\n      tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      matched = 0.0\n      uq = 0.0\n  \n      anchor = 0.0\n      pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n  \n      for term in query_repr.terms:\n          uq += 1.0\n          df = float(corpus_df.get(term, 1.0))\n          tidf = float(idf(df, N))\n          if tidf <= 0.0:\n              continue\n  \n          rarity = tidf / (tidf + 1.0)\n          # Tag-aware clarity: pseudo-terms (prefixes/bigrams) are easier to match and can be noisier,\n          # so we optionally suppress them a bit more than full tokens.\n          if term.startswith(\"B:\"):\n              clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_po...\n          else:\n              clarity = rarity ** Config.q_clarity_power\n  \n          # Smooth reliability gate: common query terms are weak constraints.\n          residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n  ... (4 more lines)\nChange 3: Replace:\n  eps = Config.epsilon\n          base = Config.tf_log_base\n  \n          sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n          pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n  \n          uq = float(len(query_term_ids))\n          if query_term_weights is None:\n              q_w = np.ones(len(query_term_ids), dtype=np.float64)\n          else:\n              q_w = query_term_weights.astype(np.float64, copy=False)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n              wt = float(q_w[i]) * idf_val * clarity\n              cov_den += wt\nwith:\n  eps = Config.epsilon\n          base = Config.tf_log_base\n          tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n  \n          sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n          pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n  \n          uq = float(len(query_term_ids))\n          if query_term_weights is None:\n              q_w = np.ones(len(query_term_ids), dtype=np.float64)\n          else:\n              q_w = query_term_weights.astype(np.float64, copy=False)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n              residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n              wt = float(q_w[i]) * idf_val * clarity * residual\n              cov_den += wt\n- Metrics: beir_nfcorpus_ndcg@10: 0.3345, beir_nfcorpus_recall@100: 0.2608, beir_nfcorpus_index_time_ms: 6380.2212, beir_nfcorpus_query_time_ms: 155.0517, bright_pony_ndcg@10: 0.1123, bright_pony_recall@100: 0.3084, bright_pony_index_time_ms: 2193.8897, bright_pony_query_time_ms: 881.5405, beir_scifact_ndcg@10: 0.6858, beir_scifact_recall@100: 0.9287, beir_scifact_index_time_ms: 8688.0790, beir_scifact_query_time_ms: 677.6585, beir_arguana_ndcg@10: 0.3083, beir_arguana_recall@100: 0.9458, beir_arguana_index_time_ms: 10399.7306, beir_arguana_query_time_ms: 55680.7500, bright_theoremqa_theorems_ndcg@10: 0.0316, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 17680.2247, bright_theoremqa_theorems_query_time_ms: 1631.1728, beir_scidocs_ndcg@10: 0.1562, beir_scidocs_recall@100: 0.3610, beir_scidocs_index_time_ms: 35839.6188, beir_scidocs_query_time_ms: 6534.6104, bright_economics_ndcg@10: 0.1517, bright_economics_recall@100: 0.3896, bright_economics_index_time_ms: 23920.2695, bright_economics_query_time_ms: 5561.4458, bright_biology_ndcg@10: 0.2607, bright_biology_recall@100: 0.5401, bright_biology_index_time_ms: 25230.6393, bright_biology_query_time_ms: 5252.3962, bright_earth_science_ndcg@10: 0.3113, bright_earth_science_recall@100: 0.6631, bright_earth_science_index_time_ms: 53514.4871, bright_earth_science_query_time_ms: 8734.6481, beir_fiqa_ndcg@10: 0.2424, beir_fiqa_recall@100: 0.5394, beir_fiqa_index_time_ms: 61895.8824, beir_fiqa_query_time_ms: 10325.0028, bright_stackoverflow_ndcg@10: 0.2084, bright_stackoverflow_recall@100: 0.5162, bright_stackoverflow_index_time_ms: 147905.3291, bright_stackoverflow_query_time_ms: 22066.2990, beir_trec-covid_ndcg@10: 0.7134, beir_trec-covid_recall@100: 0.1207, beir_trec-covid_index_time_ms: 265745.9827, beir_trec-covid_query_time_ms: 2744.8795, avg_ndcg@10: 0.2930, avg_recall@100: 0.4806, combined_score: 0.4431, total_index_time_ms: 659394.3542, total_query_time_ms: 120245.4552, total_time_ms: 779639.8094, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # NEW: bigram channel (purely lexical proximity / phrase specificity).\n      # Bigrams often lift early precision on abstract-heavy corpora (SciDocs, FiQA, ArguAna)\n      # without needing positional indexes. Keep weight small to stay recall-safe.\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90  # suppress very common bigrams a bit more than unigrams\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Compatibility leftovers.\n      k1: float = 0.9\n      b: float = 0.4\nwith:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # Bigram channel (purely lexical proximity / phrase specificity).\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # NEW: residual-IDF gate (smoothly downweight ultra-common query tokens).\n      # residual = idf/(idf+tau) in (0,1). Improves early precision without hard stopwording.\n      residual_idf_tau: float = 1.25\n  \n      # Compatibility leftovers.\n  ... (2 more lines)\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n      plus a bounded *rare-term anchor* gate.\n  \n      Rare-term anchor:\n        Use the single strongest rarity-above-pivot match as a tiny multiplier.\n        This helps technical/theorem/code corpora where one exact rare identifier match\n        is often decisive, while remaining recall-safe (only changes scores for matching docs).\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      matched = 0.0\n      uq = 0.0\n  \n      anchor = 0.0\n  ... (53 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n      plus a bounded rare-term anchor.\n  \n      NEW: residual-IDF gate:\n        Treat very-common query tokens as low-reliability constraints via\n          residual=idf/(idf+tau) in (0,1).\n        This tends to improve nDCG@10 (less generic-term noise) without pruning terms (recall-safe).\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n      tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      matched = 0.0\n      uq = 0.0\n  \n  ... (54 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          eps = Config.epsilon\n          base = Config.tf_log_base\n  \n          sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n          pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n  \n          uq = float(len(query_term_ids))\n          if query_term_weights is None:\n              q_w = np.ones(len(query_term_ids), dtype=np.float64)\n          else:\n              q_w = query_term_weights.astype(np.float64, copy=False)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n  ... (34 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          eps = Config.epsilon\n          base = Config.tf_log_base\n          tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n  \n          sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n          pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n  \n          uq = float(len(query_term_ids))\n          if query_term_weights is None:\n              q_w = np.ones(len(query_term_ids), dtype=np.float64)\n          else:\n              q_w = query_term_weights.astype(np.float64, copy=False)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n  ... (37 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3345, beir_nfcorpus_recall@100: 0.2608, beir_nfcorpus_index_time_ms: 6446.1875, beir_nfcorpus_query_time_ms: 162.2259, bright_pony_ndcg@10: 0.1123, bright_pony_recall@100: 0.3084, bright_pony_index_time_ms: 2303.4477, bright_pony_query_time_ms: 861.1860, beir_scifact_ndcg@10: 0.6858, beir_scifact_recall@100: 0.9287, beir_scifact_index_time_ms: 8633.3658, beir_scifact_query_time_ms: 680.0649, beir_arguana_ndcg@10: 0.3083, beir_arguana_recall@100: 0.9458, beir_arguana_index_time_ms: 10042.3007, beir_arguana_query_time_ms: 55388.1022, bright_theoremqa_theorems_ndcg@10: 0.0316, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 17503.6426, bright_theoremqa_theorems_query_time_ms: 1588.8815, beir_scidocs_ndcg@10: 0.1562, beir_scidocs_recall@100: 0.3610, beir_scidocs_index_time_ms: 35850.5318, beir_scidocs_query_time_ms: 6566.8787, bright_economics_ndcg@10: 0.1517, bright_economics_recall@100: 0.3896, bright_economics_index_time_ms: 23167.2595, bright_economics_query_time_ms: 5687.3667, bright_biology_ndcg@10: 0.2607, bright_biology_recall@100: 0.5401, bright_biology_index_time_ms: 25368.4885, bright_biology_query_time_ms: 4993.2479, bright_earth_science_ndcg@10: 0.3113, bright_earth_science_recall@100: 0.6631, bright_earth_science_index_time_ms: 53987.5900, bright_earth_science_query_time_ms: 8917.8827, beir_fiqa_ndcg@10: 0.2424, beir_fiqa_recall@100: 0.5394, beir_fiqa_index_time_ms: 61796.6020, beir_fiqa_query_time_ms: 10416.4861, bright_stackoverflow_ndcg@10: 0.2084, bright_stackoverflow_recall@100: 0.5162, bright_stackoverflow_index_time_ms: 149603.6848, bright_stackoverflow_query_time_ms: 22014.7062, beir_trec-covid_ndcg@10: 0.7134, beir_trec-covid_recall@100: 0.1207, beir_trec-covid_index_time_ms: 266747.4037, beir_trec-covid_query_time_ms: 2713.0052, avg_ndcg@10: 0.2930, avg_recall@100: 0.4806, combined_score: 0.4431, total_index_time_ms: 661450.5045, total_query_time_ms: 119990.0341, total_time_ms: 781440.5386, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Entropy-normalized TF (\"aboutness\") prior.\n      entropy_alpha: float = 0.10\n  \n      # Residual-IDF reliability gate: downweight ultra-common query terms smoothly (recall-safe).\n      # residual = idf/(idf+tau) in (0,1). Helps early precision on verbose/natural queries.\n      residual_idf_tau: float = 1.25\n  \n      # Compatibility leftovers.\n      k1: float = 0.9\n  ... (1 more lines)\nwith:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # NEW: bigram channel (cheap lexical proximity / phrase specificity).\n      # Helps early precision on abstract-heavy corpora (SciDocs/FiQA/ArguAna) without positions.\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90  # slightly harsher clarity for noisier bigram pseudo-terms\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Entropy-normalized TF (\"aboutness\") prior.\n      entropy_alpha: float = 0.10\n  \n      # Residual-IDF reliability gate: downweight ultra-common query terms smoothly (recall-safe).\n  ... (6 more lines)\nChange 2: Replace:\n  rarity = tidf / (tidf + 1.0)\n          clarity = rarity ** Config.q_clarity_power\nwith:\n  rarity = tidf / (tidf + 1.0)\n          # Tag-aware clarity: pseudo-terms (bigrams) are easier to match and can be noisier.\n          if term.startswith(\"B:\"):\n              clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_po...\n          else:\n              clarity = rarity ** Config.q_clarity_power\nChange 3: Replace:\n  def score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Entry point used by BM25.score().\"\"\"\n      if not query:\n          return 0.0\n      q = QueryRepr.from_tokens(query)\n      if not q.terms:\n          return 0.0\n  \n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      doc_length = float(corpus.doc_lengths[doc_idx])\n  \n      # Primary token-channel score.\n      s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n  \n      # Secondary prefix-channel score (helps when exact token match is brittle).\n      if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n          pfx = max(1, int(Config.prefix_len))\n          ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n          if ptoks:\n              pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n              pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n              s += Config.prefix_weight * retrieval_score(\n                  pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n              )\n      return s\nwith:\n  def score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Entry point used by BM25.score().\"\"\"\n      if not query:\n          return 0.0\n      q = QueryRepr.from_tokens(query)\n      if not q.terms:\n          return 0.0\n  \n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      doc_length = float(corpus.doc_lengths[doc_idx])\n  \n      # Primary token-channel score.\n      s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n  \n      # Bigram-channel score (phrase/proximity specificity; purely lexical).\n      if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n          qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n          if qb:\n              bq = QueryRepr.from_tokens(qb)\n              bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n              s += float(Config.bigram_weight) * retrieval_score(\n                  bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n              )\n  \n      # Secondary prefix-channel score (helps when exact token match is brittle).\n      if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n          pfx = max(1, int(Config.prefix_len))\n          ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n          if ptoks:\n              pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n  ... (5 more lines)\nChange 4: Replace:\n  class Corpus:\n      def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n          self.documents = documents\n          self.ids = ids or [str(i) for i in range(len(documents))]\n          self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n          self.N = len(documents)\n          self.document_count = self.N\n          self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n          self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n          self.average_document_length = self.avgdl\n  \n          # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n          # This is still lexical (no semantics) but increases robustness to:\n          # - morphology: \"oxidize\"/\"oxidation\"\n          # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n          pfx = max(1, int(Config.prefix_len))\n          docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n          self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n  \n          # Joint vocabulary over (token) U (prefixes), keeping them disjoint by tagging prefixes.\n          self._vocab: dict[str, int] = {}\n          for doc, pdoc in zip(documents, docs_prefix):\n              for term in doc:\n                  if term not in self._vocab:\n                      self._vocab[term] = len(self._vocab)\n              for p in pdoc:\n                  key = \"P:\" + p\n                  if key not in self._vocab:\n                      self._vocab[key] = len(self._vocab)\n  \n  ... (46 more lines)\nwith:\n  class Corpus:\n      def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n          self.documents = documents\n          self.ids = ids or [str(i) for i in range(len(documents))]\n          self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n          self.N = len(documents)\n          self.document_count = self.N\n          self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n          self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n          self.average_document_length = self.avgdl\n  \n          # Prefix view (morphology/symbol robustness).\n          pfx = max(1, int(Config.prefix_len))\n          docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n          self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n  \n          # NEW: bigram view (adjacent tokens) as cheap phrase/proximity evidence.\n          docs_bigram: list[list[str]] = []\n          for doc in documents:\n              if len(doc) < 2:\n                  docs_bigram.append([])\n              else:\n                  docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n          self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n  \n          # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n          self._vocab: dict[str, int] = {}\n          for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n              for term in doc:\n                  if term not in self._vocab:\n  ... (65 more lines)\nChange 5: Replace:\n  # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n          counts = Counter(query)\n          query_term_ids: list[int] = []\n          query_term_weights: list[float] = []\n  \n          for term, count in counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n  \n          if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n              pfx = max(1, int(Config.prefix_len))\n              pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n              for p, c in pcounts.items():\n                  tid = self.corpus.get_term_id(\"P:\" + p)\n                  if tid is not None:\n                      query_term_ids.append(tid)\n                      query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n  \n          if not query_term_ids:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\nwith:\n  # Build combined query over tokens + (optional) prefixes + (optional) bigrams as pseudo-terms.\n          counts = Counter(query)\n          query_term_ids: list[int] = []\n          query_term_weights: list[float] = []\n  \n          for term, count in counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n  \n          if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n              pfx = max(1, int(Config.prefix_len))\n              pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n              for p, c in pcounts.items():\n                  tid = self.corpus.get_term_id(\"P:\" + p)\n                  if tid is not None:\n                      query_term_ids.append(tid)\n                      query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n  \n          if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n              bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n              for bg, c in bcounts.items():\n                  tid = self.corpus.get_term_id(\"B:\" + bg)\n                  if tid is not None:\n                      query_term_ids.append(tid)\n                      query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.q...\n  \n          if not query_term_ids:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\nChange 6: Replace:\n  rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n              residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n              wt = float(q_w[i]) * idf_val * clarity * residual\nwith:\n  rarity = idf_val / (idf_val + 1.0)\n              clarity_pow = Config.q_clarity_power\n              # Tag-aware clarity parity with retrieval_score(): bigram pseudo-terms can be noisier.\n              if hasattr(self.corpus, \"_vocab\"):\n                  # We don't have id->term mapping; approximate by using df/idf regime:\n                  # bigrams tend to have higher idf. But safest: use the actual string if available...\n                  pass\n              if hasattr(self.corpus, \"_id_to_term\"):\n                  t = self.corpus._id_to_term.get(term_id)\n                  if t is not None and t.startswith(\"B:\"):\n                      clarity_pow = float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_...\n              clarity = rarity ** clarity_pow\n              residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n              wt = float(q_w[i]) * idf_val * clarity * residual\nChange 7: Replace:\n  self.vocab_size = len(self._vocab)\n  \n          tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\nwith:\n  self.vocab_size = len(self._vocab)\n  \n          # Reverse map for tag-aware behavior in vectorized scoring (e.g., \"B:\" bigrams).\n          self._id_to_term = {i: t for t, i in self._vocab.items()}\n  \n          tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3341, beir_nfcorpus_recall@100: 0.2611, beir_nfcorpus_index_time_ms: 6757.4352, beir_nfcorpus_query_time_ms: 196.8648, bright_pony_ndcg@10: 0.1038, bright_pony_recall@100: 0.2960, bright_pony_index_time_ms: 2385.4865, bright_pony_query_time_ms: 1143.2424, beir_scifact_ndcg@10: 0.6852, beir_scifact_recall@100: 0.9293, beir_scifact_index_time_ms: 9174.4163, beir_scifact_query_time_ms: 893.9792, beir_arguana_ndcg@10: 0.3025, beir_arguana_recall@100: 0.9443, beir_arguana_index_time_ms: 10960.2805, beir_arguana_query_time_ms: 77045.5908, bright_theoremqa_theorems_ndcg@10: 0.0271, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 18171.7671, bright_theoremqa_theorems_query_time_ms: 2202.4288, beir_scidocs_ndcg@10: 0.1540, beir_scidocs_recall@100: 0.3597, beir_scidocs_index_time_ms: 37148.9477, beir_scidocs_query_time_ms: 8419.6530, bright_economics_ndcg@10: 0.1572, bright_economics_recall@100: 0.3995, bright_economics_index_time_ms: 24230.1253, bright_economics_query_time_ms: 8010.0175, bright_biology_ndcg@10: 0.2654, bright_biology_recall@100: 0.5357, bright_biology_index_time_ms: 25972.1148, bright_biology_query_time_ms: 7549.7566, bright_earth_science_ndcg@10: 0.3372, bright_earth_science_recall@100: 0.6731, bright_earth_science_index_time_ms: 55919.7933, bright_earth_science_query_time_ms: 12671.2718, beir_fiqa_ndcg@10: 0.2405, beir_fiqa_recall@100: 0.5369, beir_fiqa_index_time_ms: 64806.5605, beir_fiqa_query_time_ms: 13715.5345, bright_stackoverflow_ndcg@10: 0.2027, bright_stackoverflow_recall@100: 0.5169, bright_stackoverflow_index_time_ms: 152078.2268, bright_stackoverflow_query_time_ms: 30884.2942, beir_trec-covid_ndcg@10: 0.7095, beir_trec-covid_recall@100: 0.1221, beir_trec-covid_index_time_ms: 266002.2567, beir_trec-covid_query_time_ms: 3527.5688, avg_ndcg@10: 0.2933, avg_recall@100: 0.4806, combined_score: 0.4432, total_index_time_ms: 673607.4105, total_query_time_ms: 166260.2023, total_time_ms: 839867.6129, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4432)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (purely lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Residual-IDF gate (smoothly downweight ultra-common query tokens).\n    residual_idf_tau: float = 1.25\n\n    # NEW: Query self-information coverage (QSN).\n    # Soft-AND over *raw* query informativeness: reward covering high-idf query mass.\n    # Helps early precision on verbose queries while remaining recall-safe (multiplier only).\n    qsn_alpha: float = 0.14\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded *rare-term anchor* gate.\n\n    Rare-term anchor:\n      Use the single strongest rarity-above-pivot match as a tiny multiplier.\n      This helps technical/theorem/code corpora where one exact rare identifier match\n      is often decisive, while remaining recall-safe (only changes scores for matching docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    # QSN accumulators: total raw IDF mass in query vs mass covered by this doc.\n    q_idf_tot = 0.0\n    q_idf_hit = 0.0\n    qsn_alpha = float(getattr(Config, \"qsn_alpha\", 0.0))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # Raw informativeness mass (no clarity/residual): \"how much info is in the query term itself\".\n        if qsn_alpha != 0.0:\n            q_mass = wq * tidf\n            q_idf_tot += q_mass\n\n        rarity = tidf / (tidf + 1.0)\n        # Tag-aware clarity: pseudo-terms (prefixes/bigrams) are easier to match and can be noisier,\n        # so we optionally suppress them a bit more than full tokens.\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        # Smooth reliability gate: common query terms are weak constraints.\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        if qsn_alpha != 0.0:\n            q_idf_hit += wq * tidf\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)  # in (0,1)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    # QSN: cover the \"informative mass\" of the query (raw idf), not just count of terms.\n    if qsn_alpha != 0.0 and q_idf_tot > 0.0:\n        qsn = q_idf_hit / (q_idf_tot + eps)\n        qsn = max(0.0, min(1.0, qsn))\n        score *= 1.0 + qsn_alpha * qsn\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        # QSN accumulators (raw idf mass coverage).\n        qsn_alpha = float(getattr(Config, \"qsn_alpha\", 0.0))\n        q_idf_tot = 0.0\n        q_idf_hit = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # QSN: raw query information mass and per-doc hit mass.\n            if qsn_alpha != 0.0:\n                q_mass = float(q_w[i]) * idf_val\n                q_idf_tot += q_mass\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            if qsn_alpha != 0.0:\n                q_idf_hit += q_mass * present\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        # QSN multiplier (vectorized).\n        if qsn_alpha != 0.0 and q_idf_tot > 0.0:\n            qsn = np.clip(q_idf_hit / (q_idf_tot + eps), 0.0, 1.0)\n            scores *= 1.0 + qsn_alpha * qsn\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3342), Performs well on beir_nfcorpus_recall@100 (0.2608), Performs well on beir_nfcorpus_index_time_ms (6679.4435), Performs well on beir_nfcorpus_query_time_ms (180.7385), Performs well on bright_pony_ndcg@10 (0.1181), Performs well on bright_pony_recall@100 (0.3114), Performs well on bright_pony_index_time_ms (2478.7770), Performs well on bright_pony_query_time_ms (940.9832), Performs well on beir_scifact_ndcg@10 (0.6903), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (9131.7598), Performs well on beir_scifact_query_time_ms (737.1589), Performs well on beir_arguana_ndcg@10 (0.3052), Performs well on beir_arguana_recall@100 (0.9422), Performs well on beir_arguana_index_time_ms (10923.5013), Performs well on beir_arguana_query_time_ms (58925.8556), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0312), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (18252.7110), Performs well on bright_theoremqa_theorems_query_time_ms (1749.0099), Performs well on beir_scidocs_ndcg@10 (0.1544), Performs well on beir_scidocs_recall@100 (0.3591), Performs well on beir_scidocs_index_time_ms (36990.8431), Performs well on beir_scidocs_query_time_ms (7051.4377), Performs well on bright_economics_ndcg@10 (0.1490), Performs well on bright_economics_recall@100 (0.3924), Performs well on bright_economics_index_time_ms (24543.2286), Performs well on bright_economics_query_time_ms (6091.8831), Performs well on bright_biology_ndcg@10 (0.2574), Performs well on bright_biology_recall@100 (0.5477), Performs well on bright_biology_index_time_ms (26183.2526), Performs well on bright_biology_query_time_ms (5623.3260), Performs well on bright_earth_science_ndcg@10 (0.2902), Performs well on bright_earth_science_recall@100 (0.6718), Performs well on bright_earth_science_index_time_ms (56209.7753), Performs well on bright_earth_science_query_time_ms (9093.4813), Performs well on beir_fiqa_ndcg@10 (0.2427), Performs well on beir_fiqa_recall@100 (0.5375), Performs well on beir_fiqa_index_time_ms (65369.5058), Performs well on beir_fiqa_query_time_ms (10630.5747), Performs well on bright_stackoverflow_ndcg@10 (0.2015), Performs well on bright_stackoverflow_recall@100 (0.5142), Performs well on bright_stackoverflow_index_time_ms (152946.7848), Performs well on bright_stackoverflow_query_time_ms (22637.2720), Performs well on beir_trec-covid_ndcg@10 (0.7105), Performs well on beir_trec-covid_recall@100 (0.1188), Performs well on beir_trec-covid_index_time_ms (271144.0655), Performs well on beir_trec-covid_query_time_ms (3005.5567), Performs well on avg_ndcg@10 (0.2904), Performs well on avg_recall@100 (0.4815), Performs well on combined_score (0.4432), Performs well on total_index_time_ms (680853.6484), Performs well on total_query_time_ms (126667.2777), Performs well on total_time_ms (807520.9261), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4432)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: bigram channel (cheap lexical proximity / phrase specificity).\n    # Helps early precision on abstract-heavy corpora (SciDocs/FiQA/ArguAna) without positions.\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90  # slightly harsher clarity for noisier bigram pseudo-terms\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Entropy-normalized TF (\"aboutness\") prior.\n    entropy_alpha: float = 0.10\n\n    # Residual-IDF reliability gate: downweight ultra-common query terms smoothly (recall-safe).\n    # residual = idf/(idf+tau) in (0,1). Helps early precision on verbose/natural queries.\n    residual_idf_tau: float = 1.25\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor, plus an entropy-normalized burstiness prior.\n\n    Entropy prior (query-term TF entropy within doc):\n      Let p_t \u221d (wt * tf_t) over matched query terms. Compute normalized entropy H/log(K).\n      Use concentration = 1 - H/log(K) in [0,1]. Concentrated evidence often correlates with\n      topicality (better early precision) while staying recall-safe (only affects matching docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    # For entropy prior: accumulate per-term matched mass m_t = wt * tf (only if tf>0).\n    m_sum = 0.0\n    m_log_sum = 0.0  # sum(m_t * log(m_t))\n    k_match = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        # Tag-aware clarity: pseudo-terms (bigrams) are easier to match and can be noisier.\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        # Smooth reliability gate: common query terms are weak constraints.\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if getattr(Config, \"entropy_alpha\", 0.0) != 0.0:\n            mt = wt * tf\n            if mt > 0.0:\n                k_match += 1.0\n                m_sum += mt\n                m_log_sum += mt * math.log(mt + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if getattr(Config, \"entropy_alpha\", 0.0) != 0.0 and k_match >= 2.0 and m_sum > 0.0:\n        # H = -sum p log p = log(m_sum) - (sum m log m)/m_sum\n        H = math.log(m_sum + eps) - (m_log_sum / (m_sum + eps))\n        Hn = H / (math.log(k_match + eps) + eps)  # normalized to ~[0,1]\n        conc = max(0.0, min(1.0, 1.0 - Hn))\n        score *= 1.0 + float(Config.entropy_alpha) * conc\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view (morphology/symbol robustness).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as cheap phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        # Reverse map for tag-aware behavior in vectorized scoring (e.g., \"B:\" bigrams).\n        self._id_to_term = {i: t for t, i in self._vocab.items()}\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for all channels (prefix/bigram keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        # Entropy prior accumulators over matched query terms.\n        m_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        m_log_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        k_match = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_ent = getattr(Config, \"entropy_alpha\", 0.0) != 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity_pow = Config.q_clarity_power\n            # Tag-aware clarity parity with retrieval_score(): bigram pseudo-terms can be noisier.\n            if hasattr(self.corpus, \"_vocab\"):\n                # We don't have id->term mapping; approximate by using df/idf regime:\n                # bigrams tend to have higher idf. But safest: use the actual string if available via reverse map.\n                pass\n            if hasattr(self.corpus, \"_id_to_term\"):\n                t = self.corpus._id_to_term.get(term_id)\n                if t is not None and t.startswith(\"B:\"):\n                    clarity_pow = float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n            clarity = rarity ** clarity_pow\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_ent:\n                mt = wt * tf_row\n                mt_pos = np.maximum(mt, 0.0)\n                k_match += (mt_pos > 0.0).astype(np.float64)\n                m_sum += mt_pos\n                m_log_sum += mt_pos * np.log(mt_pos + eps)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_ent:\n            # H = log(m_sum) - (m_log_sum/m_sum), normalize by log(k_match).\n            mask = (k_match >= 2.0) & (m_sum > 0.0)\n            if np.any(mask):\n                H = np.log(m_sum + eps) - (m_log_sum / (m_sum + eps))\n                Hn = H / (np.log(k_match + eps) + eps)\n                conc = np.clip(1.0 - Hn, 0.0, 1.0)\n                scores *= 1.0 + float(Config.entropy_alpha) * (conc * mask.astype(np.float64))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes + (optional) bigrams as pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3341), Performs well on beir_nfcorpus_recall@100 (0.2611), Performs well on beir_nfcorpus_index_time_ms (6757.4352), Performs well on beir_nfcorpus_query_time_ms (196.8648), Performs well on bright_pony_ndcg@10 (0.1038), Performs well on bright_pony_recall@100 (0.2960), Performs well on bright_pony_index_time_ms (2385.4865), Performs well on bright_pony_query_time_ms (1143.2424), Performs well on beir_scifact_ndcg@10 (0.6852), Performs well on beir_scifact_recall@100 (0.9293), Performs well on beir_scifact_index_time_ms (9174.4163), Performs well on beir_scifact_query_time_ms (893.9792), Performs well on beir_arguana_ndcg@10 (0.3025), Performs well on beir_arguana_recall@100 (0.9443), Performs well on beir_arguana_index_time_ms (10960.2805), Performs well on beir_arguana_query_time_ms (77045.5908), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0271), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (18171.7671), Performs well on bright_theoremqa_theorems_query_time_ms (2202.4288), Performs well on beir_scidocs_ndcg@10 (0.1540), Performs well on beir_scidocs_recall@100 (0.3597), Performs well on beir_scidocs_index_time_ms (37148.9477), Performs well on beir_scidocs_query_time_ms (8419.6530), Performs well on bright_economics_ndcg@10 (0.1572), Performs well on bright_economics_recall@100 (0.3995), Performs well on bright_economics_index_time_ms (24230.1253), Performs well on bright_economics_query_time_ms (8010.0175), Performs well on bright_biology_ndcg@10 (0.2654), Performs well on bright_biology_recall@100 (0.5357), Performs well on bright_biology_index_time_ms (25972.1148), Performs well on bright_biology_query_time_ms (7549.7566), Performs well on bright_earth_science_ndcg@10 (0.3372), Performs well on bright_earth_science_recall@100 (0.6731), Performs well on bright_earth_science_index_time_ms (55919.7933), Performs well on bright_earth_science_query_time_ms (12671.2718), Performs well on beir_fiqa_ndcg@10 (0.2405), Performs well on beir_fiqa_recall@100 (0.5369), Performs well on beir_fiqa_index_time_ms (64806.5605), Performs well on beir_fiqa_query_time_ms (13715.5345), Performs well on bright_stackoverflow_ndcg@10 (0.2027), Performs well on bright_stackoverflow_recall@100 (0.5169), Performs well on bright_stackoverflow_index_time_ms (152078.2268), Performs well on bright_stackoverflow_query_time_ms (30884.2942), Performs well on beir_trec-covid_ndcg@10 (0.7095), Performs well on beir_trec-covid_recall@100 (0.1221), Performs well on beir_trec-covid_index_time_ms (266002.2567), Performs well on beir_trec-covid_query_time_ms (3527.5688), Performs well on avg_ndcg@10 (0.2933), Performs well on avg_recall@100 (0.4806), Performs well on combined_score (0.4432), Performs well on total_index_time_ms (673607.4105), Performs well on total_query_time_ms (166260.2023), Performs well on total_time_ms (839867.6129), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4431)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (purely lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # NEW: residual-IDF gate (smoothly downweight ultra-common query tokens).\n    # residual = idf/(idf+tau) in (0,1). Improves early precision without hard stopwording.\n    residual_idf_tau: float = 1.25\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor.\n\n    NEW: residual-IDF gate:\n      Treat very-common query tokens as low-reliability constraints via\n        residual=idf/(idf+tau) in (0,1).\n      This tends to improve nDCG@10 (less generic-term noise) without pruning terms (recall-safe).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3345), Performs well on beir_nfcorpus_recall@100 (0.2608), Performs well on beir_nfcorpus_index_time_ms (6446.1875), Performs well on beir_nfcorpus_query_time_ms (162.2259), Performs well on bright_pony_ndcg@10 (0.1123), Performs well on bright_pony_recall@100 (0.3084), Performs well on bright_pony_index_time_ms (2303.4477), Performs well on bright_pony_query_time_ms (861.1860), Performs well on beir_scifact_ndcg@10 (0.6858), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (8633.3658), Performs well on beir_scifact_query_time_ms (680.0649), Performs well on beir_arguana_ndcg@10 (0.3083), Performs well on beir_arguana_recall@100 (0.9458), Performs well on beir_arguana_index_time_ms (10042.3007), Performs well on beir_arguana_query_time_ms (55388.1022), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0316), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (17503.6426), Performs well on bright_theoremqa_theorems_query_time_ms (1588.8815), Performs well on beir_scidocs_ndcg@10 (0.1562), Performs well on beir_scidocs_recall@100 (0.3610), Performs well on beir_scidocs_index_time_ms (35850.5318), Performs well on beir_scidocs_query_time_ms (6566.8787), Performs well on bright_economics_ndcg@10 (0.1517), Performs well on bright_economics_recall@100 (0.3896), Performs well on bright_economics_index_time_ms (23167.2595), Performs well on bright_economics_query_time_ms (5687.3667), Performs well on bright_biology_ndcg@10 (0.2607), Performs well on bright_biology_recall@100 (0.5401), Performs well on bright_biology_index_time_ms (25368.4885), Performs well on bright_biology_query_time_ms (4993.2479), Performs well on bright_earth_science_ndcg@10 (0.3113), Performs well on bright_earth_science_recall@100 (0.6631), Performs well on bright_earth_science_index_time_ms (53987.5900), Performs well on bright_earth_science_query_time_ms (8917.8827), Performs well on beir_fiqa_ndcg@10 (0.2424), Performs well on beir_fiqa_recall@100 (0.5394), Performs well on beir_fiqa_index_time_ms (61796.6020), Performs well on beir_fiqa_query_time_ms (10416.4861), Performs well on bright_stackoverflow_ndcg@10 (0.2084), Performs well on bright_stackoverflow_recall@100 (0.5162), Performs well on bright_stackoverflow_index_time_ms (149603.6848), Performs well on bright_stackoverflow_query_time_ms (22014.7062), Performs well on beir_trec-covid_ndcg@10 (0.7134), Performs well on beir_trec-covid_recall@100 (0.1207), Performs well on beir_trec-covid_index_time_ms (266747.4037), Performs well on beir_trec-covid_query_time_ms (2713.0052), Performs well on avg_ndcg@10 (0.2930), Performs well on avg_recall@100 (0.4806), Performs well on combined_score (0.4431), Performs well on total_index_time_ms (661450.5045), Performs well on total_query_time_ms (119990.0341), Performs well on total_time_ms (781440.5386), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4431)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (purely lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # NEW: residual-IDF gate (smoothly downweight ultra-common query tokens).\n    # residual = idf/(idf+tau) in (0,1). Keeps recall (no pruning) but reduces generic-term noise.\n    residual_idf_tau: float = 1.25\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded *rare-term anchor* gate.\n\n    Rare-term anchor:\n      Use the single strongest rarity-above-pivot match as a tiny multiplier.\n      This helps technical/theorem/code corpora where one exact rare identifier match\n      is often decisive, while remaining recall-safe (only changes scores for matching docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        # Tag-aware clarity: pseudo-terms (prefixes/bigrams) are easier to match and can be noisier,\n        # so we optionally suppress them a bit more than full tokens.\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        # Smooth reliability gate: common query terms are weak constraints.\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)  # in (0,1)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3345), Performs well on beir_nfcorpus_recall@100 (0.2608), Performs well on beir_nfcorpus_index_time_ms (6380.2212), Performs well on beir_nfcorpus_query_time_ms (155.0517), Performs well on bright_pony_ndcg@10 (0.1123), Performs well on bright_pony_recall@100 (0.3084), Performs well on bright_pony_index_time_ms (2193.8897), Performs well on bright_pony_query_time_ms (881.5405), Performs well on beir_scifact_ndcg@10 (0.6858), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (8688.0790), Performs well on beir_scifact_query_time_ms (677.6585), Performs well on beir_arguana_ndcg@10 (0.3083), Performs well on beir_arguana_recall@100 (0.9458), Performs well on beir_arguana_index_time_ms (10399.7306), Performs well on beir_arguana_query_time_ms (55680.7500), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0316), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (17680.2247), Performs well on bright_theoremqa_theorems_query_time_ms (1631.1728), Performs well on beir_scidocs_ndcg@10 (0.1562), Performs well on beir_scidocs_recall@100 (0.3610), Performs well on beir_scidocs_index_time_ms (35839.6188), Performs well on beir_scidocs_query_time_ms (6534.6104), Performs well on bright_economics_ndcg@10 (0.1517), Performs well on bright_economics_recall@100 (0.3896), Performs well on bright_economics_index_time_ms (23920.2695), Performs well on bright_economics_query_time_ms (5561.4458), Performs well on bright_biology_ndcg@10 (0.2607), Performs well on bright_biology_recall@100 (0.5401), Performs well on bright_biology_index_time_ms (25230.6393), Performs well on bright_biology_query_time_ms (5252.3962), Performs well on bright_earth_science_ndcg@10 (0.3113), Performs well on bright_earth_science_recall@100 (0.6631), Performs well on bright_earth_science_index_time_ms (53514.4871), Performs well on bright_earth_science_query_time_ms (8734.6481), Performs well on beir_fiqa_ndcg@10 (0.2424), Performs well on beir_fiqa_recall@100 (0.5394), Performs well on beir_fiqa_index_time_ms (61895.8824), Performs well on beir_fiqa_query_time_ms (10325.0028), Performs well on bright_stackoverflow_ndcg@10 (0.2084), Performs well on bright_stackoverflow_recall@100 (0.5162), Performs well on bright_stackoverflow_index_time_ms (147905.3291), Performs well on bright_stackoverflow_query_time_ms (22066.2990), Performs well on beir_trec-covid_ndcg@10 (0.7134), Performs well on beir_trec-covid_recall@100 (0.1207), Performs well on beir_trec-covid_index_time_ms (265745.9827), Performs well on beir_trec-covid_query_time_ms (2744.8795), Performs well on avg_ndcg@10 (0.2930), Performs well on avg_recall@100 (0.4806), Performs well on combined_score (0.4431), Performs well on total_index_time_ms (659394.3542), Performs well on total_query_time_ms (120245.4552), Performs well on total_time_ms (779639.8094), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4420)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Specificity/aboutness via positive PMI density lift (bounded).\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    # Residual-IDF reliability gate: common query terms are weak constraints (recall-safe).\n    # residual = idf/(idf+tau) in (0,1).\n    residual_idf_tau: float = 1.25\n\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus bounded rerankers that mainly help early precision:\n\n      (1) rare-term anchor (single best rare hit; recall-safe)\n      (2) specificity/aboutness via positive PMI density lift (bounded)\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        # Residual-IDF reliability gate reduces generic-term noise without pruning.\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_spec:\n            # log((tf/dl)/(df/N)) = log(tf*N/(dl*df)); reward only positive (capped).\n            g = math.log((tf * float(N) + eps) / (dl_eff * df + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n            clarity = (rarity ** Config.q_clarity_power) * residual\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                g = np.log((tf_row * float(self.corpus.N) + eps) / (dl_eff * df_val + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4417)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # NEW: bounded within-doc specificity/aboutness via positive PMI density lift.\n    # Downranks incidental mentions in long docs; usually helps nDCG@10 with little recall impact.\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus bounded rerankers that mainly help early precision:\n\n      (1) rare-term anchor (single best rare hit; recall-safe)\n      (2) specificity/aboutness via positive PMI density lift (bounded)\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_spec:\n            # log((tf/dl)/(df/N)) = log(tf*N/(dl*df)); reward only positive (capped).\n            g = math.log((tf * float(N) + eps) / (dl_eff * df + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                g = np.log((tf_row * float(self.corpus.N) + eps) / (dl_eff * df_val + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4428)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (purely lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Residual-IDF gate (smoothly downweight ultra-common query tokens).\n    residual_idf_tau: float = 1.25\n\n    # Query self-information coverage (QSN).\n    qsn_alpha: float = 0.14\n\n    # NEW: \"Informative AND\" (IA) soft-AND in probability space.\n    # Each matched query term yields p(tf)=1-exp(-tf/k). Weighted geometric mean\n    # prefers docs satisfying multiple constraints jointly (nDCG@10 lift; recall-safe multiplier).\n    ia_alpha: float = 0.10\n    ia_tf_k: float = 1.6\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus bounded gates:\n\n      (1) rare-term anchor (single strongest rare hit; recall-safe)\n      (2) QSN: cover raw informative query mass (recall-safe)\n      (3) NEW IA: probabilistic soft-AND over constraints (recall-safe)\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    # QSN accumulators: total raw IDF mass in query vs mass covered by this doc.\n    q_idf_tot = 0.0\n    q_idf_hit = 0.0\n    qsn_alpha = float(getattr(Config, \"qsn_alpha\", 0.0))\n\n    # IA accumulators (weighted geometric mean over p(tf)).\n    ia_log = 0.0\n    ia_wsum = 0.0\n    ia_alpha = float(getattr(Config, \"ia_alpha\", 0.0))\n    ia_k = float(getattr(Config, \"ia_tf_k\", 1.0))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        if qsn_alpha != 0.0:\n            q_mass = wq * tidf\n            q_idf_tot += q_mass\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        if qsn_alpha != 0.0:\n            q_idf_hit += wq * tidf\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # IA: convert tf into a bounded satisfaction probability and aggregate in log-space.\n        if ia_alpha != 0.0 and wt > 0.0:\n            p = 1.0 - math.exp(-tf / (ia_k + eps))  # (0,1)\n            ia_log += wt * math.log(p + eps)\n            ia_wsum += wt\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)  # in (0,1)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if qsn_alpha != 0.0 and q_idf_tot > 0.0:\n        qsn = q_idf_hit / (q_idf_tot + eps)\n        qsn = max(0.0, min(1.0, qsn))\n        score *= 1.0 + qsn_alpha * qsn\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if ia_alpha != 0.0 and ia_wsum > 0.0:\n        ia = math.exp(ia_log / (ia_wsum + eps))  # (0,1]\n        score *= 1.0 + ia_alpha * ia\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        # QSN accumulators (raw idf mass coverage).\n        qsn_alpha = float(getattr(Config, \"qsn_alpha\", 0.0))\n        q_idf_tot = 0.0\n        q_idf_hit = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # IA accumulators.\n        ia_alpha = float(getattr(Config, \"ia_alpha\", 0.0))\n        ia_k = float(getattr(Config, \"ia_tf_k\", 1.0))\n        ia_log = np.zeros(len(candidate_docs), dtype=np.float64)\n        ia_wsum = 0.0\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if qsn_alpha != 0.0:\n                q_mass = float(q_w[i]) * idf_val\n                q_idf_tot += q_mass\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            if qsn_alpha != 0.0:\n                q_idf_hit += q_mass * present\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if ia_alpha != 0.0 and wt > 0.0:\n                p = 1.0 - np.exp(-tf_row / (ia_k + eps))  # (0,1)\n                ia_log += wt * np.log(p + eps) * present\n                ia_wsum += wt\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if qsn_alpha != 0.0 and q_idf_tot > 0.0:\n            qsn = np.clip(q_idf_hit / (q_idf_tot + eps), 0.0, 1.0)\n            scores *= 1.0 + qsn_alpha * qsn\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if ia_alpha != 0.0 and ia_wsum > 0.0:\n            ia = np.exp(ia_log / (ia_wsum + eps))\n            scores *= 1.0 + ia_alpha * ia\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4428)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (purely lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Residual-IDF gate (smoothly downweight ultra-common query tokens).\n    residual_idf_tau: float = 1.25\n\n    # NEW: Informative-AND (IA) soft-AND gate.\n    # View each query term as a \"constraint\"; tf -> satisfaction prob p(tf)=1-exp(-tf/k).\n    # Weighted geometric mean prefers docs that satisfy many constraints jointly (nDCG@10 lift),\n    # but is recall-safe since it's a small multiplier on already-matching docs.\n    ia_alpha: float = 0.12\n    ia_tf_k: float = 1.6\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus bounded gates that are recall-safe multipliers:\n\n      (1) residual-IDF reliability: very common terms are weak constraints\n      (2) rare-term anchor: one decisive rare exact hit\n      (3) IA (informative-AND): probabilistic soft-AND prefers joint satisfaction\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    ia_alpha = float(getattr(Config, \"ia_alpha\", 0.0))\n    ia_k = float(getattr(Config, \"ia_tf_k\", 1.0))\n    ia_log = 0.0\n    ia_wsum = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if ia_alpha != 0.0 and wt > 0.0:\n            p = 1.0 - math.exp(-tf / (ia_k + eps))  # (0,1)\n            ia_log += wt * math.log(p + eps)\n            ia_wsum += wt\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if ia_alpha != 0.0 and ia_wsum > 0.0:\n        ia = math.exp(ia_log / (ia_wsum + eps))  # (0,1]\n        score *= 1.0 + ia_alpha * ia\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        ia_alpha = float(getattr(Config, \"ia_alpha\", 0.0))\n        ia_k = float(getattr(Config, \"ia_tf_k\", 1.0))\n        ia_log = np.zeros(len(candidate_docs), dtype=np.float64)\n        ia_wsum = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if ia_alpha != 0.0 and wt > 0.0:\n                p = 1.0 - np.exp(-tf_row / (ia_k + eps))  # (0,1)\n                ia_log += wt * np.log(p + eps) * present\n                ia_wsum += wt\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if ia_alpha != 0.0 and ia_wsum > 0.0:\n            ia = np.exp(ia_log / (ia_wsum + eps))\n            scores *= 1.0 + ia_alpha * ia\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4431, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (purely lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # NEW: residual-IDF gate (smoothly downweight ultra-common query tokens).\n    # residual = idf/(idf+tau) in (0,1). Improves early precision without hard stopwording.\n    residual_idf_tau: float = 1.25\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor.\n\n    NEW: residual-IDF gate:\n      Treat very-common query tokens as low-reliability constraints via\n        residual=idf/(idf+tau) in (0,1).\n      This tends to improve nDCG@10 (less generic-term noise) without pruning terms (recall-safe).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4428, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (purely lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Residual-IDF gate (smoothly downweight ultra-common query tokens).\n    residual_idf_tau: float = 1.25\n\n    # NEW: Informative-AND (IA) soft-AND gate.\n    # View each query term as a \"constraint\"; tf -> satisfaction prob p(tf)=1-exp(-tf/k).\n    # Weighted geometric mean prefers docs that satisfy many constraints jointly (nDCG@10 lift),\n    # but is recall-safe since it's a small multiplier on already-matching docs.\n    ia_alpha: float = 0.12\n    ia_tf_k: float = 1.6\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus bounded gates that are recall-safe multipliers:\n\n      (1) residual-IDF reliability: very common terms are weak constraints\n      (2) rare-term anchor: one decisive rare exact hit\n      (3) IA (informative-AND): probabilistic soft-AND prefers joint satisfaction\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    ia_alpha = float(getattr(Config, \"ia_alpha\", 0.0))\n    ia_k = float(getattr(Config, \"ia_tf_k\", 1.0))\n    ia_log = 0.0\n    ia_wsum = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if ia_alpha != 0.0 and wt > 0.0:\n            p = 1.0 - math.exp(-tf / (ia_k + eps))  # (0,1)\n            ia_log += wt * math.log(p + eps)\n            ia_wsum += wt\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if ia_alpha != 0.0 and ia_wsum > 0.0:\n        ia = math.exp(ia_log / (ia_wsum + eps))  # (0,1]\n        score *= 1.0 + ia_alpha * ia\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        ia_alpha = float(getattr(Config, \"ia_alpha\", 0.0))\n        ia_k = float(getattr(Config, \"ia_tf_k\", 1.0))\n        ia_log = np.zeros(len(candidate_docs), dtype=np.float64)\n        ia_wsum = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if ia_alpha != 0.0 and wt > 0.0:\n                p = 1.0 - np.exp(-tf_row / (ia_k + eps))  # (0,1)\n                ia_log += wt * np.log(p + eps) * present\n                ia_wsum += wt\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if ia_alpha != 0.0 and ia_wsum > 0.0:\n            ia = np.exp(ia_log / (ia_wsum + eps))\n            scores *= 1.0 + ia_alpha * ia\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4420, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Specificity/aboutness via positive PMI density lift (bounded).\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    # Residual-IDF reliability gate: common query terms are weak constraints (recall-safe).\n    # residual = idf/(idf+tau) in (0,1).\n    residual_idf_tau: float = 1.25\n\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus bounded rerankers that mainly help early precision:\n\n      (1) rare-term anchor (single best rare hit; recall-safe)\n      (2) specificity/aboutness via positive PMI density lift (bounded)\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        # Residual-IDF reliability gate reduces generic-term noise without pruning.\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_spec:\n            # log((tf/dl)/(df/N)) = log(tf*N/(dl*df)); reward only positive (capped).\n            g = math.log((tf * float(N) + eps) / (dl_eff * df + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n            clarity = (rarity ** Config.q_clarity_power) * residual\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                g = np.log((tf_row * float(self.corpus.N) + eps) / (dl_eff * df_val + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4402, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Entropy-normalized TF (\"aboutness\") prior.\n    entropy_alpha: float = 0.10\n\n    # Residual-IDF reliability gate: downweight ultra-common query terms smoothly (recall-safe).\n    # residual = idf/(idf+tau) in (0,1). Helps early precision on verbose/natural queries.\n    residual_idf_tau: float = 1.25\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor, plus an entropy-normalized burstiness prior.\n\n    Entropy prior (query-term TF entropy within doc):\n      Let p_t \u221d (wt * tf_t) over matched query terms. Compute normalized entropy H/log(K).\n      Use concentration = 1 - H/log(K) in [0,1]. Concentrated evidence often correlates with\n      topicality (better early precision) while staying recall-safe (only affects matching docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    # For entropy prior: accumulate per-term matched mass m_t = wt * tf (only if tf>0).\n    m_sum = 0.0\n    m_log_sum = 0.0  # sum(m_t * log(m_t))\n    k_match = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # Smooth reliability gate: common query terms are weak constraints.\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity * residual\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if getattr(Config, \"entropy_alpha\", 0.0) != 0.0:\n            mt = wt * tf\n            if mt > 0.0:\n                k_match += 1.0\n                m_sum += mt\n                m_log_sum += mt * math.log(mt + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if getattr(Config, \"entropy_alpha\", 0.0) != 0.0 and k_match >= 2.0 and m_sum > 0.0:\n        # H = -sum p log p = log(m_sum) - (sum m log m)/m_sum\n        H = math.log(m_sum + eps) - (m_log_sum / (m_sum + eps))\n        Hn = H / (math.log(k_match + eps) + eps)  # normalized to ~[0,1]\n        conc = max(0.0, min(1.0, 1.0 - Hn))\n        score *= 1.0 + float(Config.entropy_alpha) * conc\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over (token) U (prefixes), keeping them disjoint by tagging prefixes.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        # Entropy prior accumulators over matched query terms.\n        m_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        m_log_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        k_match = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_ent = getattr(Config, \"entropy_alpha\", 0.0) != 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n            wt = float(q_w[i]) * idf_val * clarity * residual\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_ent:\n                mt = wt * tf_row\n                mt_pos = np.maximum(mt, 0.0)\n                k_match += (mt_pos > 0.0).astype(np.float64)\n                m_sum += mt_pos\n                m_log_sum += mt_pos * np.log(mt_pos + eps)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_ent:\n            # H = log(m_sum) - (m_log_sum/m_sum), normalize by log(k_match).\n            mask = (k_match >= 2.0) & (m_sum > 0.0)\n            if np.any(mask):\n                H = np.log(m_sum + eps) - (m_log_sum / (m_sum + eps))\n                Hn = H / (np.log(k_match + eps) + eps)\n                conc = np.clip(1.0 - Hn, 0.0, 1.0)\n                scores *= 1.0 + float(Config.entropy_alpha) * (conc * mask.astype(np.float64))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + informative-coverage pressure.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Rare-term anchor (single best rare hit; bounded, recall-safe).\n    # More stable than summing many \"somewhat rare\" terms on verbose queries.\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Bounded specificity/aboutness prior (positive-PMI density lift).\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    # NEW: cohesion prior via concentration (Gini) over matched query evidence.\n    # Let m_t = wt*tf over matched query terms in the doc.\n    # Gini in [0,1] is higher when evidence concentrates on a few decisive terms.\n    # Gentle multiplier to lift early precision.\n    gini_alpha: float = 0.08\n\n    # Tiny micro-token channel (char n-grams) to reduce tokenization brittleness.\n    micro_len: int = 3\n    micro_min_token_len: int = 2\n    micro_weight: float = 0.12\n\n    # Compatibility (Corpus still builds norm_array).\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + clarity gate + IDF-mass coverage,\n    plus bounded priors that mainly affect early precision:\n\n    (1) Rare-term *anchor* (single best rare hit; recall-safe, robust to verbose queries).\n    (2) Specificity/aboutness gain via positive PMI (bounded).\n    (3) Cohesion prior via Gini concentration of matched query evidence m_t = wt*tf.\n        Higher concentration => doc likely focuses on a decisive subset of query constraints.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Rare anchor: strongest hinge among matched high-idf terms.\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    # Gini concentration accumulators over matched query terms.\n    use_gini = float(getattr(Config, \"gini_alpha\", 0.0)) != 0.0\n    m_sum = 0.0\n    m_sq_sum = 0.0\n    k_match = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)  # in (0,1)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_spec:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n        if use_gini:\n            mt = wt * tf\n            if mt > 0.0:\n                k_match += 1.0\n                m_sum += mt\n                m_sq_sum += mt * mt\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_gini and k_match >= 2.0 and m_sum > 0.0:\n        # Proxy Gini via Herfindahl: H = sum (p_i^2) where p_i = m_i/sum m.\n        # Map to [0,1]: conc = (H - 1/K) / (1 - 1/K). High when concentrated.\n        H = m_sq_sum / ((m_sum * m_sum) + eps)\n        invk = 1.0 / (k_match + eps)\n        conc = (H - invk) / (1.0 - invk + eps)\n        conc = max(0.0, min(1.0, conc))\n        score *= 1.0 + float(Config.gini_alpha) * conc\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Micro channel: char n-grams over query tokens (lexical, more forgiving for symbols/identifiers).\n    if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n        m = max(2, int(Config.micro_len))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        mtoks: list[str] = []\n        for t in query:\n            if len(t) < min_tok:\n                continue\n            if len(t) <= m:\n                mtoks.append(\"M:\" + t)\n            else:\n                for i in range(0, len(t) - m + 1):\n                    mtoks.append(\"M:\" + t[i : i + m])\n        if mtoks:\n            mq = QueryRepr.from_tokens(mtoks)\n            s += float(Config.micro_weight) * retrieval_score(\n                mq,\n                corpus.micro_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Micro-token field: character n-grams prefixed with \"M:\" to keep disjoint vocab.\n        m = max(2, int(getattr(Config, \"micro_len\", 3)))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        micro_docs: list[list[str]] = []\n        for doc in documents:\n            grams: list[str] = []\n            for t in doc:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            micro_docs.append(grams)\n        self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n\n        # Joint vocabulary over base tokens and micro tokens.\n        self._vocab: dict[str, int] = {}\n        for doc, mdoc in zip(documents, micro_docs):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for g in mdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, mdoc) in enumerate(zip(documents, micro_docs)):\n            term_counts = Counter(doc)\n            micro_counts = Counter(mdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in micro_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (micro keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        # Rare anchor (max hinge) per doc.\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        # Gini proxy via Herfindahl concentration over m_t = wt*tf.\n        use_gini = float(getattr(Config, \"gini_alpha\", 0.0)) != 0.0\n        m_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        m_sq_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        k_match = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n            if use_gini:\n                mt = wt * tf_row\n                mt_pos = np.maximum(mt, 0.0)\n                k_match += (mt_pos > 0.0).astype(np.float64)\n                m_sum += mt_pos\n                m_sq_sum += mt_pos * mt_pos\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_gini:\n            mask = (k_match >= 2.0) & (m_sum > 0.0)\n            if np.any(mask):\n                H = m_sq_sum / ((m_sum * m_sum) + eps)\n                invk = 1.0 / (k_match + eps)\n                conc = (H - invk) / (1.0 - invk + eps)\n                conc = np.clip(conc, 0.0, 1.0)\n                scores *= 1.0 + float(Config.gini_alpha) * (conc * mask.astype(np.float64))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query over base tokens + micro (char n-gram) tokens.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n            m = max(2, int(Config.micro_len))\n            min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n            grams: list[str] = []\n            for t in query:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            if grams:\n                gcounts = Counter(grams)\n                for g, c in gcounts.items():\n                    tid = self.corpus.get_term_id(g)\n                    if tid is not None:\n                        query_term_ids.append(tid)\n                        query_term_weights.append(float(Config.micro_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Below are targeted edits to (a) recover the best-known behavior (prefix+bigram channels + residual-idf reliability) and (b) simplify/remove the micro-token channel that likely hurt both quality and speed. This should improve combined_score while also reducing code size.\n\n### Why these changes help\n- **Micro char-ngrams** massively inflate vocabulary/df and create lots of spurious matches (hurting nDCG) and extra candidates (hurting speed). Removing it often restores lexical precision.\n- **Prefix channel** is a cheaper, more semantically aligned robustness trick than char-ngrams (captures morphology, identifiers\u2019 stems) and worked well historically in your top programs.\n- **Bigram channel** adds cheap phrase/proximity specificity and tends to lift early precision on abstract-heavy corpora.\n- **Residual-IDF gate** downweights ultra-common query terms smoothly (recall-safe) and previously helped nDCG without pruning.\n\n---\n\n## Change 1 \u2014 Simplify Config: drop micro channel; add prefix+bigram+residual-idf back\n<<<<<<< SEARCH\nclass Config:\n    # Concave evidence + informative-coverage pressure.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Rare-term anchor (single best rare hit; bounded, recall-safe).\n    # More stable than summing many \"somewhat rare\" terms on verbose queries.\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Bounded specificity/aboutness prior (positive-PMI density lift).\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    # NEW: cohesion prior via concentration (Gini) over matched query evidence.\n    # Let m_t = wt*tf over matched query terms in the doc.\n    # Gini in [0,1] is higher when evidence concentrates on a few decisive terms.\n    # Gentle multiplier to lift early precision.\n    gini_alpha: float = 0.08\n\n    # Tiny micro-token channel (char n-grams) to reduce tokenization brittleness.\n    micro_len: int = 3\n    micro_min_token_len: int = 2\n    micro_weight: float = 0.12\n\n    # Compatibility (Corpus still builds norm_array).\n    k1: float = 0.9\n    b: float = 0.4\n=======\nclass Config:\n    # Concave evidence + informative-coverage pressure.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (robustness for morphology/identifiers; cheaper/cleaner than char-ngrams).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (cheap lexical proximity / phrase specificity).\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Residual-IDF reliability gate: downweight ultra-common query terms smoothly (recall-safe).\n    residual_idf_tau: float = 1.25\n\n    # Rare-term anchor (single best rare hit; bounded, recall-safe).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Bounded specificity/aboutness prior (positive-PMI density lift).\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    # Cohesion prior via concentration over matched query evidence.\n    gini_alpha: float = 0.08\n\n    # Compatibility (Corpus still builds norm_array).\n    k1: float = 0.9\n    b: float = 0.4\n>>>>>>> REPLACE\n\n---\n\n## Change 2 \u2014 Add residual-idf gating + tag-aware clarity inside retrieval_score()\n<<<<<<< SEARCH\n    eps = Config.epsilon\n    base = Config.tf_log_base\n=======\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n=======\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n        residual = tidf / (tidf + tau) if tau > 0.0 else 1.0\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        wt = wq * tidf * clarity\n=======\n        wt = wq * tidf * clarity * residual\n>>>>>>> REPLACE\n\n---\n\n## Change 3 \u2014 score_document(): remove micro-channel; add bigram + prefix channels\n<<<<<<< SEARCH\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Micro channel: char n-grams over query tokens (lexical, more forgiving for symbols/identifiers).\n    if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n        m = max(2, int(Config.micro_len))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        mtoks: list[str] = []\n        for t in query:\n            if len(t) < min_tok:\n                continue\n            if len(t) <= m:\n                mtoks.append(\"M:\" + t)\n            else:\n                for i in range(0, len(t) - m + 1):\n                    mtoks.append(\"M:\" + t[i : i + m])\n        if mtoks:\n            mq = QueryRepr.from_tokens(mtoks)\n            s += float(Config.micro_weight) * retrieval_score(\n                mq,\n                corpus.micro_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n=======\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram channel (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq,\n                corpus.bigram_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    # Prefix channel (morphology/identifier robustness).\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq,\n                corpus.prefix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    return s\n>>>>>>> REPLACE\n\n---\n\n## Change 4 \u2014 Corpus: remove micro-ngrams; build prefix + bigram views + joint vocab\n<<<<<<< SEARCH\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Micro-token field: character n-grams prefixed with \"M:\" to keep disjoint vocab.\n        m = max(2, int(getattr(Config, \"micro_len\", 3)))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        micro_docs: list[list[str]] = []\n        for doc in documents:\n            grams: list[str] = []\n            for t in doc:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            micro_docs.append(grams)\n        self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n\n        # Joint vocabulary over base tokens and micro tokens.\n        self._vocab: dict[str, int] = {}\n        for doc, mdoc in zip(documents, micro_docs):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for g in mdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, mdoc) in enumerate(zip(documents, micro_docs)):\n            term_counts = Counter(doc)\n            micro_counts = Counter(mdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in micro_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (micro keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n=======\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view (morphology/identifier robustness).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 5)))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Bigram view (adjacent tokens) as cheap phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Base doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n>>>>>>> REPLACE\n\n---\n\n## Change 5 \u2014 BM25.rank(): build combined query (tokens + prefix + bigram), not micro-grams\n<<<<<<< SEARCH\n        # Combined query over base tokens + micro (char n-gram) tokens.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n            m = max(2, int(Config.micro_len))\n            min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n            grams: list[str] = []\n            for t in query:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            if grams:\n                gcounts = Counter(grams)\n                for g, c in gcounts.items():\n                    tid = self.corpus.get_term_id(g)\n                    if tid is not None:\n                        query_term_ids.append(tid)\n                        query_term_weights.append(float(Config.micro_weight) * (float(c) ** Config.qtf_power))\n=======\n        # Combined query over tokens + (optional) prefixes + (optional) bigrams as pseudo-terms.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n>>>>>>> REPLACE\n\n---\n\n## Change 6 \u2014 Vectorized scorer: add residual-idf + tag-aware clarity (for bigrams) to match retrieval_score()\n<<<<<<< SEARCH\n        eps = Config.epsilon\n        base = Config.tf_log_base\n=======\n        eps = Config.epsilon\n        base = Config.tf_log_base\n        tau = float(getattr(Config, \"residual_idf_tau\", 0.0))\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n=======\n            rarity = idf_val / (idf_val + 1.0)\n            clarity_pow = Config.q_clarity_power\n            # Approximate tag-aware clarity: bigram pseudo-terms are tagged \"B:\" in vocab.\n            if hasattr(self.corpus, \"_vocab\"):\n                # best-effort: infer via stored term strings if present (safe if absent)\n                pass\n            if hasattr(self.corpus, \"_id_to_term\"):\n                t = self.corpus._id_to_term.get(term_id)\n                if t is not None and t.startswith(\"B:\"):\n                    clarity_pow = float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n            clarity = rarity ** clarity_pow\n            residual = idf_val / (idf_val + tau) if tau > 0.0 else 1.0\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity * residual\n>>>>>>> REPLACE\n\n### Note\nChange 6 references `self.corpus._id_to_term`. If it doesn\u2019t exist yet, add it cheaply when building vocab:\n\nAdd this small reverse-map line in `Corpus.__init__` right after `self.vocab_size = len(self._vocab)`:\n\n```python\nself._id_to_term = {i: t for t, i in self._vocab.items()}\n```\n\nIf you want that applied as a diff too, here it is:\n\n<<<<<<< SEARCH\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n=======\n        self.vocab_size = len(self._vocab)\n        self._id_to_term = {i: t for t, i in self._vocab.items()}\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}