{"id": "9baac4e2-3760-4a5a-adf9-ea3ac582a79b", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (phrase specificity)\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Background-corrected aboutness (positive PMI density).\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # NEW: local-coherence prior (position-free proxy for \"do query terms appear together in a snippet?\")\n    # Use a Poisson window model: u_w = 1-exp(-tf*w/dl). Then average u_w by informative weights.\n    # Bounded => mostly reorders early ranks; tends to help nDCG@10 on long documents.\n    window_size: int = 28\n    window_gamma: float = 0.14\n    window_cap: float = 0.85\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor gate, plus a bounded aboutness prior,\n    plus a bounded local-coherence prior (window-Poisson).\n\n    Window-Poisson coherence:\n      Approximate \"term appears in a random snippet of length w\" by:\n          u_w(tf,dl)=1-exp(-tf*w/dl)\n      and average u_w over query terms (weighted by wt). This is position-free but\n      correlates with true proximity/cohesion, improving early precision on long docs.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    use_win = float(getattr(Config, \"window_gamma\", 0.0)) != 0.0\n    win_num = 0.0\n    w = float(max(1, int(getattr(Config, \"window_size\", 1))))\n    win_cap = float(getattr(Config, \"window_cap\", 0.85))\n    dl_snip = max(float(doc_length), 1.0)\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n        if use_win:\n            u_w = 1.0 - math.exp(-(tf * w) / (dl_snip + eps))\n            win_num += wt * u_w\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    if use_win and win_num > 0.0 and cov_den > 0.0:\n        win = min(win_num / (cov_den + eps), win_cap)\n        score *= 1.0 + float(Config.window_gamma) * win\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        use_win = float(getattr(Config, \"window_gamma\", 0.0)) != 0.0\n        win_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        w = float(max(1, int(getattr(Config, \"window_size\", 1))))\n        win_cap = float(getattr(Config, \"window_cap\", 0.85))\n        dl_snip = np.maximum(self.corpus.doc_lengths[candidate_docs], 1.0)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n            if use_win:\n                u_w = 1.0 - np.exp(-(tf_row * w) / (dl_snip + eps))\n                win_num += wt * u_w\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        if use_win and cov_den > 0.0:\n            win = np.minimum(win_num / (cov_den + eps), win_cap)\n            scores *= 1.0 + float(Config.window_gamma) * win\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "5fee1889-6696-4f48-a3dc-3dff5a74bfe7", "generation": 9, "timestamp": 1770296517.823654, "iteration_found": 235, "metrics": {"beir_nfcorpus_ndcg@10": 0.3337492541680015, "beir_nfcorpus_recall@100": 0.26152528221997456, "beir_nfcorpus_index_time_ms": 6492.008000030182, "beir_nfcorpus_query_time_ms": 200.87129098828882, "bright_pony_ndcg@10": 0.10971807941021404, "bright_pony_recall@100": 0.29720347605527603, "bright_pony_index_time_ms": 2372.897708090022, "bright_pony_query_time_ms": 1289.7657079156488, "beir_scifact_ndcg@10": 0.698132887786374, "beir_scifact_recall@100": 0.922, "beir_scifact_index_time_ms": 8825.138292042539, "beir_scifact_query_time_ms": 955.1730829989538, "beir_arguana_ndcg@10": 0.31601713522329583, "beir_arguana_recall@100": 0.9414703783012134, "beir_arguana_index_time_ms": 10528.605957981199, "beir_arguana_query_time_ms": 87664.21900002751, "bright_theoremqa_theorems_ndcg@10": 0.02996926921934093, "bright_theoremqa_theorems_recall@100": 0.19298245614035087, "bright_theoremqa_theorems_index_time_ms": 17630.846832995303, "bright_theoremqa_theorems_query_time_ms": 2451.3347500469536, "beir_scidocs_ndcg@10": 0.15981259647661825, "beir_scidocs_recall@100": 0.36260000000000003, "beir_scidocs_index_time_ms": 35057.953375042416, "beir_scidocs_query_time_ms": 9163.095999974757, "bright_economics_ndcg@10": 0.1481344988425449, "bright_economics_recall@100": 0.3869637517463147, "bright_economics_index_time_ms": 23630.067624966614, "bright_economics_query_time_ms": 9017.576625104994, "bright_biology_ndcg@10": 0.232932175523321, "bright_biology_recall@100": 0.5155454918101826, "bright_biology_index_time_ms": 25385.497250012122, "bright_biology_query_time_ms": 8346.93545801565, "beir_fiqa_ndcg@10": 0.25000203724689435, "beir_fiqa_recall@100": 0.5474329682663016, "beir_fiqa_index_time_ms": 62034.14195799269, "beir_fiqa_query_time_ms": 14422.547125024721, "bright_earth_science_ndcg@10": 0.31365261378012466, "bright_earth_science_recall@100": 0.6546187503764638, "bright_earth_science_index_time_ms": 53885.04974998068, "bright_earth_science_query_time_ms": 14350.242333952338, "bright_stackoverflow_ndcg@10": 0.20528578107820797, "bright_stackoverflow_recall@100": 0.5184348081246454, "bright_stackoverflow_index_time_ms": 149281.98670805432, "bright_stackoverflow_query_time_ms": 34744.91554207634, "beir_trec-covid_ndcg@10": 0.7179949816586572, "beir_trec-covid_recall@100": 0.12211521605176953, "beir_trec-covid_index_time_ms": 259857.020916068, "beir_trec-covid_query_time_ms": 3797.5412919186056, "avg_ndcg@10": 0.2929501092011329, "avg_recall@100": 0.4769077149243744, "combined_score": 0.44011619377972616, "total_index_time_ms": 654981.2143732561, "total_query_time_ms": 186404.21820804477, "total_time_ms": 841385.4325813008, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # Bigram channel (phrase specificity)\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Background-corrected aboutness (positive PMI density).\n      # For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N; add only posi...\n      # This is robust across corpora and tends to improve early precision without killing recall.\n      about_beta: float = 0.08\n      about_cap: float = 2.5\n  ... (5 more lines)\nwith:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # Bigram channel (phrase specificity)\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Background-corrected aboutness (positive PMI density).\n      about_beta: float = 0.08\n      about_cap: float = 2.5\n      about_len_floor: float = 40.0\n  \n  ... (10 more lines)\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n      plus a bounded rare-term anchor gate, plus a bounded aboutness prior.\n  \n      Aboutness prior (positive PMI density, position-free):\n        For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N.\n        Add only positive PMI (capped) weighted by the same informative wt, then apply\n        a small multiplier. Downranks incidental mentions in long docs => better nDCG@10.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      matched = 0.0\n      uq = 0.0\n  \n      anchor = 0.0\n  ... (66 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n      plus a bounded rare-term anchor gate, plus a bounded aboutness prior,\n      plus a bounded local-coherence prior (window-Poisson).\n  \n      Window-Poisson coherence:\n        Approximate \"term appears in a random snippet of length w\" by:\n            u_w(tf,dl)=1-exp(-tf*w/dl)\n        and average u_w over query terms (weighted by wt). This is position-free but\n        correlates with true proximity/cohesion, improving early precision on long docs.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      matched = 0.0\n      uq = 0.0\n  ... (82 more lines)\nChange 3: Replace:\n  use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n          about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n          about_cap = float(getattr(Config, \"about_cap\", 2.5))\n          dl_eff = np.maximum(\n              self.corpus.doc_lengths[candidate_docs],\n              float(getattr(Config, \"about_len_floor\", 0.0)),\n          )\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n              wt = float(q_w[i]) * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n  \n              matched += present\n              cov_num += wt * present\n              sum_evidence += wt * np.log1p(tf_row / (base + eps))\n  \n              if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                  hinge = (idf_val - pivot) / (idf_val + eps)\n                  anchor = np.maximum(anchor, present * hinge)\n  \n              if use_about:\n  ... (26 more lines)\nwith:\n  use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n          about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n          about_cap = float(getattr(Config, \"about_cap\", 2.5))\n          dl_eff = np.maximum(\n              self.corpus.doc_lengths[candidate_docs],\n              float(getattr(Config, \"about_len_floor\", 0.0)),\n          )\n  \n          use_win = float(getattr(Config, \"window_gamma\", 0.0)) != 0.0\n          win_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          w = float(max(1, int(getattr(Config, \"window_size\", 1))))\n          win_cap = float(getattr(Config, \"window_cap\", 0.85))\n          dl_snip = np.maximum(self.corpus.doc_lengths[candidate_docs], 1.0)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n              wt = float(q_w[i]) * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n  \n              matched += present\n              cov_num += wt * present\n              sum_evidence += wt * np.log1p(tf_row / (base + eps))\n  ... (40 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3338263079380546, "beir_nfcorpus_recall@100": 0.2613406222785296, "beir_nfcorpus_index_time_ms": 6394.722749944776, "beir_nfcorpus_query_time_ms": 180.15362496953458, "bright_pony_ndcg@10": 0.11234510982510991, "bright_pony_recall@100": 0.29929350903816615, "bright_pony_index_time_ms": 2297.7155409753323, "bright_pony_query_time_ms": 1080.5653750430793, "beir_scifact_ndcg@10": 0.6949039641631434, "beir_scifact_recall@100": 0.922, "beir_scifact_index_time_ms": 8794.436583062634, "beir_scifact_query_time_ms": 851.3867909787223, "beir_arguana_ndcg@10": 0.3127337345170438, "beir_arguana_recall@100": 0.940756602426838, "beir_arguana_index_time_ms": 10467.386833042838, "beir_arguana_query_time_ms": 73900.84729203954, "bright_theoremqa_theorems_ndcg@10": 0.027434003533501748, "bright_theoremqa_theorems_recall@100": 0.17982456140350878, "bright_theoremqa_theorems_index_time_ms": 17562.42208299227, "bright_theoremqa_theorems_query_time_ms": 2087.49824995175, "beir_scidocs_ndcg@10": 0.15811723170033293, "beir_scidocs_recall@100": 0.36160000000000003, "beir_scidocs_index_time_ms": 34870.55516697001, "beir_scidocs_query_time_ms": 7955.679167062044, "bright_economics_ndcg@10": 0.15058255080858768, "bright_economics_recall@100": 0.3977187946934884, "bright_economics_index_time_ms": 23760.912999976426, "bright_economics_query_time_ms": 7574.188707978465, "bright_biology_ndcg@10": 0.2463968420517151, "bright_biology_recall@100": 0.5271959772470759, "bright_biology_index_time_ms": 25136.377667076886, "bright_biology_query_time_ms": 6959.2114590341225, "bright_earth_science_ndcg@10": 0.3155742979485442, "bright_earth_science_recall@100": 0.6572870590792551, "bright_earth_science_index_time_ms": 54014.44825006183, "bright_earth_science_query_time_ms": 11291.457667015493, "beir_fiqa_ndcg@10": 0.24796289218422524, "beir_fiqa_recall@100": 0.5431278084055862, "beir_fiqa_index_time_ms": 61572.31770805083, "beir_fiqa_query_time_ms": 12212.24887506105, "bright_stackoverflow_ndcg@10": 0.20665617523255195, "bright_stackoverflow_recall@100": 0.5184348081246454, "bright_stackoverflow_index_time_ms": 149864.29933295585, "bright_stackoverflow_query_time_ms": 29356.078917044215, "beir_trec-covid_ndcg@10": 0.7191454811774601, "beir_trec-covid_recall@100": 0.12192049988472606, "beir_trec-covid_index_time_ms": 267518.8933329191, "beir_trec-covid_query_time_ms": 3274.995082989335, "avg_ndcg@10": 0.29380654925668925, "avg_recall@100": 0.4775416868818183, "combined_score": 0.4407946593567925, "total_index_time_ms": 662254.4882480288, "total_query_time_ms": 156724.31120916735, "total_time_ms": 818978.7994571961, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4408\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.4387 \u2192 0.4408\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      # Concave evidence + informative-coverage pressure.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # NEW: rare-constraint gate (bounded) to help identifier/symbol-heavy queries (BRIGHT).\n      # It only reorders already-matching docs (present==1), so it is recall-safe.\n      rare_idf_pivot: float = 4.2\n      rare_boost: float = 0.18\n  \n      # Compatibility (Corpus still builds norm_array).\n      k1: float = 0.9\n      b: float = 0.4\nwith:\n  class Config:\n      # Concave evidence + informative-coverage pressure.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Rare-constraint gate (bounded) for identifier/symbol-heavy queries (recall-safe).\n      rare_idf_pivot: float = 4.2\n      rare_boost: float = 0.18\n  \n      # NEW: bounded specificity/aboutness prior (positive-PMI density lift).\n      # Helps nDCG@10 by downranking docs that mention query terms only incidentally.\n      spec_beta: float = 0.10\n      spec_cap: float = 3.0\n      spec_len_floor: float = 25.0\n  \n      # NEW: tiny micro-token channel (char n-grams) to reduce tokenization brittleness.\n      micro_len: int = 3\n      micro_min_token_len: int = 2\n      micro_weight: float = 0.12\n  \n      # Compatibility (Corpus still builds norm_array).\n      k1: float = 0.9\n      b: float = 0.4\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + clarity gate + IDF-mass coverage,\n      plus a bounded rare-term \"key match\" gate.\n  \n      The rare gate targets domains where a single exact match on a very rare token\n      (identifier/symbol/name) is decisive. It is recall-safe because it only\n      multiplies scores for docs that already match the term at least once.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      rare_hits = 0.0\n  \n      for term in query_repr.terms:\n          df = float(corpus_df.get(term, 1.0))\n          tidf = float(idf(df, N))\n  ... (34 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + clarity gate + IDF-mass coverage,\n      plus two bounded priors that mostly affect early precision:\n  \n      (1) Rare-term key-match gate (recall-safe reordering among matching docs).\n      (2) Specificity/aboutness gain: reward cases where p(t|d)=tf/dl exceeds background p(t)=df/N\n          (positive PMI). Bounded & small => tends to improve nDCG@10 without killing recall@100.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      rare_hits = 0.0\n  \n      use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n      spec_sum = 0.0\n      spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n  ... (49 more lines)\nChange 3: Replace:\n  class Corpus:\n      def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n          self.documents = documents\n          self.ids = ids or [str(i) for i in range(len(documents))]\n          self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n          self.N = len(documents)\n          self.document_count = self.N\n          self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n          self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n          self.average_document_length = self.avgdl\n  \n          self._vocab: dict[str, int] = {}\n          for doc in documents:\n              for term in doc:\n                  if term not in self._vocab:\n                      self._vocab[term] = len(self._vocab)\n          self.vocab_size = len(self._vocab)\n  \n          tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n          self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n          self._df = np.zeros(self.vocab_size, dtype=np.float64)\n          self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n  \n          for doc_idx, doc in enumerate(documents):\n              term_counts = Counter(doc)\n              seen = set()\n              for term, count in term_counts.items():\n                  tid = self._vocab[term]\n                  tf_matrix_lil[tid, doc_idx] = count\n                  if tid not in seen:\n  ... (18 more lines)\nwith:\n  class Corpus:\n      def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n          self.documents = documents\n          self.ids = ids or [str(i) for i in range(len(documents))]\n          self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n          self.N = len(documents)\n          self.document_count = self.N\n          self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n          self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n          self.average_document_length = self.avgdl\n  \n          # Micro-token field: character n-grams prefixed with \"M:\" to keep disjoint vocab.\n          m = max(2, int(getattr(Config, \"micro_len\", 3)))\n          min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n          micro_docs: list[list[str]] = []\n          for doc in documents:\n              grams: list[str] = []\n              for t in doc:\n                  if len(t) < min_tok:\n                      continue\n                  if len(t) <= m:\n                      grams.append(\"M:\" + t)\n                  else:\n                      for i in range(0, len(t) - m + 1):\n                          grams.append(\"M:\" + t[i : i + m])\n              micro_docs.append(grams)\n          self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n  \n          # Joint vocabulary over base tokens and micro tokens.\n          self._vocab: dict[str, int] = {}\n  ... (52 more lines)\nChange 4: Replace:\n  def score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n      if not query:\n          return 0.0\n      q = QueryRepr.from_tokens(query)\n      if not q.terms:\n          return 0.0\n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      doc_length = float(corpus.doc_lengths[doc_idx])\n      return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\nwith:\n  def score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n      if not query:\n          return 0.0\n      q = QueryRepr.from_tokens(query)\n      if not q.terms:\n          return 0.0\n  \n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      doc_length = float(corpus.doc_lengths[doc_idx])\n  \n      s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n  \n      # Micro channel: char n-grams over query tokens (lexical, more forgiving for symbols/identifi...\n      if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n          m = max(2, int(Config.micro_len))\n          min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n          mtoks: list[str] = []\n          for t in query:\n              if len(t) < min_tok:\n                  continue\n              if len(t) <= m:\n                  mtoks.append(\"M:\" + t)\n              else:\n                  for i in range(0, len(t) - m + 1):\n                      mtoks.append(\"M:\" + t[i : i + m])\n          if mtoks:\n              mq = QueryRepr.from_tokens(mtoks)\n              s += float(Config.micro_weight) * retrieval_score(\n                  mq,\n  ... (7 more lines)\nChange 5: Replace:\n  def rank(\n          self,\n          query: list[str],\n          top_k: int | None = None,\n      ) -> tuple[np.ndarray, np.ndarray]:\n          if not query:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n          term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n          if not query_term_ids:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n          qtf = np.array(query_term_weights, dtype=np.float64)\n          candidate_set: set[int] = set()\n          for tid in query_term_ids:\n              candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)...\n          candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n          candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n          all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n          all_scores[candidate_docs] = candidate_scores\n          sorted_indices = np.argsort(-all_scores).astype(np.int64)\n          sorted_scores = all_scores[sorted_indices]\n          if top_k is not None:\n              sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n          return sorted_indices, sorted_scores\nwith:\n  def rank(\n          self,\n          query: list[str],\n          top_k: int | None = None,\n      ) -> tuple[np.ndarray, np.ndarray]:\n          if not query:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n  \n          # Combined query over base tokens + micro (char n-gram) tokens.\n          term_counts = Counter(query)\n          query_term_ids: list[int] = []\n          query_term_weights: list[float] = []\n  \n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n  \n          if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n              m = max(2, int(Config.micro_len))\n              min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n              grams: list[str] = []\n              for t in query:\n                  if len(t) < min_tok:\n                      continue\n                  if len(t) <= m:\n                      grams.append(\"M:\" + t)\n                  else:\n                      for i in range(0, len(t) - m + 1):\n  ... (27 more lines)\nChange 6: Replace:\n  sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = wq * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n              cov_num += wt * present\n              sum_evidence += wt * np.log1p(tf_row / (base + eps))\n  \n              if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                  rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n  \n          scores = np.log1p(np.maximum(sum_evidence, 0.0))\n  \n          if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n              scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n  \n  ... (6 more lines)\nwith:\n  sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n          spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n          spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n          dl_eff = np.maximum(\n              self.corpus.doc_lengths[candidate_docs],\n              float(getattr(Config, \"spec_len_floor\", 0.0)),\n          )\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = wq * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n              cov_num += wt * present\n              sum_evidence += wt * np.log1p(tf_row / (base + eps))\n  \n  ... (25 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3340, beir_nfcorpus_recall@100: 0.2612, beir_nfcorpus_index_time_ms: 6844.2048, beir_nfcorpus_query_time_ms: 594.0701, bright_pony_ndcg@10: 0.1342, bright_pony_recall@100: 0.3594, bright_pony_index_time_ms: 2336.4173, bright_pony_query_time_ms: 2267.7455, beir_scifact_ndcg@10: 0.6827, beir_scifact_recall@100: 0.9417, beir_scifact_index_time_ms: 9444.3728, beir_scifact_query_time_ms: 2309.2160, beir_arguana_ndcg@10: 0.2865, beir_arguana_recall@100: 0.9208, beir_arguana_index_time_ms: 11591.3896, beir_arguana_query_time_ms: 111711.7765, bright_theoremqa_theorems_ndcg@10: 0.0501, bright_theoremqa_theorems_recall@100: 0.1842, bright_theoremqa_theorems_index_time_ms: 16609.7548, bright_theoremqa_theorems_query_time_ms: 4720.3368, beir_scidocs_ndcg@10: 0.1530, beir_scidocs_recall@100: 0.3542, beir_scidocs_index_time_ms: 37338.8058, beir_scidocs_query_time_ms: 26830.2422, bright_economics_ndcg@10: 0.1271, bright_economics_recall@100: 0.3600, bright_economics_index_time_ms: 26739.9090, bright_economics_query_time_ms: 20773.2117, bright_biology_ndcg@10: 0.2487, bright_biology_recall@100: 0.5663, bright_biology_index_time_ms: 32235.8387, bright_biology_query_time_ms: 19263.6985, beir_fiqa_ndcg@10: 0.2405, beir_fiqa_recall@100: 0.5349, beir_fiqa_index_time_ms: 72211.6663, beir_fiqa_query_time_ms: 30118.0791, bright_earth_science_ndcg@10: 0.2733, bright_earth_science_recall@100: 0.6385, bright_earth_science_index_time_ms: 70259.4775, bright_earth_science_query_time_ms: 44627.6686, bright_stackoverflow_ndcg@10: 0.1810, bright_stackoverflow_recall@100: 0.4863, bright_stackoverflow_index_time_ms: 222348.7500, bright_stackoverflow_query_time_ms: 68213.9805, beir_trec-covid_ndcg@10: 0.7017, beir_trec-covid_recall@100: 0.1201, beir_trec-covid_index_time_ms: 384450.7404, beir_trec-covid_query_time_ms: 9410.1414, avg_ndcg@10: 0.2844, avg_recall@100: 0.4773, combined_score: 0.4387, total_index_time_ms: 892411.3270, total_query_time_ms: 340840.1670, total_time_ms: 1233251.4939, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n  \n      # NEW: bounded rare-term \"anchor\" (single best rare exact hit).\n      # Intuition: in symbol/identifier-heavy queries, one ultra-rare token match is decisive;\n      # using max(hinge) avoids overweighting verbose queries with many rare-ish terms.\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Compatibility leftovers.\n      k1: float = 0.9\n      b: float = 0.4\nwith:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # NEW: bigram channel (purely lexical proximity / phrase specificity).\n      # Bigrams often lift early precision on abstract-heavy corpora (SciDocs, FiQA, ArguAna)\n      # without needing positional indexes. Keep weight small to stay recall-safe.\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90  # suppress very common bigrams a bit more than unigrams\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Compatibility leftovers.\n      k1: float = 0.9\n      b: float = 0.4\nChange 2: Replace:\n  rarity = tidf / (tidf + 1.0)\n          clarity = rarity ** Config.q_clarity_power\nwith:\n  rarity = tidf / (tidf + 1.0)\n          # Tag-aware clarity: pseudo-terms (prefixes/bigrams) are easier to match and can be noisier,\n          # so we optionally suppress them a bit more than full tokens.\n          if term.startswith(\"B:\"):\n              clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_po...\n          else:\n              clarity = rarity ** Config.q_clarity_power\nChange 3: Replace:\n  # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n          # This is still lexical (no semantics) but increases robustness to:\n          # - morphology: \"oxidize\"/\"oxidation\"\n          # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n          pfx = max(1, int(Config.prefix_len))\n          docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n          self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n  \n          # Joint vocabulary over (token) U (prefixes), keeping them disjoint by tagging prefixes.\n          self._vocab: dict[str, int] = {}\n          for doc, pdoc in zip(documents, docs_prefix):\n              for term in doc:\n                  if term not in self._vocab:\n                      self._vocab[term] = len(self._vocab)\n              for p in pdoc:\n                  key = \"P:\" + p\n                  if key not in self._vocab:\n                      self._vocab[key] = len(self._vocab)\n  \n          self.vocab_size = len(self._vocab)\n  \n          tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n          self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n          self._df = np.zeros(self.vocab_size, dtype=np.float64)\n  \n          # Keep original doc TF dicts for score() path.\n          self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n  \n          for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n              term_counts = Counter(doc)\n  ... (18 more lines)\nwith:\n  # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n          # This is still lexical (no semantics) but increases robustness to:\n          # - morphology: \"oxidize\"/\"oxidation\"\n          # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n          pfx = max(1, int(Config.prefix_len))\n          docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n          self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n  \n          # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n          docs_bigram: list[list[str]] = []\n          for doc in documents:\n              if len(doc) < 2:\n                  docs_bigram.append([])\n              else:\n                  docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n          self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n  \n          # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n          self._vocab: dict[str, int] = {}\n          for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n              for term in doc:\n                  if term not in self._vocab:\n                      self._vocab[term] = len(self._vocab)\n              for p in pdoc:\n                  key = \"P:\" + p\n                  if key not in self._vocab:\n                      self._vocab[key] = len(self._vocab)\n              for bg in bdoc:\n                  key = \"B:\" + bg\n                  if key not in self._vocab:\n  ... (40 more lines)\nChange 4: Replace:\n  # Secondary prefix-channel score (helps when exact token match is brittle).\n      if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n          pfx = max(1, int(Config.prefix_len))\n          ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n          if ptoks:\n              pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n              pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n              s += Config.prefix_weight * retrieval_score(\n                  pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n              )\n      return s\nwith:\n  # Bigram-channel score (phrase/proximity specificity; purely lexical).\n      if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n          qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n          if qb:\n              bq = QueryRepr.from_tokens(qb)\n              bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n              s += float(Config.bigram_weight) * retrieval_score(\n                  bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n              )\n  \n      # Secondary prefix-channel score (helps when exact token match is brittle).\n      if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n          pfx = max(1, int(Config.prefix_len))\n          ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n          if ptoks:\n              pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n              pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n              s += Config.prefix_weight * retrieval_score(\n                  pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n              )\n      return s\nChange 5: Replace:\n  if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n              pfx = max(1, int(Config.prefix_len))\n              pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n              for p, c in pcounts.items():\n                  tid = self.corpus.get_term_id(\"P:\" + p)\n                  if tid is not None:\n                      query_term_ids.append(tid)\n                      query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n  \n          if not query_term_ids:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\nwith:\n  if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n              pfx = max(1, int(Config.prefix_len))\n              pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n              for p, c in pcounts.items():\n                  tid = self.corpus.get_term_id(\"P:\" + p)\n                  if tid is not None:\n                      query_term_ids.append(tid)\n                      query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n  \n          # Bigram pseudo-terms (phrase/proximity evidence).\n          if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n              bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n              for bg, c in bcounts.items():\n                  tid = self.corpus.get_term_id(\"B:\" + bg)\n                  if tid is not None:\n                      query_term_ids.append(tid)\n                      query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.q...\n  \n          if not query_term_ids:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n- Metrics: beir_nfcorpus_ndcg@10: 0.3351, beir_nfcorpus_recall@100: 0.2614, beir_nfcorpus_index_time_ms: 7017.3768, beir_nfcorpus_query_time_ms: 162.4492, bright_pony_ndcg@10: 0.1196, bright_pony_recall@100: 0.3033, bright_pony_index_time_ms: 2550.2317, bright_pony_query_time_ms: 852.4628, beir_scifact_ndcg@10: 0.6915, beir_scifact_recall@100: 0.9220, beir_scifact_index_time_ms: 9248.0411, beir_scifact_query_time_ms: 705.9385, beir_arguana_ndcg@10: 0.3090, beir_arguana_recall@100: 0.9393, beir_arguana_index_time_ms: 11194.1028, beir_arguana_query_time_ms: 58239.5352, bright_theoremqa_theorems_ndcg@10: 0.0274, bright_theoremqa_theorems_recall@100: 0.1798, bright_theoremqa_theorems_index_time_ms: 19644.0622, bright_theoremqa_theorems_query_time_ms: 1870.8683, beir_scidocs_ndcg@10: 0.1551, beir_scidocs_recall@100: 0.3589, beir_scidocs_index_time_ms: 40357.5348, beir_scidocs_query_time_ms: 7133.2668, bright_economics_ndcg@10: 0.1491, bright_economics_recall@100: 0.3884, bright_economics_index_time_ms: 28529.9027, bright_economics_query_time_ms: 7346.9787, bright_biology_ndcg@10: 0.2515, bright_biology_recall@100: 0.5256, bright_biology_index_time_ms: 30897.5011, bright_biology_query_time_ms: 6956.3096, bright_earth_science_ndcg@10: 0.3096, bright_earth_science_recall@100: 0.6573, bright_earth_science_index_time_ms: 67645.2750, bright_earth_science_query_time_ms: 9667.5022, beir_fiqa_ndcg@10: 0.2440, beir_fiqa_recall@100: 0.5316, beir_fiqa_index_time_ms: 74600.4033, beir_fiqa_query_time_ms: 11378.9218, bright_stackoverflow_ndcg@10: 0.2006, bright_stackoverflow_recall@100: 0.5172, bright_stackoverflow_index_time_ms: 175556.7606, bright_stackoverflow_query_time_ms: 24694.1712, beir_trec-covid_ndcg@10: 0.7122, beir_trec-covid_recall@100: 0.1202, beir_trec-covid_index_time_ms: 312871.6456, beir_trec-covid_query_time_ms: 3171.2123, avg_ndcg@10: 0.2921, avg_recall@100: 0.4754, combined_score: 0.4387, total_index_time_ms: 780112.8377, total_query_time_ms: 132179.6166, total_time_ms: 912292.4544, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # NEW: bigram channel (purely lexical proximity / phrase specificity).\n      # Bigrams often lift early precision on abstract-heavy corpora (SciDocs, FiQA, ArguAna)\n      # without needing positional indexes. Keep weight small to stay recall-safe.\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90  # suppress very common bigrams a bit more than unigrams\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # Compatibility leftovers.\n      k1: float = 0.9\n      b: float = 0.4\nwith:\n  class Config:\n      # Core: concave evidence + informative coverage.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Prefix channel (morphology/symbol robustness)\n      prefix_len: int = 5\n      prefix_weight: float = 0.18\n  \n      # Bigram channel (phrase specificity)\n      bigram_weight: float = 0.10\n      bigram_clarity_power: float = 0.90\n  \n      # Calibrated coordination boost.\n      coord_gamma: float = 0.22\n      coord_mass_tau: float = 2.5\n  \n      # Bounded rare-term \"anchor\" (single best rare exact hit).\n      rare_idf_pivot: float = 4.2\n      anchor_boost: float = 0.14\n  \n      # NEW: background-corrected aboutness (position-free \"is this doc *about* the query?\" prior).\n      # For each matched term, compare within-doc rate tf/dl to collection rate df/N and take\n      # positive PMI, capped. Small bounded multiplier => tends to lift nDCG@10 with minimal recall...\n      about_beta: float = 0.08\n      about_cap: float = 2.5\n  ... (5 more lines)\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n      plus a bounded *rare-term anchor* gate.\n  \n      Rare-term anchor:\n        Use the single strongest rarity-above-pivot match as a tiny multiplier.\n        This helps technical/theorem/code corpora where one exact rare identifier match\n        is often decisive, while remaining recall-safe (only changes scores for matching docs).\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      matched = 0.0\n      uq = 0.0\n  \n      anchor = 0.0\n  ... (53 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n      plus (i) a bounded rare-term anchor and (ii) a bounded aboutness prior.\n  \n      Aboutness prior:\n        For each matched term, compare within-doc rate tf/dl_eff to collection rate df/N.\n        Add positive PMI (capped) weighted by the same informative wt, then apply a small\n        multiplier. Intuition: downrank incidental mentions; reward docs where query terms\n        are unusually dense => improves early precision (nDCG@10) in long corpora.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      matched = 0.0\n      uq = 0.0\n  \n  ... (67 more lines)\nChange 3: Replace:\n  for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n              wt = float(q_w[i]) * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n  \n              matched += present\n              cov_num += wt * present\n              sum_evidence += wt * np.log1p(tf_row / (base + eps))\n  \n              if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                  hinge = (idf_val - pivot) / (idf_val + eps)\n                  anchor = np.maximum(anchor, present * hinge)\n  \n          scores = np.log1p(np.maximum(sum_evidence, 0.0))\n  \n          if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n              scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n  \n          if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n              q_mass = np.log1p(max(cov_den, 0.0))\n              cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n              scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n  ... (7 more lines)\nwith:\n  use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n          about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n          about_cap = float(getattr(Config, \"about_cap\", 2.5))\n          dl_eff = np.maximum(\n              self.corpus.doc_lengths[candidate_docs],\n              float(getattr(Config, \"about_len_floor\", 0.0)),\n          )\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n              wt = float(q_w[i]) * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n  \n              matched += present\n              cov_num += wt * present\n              sum_evidence += wt * np.log1p(tf_row / (base + eps))\n  \n              if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                  hinge = (idf_val - pivot) / (idf_val + eps)\n                  anchor = np.maximum(anchor, present * hinge)\n  \n              if use_about:\n  ... (26 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3338, beir_nfcorpus_recall@100: 0.2613, beir_nfcorpus_index_time_ms: 7223.7757, beir_nfcorpus_query_time_ms: 191.6053, bright_pony_ndcg@10: 0.1123, bright_pony_recall@100: 0.2993, bright_pony_index_time_ms: 2365.1067, bright_pony_query_time_ms: 1141.9732, beir_scifact_ndcg@10: 0.6949, beir_scifact_recall@100: 0.9220, beir_scifact_index_time_ms: 9641.5895, beir_scifact_query_time_ms: 875.7637, beir_arguana_ndcg@10: 0.3127, beir_arguana_recall@100: 0.9408, beir_arguana_index_time_ms: 11402.8388, beir_arguana_query_time_ms: 75013.6941, bright_theoremqa_theorems_ndcg@10: 0.0274, bright_theoremqa_theorems_recall@100: 0.1798, bright_theoremqa_theorems_index_time_ms: 17489.0313, bright_theoremqa_theorems_query_time_ms: 2096.9761, beir_scidocs_ndcg@10: 0.1581, beir_scidocs_recall@100: 0.3616, beir_scidocs_index_time_ms: 35144.7745, beir_scidocs_query_time_ms: 7980.1447, bright_economics_ndcg@10: 0.1506, bright_economics_recall@100: 0.3977, bright_economics_index_time_ms: 24352.1843, bright_economics_query_time_ms: 7668.6563, bright_biology_ndcg@10: 0.2464, bright_biology_recall@100: 0.5272, bright_biology_index_time_ms: 25810.7817, bright_biology_query_time_ms: 7142.2859, bright_earth_science_ndcg@10: 0.3156, bright_earth_science_recall@100: 0.6573, bright_earth_science_index_time_ms: 54695.8977, bright_earth_science_query_time_ms: 11686.6149, beir_fiqa_ndcg@10: 0.2480, beir_fiqa_recall@100: 0.5431, beir_fiqa_index_time_ms: 63798.5688, beir_fiqa_query_time_ms: 12311.1400, bright_stackoverflow_ndcg@10: 0.2067, bright_stackoverflow_recall@100: 0.5184, bright_stackoverflow_index_time_ms: 154150.2884, bright_stackoverflow_query_time_ms: 30524.1904, beir_trec-covid_ndcg@10: 0.7191, beir_trec-covid_recall@100: 0.1219, beir_trec-covid_index_time_ms: 270463.5123, beir_trec-covid_query_time_ms: 3354.4654, avg_ndcg@10: 0.2938, avg_recall@100: 0.4775, combined_score: 0.4408, total_index_time_ms: 676538.3497, total_query_time_ms: 159987.5100, total_time_ms: 836525.8597, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4408)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (phrase specificity)\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Background-corrected aboutness (positive PMI density).\n    # For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N; add only positive PMI.\n    # This is robust across corpora and tends to improve early precision without killing recall.\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor gate, plus a bounded aboutness prior.\n\n    Aboutness prior (positive PMI density, position-free):\n      For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N.\n      Add only positive PMI (capped) weighted by the same informative wt, then apply\n      a small multiplier. Downranks incidental mentions in long docs => better nDCG@10.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3338), Performs well on beir_nfcorpus_recall@100 (0.2613), Performs well on beir_nfcorpus_index_time_ms (6394.7227), Performs well on beir_nfcorpus_query_time_ms (180.1536), Performs well on bright_pony_ndcg@10 (0.1123), Performs well on bright_pony_recall@100 (0.2993), Performs well on bright_pony_index_time_ms (2297.7155), Performs well on bright_pony_query_time_ms (1080.5654), Performs well on beir_scifact_ndcg@10 (0.6949), Performs well on beir_scifact_recall@100 (0.9220), Performs well on beir_scifact_index_time_ms (8794.4366), Performs well on beir_scifact_query_time_ms (851.3868), Performs well on beir_arguana_ndcg@10 (0.3127), Performs well on beir_arguana_recall@100 (0.9408), Performs well on beir_arguana_index_time_ms (10467.3868), Performs well on beir_arguana_query_time_ms (73900.8473), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0274), Performs well on bright_theoremqa_theorems_recall@100 (0.1798), Performs well on bright_theoremqa_theorems_index_time_ms (17562.4221), Performs well on bright_theoremqa_theorems_query_time_ms (2087.4982), Performs well on beir_scidocs_ndcg@10 (0.1581), Performs well on beir_scidocs_recall@100 (0.3616), Performs well on beir_scidocs_index_time_ms (34870.5552), Performs well on beir_scidocs_query_time_ms (7955.6792), Performs well on bright_economics_ndcg@10 (0.1506), Performs well on bright_economics_recall@100 (0.3977), Performs well on bright_economics_index_time_ms (23760.9130), Performs well on bright_economics_query_time_ms (7574.1887), Performs well on bright_biology_ndcg@10 (0.2464), Performs well on bright_biology_recall@100 (0.5272), Performs well on bright_biology_index_time_ms (25136.3777), Performs well on bright_biology_query_time_ms (6959.2115), Performs well on bright_earth_science_ndcg@10 (0.3156), Performs well on bright_earth_science_recall@100 (0.6573), Performs well on bright_earth_science_index_time_ms (54014.4483), Performs well on bright_earth_science_query_time_ms (11291.4577), Performs well on beir_fiqa_ndcg@10 (0.2480), Performs well on beir_fiqa_recall@100 (0.5431), Performs well on beir_fiqa_index_time_ms (61572.3177), Performs well on beir_fiqa_query_time_ms (12212.2489), Performs well on bright_stackoverflow_ndcg@10 (0.2067), Performs well on bright_stackoverflow_recall@100 (0.5184), Performs well on bright_stackoverflow_index_time_ms (149864.2993), Performs well on bright_stackoverflow_query_time_ms (29356.0789), Performs well on beir_trec-covid_ndcg@10 (0.7191), Performs well on beir_trec-covid_recall@100 (0.1219), Performs well on beir_trec-covid_index_time_ms (267518.8933), Performs well on beir_trec-covid_query_time_ms (3274.9951), Performs well on avg_ndcg@10 (0.2938), Performs well on avg_recall@100 (0.4775), Performs well on combined_score (0.4408), Performs well on total_index_time_ms (662254.4882), Performs well on total_query_time_ms (156724.3112), Performs well on total_time_ms (818978.7995), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4408)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (phrase specificity)\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # NEW: background-corrected aboutness (position-free \"is this doc *about* the query?\" prior).\n    # For each matched term, compare within-doc rate tf/dl to collection rate df/N and take\n    # positive PMI, capped. Small bounded multiplier => tends to lift nDCG@10 with minimal recall loss.\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus (i) a bounded rare-term anchor and (ii) a bounded aboutness prior.\n\n    Aboutness prior:\n      For each matched term, compare within-doc rate tf/dl_eff to collection rate df/N.\n      Add positive PMI (capped) weighted by the same informative wt, then apply a small\n      multiplier. Intuition: downrank incidental mentions; reward docs where query terms\n      are unusually dense => improves early precision (nDCG@10) in long corpora.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3338), Performs well on beir_nfcorpus_recall@100 (0.2613), Performs well on beir_nfcorpus_index_time_ms (7223.7757), Performs well on beir_nfcorpus_query_time_ms (191.6053), Performs well on bright_pony_ndcg@10 (0.1123), Performs well on bright_pony_recall@100 (0.2993), Performs well on bright_pony_index_time_ms (2365.1067), Performs well on bright_pony_query_time_ms (1141.9732), Performs well on beir_scifact_ndcg@10 (0.6949), Performs well on beir_scifact_recall@100 (0.9220), Performs well on beir_scifact_index_time_ms (9641.5895), Performs well on beir_scifact_query_time_ms (875.7637), Performs well on beir_arguana_ndcg@10 (0.3127), Performs well on beir_arguana_recall@100 (0.9408), Performs well on beir_arguana_index_time_ms (11402.8388), Performs well on beir_arguana_query_time_ms (75013.6941), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0274), Performs well on bright_theoremqa_theorems_recall@100 (0.1798), Performs well on bright_theoremqa_theorems_index_time_ms (17489.0313), Performs well on bright_theoremqa_theorems_query_time_ms (2096.9761), Performs well on beir_scidocs_ndcg@10 (0.1581), Performs well on beir_scidocs_recall@100 (0.3616), Performs well on beir_scidocs_index_time_ms (35144.7745), Performs well on beir_scidocs_query_time_ms (7980.1447), Performs well on bright_economics_ndcg@10 (0.1506), Performs well on bright_economics_recall@100 (0.3977), Performs well on bright_economics_index_time_ms (24352.1843), Performs well on bright_economics_query_time_ms (7668.6563), Performs well on bright_biology_ndcg@10 (0.2464), Performs well on bright_biology_recall@100 (0.5272), Performs well on bright_biology_index_time_ms (25810.7817), Performs well on bright_biology_query_time_ms (7142.2859), Performs well on bright_earth_science_ndcg@10 (0.3156), Performs well on bright_earth_science_recall@100 (0.6573), Performs well on bright_earth_science_index_time_ms (54695.8977), Performs well on bright_earth_science_query_time_ms (11686.6149), Performs well on beir_fiqa_ndcg@10 (0.2480), Performs well on beir_fiqa_recall@100 (0.5431), Performs well on beir_fiqa_index_time_ms (63798.5688), Performs well on beir_fiqa_query_time_ms (12311.1400), Performs well on bright_stackoverflow_ndcg@10 (0.2067), Performs well on bright_stackoverflow_recall@100 (0.5184), Performs well on bright_stackoverflow_index_time_ms (154150.2884), Performs well on bright_stackoverflow_query_time_ms (30524.1904), Performs well on beir_trec-covid_ndcg@10 (0.7191), Performs well on beir_trec-covid_recall@100 (0.1219), Performs well on beir_trec-covid_index_time_ms (270463.5123), Performs well on beir_trec-covid_query_time_ms (3354.4654), Performs well on avg_ndcg@10 (0.2938), Performs well on avg_recall@100 (0.4775), Performs well on combined_score (0.4408), Performs well on total_index_time_ms (676538.3497), Performs well on total_query_time_ms (159987.5100), Performs well on total_time_ms (836525.8597), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4387)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: bigram channel (purely lexical proximity / phrase specificity).\n    # Bigrams often lift early precision on abstract-heavy corpora (SciDocs, FiQA, ArguAna)\n    # without needing positional indexes. Keep weight small to stay recall-safe.\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90  # suppress very common bigrams a bit more than unigrams\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded *rare-term anchor* gate.\n\n    Rare-term anchor:\n      Use the single strongest rarity-above-pivot match as a tiny multiplier.\n      This helps technical/theorem/code corpora where one exact rare identifier match\n      is often decisive, while remaining recall-safe (only changes scores for matching docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        # Tag-aware clarity: pseudo-terms (prefixes/bigrams) are easier to match and can be noisier,\n        # so we optionally suppress them a bit more than full tokens.\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)  # in (0,1)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3351), Performs well on beir_nfcorpus_recall@100 (0.2614), Performs well on beir_nfcorpus_index_time_ms (7017.3768), Performs well on beir_nfcorpus_query_time_ms (162.4492), Performs well on bright_pony_ndcg@10 (0.1196), Performs well on bright_pony_recall@100 (0.3033), Performs well on bright_pony_index_time_ms (2550.2317), Performs well on bright_pony_query_time_ms (852.4628), Performs well on beir_scifact_ndcg@10 (0.6915), Performs well on beir_scifact_recall@100 (0.9220), Performs well on beir_scifact_index_time_ms (9248.0411), Performs well on beir_scifact_query_time_ms (705.9385), Performs well on beir_arguana_ndcg@10 (0.3090), Performs well on beir_arguana_recall@100 (0.9393), Performs well on beir_arguana_index_time_ms (11194.1028), Performs well on beir_arguana_query_time_ms (58239.5352), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0274), Performs well on bright_theoremqa_theorems_recall@100 (0.1798), Performs well on bright_theoremqa_theorems_index_time_ms (19644.0622), Performs well on bright_theoremqa_theorems_query_time_ms (1870.8683), Performs well on beir_scidocs_ndcg@10 (0.1551), Performs well on beir_scidocs_recall@100 (0.3589), Performs well on beir_scidocs_index_time_ms (40357.5348), Performs well on beir_scidocs_query_time_ms (7133.2668), Performs well on bright_economics_ndcg@10 (0.1491), Performs well on bright_economics_recall@100 (0.3884), Performs well on bright_economics_index_time_ms (28529.9027), Performs well on bright_economics_query_time_ms (7346.9787), Performs well on bright_biology_ndcg@10 (0.2515), Performs well on bright_biology_recall@100 (0.5256), Performs well on bright_biology_index_time_ms (30897.5011), Performs well on bright_biology_query_time_ms (6956.3096), Performs well on bright_earth_science_ndcg@10 (0.3096), Performs well on bright_earth_science_recall@100 (0.6573), Performs well on bright_earth_science_index_time_ms (67645.2750), Performs well on bright_earth_science_query_time_ms (9667.5022), Performs well on beir_fiqa_ndcg@10 (0.2440), Performs well on beir_fiqa_recall@100 (0.5316), Performs well on beir_fiqa_index_time_ms (74600.4033), Performs well on beir_fiqa_query_time_ms (11378.9218), Performs well on bright_stackoverflow_ndcg@10 (0.2006), Performs well on bright_stackoverflow_recall@100 (0.5172), Performs well on bright_stackoverflow_index_time_ms (175556.7606), Performs well on bright_stackoverflow_query_time_ms (24694.1712), Performs well on beir_trec-covid_ndcg@10 (0.7122), Performs well on beir_trec-covid_recall@100 (0.1202), Performs well on beir_trec-covid_index_time_ms (312871.6456), Performs well on beir_trec-covid_query_time_ms (3171.2123), Performs well on avg_ndcg@10 (0.2921), Performs well on avg_recall@100 (0.4754), Performs well on combined_score (0.4387), Performs well on total_index_time_ms (780112.8377), Performs well on total_query_time_ms (132179.6166), Performs well on total_time_ms (912292.4544), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4387)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + informative-coverage pressure.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Rare-constraint gate (bounded) for identifier/symbol-heavy queries (recall-safe).\n    rare_idf_pivot: float = 4.2\n    rare_boost: float = 0.18\n\n    # NEW: bounded specificity/aboutness prior (positive-PMI density lift).\n    # Helps nDCG@10 by downranking docs that mention query terms only incidentally.\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    # NEW: tiny micro-token channel (char n-grams) to reduce tokenization brittleness.\n    micro_len: int = 3\n    micro_min_token_len: int = 2\n    micro_weight: float = 0.12\n\n    # Compatibility (Corpus still builds norm_array).\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + clarity gate + IDF-mass coverage,\n    plus two bounded priors that mostly affect early precision:\n\n    (1) Rare-term key-match gate (recall-safe reordering among matching docs).\n    (2) Specificity/aboutness gain: reward cases where p(t|d)=tf/dl exceeds background p(t)=df/N\n        (positive PMI). Bounded & small => tends to improve nDCG@10 without killing recall@100.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n        if use_spec:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Micro channel: char n-grams over query tokens (lexical, more forgiving for symbols/identifiers).\n    if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n        m = max(2, int(Config.micro_len))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        mtoks: list[str] = []\n        for t in query:\n            if len(t) < min_tok:\n                continue\n            if len(t) <= m:\n                mtoks.append(\"M:\" + t)\n            else:\n                for i in range(0, len(t) - m + 1):\n                    mtoks.append(\"M:\" + t[i : i + m])\n        if mtoks:\n            mq = QueryRepr.from_tokens(mtoks)\n            s += float(Config.micro_weight) * retrieval_score(\n                mq,\n                corpus.micro_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Micro-token field: character n-grams prefixed with \"M:\" to keep disjoint vocab.\n        m = max(2, int(getattr(Config, \"micro_len\", 3)))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        micro_docs: list[list[str]] = []\n        for doc in documents:\n            grams: list[str] = []\n            for t in doc:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            micro_docs.append(grams)\n        self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n\n        # Joint vocabulary over base tokens and micro tokens.\n        self._vocab: dict[str, int] = {}\n        for doc, mdoc in zip(documents, micro_docs):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for g in mdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, mdoc) in enumerate(zip(documents, micro_docs)):\n            term_counts = Counter(doc)\n            micro_counts = Counter(mdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in micro_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (micro keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query over base tokens + micro (char n-gram) tokens.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n            m = max(2, int(Config.micro_len))\n            min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n            grams: list[str] = []\n            for t in query:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            if grams:\n                gcounts = Counter(grams)\n                for g, c in gcounts.items():\n                    tid = self.corpus.get_term_id(g)\n                    if tid is not None:\n                        query_term_ids.append(tid)\n                        query_term_weights.append(float(Config.micro_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3340), Performs well on beir_nfcorpus_recall@100 (0.2612), Performs well on beir_nfcorpus_index_time_ms (6844.2048), Performs well on beir_nfcorpus_query_time_ms (594.0701), Performs well on bright_pony_ndcg@10 (0.1342), Performs well on bright_pony_recall@100 (0.3594), Performs well on bright_pony_index_time_ms (2336.4173), Performs well on bright_pony_query_time_ms (2267.7455), Performs well on beir_scifact_ndcg@10 (0.6827), Performs well on beir_scifact_recall@100 (0.9417), Performs well on beir_scifact_index_time_ms (9444.3728), Performs well on beir_scifact_query_time_ms (2309.2160), Performs well on beir_arguana_ndcg@10 (0.2865), Performs well on beir_arguana_recall@100 (0.9208), Performs well on beir_arguana_index_time_ms (11591.3896), Performs well on beir_arguana_query_time_ms (111711.7765), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0501), Performs well on bright_theoremqa_theorems_recall@100 (0.1842), Performs well on bright_theoremqa_theorems_index_time_ms (16609.7548), Performs well on bright_theoremqa_theorems_query_time_ms (4720.3368), Performs well on beir_scidocs_ndcg@10 (0.1530), Performs well on beir_scidocs_recall@100 (0.3542), Performs well on beir_scidocs_index_time_ms (37338.8058), Performs well on beir_scidocs_query_time_ms (26830.2422), Performs well on bright_economics_ndcg@10 (0.1271), Performs well on bright_economics_recall@100 (0.3600), Performs well on bright_economics_index_time_ms (26739.9090), Performs well on bright_economics_query_time_ms (20773.2117), Performs well on bright_biology_ndcg@10 (0.2487), Performs well on bright_biology_recall@100 (0.5663), Performs well on bright_biology_index_time_ms (32235.8387), Performs well on bright_biology_query_time_ms (19263.6985), Performs well on beir_fiqa_ndcg@10 (0.2405), Performs well on beir_fiqa_recall@100 (0.5349), Performs well on beir_fiqa_index_time_ms (72211.6663), Performs well on beir_fiqa_query_time_ms (30118.0791), Performs well on bright_earth_science_ndcg@10 (0.2733), Performs well on bright_earth_science_recall@100 (0.6385), Performs well on bright_earth_science_index_time_ms (70259.4775), Performs well on bright_earth_science_query_time_ms (44627.6686), Performs well on bright_stackoverflow_ndcg@10 (0.1810), Performs well on bright_stackoverflow_recall@100 (0.4863), Performs well on bright_stackoverflow_index_time_ms (222348.7500), Performs well on bright_stackoverflow_query_time_ms (68213.9805), Performs well on beir_trec-covid_ndcg@10 (0.7017), Performs well on beir_trec-covid_recall@100 (0.1201), Performs well on beir_trec-covid_index_time_ms (384450.7404), Performs well on beir_trec-covid_query_time_ms (9410.1414), Performs well on avg_ndcg@10 (0.2844), Performs well on avg_recall@100 (0.4773), Performs well on combined_score (0.4387), Performs well on total_index_time_ms (892411.3270), Performs well on total_query_time_ms (340840.1670), Performs well on total_time_ms (1233251.4939), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4382)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # NEW: entropy-normalized TF (\"burstiness\") prior.\n    # Idea: treat the query-term counts within a doc as a distribution p(t|d,q).\n    # Lower entropy => evidence concentrated on fewer query terms => more \"about\" the query.\n    # This is a gentle multiplier, intended to improve nDCG@10 without killing recall.\n    entropy_alpha: float = 0.10\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor, plus an entropy-normalized burstiness prior.\n\n    Entropy prior (query-term TF entropy within doc):\n      Let p_t \u221d (wt * tf_t) over matched query terms. Compute normalized entropy H/log(K).\n      Use concentration = 1 - H/log(K) in [0,1]. Concentrated evidence often correlates with\n      topicality (better early precision) while staying recall-safe (only affects matching docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    # For entropy prior: accumulate per-term matched mass m_t = wt * tf (only if tf>0).\n    m_sum = 0.0\n    m_log_sum = 0.0  # sum(m_t * log(m_t))\n    k_match = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if getattr(Config, \"entropy_alpha\", 0.0) != 0.0:\n            mt = wt * tf\n            if mt > 0.0:\n                k_match += 1.0\n                m_sum += mt\n                m_log_sum += mt * math.log(mt + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if getattr(Config, \"entropy_alpha\", 0.0) != 0.0 and k_match >= 2.0 and m_sum > 0.0:\n        # H = -sum p log p = log(m_sum) - (sum m log m)/m_sum\n        H = math.log(m_sum + eps) - (m_log_sum / (m_sum + eps))\n        Hn = H / (math.log(k_match + eps) + eps)  # normalized to ~[0,1]\n        conc = max(0.0, min(1.0, 1.0 - Hn))\n        score *= 1.0 + float(Config.entropy_alpha) * conc\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over (token) U (prefixes), keeping them disjoint by tagging prefixes.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        # Entropy prior accumulators over matched query terms.\n        m_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        m_log_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        k_match = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_ent = getattr(Config, \"entropy_alpha\", 0.0) != 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_ent:\n                mt = wt * tf_row\n                mt_pos = np.maximum(mt, 0.0)\n                k_match += (mt_pos > 0.0).astype(np.float64)\n                m_sum += mt_pos\n                m_log_sum += mt_pos * np.log(mt_pos + eps)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_ent:\n            # H = log(m_sum) - (m_log_sum/m_sum), normalize by log(k_match).\n            mask = (k_match >= 2.0) & (m_sum > 0.0)\n            if np.any(mask):\n                H = np.log(m_sum + eps) - (m_log_sum / (m_sum + eps))\n                Hn = H / (np.log(k_match + eps) + eps)\n                conc = np.clip(1.0 - Hn, 0.0, 1.0)\n                scores *= 1.0 + float(Config.entropy_alpha) * (conc * mask.astype(np.float64))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core signals: concave evidence + informative coverage + gentle verbosity prior.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    qtf_power: float = 0.5\n    dl_alpha: float = 0.15\n\n    # Micro channel (character n-grams) for robustness to tokenization / symbols.\n    micro_len: int = 3\n    micro_min_token_len: int = 2\n    micro_weight: float = 0.12\n\n    # Specificity / aboutness prior (bounded, position-free).\n    spec_beta: float = 0.08\n    spec_cap: float = 2.5\n    spec_len_floor: float = 35.0\n\n    # Calibrated coordination (soft-AND) over matched constraints.\n    coord_gamma: float = 0.20\n    coord_mass_tau: float = 2.2\n\n    # Rare-term anchor (single strongest rare exact hit), recall-safe (reorders matching docs).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.12\n\n    # NEW: residual-IDF synergy for query term pairs (weak lexical \"cohesion\" signal).\n    # Intuition (PRF-free): if two query constraints co-occur in far fewer documents than expected\n    # under independence, their joint presence is highly diagnostic. This can lift nDCG@10 by\n    # rewarding docs that satisfy \"meaningful\" combinations, while staying recall-safe because\n    # it only applies to already-matching docs.\n    pair_beta: float = 0.06\n    pair_cap: float = 1.8\n    pair_max_terms: int = 8  # keep O(|q|^2) small; focus on top informative constraints\n\n    # Compatibility leftovers\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + informative coverage + calibrated coordination,\n    plus (optional) bounded specificity and a rare-term anchor.\n\n    NEW: residual-IDF *pair synergy* (query cohesion):\n      For a small set of informative query terms, reward documents that satisfy pairs\n      whose co-occurrence is rarer than expected under independence. Approximate\n      pair-IDF as min(idf_i, idf_j), then subtract expected single-term evidence.\n      This adds a weak \"AND beyond coverage\" signal without positions/phrases.\n\n      Why this can help: many corpora have documents that match several query terms\n      but in unrelated contexts. A doc matching a *rare combination* is more likely\n      to be truly on-topic => boosts early precision (nDCG@10) while being recall-safe.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    matched = 0.0\n    uq = 0.0\n\n    # For pair synergy we need per-term weights and match flags.\n    per_term_wt: dict[str, float] = {}\n    matched_terms: list[str] = []\n\n    # Specificity/aboutness accumulators.\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    # Rare anchor (single strongest).\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        per_term_wt[term] = wt\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched_terms.append(term)\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and term_idf > pivot:\n            hinge = (term_idf - pivot) / (term_idf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_spec:\n            # Positive PMI density, capped.\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    # Calibrated coordination boost.\n    if uq > 0.0 and float(getattr(Config, \"coord_gamma\", 0.0)) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (float(getattr(Config, \"coord_mass_tau\", 2.2)) + eps))\n        score *= 1.0 + (float(Config.coord_gamma) * cal) * (matched / (uq + eps))\n\n    # Pair synergy (bounded multiplier).\n    pair_beta = float(getattr(Config, \"pair_beta\", 0.0))\n    if pair_beta != 0.0 and len(matched_terms) >= 2:\n        # Focus on most informative constraints in this query (by wt).\n        m = int(getattr(Config, \"pair_max_terms\", 8))\n        m = max(2, m)\n        # Sort matched terms by wt descending and keep top-m.\n        mt_sorted = sorted(matched_terms, key=lambda t: per_term_wt.get(t, 0.0), reverse=True)[:m]\n\n        # Residual-idf co-occurrence: idf_pair \u2248 min(idf_i,idf_j).\n        # residual = max(0, idf_pair - 0.5*(idf_i+idf_j)) in [0, +]\n        # Use wt magnitudes for scaling but cap the overall gain.\n        pair_cap = float(getattr(Config, \"pair_cap\", 1.8))\n        pair_sum = 0.0\n        # Precompute term idfs to avoid recomputing idf(df,N) repeatedly.\n        term_idf_cache: dict[str, float] = {}\n        for t in mt_sorted:\n            df = float(corpus_df.get(t, 1.0))\n            term_idf_cache[t] = float(idf(df, N))\n\n        for i in range(len(mt_sorted)):\n            ti = mt_sorted[i]\n            idfi = term_idf_cache.get(ti, 0.0)\n            wti = per_term_wt.get(ti, 0.0)\n            for j in range(i + 1, len(mt_sorted)):\n                tj = mt_sorted[j]\n                idfj = term_idf_cache.get(tj, 0.0)\n                wtj = per_term_wt.get(tj, 0.0)\n\n                idf_pair = min(idfi, idfj)\n                expected = 0.5 * (idfi + idfj)\n                resid = idf_pair - expected\n                if resid > 0.0:\n                    pair_sum += resid * math.sqrt(max(wti, 0.0) * max(wtj, 0.0))\n\n        if pair_sum > 0.0 and cov_den > 0.0:\n            # Normalize by query mass so effect is stable across query lengths.\n            g = min(pair_sum / (cov_den + eps), pair_cap)\n            score *= 1.0 + pair_beta * g\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Micro channel: character n-grams over query tokens (lexical, but more forgiving).\n    if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n        m = max(2, int(Config.micro_len))\n        mtoks: list[str] = []\n        for t in query:\n            if len(t) < max(1, int(Config.micro_min_token_len)):\n                continue\n            if len(t) <= m:\n                mtoks.append(\"M:\" + t)\n            else:\n                for i in range(0, len(t) - m + 1):\n                    mtoks.append(\"M:\" + t[i : i + m])\n        if mtoks:\n            mq = QueryRepr.from_tokens(mtoks)\n            s += float(Config.micro_weight) * retrieval_score(\n                mq,\n                corpus.micro_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build micro-token view (character n-grams) as a separate lexical field.\n        m = max(2, int(getattr(Config, \"micro_len\", 3)))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        micro_docs: list[list[str]] = []\n        for doc in documents:\n            grams: list[str] = []\n            for t in doc:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            micro_docs.append(grams)\n        self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n\n        # Joint vocabulary over base tokens and micro tokens (kept disjoint via \"M:\" prefix).\n        self._vocab: dict[str, int] = {}\n        for doc, mdoc in zip(documents, micro_docs):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for g in mdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, mdoc) in enumerate(zip(documents, micro_docs)):\n            term_counts = Counter(doc)\n            micro_counts = Counter(mdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in micro_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # Expose df for both channels (micro keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Rare anchor (single strongest).\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        uq = float(len(query_term_ids))\n\n        # For pair synergy: keep per-term wt and presence vectors (top-m only).\n        pair_beta = float(getattr(Config, \"pair_beta\", 0.0))\n        m = int(getattr(Config, \"pair_max_terms\", 8))\n        m = max(2, m)\n        top_term_ids: list[int] = []\n        top_wt_scalars: list[float] = []\n        top_present: list[np.ndarray] = []\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n            if pair_beta != 0.0:\n                # Maintain a small top-m list by wt.\n                if len(top_term_ids) < m:\n                    top_term_ids.append(term_id)\n                    top_wt_scalars.append(float(wt))\n                    top_present.append(present)\n                else:\n                    jmin = int(np.argmin(np.asarray(top_wt_scalars)))\n                    if wt > top_wt_scalars[jmin]:\n                        top_term_ids[jmin] = term_id\n                        top_wt_scalars[jmin] = float(wt)\n                        top_present[jmin] = present\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and float(getattr(Config, \"coord_gamma\", 0.0)) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (float(getattr(Config, \"coord_mass_tau\", 2.2)) + eps))\n            scores *= 1.0 + (float(Config.coord_gamma) * cal) * (matched / (uq + eps))\n\n        # Pair synergy (vectorized): use co-presence present_i*present_j and residual-idf heuristic.\n        if pair_beta != 0.0 and len(top_term_ids) >= 2 and cov_den > 0.0:\n            pair_cap = float(getattr(Config, \"pair_cap\", 1.8))\n            pair_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n\n            idf_vals = np.array([float(self.corpus.idf_array[tid]) for tid in top_term_ids], dtype=np.float64)\n            wt_vals = np.array(top_wt_scalars, dtype=np.float64)\n\n            for i in range(len(top_term_ids)):\n                pi = top_present[i]\n                idfi = float(idf_vals[i])\n                wti = float(wt_vals[i])\n                for j in range(i + 1, len(top_term_ids)):\n                    pj = top_present[j]\n                    idfj = float(idf_vals[j])\n                    wtj = float(wt_vals[j])\n\n                    idf_pair = idfi if idfi < idfj else idfj\n                    expected = 0.5 * (idfi + idfj)\n                    resid = idf_pair - expected\n                    if resid <= 0.0:\n                        continue\n                    co = pi * pj\n                    pair_sum += resid * math.sqrt(max(wti, 0.0) * max(wtj, 0.0)) * co\n\n            g = np.minimum(pair_sum / (cov_den + eps), pair_cap)\n            scores *= 1.0 + pair_beta * g\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build a combined query over: base tokens + micro (char n-gram) tokens.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n            m = max(2, int(Config.micro_len))\n            min_tok = max(1, int(Config.micro_min_token_len))\n            grams: list[str] = []\n            for t in query:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            if grams:\n                gcounts = Counter(grams)\n                for g, c in gcounts.items():\n                    tid = self.corpus.get_term_id(g)\n                    if tid is not None:\n                        query_term_ids.append(tid)\n                        query_term_weights.append(float(Config.micro_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4387)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5  # larger => weaker dependence on query mass\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Entropy-normalized TF (\"burstiness\") prior.\n    entropy_alpha: float = 0.10\n\n    # NEW: aboutness / density lift (positive PMI) prior.\n    # Reward docs where matched query terms are unusually dense vs background df/N.\n    # Helps early precision on long-document corpora with many incidental mentions.\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + informative coverage + calibrated coordination\n    + rare-anchor gate + (optional) aboutness prior + entropy burstiness prior.\n\n    Aboutness prior (positive PMI density):\n      For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N.\n      Add only positive PMI, capped. This downranks incidental mentions in long docs,\n      typically improving nDCG@10 with minimal recall impact (reorders matching docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    # Entropy prior accumulators: m_t = wt * tf over matched terms.\n    m_sum = 0.0\n    m_log_sum = 0.0\n    k_match = 0.0\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if getattr(Config, \"entropy_alpha\", 0.0) != 0.0:\n            mt = wt * tf\n            if mt > 0.0:\n                k_match += 1.0\n                m_sum += mt\n                m_log_sum += mt * math.log(mt + eps)\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if getattr(Config, \"entropy_alpha\", 0.0) != 0.0 and k_match >= 2.0 and m_sum > 0.0:\n        H = math.log(m_sum + eps) - (m_log_sum / (m_sum + eps))\n        Hn = H / (math.log(k_match + eps) + eps)\n        conc = max(0.0, min(1.0, 1.0 - Hn))\n        score *= 1.0 + float(Config.entropy_alpha) * conc\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over (token) U (prefixes), keeping them disjoint by tagging prefixes.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        # Entropy prior accumulators over matched query terms.\n        m_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        m_log_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        k_match = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_ent = getattr(Config, \"entropy_alpha\", 0.0) != 0.0\n\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_ent:\n                mt = wt * tf_row\n                mt_pos = np.maximum(mt, 0.0)\n                k_match += (mt_pos > 0.0).astype(np.float64)\n                m_sum += mt_pos\n                m_log_sum += mt_pos * np.log(mt_pos + eps)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_ent:\n            # H = log(m_sum) - (m_log_sum/m_sum), normalize by log(k_match).\n            mask = (k_match >= 2.0) & (m_sum > 0.0)\n            if np.any(mask):\n                H = np.log(m_sum + eps) - (m_log_sum / (m_sum + eps))\n                Hn = H / (np.log(k_match + eps) + eps)\n                conc = np.clip(1.0 - Hn, 0.0, 1.0)\n                scores *= 1.0 + float(Config.entropy_alpha) * (conc * mask.astype(np.float64))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4383)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + IDF-mass coverage + mild length prior.\n    epsilon: float = 1e-9\n\n    # Evidence uses log1p(tf/base) then log1p(total evidence).\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the *informative* query mass.\n    coverage_gamma: float = 0.25\n\n    # Query-side clarity gate: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting: count**p\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior.\n    dl_alpha: float = 0.15\n\n    # Prefix pseudo-term channel (morphology/symbol robustness).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # \"Anchor\" rare-token gate (bounded, recall-safe).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Position-free \"aboutness\" prior (positive PMI density).\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # Calibrated coordination (soft-AND) over *constraints matched*.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # NEW: query-level \"specificity pressure\" to favor rare constraints when query is technical.\n    # Intuition: when a query contains very rare tokens, they should act more like hard constraints.\n    # We implement this by tilting the query-term weight by (idf / mean_idf)^power, bounded.\n    q_spec_power: float = 0.35\n    q_spec_cap: float = 2.2\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + informative coverage + calibrated coordination\n    + bounded rare-anchor, plus a bounded aboutness prior.\n\n    NEW: query-specificity tilt (rare constraints behave more like hard constraints).\n    We estimate the query's \"rarity profile\" via mean IDF and upweight terms by\n      tilt_t = clip((idf_t / mean_idf)^q_spec_power, 1/q_spec_cap, q_spec_cap)\n    This is a principled way to adapt between:\n      - keywordy/common queries (tilt\u22481)\n      - technical queries with a few very rare tokens (tilt boosts those tokens)\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # Precompute query mean-IDF to define a relative rarity scale for this query.\n    idfs: list[float] = []\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        v = float(idf(df, N))\n        if v > 0.0:\n            idfs.append(v)\n    mean_idf = float(np.mean(idfs)) if idfs else 1.0\n\n    qspec_p = float(getattr(Config, \"q_spec_power\", 0.0))\n    qspec_cap = float(getattr(Config, \"q_spec_cap\", 2.2))\n    inv_cap = 1.0 / max(qspec_cap, 1.0 + eps)\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # Query-specificity tilt (bounded, so it reorders rather than destabilizes).\n        tilt = 1.0\n        if qspec_p != 0.0 and mean_idf > 0.0:\n            r = term_idf / (mean_idf + eps)\n            tilt = r**qspec_p\n            if tilt > qspec_cap:\n                tilt = qspec_cap\n            elif tilt < inv_cap:\n                tilt = inv_cap\n\n        wt = (wq * tilt) * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and term_idf > pivot:\n            hinge = (term_idf - pivot) / (term_idf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and float(getattr(Config, \"coord_gamma\", 0.0)) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (float(getattr(Config, \"coord_mass_tau\", 2.5)) + eps))\n        score *= 1.0 + (float(Config.coord_gamma) * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary prefix-channel score (robust to morphology/symbol splitting).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view for tokenization/morphology robustness.\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:\").\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original token TF dicts for score_document().\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only (current scoring does not use BM25 norms).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        # Query-specificity tilt computed from mean idf of query terms (vectorized path).\n        qspec_p = float(getattr(Config, \"q_spec_power\", 0.0))\n        qspec_cap = float(getattr(Config, \"q_spec_cap\", 2.2))\n        inv_cap = 1.0 / max(qspec_cap, 1.0 + eps)\n        mean_idf = float(np.mean([float(self.corpus.idf_array[tid]) for tid in query_term_ids])) if query_term_ids else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            tilt = 1.0\n            if qspec_p != 0.0 and mean_idf > 0.0:\n                r = idf_val / (mean_idf + eps)\n                tilt = r**qspec_p\n                if tilt > qspec_cap:\n                    tilt = qspec_cap\n                elif tilt < inv_cap:\n                    tilt = inv_cap\n\n            wt = (float(q_w[i]) * tilt) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)  # scalar in (0,1)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and float(getattr(Config, \"coord_gamma\", 0.0)) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (float(getattr(Config, \"coord_mass_tau\", 2.5)) + eps))\n            scores *= 1.0 + (float(Config.coord_gamma) * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query: tokens + (optional) prefixes as extra pseudo-terms.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(\n                self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist()\n            )\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4355, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + informative coverage, but add a *micro field* for short tokens.\n    # Motivation: many weak datasets here (BRIGHT theoremqa/pony/stackoverflow) contain\n    # identifier-like tokens where exact short-term matches are decisive.\n    epsilon: float = 1e-9\n\n    # Evidence: wt * log1p(tf/base), then log1p(total evidence).\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the *informative* query mass.\n    coverage_gamma: float = 0.25\n\n    # Query-side clarity gate: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting: count**p (keeps emphasis w/o verbosity blowups)\n    qtf_power: float = 0.5\n\n    # Mild length prior: downweight extremely long docs gently (helps precision w/o killing recall).\n    dl_alpha: float = 0.15\n\n    # Micro channel (character n-grams) for robustness to tokenization / symbols.\n    micro_len: int = 3\n    micro_min_token_len: int = 2\n    micro_weight: float = 0.12\n\n    # NEW: local-coherence prior (best-window informative coverage).\n    # Intuition: relevant docs often mention multiple query constraints in the same\n    # local region (definition/theorem statement/answer snippet).\n    window_size: int = 28\n    window_gamma: float = 0.18\n    window_cap: float = 0.90  # keep bounded so it mostly reorders top results\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage,\n    plus a bounded *local-coherence* prior computed without positions.\n\n    Key fix vs previous \"focus\":\n      Previous u(tf)=1-exp(-tf/base) ignores document length, so long documents that\n      mention terms sparsely can look overly \"focused\".\n\n    New window-Poisson coherence:\n      Interpret tf/dl as a background rate. For a window of length w, expected count\n      in a random window is tf * w / dl, so:\n          u_w(tf,dl) = 1 - exp(-tf * w / dl)\n      This approximates \"probability the term appears at least once in a random snippet\"\n      and is therefore a true (position-free) proxy for local co-occurrence.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    use_focus = float(getattr(Config, \"window_gamma\", 0.0)) != 0.0\n    focus_num = 0.0\n    focus_cap = float(getattr(Config, \"window_cap\", 0.9))\n    w = float(max(1, int(getattr(Config, \"window_size\", 1))))\n    dl_eff = max(float(doc_length), 1.0)\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if use_focus:\n            # probability term appears in a random window of length w\n            u_w = 1.0 - math.exp(-(tf * w) / (dl_eff + eps))\n            focus_num += wt * u_w\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0:\n        if Config.coverage_gamma != 0.0:\n            score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if use_focus and focus_num > 0.0:\n            focus = focus_num / (cov_den + eps)\n            focus = min(focus, focus_cap)\n            score *= 1.0 + float(Config.window_gamma) * focus\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Micro channel: character n-grams over query tokens (lexical, but more forgiving).\n    if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n        m = max(2, int(Config.micro_len))\n        mtoks: list[str] = []\n        for t in query:\n            if len(t) < max(1, int(Config.micro_min_token_len)):\n                continue\n            if len(t) <= m:\n                mtoks.append(\"M:\" + t)\n            else:\n                for i in range(0, len(t) - m + 1):\n                    mtoks.append(\"M:\" + t[i : i + m])\n        if mtoks:\n            mq = QueryRepr.from_tokens(mtoks)\n            s += float(Config.micro_weight) * retrieval_score(\n                mq,\n                corpus.micro_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build micro-token view (character n-grams) as a separate lexical field.\n        m = max(2, int(getattr(Config, \"micro_len\", 3)))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        micro_docs: list[list[str]] = []\n        for doc in documents:\n            grams: list[str] = []\n            for t in doc:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            micro_docs.append(grams)\n        self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n\n        # Joint vocabulary over base tokens and micro tokens (kept disjoint via \"M:\" prefix).\n        self._vocab: dict[str, int] = {}\n        for doc, mdoc in zip(documents, micro_docs):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for g in mdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, mdoc) in enumerate(zip(documents, micro_docs)):\n            term_counts = Counter(doc)\n            micro_counts = Counter(mdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in micro_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # Expose df for both channels (micro keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        use_focus = float(getattr(Config, \"window_gamma\", 0.0)) != 0.0\n        focus_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        focus_cap = float(getattr(Config, \"window_cap\", 0.9))\n        w = float(max(1, int(getattr(Config, \"window_size\", 1))))\n        dl_eff = np.maximum(self.corpus.doc_lengths[candidate_docs], 1.0)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if use_focus:\n                u_w = 1.0 - np.exp(-(tf_row * w) / (dl_eff + eps))\n                focus_num += wt * u_w\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0:\n            if Config.coverage_gamma != 0.0:\n                scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n            if use_focus:\n                focus = focus_num / (cov_den + eps)\n                focus = np.minimum(focus, focus_cap)\n                scores *= 1.0 + float(Config.window_gamma) * focus\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build a combined query over: base tokens + micro (char n-gram) tokens.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n            m = max(2, int(Config.micro_len))\n            min_tok = max(1, int(Config.micro_min_token_len))\n            grams: list[str] = []\n            for t in query:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            if grams:\n                gcounts = Counter(grams)\n                for g, c in gcounts.items():\n                    tid = self.corpus.get_term_id(g)\n                    if tid is not None:\n                        query_term_ids.append(tid)\n                        query_term_weights.append(float(Config.micro_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4377, Type: Migrant)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + IDF-mass coverage + mild length prior.\n    epsilon: float = 1e-9\n\n    # Evidence uses log1p(tf/base) then log1p(total evidence).\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the *informative* query mass.\n    coverage_gamma: float = 0.25\n\n    # Query-side clarity gate: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting: count**p\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior.\n    dl_alpha: float = 0.15\n\n    # Prefix pseudo-term channel (morphology/symbol robustness).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: \"anchor\" rare-token gate (bounded, recall-safe).\n    # Intuition: for identifier/symbol/name-heavy queries, one very-rare exact token match\n    # is often decisive. Use only the single strongest rarity-above-pivot hit.\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage,\n    plus a bounded *anchor rare-token* gate.\n\n    Anchor gate intuition:\n      Some domains (BRIGHT theorem/tech, StackOverflow identifiers) often have one\n      decisive ultra-rare token. Instead of aggregating rare hits (which can overweight\n      verbose queries), we use the single strongest rarity-above-pivot match as a\n      small multiplicative bump. This is recall-safe (only affects already-matching docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # strongest rare-match hinge among matched query terms (bounded in (0,1))\n    anchor = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        # bounded clarity gate in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 0.0):\n            pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n            hinge = (term_idf - pivot) / (term_idf + eps)  # in (0,1)\n            if hinge > anchor:\n                anchor = hinge\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    # mild verbosity prior\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary prefix-channel score (robust to morphology/symbol splitting).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view for tokenization/morphology robustness.\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:\").\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original token TF dicts for score_document().\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only (current scoring does not use BM25 norms).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)  # scalar in (0,1)\n                anchor = np.maximum(anchor, present * hinge)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query: tokens + (optional) prefixes as extra pseudo-terms.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(\n                self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist()\n            )\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4383, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + IDF-mass coverage + mild length prior.\n    epsilon: float = 1e-9\n\n    # Evidence uses log1p(tf/base) then log1p(total evidence).\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the *informative* query mass.\n    coverage_gamma: float = 0.25\n\n    # Query-side clarity gate: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting: count**p\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior.\n    dl_alpha: float = 0.15\n\n    # Prefix pseudo-term channel (morphology/symbol robustness).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # \"Anchor\" rare-token gate (bounded, recall-safe).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Position-free \"aboutness\" prior (positive PMI density).\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # Calibrated coordination (soft-AND) over *constraints matched*.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # NEW: query-level \"specificity pressure\" to favor rare constraints when query is technical.\n    # Intuition: when a query contains very rare tokens, they should act more like hard constraints.\n    # We implement this by tilting the query-term weight by (idf / mean_idf)^power, bounded.\n    q_spec_power: float = 0.35\n    q_spec_cap: float = 2.2\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + informative coverage + calibrated coordination\n    + bounded rare-anchor, plus a bounded aboutness prior.\n\n    NEW: query-specificity tilt (rare constraints behave more like hard constraints).\n    We estimate the query's \"rarity profile\" via mean IDF and upweight terms by\n      tilt_t = clip((idf_t / mean_idf)^q_spec_power, 1/q_spec_cap, q_spec_cap)\n    This is a principled way to adapt between:\n      - keywordy/common queries (tilt\u22481)\n      - technical queries with a few very rare tokens (tilt boosts those tokens)\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # Precompute query mean-IDF to define a relative rarity scale for this query.\n    idfs: list[float] = []\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        v = float(idf(df, N))\n        if v > 0.0:\n            idfs.append(v)\n    mean_idf = float(np.mean(idfs)) if idfs else 1.0\n\n    qspec_p = float(getattr(Config, \"q_spec_power\", 0.0))\n    qspec_cap = float(getattr(Config, \"q_spec_cap\", 2.2))\n    inv_cap = 1.0 / max(qspec_cap, 1.0 + eps)\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # Query-specificity tilt (bounded, so it reorders rather than destabilizes).\n        tilt = 1.0\n        if qspec_p != 0.0 and mean_idf > 0.0:\n            r = term_idf / (mean_idf + eps)\n            tilt = r**qspec_p\n            if tilt > qspec_cap:\n                tilt = qspec_cap\n            elif tilt < inv_cap:\n                tilt = inv_cap\n\n        wt = (wq * tilt) * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and term_idf > pivot:\n            hinge = (term_idf - pivot) / (term_idf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and float(getattr(Config, \"coord_gamma\", 0.0)) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (float(getattr(Config, \"coord_mass_tau\", 2.5)) + eps))\n        score *= 1.0 + (float(Config.coord_gamma) * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary prefix-channel score (robust to morphology/symbol splitting).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view for tokenization/morphology robustness.\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:\").\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original token TF dicts for score_document().\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only (current scoring does not use BM25 norms).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        # Query-specificity tilt computed from mean idf of query terms (vectorized path).\n        qspec_p = float(getattr(Config, \"q_spec_power\", 0.0))\n        qspec_cap = float(getattr(Config, \"q_spec_cap\", 2.2))\n        inv_cap = 1.0 / max(qspec_cap, 1.0 + eps)\n        mean_idf = float(np.mean([float(self.corpus.idf_array[tid]) for tid in query_term_ids])) if query_term_ids else 1.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            tilt = 1.0\n            if qspec_p != 0.0 and mean_idf > 0.0:\n                r = idf_val / (mean_idf + eps)\n                tilt = r**qspec_p\n                if tilt > qspec_cap:\n                    tilt = qspec_cap\n                elif tilt < inv_cap:\n                    tilt = inv_cap\n\n            wt = (float(q_w[i]) * tilt) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)  # scalar in (0,1)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and float(getattr(Config, \"coord_gamma\", 0.0)) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (float(getattr(Config, \"coord_mass_tau\", 2.5)) + eps))\n            scores *= 1.0 + (float(Config.coord_gamma) * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query: tokens + (optional) prefixes as extra pseudo-terms.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(\n                self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist()\n            )\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4379, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core signals: concave evidence + informative coverage + gentle verbosity prior.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    qtf_power: float = 0.5\n    dl_alpha: float = 0.15\n\n    # Micro channel (character n-grams) for robustness to tokenization / symbols.\n    micro_len: int = 3\n    micro_min_token_len: int = 2\n    micro_weight: float = 0.12\n\n    # Specificity / aboutness prior (bounded, position-free).\n    spec_beta: float = 0.08\n    spec_cap: float = 2.5\n    spec_len_floor: float = 35.0\n\n    # NEW: calibrated coordination (soft-AND) over matched constraints.\n    # Helps multi-term queries (SciDocs/FiQA/ArguAna) without over-penalizing verbose queries.\n    coord_gamma: float = 0.20\n    coord_mass_tau: float = 2.2\n\n    # NEW: rare-term anchor (single strongest rare exact hit), recall-safe (reorders matching docs).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.12\n\n    # Compatibility leftovers\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + informative coverage + calibrated coordination,\n    plus (optional) bounded specificity and a rare-term anchor.\n\n    Coordination: encourages satisfying more query constraints (soft-AND), but is\n    tapered by query \"mass\" so long/verbose queries don't get over-ANDed.\n\n    Rare anchor: for symbol/identifier-heavy queries, a single ultra-rare exact hit\n    is often decisive; use max hinge (bounded) to avoid overcounting.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    matched = 0.0\n    uq = 0.0\n\n    # Specificity/aboutness accumulators.\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    # Rare anchor (single strongest).\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and term_idf > pivot:\n            hinge = (term_idf - pivot) / (term_idf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_spec:\n            # Positive PMI density, capped.\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    # Calibrated coordination boost.\n    if uq > 0.0 and float(getattr(Config, \"coord_gamma\", 0.0)) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (float(getattr(Config, \"coord_mass_tau\", 2.2)) + eps))\n        score *= 1.0 + (float(Config.coord_gamma) * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Micro channel: character n-grams over query tokens (lexical, but more forgiving).\n    if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n        m = max(2, int(Config.micro_len))\n        mtoks: list[str] = []\n        for t in query:\n            if len(t) < max(1, int(Config.micro_min_token_len)):\n                continue\n            if len(t) <= m:\n                mtoks.append(\"M:\" + t)\n            else:\n                for i in range(0, len(t) - m + 1):\n                    mtoks.append(\"M:\" + t[i : i + m])\n        if mtoks:\n            mq = QueryRepr.from_tokens(mtoks)\n            s += float(Config.micro_weight) * retrieval_score(\n                mq,\n                corpus.micro_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build micro-token view (character n-grams) as a separate lexical field.\n        m = max(2, int(getattr(Config, \"micro_len\", 3)))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        micro_docs: list[list[str]] = []\n        for doc in documents:\n            grams: list[str] = []\n            for t in doc:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            micro_docs.append(grams)\n        self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n\n        # Joint vocabulary over base tokens and micro tokens (kept disjoint via \"M:\" prefix).\n        self._vocab: dict[str, int] = {}\n        for doc, mdoc in zip(documents, micro_docs):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for g in mdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, mdoc) in enumerate(zip(documents, micro_docs)):\n            term_counts = Counter(doc)\n            micro_counts = Counter(mdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in micro_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # Expose df for both channels (micro keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Rare anchor (single strongest).\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        uq = float(len(query_term_ids))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and float(getattr(Config, \"coord_gamma\", 0.0)) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (float(getattr(Config, \"coord_mass_tau\", 2.2)) + eps))\n            scores *= 1.0 + (float(Config.coord_gamma) * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build a combined query over: base tokens + micro (char n-gram) tokens.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n            m = max(2, int(Config.micro_len))\n            min_tok = max(1, int(Config.micro_min_token_len))\n            grams: list[str] = []\n            for t in query:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            if grams:\n                gcounts = Counter(grams)\n                for g, c in gcounts.items():\n                    tid = self.corpus.get_term_id(g)\n                    if tid is not None:\n                        query_term_ids.append(tid)\n                        query_term_weights.append(float(Config.micro_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (phrase specificity)\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Background-corrected aboutness (positive PMI density).\n    # For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N; add only positive PMI.\n    # This is robust across corpora and tends to improve early precision without killing recall.\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor gate, plus a bounded aboutness prior.\n\n    Aboutness prior (positive PMI density, position-free):\n      For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N.\n      Add only positive PMI (capped) weighted by the same informative wt, then apply\n      a small multiplier. Downranks incidental mentions in long docs => better nDCG@10.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Bigram-channel score (phrase/proximity specificity; purely lexical).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        qb = [\"B:\" + query[i] + \" \" + query[i + 1] for i in range(len(query) - 1)]\n        if qb:\n            bq = QueryRepr.from_tokens(qb)\n            bdoc_tf = corpus.bigram_doc_tf_dicts[doc_idx]\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq, bdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: bigram view (adjacent tokens) as phrase/proximity evidence.\n        docs_bigram: list[list[str]] = []\n        for doc in documents:\n            if len(doc) < 2:\n                docs_bigram.append([])\n            else:\n                docs_bigram.append([doc[i] + \" \" + doc[i + 1] for i in range(len(doc) - 1)])\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigram]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigram):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                key = \"B:\" + bg\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigram)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(\"B:\" + bg for bg in bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        anchor = np.zeros(len(candidate_docs), dtype=np.float64)\n        pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n        uq = float(len(query_term_ids))\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (phrase/proximity evidence).\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(query[i] + \" \" + query[i + 1] for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(\"B:\" + bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (phrase specificity)\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Background-corrected aboutness (positive PMI density).\n    # For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N; add only positive PMI.\n    # This is robust across corpora and tends to improve early precision without killing recall.\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n=======\nclass Config:\n    # Core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Prefix channel (morphology/symbol robustness)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bigram channel (phrase specificity)\n    bigram_weight: float = 0.10\n    bigram_clarity_power: float = 0.90\n\n    # Calibrated coordination boost.\n    coord_gamma: float = 0.22\n    coord_mass_tau: float = 2.5\n\n    # Bounded rare-term \"anchor\" (single best rare exact hit).\n    rare_idf_pivot: float = 4.2\n    anchor_boost: float = 0.14\n\n    # Background-corrected aboutness (positive PMI density).\n    about_beta: float = 0.08\n    about_cap: float = 2.5\n    about_len_floor: float = 40.0\n\n    # NEW: local-coherence prior (position-free proxy for \"do query terms appear together in a snippet?\")\n    # Use a Poisson window model: u_w = 1-exp(-tf*w/dl). Then average u_w by informative weights.\n    # Bounded => mostly reorders early ranks; tends to help nDCG@10 on long documents.\n    window_size: int = 28\n    window_gamma: float = 0.14\n    window_cap: float = 0.85\n\n    # Compatibility leftovers.\n    k1: float = 0.9\n    b: float = 0.4\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor gate, plus a bounded aboutness prior.\n\n    Aboutness prior (positive PMI density, position-free):\n      For matched terms, compare within-doc rate tf/dl_eff to collection rate df/N.\n      Add only positive PMI (capped) weighted by the same informative wt, then apply\n      a small multiplier. Downranks incidental mentions in long docs => better nDCG@10.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + calibrated coordination,\n    plus a bounded rare-term anchor gate, plus a bounded aboutness prior,\n    plus a bounded local-coherence prior (window-Poisson).\n\n    Window-Poisson coherence:\n      Approximate \"term appears in a random snippet of length w\" by:\n          u_w(tf,dl)=1-exp(-tf*w/dl)\n      and average u_w over query terms (weighted by wt). This is position-free but\n      correlates with true proximity/cohesion, improving early precision on long docs.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    anchor = 0.0\n    pivot = float(getattr(Config, \"rare_idf_pivot\", 0.0))\n\n    use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n    about_sum = 0.0\n    about_cap = float(getattr(Config, \"about_cap\", 2.5))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"about_len_floor\", 0.0)))\n\n    use_win = float(getattr(Config, \"window_gamma\", 0.0)) != 0.0\n    win_num = 0.0\n    w = float(max(1, int(getattr(Config, \"window_size\", 1))))\n    win_cap = float(getattr(Config, \"window_cap\", 0.85))\n    dl_snip = max(float(doc_length), 1.0)\n\n    for term in query_repr.terms:\n        uq += 1.0\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        if term.startswith(\"B:\"):\n            clarity = rarity ** float(getattr(Config, \"bigram_clarity_power\", Config.q_clarity_power))\n        else:\n            clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and tidf > pivot:\n            hinge = (tidf - pivot) / (tidf + eps)\n            if hinge > anchor:\n                anchor = hinge\n\n        if use_about:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                about_sum += wt * min(g, about_cap)\n\n        if use_win:\n            u_w = 1.0 - math.exp(-(tf * w) / (dl_snip + eps))\n            win_num += wt * u_w\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n        q_mass = math.log1p(max(cov_den, 0.0))\n        cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n        score *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n    if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and anchor > 0.0:\n        score *= 1.0 + float(Config.anchor_boost) * math.log1p(anchor)\n\n    if use_about and about_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n    if use_win and win_num > 0.0 and cov_den > 0.0:\n        win = min(win_num / (cov_den + eps), win_cap)\n        score *= 1.0 + float(Config.window_gamma) * win\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n=======\n        use_about = float(getattr(Config, \"about_beta\", 0.0)) != 0.0\n        about_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        about_cap = float(getattr(Config, \"about_cap\", 2.5))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"about_len_floor\", 0.0)),\n        )\n\n        use_win = float(getattr(Config, \"window_gamma\", 0.0)) != 0.0\n        win_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        w = float(max(1, int(getattr(Config, \"window_size\", 1))))\n        win_cap = float(getattr(Config, \"window_cap\", 0.85))\n        dl_snip = np.maximum(self.corpus.doc_lengths[candidate_docs], 1.0)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"anchor_boost\", 0.0) != 0.0 and idf_val > pivot:\n                hinge = (idf_val - pivot) / (idf_val + eps)\n                anchor = np.maximum(anchor, present * hinge)\n\n            if use_about:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, about_cap)\n                about_sum += wt * np.maximum(g, 0.0)\n\n            if use_win:\n                u_w = 1.0 - np.exp(-(tf_row * w) / (dl_snip + eps))\n                win_num += wt * u_w\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if uq > 0.0 and getattr(Config, \"coord_gamma\", 0.0) != 0.0:\n            q_mass = np.log1p(max(cov_den, 0.0))\n            cal = 1.0 / (1.0 + q_mass / (Config.coord_mass_tau + eps))\n            scores *= 1.0 + (Config.coord_gamma * cal) * (matched / (uq + eps))\n\n        if getattr(Config, \"anchor_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.anchor_boost) * np.log1p(np.maximum(anchor, 0.0))\n\n        if use_about and cov_den > 0.0:\n            scores *= 1.0 + float(Config.about_beta) * (about_sum / (cov_den + eps))\n\n        if use_win and cov_den > 0.0:\n            win = np.minimum(win_num / (cov_den + eps), win_cap)\n            scores *= 1.0 + float(Config.window_gamma) * win\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}