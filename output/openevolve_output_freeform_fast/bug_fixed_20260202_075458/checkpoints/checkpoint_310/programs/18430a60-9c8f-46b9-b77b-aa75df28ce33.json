{"id": "18430a60-9c8f-46b9-b77b-aa75df28ce33", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + informative-coverage pressure.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Rare-constraint gate (bounded) for identifier/symbol-heavy queries (recall-safe).\n    rare_idf_pivot: float = 4.2\n    rare_boost: float = 0.18\n\n    # NEW: bounded specificity/aboutness prior (positive-PMI density lift).\n    # Helps nDCG@10 by downranking docs that mention query terms only incidentally.\n    spec_beta: float = 0.10\n    spec_cap: float = 3.0\n    spec_len_floor: float = 25.0\n\n    # NEW: tiny micro-token channel (char n-grams) to reduce tokenization brittleness.\n    micro_len: int = 3\n    micro_min_token_len: int = 2\n    micro_weight: float = 0.12\n\n    # Compatibility (Corpus still builds norm_array).\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + clarity gate + IDF-mass coverage,\n    plus two bounded priors that mostly affect early precision:\n\n    (1) Rare-term key-match gate (recall-safe reordering among matching docs).\n    (2) Specificity/aboutness gain: reward cases where p(t|d)=tf/dl exceeds background p(t)=df/N\n        (positive PMI). Bounded & small => tends to improve nDCG@10 without killing recall@100.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n    spec_sum = 0.0\n    spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n    dl_eff = max(float(doc_length), float(getattr(Config, \"spec_len_floor\", 0.0)))\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n        if use_spec:\n            p_td = tf / (dl_eff + eps)\n            p_t = df / (float(N) + eps)\n            g = math.log((p_td + eps) / (p_t + eps))\n            if g > 0.0:\n                spec_sum += wt * min(g, spec_cap)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    if use_spec and spec_sum > 0.0 and cov_den > 0.0:\n        score *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Micro channel: char n-grams over query tokens (lexical, more forgiving for symbols/identifiers).\n    if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n        m = max(2, int(Config.micro_len))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        mtoks: list[str] = []\n        for t in query:\n            if len(t) < min_tok:\n                continue\n            if len(t) <= m:\n                mtoks.append(\"M:\" + t)\n            else:\n                for i in range(0, len(t) - m + 1):\n                    mtoks.append(\"M:\" + t[i : i + m])\n        if mtoks:\n            mq = QueryRepr.from_tokens(mtoks)\n            s += float(Config.micro_weight) * retrieval_score(\n                mq,\n                corpus.micro_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Micro-token field: character n-grams prefixed with \"M:\" to keep disjoint vocab.\n        m = max(2, int(getattr(Config, \"micro_len\", 3)))\n        min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n        micro_docs: list[list[str]] = []\n        for doc in documents:\n            grams: list[str] = []\n            for t in doc:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            micro_docs.append(grams)\n        self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n\n        # Joint vocabulary over base tokens and micro tokens.\n        self._vocab: dict[str, int] = {}\n        for doc, mdoc in zip(documents, micro_docs):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for g in mdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, mdoc) in enumerate(zip(documents, micro_docs)):\n            term_counts = Counter(doc)\n            micro_counts = Counter(mdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in micro_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (micro keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n        spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n        spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n        dl_eff = np.maximum(\n            self.corpus.doc_lengths[candidate_docs],\n            float(getattr(Config, \"spec_len_floor\", 0.0)),\n        )\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n            if use_spec:\n                df_val = float(self.corpus._df[term_id])\n                p_td = tf_row / (dl_eff + eps)\n                p_t = df_val / (float(self.corpus.N) + eps)\n                g = np.log((p_td + eps) / (p_t + eps))\n                g = np.minimum(g, spec_cap)\n                spec_sum += wt * np.maximum(g, 0.0)\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        if use_spec and cov_den > 0.0:\n            scores *= 1.0 + float(Config.spec_beta) * (spec_sum / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query over base tokens + micro (char n-gram) tokens.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n            m = max(2, int(Config.micro_len))\n            min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n            grams: list[str] = []\n            for t in query:\n                if len(t) < min_tok:\n                    continue\n                if len(t) <= m:\n                    grams.append(\"M:\" + t)\n                else:\n                    for i in range(0, len(t) - m + 1):\n                        grams.append(\"M:\" + t[i : i + m])\n            if grams:\n                gcounts = Counter(grams)\n                for g, c in gcounts.items():\n                    tid = self.corpus.get_term_id(g)\n                    if tid is not None:\n                        query_term_ids.append(tid)\n                        query_term_weights.append(float(Config.micro_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "changes_description": null, "language": "python", "parent_id": "371d4be8-73c9-43bb-ab6c-dcdcb6e3a387", "generation": 3, "timestamp": 1770278711.382705, "iteration_found": 0, "metrics": {"beir_nfcorpus_ndcg@10": 0.33395828639019115, "beir_nfcorpus_recall@100": 0.26117332676858085, "beir_nfcorpus_index_time_ms": 6844.20479205437, "beir_nfcorpus_query_time_ms": 594.0701250219718, "bright_pony_ndcg@10": 0.13423183478736592, "bright_pony_recall@100": 0.3594133302228779, "bright_pony_index_time_ms": 2336.417334037833, "bright_pony_query_time_ms": 2267.7454999648035, "beir_scifact_ndcg@10": 0.6826577444460423, "beir_scifact_recall@100": 0.9416666666666667, "beir_scifact_index_time_ms": 9444.372790982015, "beir_scifact_query_time_ms": 2309.216000023298, "beir_arguana_ndcg@10": 0.28653682365454286, "beir_arguana_recall@100": 0.9207708779443254, "beir_arguana_index_time_ms": 11591.38958400581, "beir_arguana_query_time_ms": 111711.77649998572, "bright_theoremqa_theorems_ndcg@10": 0.05006030878610829, "bright_theoremqa_theorems_recall@100": 0.18421052631578946, "bright_theoremqa_theorems_index_time_ms": 16609.754832927138, "bright_theoremqa_theorems_query_time_ms": 4720.336833037436, "beir_scidocs_ndcg@10": 0.15304035599600285, "beir_scidocs_recall@100": 0.35420000000000007, "beir_scidocs_index_time_ms": 37338.80579099059, "beir_scidocs_query_time_ms": 26830.242207972333, "bright_economics_ndcg@10": 0.1270733057061887, "bright_economics_recall@100": 0.3599777965801892, "bright_economics_index_time_ms": 26739.90895797033, "bright_economics_query_time_ms": 20773.21166708134, "bright_biology_ndcg@10": 0.2486823545708826, "bright_biology_recall@100": 0.5662620891287218, "bright_biology_index_time_ms": 32235.83866690751, "bright_biology_query_time_ms": 19263.69849999901, "beir_fiqa_ndcg@10": 0.2405378261888178, "beir_fiqa_recall@100": 0.5349402226717042, "beir_fiqa_index_time_ms": 72211.66629204527, "beir_fiqa_query_time_ms": 30118.079083040357, "bright_earth_science_ndcg@10": 0.2732584194309856, "bright_earth_science_recall@100": 0.6385485657717964, "bright_earth_science_index_time_ms": 70259.47754201479, "bright_earth_science_query_time_ms": 44627.66862497665, "bright_stackoverflow_ndcg@10": 0.18096012669712913, "bright_stackoverflow_recall@100": 0.4863464370040458, "bright_stackoverflow_index_time_ms": 222348.75000000466, "bright_stackoverflow_query_time_ms": 68213.98054203019, "beir_trec-covid_ndcg@10": 0.7017315568340391, "beir_trec-covid_recall@100": 0.1200779985949606, "beir_trec-covid_index_time_ms": 384450.740374974, "beir_trec-covid_query_time_ms": 9410.141375032254, "avg_ndcg@10": 0.2843940786240247, "avg_recall@100": 0.47729898647247154, "combined_score": 0.43871800490278223, "total_index_time_ms": 892411.3269589143, "total_query_time_ms": 340840.16695816536, "total_time_ms": 1233251.4939170796, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Concave evidence + informative-coverage pressure.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # NEW: rare-constraint gate (bounded) to help identifier/symbol-heavy queries (BRIGHT).\n      # It only reorders already-matching docs (present==1), so it is recall-safe.\n      rare_idf_pivot: float = 4.2\n      rare_boost: float = 0.18\n  \n      # Compatibility (Corpus still builds norm_array).\n      k1: float = 0.9\n      b: float = 0.4\nwith:\n  class Config:\n      # Concave evidence + informative-coverage pressure.\n      epsilon: float = 1e-9\n      tf_log_base: float = 1.0\n      coverage_gamma: float = 0.25\n      qtf_power: float = 0.5\n      q_clarity_power: float = 0.6\n      dl_alpha: float = 0.15\n  \n      # Rare-constraint gate (bounded) for identifier/symbol-heavy queries (recall-safe).\n      rare_idf_pivot: float = 4.2\n      rare_boost: float = 0.18\n  \n      # NEW: bounded specificity/aboutness prior (positive-PMI density lift).\n      # Helps nDCG@10 by downranking docs that mention query terms only incidentally.\n      spec_beta: float = 0.10\n      spec_cap: float = 3.0\n      spec_len_floor: float = 25.0\n  \n      # NEW: tiny micro-token channel (char n-grams) to reduce tokenization brittleness.\n      micro_len: int = 3\n      micro_min_token_len: int = 2\n      micro_weight: float = 0.12\n  \n      # Compatibility (Corpus still builds norm_array).\n      k1: float = 0.9\n      b: float = 0.4\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + clarity gate + IDF-mass coverage,\n      plus a bounded rare-term \"key match\" gate.\n  \n      The rare gate targets domains where a single exact match on a very rare token\n      (identifier/symbol/name) is decisive. It is recall-safe because it only\n      multiplies scores for docs that already match the term at least once.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      rare_hits = 0.0\n  \n      for term in query_repr.terms:\n          df = float(corpus_df.get(term, 1.0))\n          tidf = float(idf(df, N))\n  ... (34 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave surprisal evidence + clarity gate + IDF-mass coverage,\n      plus two bounded priors that mostly affect early precision:\n  \n      (1) Rare-term key-match gate (recall-safe reordering among matching docs).\n      (2) Specificity/aboutness gain: reward cases where p(t|d)=tf/dl exceeds background p(t)=df/N\n          (positive PMI). Bounded & small => tends to improve nDCG@10 without killing recall@100.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      rare_hits = 0.0\n  \n      use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n      spec_sum = 0.0\n      spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n  ... (49 more lines)\nChange 3: Replace:\n  class Corpus:\n      def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n          self.documents = documents\n          self.ids = ids or [str(i) for i in range(len(documents))]\n          self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n          self.N = len(documents)\n          self.document_count = self.N\n          self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n          self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n          self.average_document_length = self.avgdl\n  \n          self._vocab: dict[str, int] = {}\n          for doc in documents:\n              for term in doc:\n                  if term not in self._vocab:\n                      self._vocab[term] = len(self._vocab)\n          self.vocab_size = len(self._vocab)\n  \n          tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n          self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n          self._df = np.zeros(self.vocab_size, dtype=np.float64)\n          self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n  \n          for doc_idx, doc in enumerate(documents):\n              term_counts = Counter(doc)\n              seen = set()\n              for term, count in term_counts.items():\n                  tid = self._vocab[term]\n                  tf_matrix_lil[tid, doc_idx] = count\n                  if tid not in seen:\n  ... (18 more lines)\nwith:\n  class Corpus:\n      def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n          self.documents = documents\n          self.ids = ids or [str(i) for i in range(len(documents))]\n          self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n          self.N = len(documents)\n          self.document_count = self.N\n          self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n          self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n          self.average_document_length = self.avgdl\n  \n          # Micro-token field: character n-grams prefixed with \"M:\" to keep disjoint vocab.\n          m = max(2, int(getattr(Config, \"micro_len\", 3)))\n          min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n          micro_docs: list[list[str]] = []\n          for doc in documents:\n              grams: list[str] = []\n              for t in doc:\n                  if len(t) < min_tok:\n                      continue\n                  if len(t) <= m:\n                      grams.append(\"M:\" + t)\n                  else:\n                      for i in range(0, len(t) - m + 1):\n                          grams.append(\"M:\" + t[i : i + m])\n              micro_docs.append(grams)\n          self.micro_doc_tf_dicts: list[Counter[str]] = [Counter(g) for g in micro_docs]\n  \n          # Joint vocabulary over base tokens and micro tokens.\n          self._vocab: dict[str, int] = {}\n  ... (52 more lines)\nChange 4: Replace:\n  def score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n      if not query:\n          return 0.0\n      q = QueryRepr.from_tokens(query)\n      if not q.terms:\n          return 0.0\n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      doc_length = float(corpus.doc_lengths[doc_idx])\n      return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\nwith:\n  def score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Entry point used by BM25.score(). Adds a tiny micro-token channel for identifiers.\"\"\"\n      if not query:\n          return 0.0\n      q = QueryRepr.from_tokens(query)\n      if not q.terms:\n          return 0.0\n  \n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      doc_length = float(corpus.doc_lengths[doc_idx])\n  \n      s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n  \n      # Micro channel: char n-grams over query tokens (lexical, more forgiving for symbols/identifi...\n      if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n          m = max(2, int(Config.micro_len))\n          min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n          mtoks: list[str] = []\n          for t in query:\n              if len(t) < min_tok:\n                  continue\n              if len(t) <= m:\n                  mtoks.append(\"M:\" + t)\n              else:\n                  for i in range(0, len(t) - m + 1):\n                      mtoks.append(\"M:\" + t[i : i + m])\n          if mtoks:\n              mq = QueryRepr.from_tokens(mtoks)\n              s += float(Config.micro_weight) * retrieval_score(\n                  mq,\n  ... (7 more lines)\nChange 5: Replace:\n  def rank(\n          self,\n          query: list[str],\n          top_k: int | None = None,\n      ) -> tuple[np.ndarray, np.ndarray]:\n          if not query:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n          term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n          if not query_term_ids:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n          qtf = np.array(query_term_weights, dtype=np.float64)\n          candidate_set: set[int] = set()\n          for tid in query_term_ids:\n              candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)...\n          candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n          candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n          all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n          all_scores[candidate_docs] = candidate_scores\n          sorted_indices = np.argsort(-all_scores).astype(np.int64)\n          sorted_scores = all_scores[sorted_indices]\n          if top_k is not None:\n              sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n          return sorted_indices, sorted_scores\nwith:\n  def rank(\n          self,\n          query: list[str],\n          top_k: int | None = None,\n      ) -> tuple[np.ndarray, np.ndarray]:\n          if not query:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n  \n          # Combined query over base tokens + micro (char n-gram) tokens.\n          term_counts = Counter(query)\n          query_term_ids: list[int] = []\n          query_term_weights: list[float] = []\n  \n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n  \n          if getattr(Config, \"micro_weight\", 0.0) != 0.0 and getattr(Config, \"micro_len\", 0) > 0:\n              m = max(2, int(Config.micro_len))\n              min_tok = max(1, int(getattr(Config, \"micro_min_token_len\", 2)))\n              grams: list[str] = []\n              for t in query:\n                  if len(t) < min_tok:\n                      continue\n                  if len(t) <= m:\n                      grams.append(\"M:\" + t)\n                  else:\n                      for i in range(0, len(t) - m + 1):\n  ... (27 more lines)\nChange 6: Replace:\n  sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = wq * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n              cov_num += wt * present\n              sum_evidence += wt * np.log1p(tf_row / (base + eps))\n  \n              if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                  rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n  \n          scores = np.log1p(np.maximum(sum_evidence, 0.0))\n  \n          if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n              scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n  \n  ... (6 more lines)\nwith:\n  sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          use_spec = float(getattr(Config, \"spec_beta\", 0.0)) != 0.0\n          spec_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n          spec_cap = float(getattr(Config, \"spec_cap\", 3.0))\n          dl_eff = np.maximum(\n              self.corpus.doc_lengths[candidate_docs],\n              float(getattr(Config, \"spec_len_floor\", 0.0)),\n          )\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = wq * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n              cov_num += wt * present\n              sum_evidence += wt * np.log1p(tf_row / (base + eps))\n  \n  ... (25 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.32990052144700627, "beir_nfcorpus_recall@100": 0.25534076992575006, "beir_nfcorpus_index_time_ms": 3784.5816250192, "beir_nfcorpus_query_time_ms": 110.63650000141934, "beir_scifact_ndcg@10": 0.6745772004636701, "beir_scifact_recall@100": 0.9182222222222223, "beir_scifact_index_time_ms": 5137.921416026074, "beir_scifact_query_time_ms": 402.626083989162, "bright_pony_ndcg@10": 0.11483595421033933, "bright_pony_recall@100": 0.3060619983661623, "bright_pony_index_time_ms": 1444.2528749932535, "bright_pony_query_time_ms": 525.1324159908108, "beir_arguana_ndcg@10": 0.29562570971558794, "beir_arguana_recall@100": 0.9293361884368309, "beir_arguana_index_time_ms": 6361.830499954522, "beir_arguana_query_time_ms": 22855.7541660266, "bright_theoremqa_theorems_ndcg@10": 0.03964707327962591, "bright_theoremqa_theorems_recall@100": 0.18421052631578946, "bright_theoremqa_theorems_index_time_ms": 11510.748166998383, "bright_theoremqa_theorems_query_time_ms": 989.8656669538468, "beir_scidocs_ndcg@10": 0.1506403632134481, "beir_scidocs_recall@100": 0.35586666666666666, "beir_scidocs_index_time_ms": 21401.78808296332, "beir_scidocs_query_time_ms": 3867.370249994565, "bright_economics_ndcg@10": 0.13850523769242282, "bright_economics_recall@100": 0.3779090848895681, "bright_economics_index_time_ms": 15155.052624992095, "bright_economics_query_time_ms": 2603.423083026428, "bright_biology_ndcg@10": 0.252829324647107, "bright_biology_recall@100": 0.5445820800445226, "bright_biology_index_time_ms": 16720.82337498432, "bright_biology_query_time_ms": 2527.68608299084, "beir_fiqa_ndcg@10": 0.2330661004885485, "beir_fiqa_recall@100": 0.5353898073805481, "beir_fiqa_index_time_ms": 36437.929500010796, "beir_fiqa_query_time_ms": 6917.8076250245795, "bright_earth_science_ndcg@10": 0.2582235626344524, "bright_earth_science_recall@100": 0.6461944519737169, "bright_earth_science_index_time_ms": 34655.64958297182, "bright_earth_science_query_time_ms": 4459.218041971326, "bright_stackoverflow_ndcg@10": 0.18947666139131186, "bright_stackoverflow_recall@100": 0.500168232065119, "bright_stackoverflow_index_time_ms": 104175.88625004282, "bright_stackoverflow_query_time_ms": 9616.684458975215, "beir_trec-covid_ndcg@10": 0.6942193317828802, "beir_trec-covid_recall@100": 0.11727832285870704, "beir_trec-covid_index_time_ms": 146976.85241594445, "beir_trec-covid_query_time_ms": 1718.8321250141598, "avg_ndcg@10": 0.2809622534138667, "avg_recall@100": 0.4725466959288001, "combined_score": 0.43422980742581346, "total_index_time_ms": 403763.31641490106, "total_query_time_ms": 56595.03649995895, "total_time_ms": 460358.35291486, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}