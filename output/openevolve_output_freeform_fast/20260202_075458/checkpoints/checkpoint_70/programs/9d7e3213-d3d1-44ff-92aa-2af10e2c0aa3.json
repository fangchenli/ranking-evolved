{"id": "9d7e3213-d3d1-44ff-92aa-2af10e2c0aa3", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: reduce over-penalizing long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination: prefer docs covering more distinct query terms.\n    coord_alpha: float = 0.18\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # Mild \"soft-AND\" over (idf-weighted) coverage.\n    cov_and_power: float = 0.10\n\n    # NEW: query-adaptive focus prior.\n    # Intuition: when a query is \"specific\" (high avg IDF), relevance is more about\n    # concentrated evidence; for broad queries, length shouldn't be punished much.\n    focus_lambda: float = 0.25\n    focus_idf_pivot: float = 2.2\n\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Self-information (surprisal) IDF: log1p(N/df).\n\n    Interprets df/N as an occurrence probability; matching a term contributes\n    roughly its surprisal. This is smoother than BM25-odds IDF and tends to\n    generalize better across heterogeneous corpora.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent the query as unique constraints but keep sublinear repetition as weight.\n        Repetition often signals emphasis; sublinear weighting avoids verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        term_weights = {t: math.sqrt(float(cnt)) for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=term_weights)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 evidence + IDF-weighted coverage shaping + query-adaptive focus prior.\n\n    New piece: a *query-adaptive* length prior. Only when the query is specific\n    (high avg IDF across its constraints) do we penalize length more strongly\n    to favor focused documents (often improves nDCG@10 on scientific/QA).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    idf_mass = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        idf_mass += term_idf\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = float(query_repr.term_weights.get(term, 1.0))\n        wt = w * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    coverage = cov_num / (cov_den + eps) if cov_den > 0.0 else 0.0\n\n    score *= 1.0 + Config.coord_alpha * coverage\n    if Config.cov_and_power != 0.0:\n        score *= (coverage + eps) ** Config.cov_and_power\n\n    # Query-adaptive focus prior (bounded, recall-safe):\n    # if avg_idf is high => apply a mild penalty when norm>1 (longer than average).\n    uq = float(len(query_repr.terms))\n    avg_idf = idf_mass / (uq + eps)\n    gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n    if gate > 0.0:\n        score *= 1.0 / (1.0 + Config.focus_lambda * gate * max(0.0, norm - 1.0))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score's sublinear length normalization.\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        idf_mass = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            idf_mass += idf_val\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * clarity * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if cov_den > 0.0:\n            coverage = cov_num / (cov_den + eps)\n            if Config.coord_alpha != 0.0:\n                scores *= 1.0 + Config.coord_alpha * coverage\n            if Config.cov_and_power != 0.0:\n                scores *= np.power(coverage + eps, Config.cov_and_power)\n\n        # Query-adaptive focus prior (matches retrieval_score)\n        uq = float(max(1, len(query_term_ids)))\n        avg_idf = idf_mass / (uq + eps)\n        gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n        if gate > 0.0:\n            scores *= 1.0 / (1.0 + Config.focus_lambda * gate * np.maximum(0.0, norms - 1.0))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(math.sqrt(float(count)))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "308104a3-757f-48d9-8285-96babbda6266", "generation": 4, "timestamp": 1770055963.786397, "iteration_found": 36, "metrics": {"beir_nfcorpus_ndcg@10": 0.317545813727539, "beir_nfcorpus_recall@100": 0.2554553930823869, "beir_nfcorpus_index_time_ms": 3530.242375039961, "beir_nfcorpus_query_time_ms": 95.40904097957537, "beir_scifact_ndcg@10": 0.678085730716795, "beir_scifact_recall@100": 0.9279999999999999, "beir_scifact_index_time_ms": 4757.406542019453, "beir_scifact_query_time_ms": 356.46370804170147, "bright_pony_ndcg@10": 0.12069945599629214, "bright_pony_recall@100": 0.3095818933017057, "bright_pony_index_time_ms": 1463.9366659685038, "bright_pony_query_time_ms": 432.69212503219023, "beir_arguana_ndcg@10": 0.2749188155350277, "beir_arguana_recall@100": 0.8972162740899358, "beir_arguana_index_time_ms": 5757.371500018053, "beir_arguana_query_time_ms": 18392.976750037633, "bright_theoremqa_theorems_ndcg@10": 0.03196984395971905, "bright_theoremqa_theorems_recall@100": 0.1513157894736842, "bright_theoremqa_theorems_index_time_ms": 10547.021875041537, "bright_theoremqa_theorems_query_time_ms": 793.2846670155413, "beir_scidocs_ndcg@10": 0.1474991387798862, "beir_scidocs_recall@100": 0.3441166666666667, "beir_scidocs_index_time_ms": 19144.644499989226, "beir_scidocs_query_time_ms": 3189.9174170102924, "bright_economics_ndcg@10": 0.15541288955799853, "bright_economics_recall@100": 0.38179544184713515, "bright_economics_index_time_ms": 13984.872290981002, "bright_economics_query_time_ms": 2153.466416988522, "bright_biology_ndcg@10": 0.16760567566353757, "bright_biology_recall@100": 0.4227118705401791, "bright_biology_index_time_ms": 15045.92604195932, "bright_biology_query_time_ms": 2120.6849169684574, "beir_fiqa_ndcg@10": 0.23298841118644065, "beir_fiqa_recall@100": 0.5230324930556411, "beir_fiqa_index_time_ms": 32487.164250051137, "beir_fiqa_query_time_ms": 5665.047665999737, "bright_earth_science_ndcg@10": 0.20240715063217235, "bright_earth_science_recall@100": 0.5364866908650938, "bright_earth_science_index_time_ms": 30802.14333400363, "bright_earth_science_query_time_ms": 3586.2573749618605, "bright_stackoverflow_ndcg@10": 0.18068000319081723, "bright_stackoverflow_recall@100": 0.4505892936540938, "bright_stackoverflow_index_time_ms": 98190.62183401547, "bright_stackoverflow_query_time_ms": 7611.653917003423, "beir_trec-covid_ndcg@10": 0.6374486735505434, "beir_trec-covid_recall@100": 0.10737727481594098, "beir_trec-covid_index_time_ms": 132224.70074996818, "beir_trec-covid_query_time_ms": 1341.6291670291685, "avg_ndcg@10": 0.26227180020806407, "avg_recall@100": 0.4423065901160386, "combined_score": 0.4062996321344437, "total_index_time_ms": 367936.0519590555, "total_query_time_ms": 45739.4831670681, "total_time_ms": 413675.5351261236, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Core saturation/length norm\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization: reduce over-penalizing long docs.\n      dl_p: float = 0.75\n  \n      # Soft coordination: prefer docs covering more distinct query terms.\n      coord_alpha: float = 0.18\n  \n      # Query-side clarity gating: downweight very common query terms smoothly.\n      q_clarity_power: float = 0.6\n  \n      # NEW: mild \"soft-AND\" over (idf-weighted) coverage.\n      # Intuition: lexical match is a conjunction of constraints; encourage satisfying\n      # more informative constraints while keeping exponent small to preserve recall@100.\n      cov_and_power: float = 0.12\n  \n      idf_floor: float = 0.0\nwith:\n  class Config:\n      # Core saturation/length norm\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization: reduce over-penalizing long docs.\n      dl_p: float = 0.75\n  \n      # Soft coordination: prefer docs covering more distinct query terms.\n      coord_alpha: float = 0.18\n  \n      # Query-side clarity gating: downweight very common query terms smoothly.\n      q_clarity_power: float = 0.6\n  \n      # Mild \"soft-AND\" over (idf-weighted) coverage.\n      cov_and_power: float = 0.10\n  \n      # NEW: query-adaptive focus prior.\n      # Intuition: when a query is \"specific\" (high avg IDF), relevance is more about\n      # concentrated evidence; for broad queries, length shouldn't be punished much.\n      focus_lambda: float = 0.25\n      focus_idf_pivot: float = 2.2\n  \n      idf_floor: float = 0.0\nChange 2: Replace:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n          return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\nwith:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"\n          Represent the query as unique constraints but keep sublinear repetition as weight.\n          Repetition often signals emphasis; sublinear weighting avoids verbosity blowups.\n          \"\"\"\n          if not tokens:\n              return cls(terms=[], term_weights={})\n          c = Counter(tokens)\n          terms = list(c.keys())\n          term_weights = {t: math.sqrt(float(cnt)) for t, cnt in c.items()}\n          return cls(terms=terms, term_weights=term_weights)\nChange 3: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 evidence + IDF-weighted coverage shaping.\n  \n      Add a *very mild* soft-AND factor over the fraction of (query-weighted, idf-weighted)\n      constraints satisfied. This often improves nDCG@10 for multi-aspect queries without\n      collapsing recall@100 (power kept small).\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      p = Config.dl_p\n  \n      dl = float(doc_length) ** p\n      adl = float(avgdl) ** p if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n  \n      for term in query_repr.terms:\n  ... (32 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 evidence + IDF-weighted coverage shaping + query-adaptive focus prior.\n  \n      New piece: a *query-adaptive* length prior. Only when the query is specific\n      (high avg IDF across its constraints) do we penalize length more strongly\n      to favor focused documents (often improves nDCG@10 on scientific/QA).\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      p = Config.dl_p\n  \n      dl = float(doc_length) ** p\n      adl = float(avgdl) ** p if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n      idf_mass = 0.0\n  \n  ... (38 more lines)\nChange 4: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = w * clarity * idf_val\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  ... (14 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n          idf_mass = 0.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n              idf_mass += idf_val\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = w * clarity * idf_val\n              cov_den += wt\n  ... (23 more lines)\nChange 5: Replace:\n  term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count))\nwith:\n  term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(math.sqrt(float(count)))", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.31596962362149583, "beir_nfcorpus_recall@100": 0.2539303945860142, "beir_nfcorpus_index_time_ms": 3494.0454169991426, "beir_nfcorpus_query_time_ms": 93.75125000951812, "beir_scifact_ndcg@10": 0.674120284003618, "beir_scifact_recall@100": 0.9279999999999999, "beir_scifact_index_time_ms": 4717.806333035696, "beir_scifact_query_time_ms": 337.39695901749656, "bright_pony_ndcg@10": 0.08222344279050578, "bright_pony_recall@100": 0.25291261059128417, "bright_pony_index_time_ms": 1357.4352920404635, "bright_pony_query_time_ms": 434.4529169611633, "beir_arguana_ndcg@10": 0.2830608809293888, "beir_arguana_recall@100": 0.9200571020699501, "beir_arguana_index_time_ms": 5683.799874968827, "beir_arguana_query_time_ms": 18443.011249997653, "bright_theoremqa_theorems_ndcg@10": 0.022577294655450365, "bright_theoremqa_theorems_recall@100": 0.11622807017543858, "bright_theoremqa_theorems_index_time_ms": 10437.288541987073, "bright_theoremqa_theorems_query_time_ms": 803.1503749662079, "beir_scidocs_ndcg@10": 0.14407774209924903, "beir_scidocs_recall@100": 0.3411166666666667, "beir_scidocs_index_time_ms": 19264.455500000622, "beir_scidocs_query_time_ms": 3165.690416994039, "bright_economics_ndcg@10": 0.13755829001113923, "bright_economics_recall@100": 0.41069968499632753, "bright_economics_index_time_ms": 13598.638332972769, "bright_economics_query_time_ms": 2127.5046250084415, "bright_biology_ndcg@10": 0.26832513333270813, "bright_biology_recall@100": 0.5291685462099361, "bright_biology_index_time_ms": 14522.742583008949, "bright_biology_query_time_ms": 2046.362749999389, "beir_fiqa_ndcg@10": 0.21768877521829236, "beir_fiqa_recall@100": 0.5078428986993802, "beir_fiqa_index_time_ms": 33173.80608298117, "beir_fiqa_query_time_ms": 5841.400959005114, "bright_earth_science_ndcg@10": 0.30051315114602495, "bright_earth_science_recall@100": 0.6513542706033632, "bright_earth_science_index_time_ms": 31585.732167004608, "bright_earth_science_query_time_ms": 3669.2152500036173, "bright_stackoverflow_ndcg@10": 0.16655295912991203, "bright_stackoverflow_recall@100": 0.46406110068590906, "bright_stackoverflow_index_time_ms": 98543.127833982, "bright_stackoverflow_query_time_ms": 7586.5466659888625, "beir_trec-covid_ndcg@10": 0.6399514634348474, "beir_trec-covid_recall@100": 0.10708005018847214, "beir_trec-covid_index_time_ms": 131873.8413329702, "beir_trec-covid_query_time_ms": 1421.431416994892, "avg_ndcg@10": 0.27105158669771934, "avg_recall@100": 0.4568709496227285, "combined_score": 0.4197070770377267, "total_index_time_ms": 368252.7192919515, "total_query_time_ms": 45969.914834946394, "total_time_ms": 414222.6341268979, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4197\n- Feature coordinates: \n- Focus areas: - Fitness unchanged at 0.4197\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      # Strong lexical baseline (BM25-like evidence) + robust cross-domain gates.\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization: reduce over-penalizing long docs.\n      dl_p: float = 0.75\n  \n      # Soft coordination: prefer docs covering more distinct query terms.\n      coord_alpha: float = 0.2\n  \n      # Query-side clarity gating: downweight very common query terms smoothly.\n      q_clarity_power: float = 0.6\n  \n      idf_floor: float = 0.0\nwith:\n  class Config:\n      # Core saturation/length norm\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization: reduce over-penalizing long docs.\n      dl_p: float = 0.75\n  \n      # Soft coordination: prefer docs covering more distinct query terms.\n      coord_alpha: float = 0.18\n  \n      # Query-side clarity gating: downweight very common query terms smoothly.\n      q_clarity_power: float = 0.6\n  \n      # NEW: mild \"soft-AND\" over (idf-weighted) coverage.\n      # Intuition: lexical match is a conjunction of constraints; encourage satisfying\n      # more informative constraints while keeping exponent small to preserve recall@100.\n      cov_and_power: float = 0.12\n  \n      idf_floor: float = 0.0\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 evidence + sublinear length norm + soft coordination.\n  \n      Evidence (per term):\n        idf(t) * sat_tf(tf, dl)  with sat_tf = tf / (tf + k1*norm)\n      Query gate:\n        clarity(t) = (idf/(idf+1))^p  in [0,1] reduces noise from common query terms.\n      Coordination:\n        multiply by 1 + alpha * (matched/|q|) to prefer multi-constraint satisfaction.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      p = Config.dl_p\n  \n      # sublinear doc-length normalization\n      dl = float(doc_length) ** p\n      adl = float(avgdl) ** p if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n  ... (20 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 evidence + IDF-weighted coverage shaping.\n  \n      Add a *very mild* soft-AND factor over the fraction of (query-weighted, idf-weighted)\n      constraints satisfied. This often improves nDCG@10 for multi-aspect queries without\n      collapsing recall@100 (power kept small).\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      p = Config.dl_p\n  \n      dl = float(doc_length) ** p\n      adl = float(avgdl) ** p if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n  \n      for term in query_repr.terms:\n  ... (32 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              present = (tf_row > 0.0).astype(np.float64)\n              matched += present\n  \n  ... (7 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = w * clarity * idf_val\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  ... (14 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3160, beir_nfcorpus_recall@100: 0.2539, beir_nfcorpus_index_time_ms: 3494.0454, beir_nfcorpus_query_time_ms: 93.7513, beir_scifact_ndcg@10: 0.6741, beir_scifact_recall@100: 0.9280, beir_scifact_index_time_ms: 4717.8063, beir_scifact_query_time_ms: 337.3970, bright_pony_ndcg@10: 0.0822, bright_pony_recall@100: 0.2529, bright_pony_index_time_ms: 1357.4353, bright_pony_query_time_ms: 434.4529, beir_arguana_ndcg@10: 0.2831, beir_arguana_recall@100: 0.9201, beir_arguana_index_time_ms: 5683.7999, beir_arguana_query_time_ms: 18443.0112, bright_theoremqa_theorems_ndcg@10: 0.0226, bright_theoremqa_theorems_recall@100: 0.1162, bright_theoremqa_theorems_index_time_ms: 10437.2885, bright_theoremqa_theorems_query_time_ms: 803.1504, beir_scidocs_ndcg@10: 0.1441, beir_scidocs_recall@100: 0.3411, beir_scidocs_index_time_ms: 19264.4555, beir_scidocs_query_time_ms: 3165.6904, bright_economics_ndcg@10: 0.1376, bright_economics_recall@100: 0.4107, bright_economics_index_time_ms: 13598.6383, bright_economics_query_time_ms: 2127.5046, bright_biology_ndcg@10: 0.2683, bright_biology_recall@100: 0.5292, bright_biology_index_time_ms: 14522.7426, bright_biology_query_time_ms: 2046.3627, beir_fiqa_ndcg@10: 0.2177, beir_fiqa_recall@100: 0.5078, beir_fiqa_index_time_ms: 33173.8061, beir_fiqa_query_time_ms: 5841.4010, bright_earth_science_ndcg@10: 0.3005, bright_earth_science_recall@100: 0.6514, bright_earth_science_index_time_ms: 31585.7322, bright_earth_science_query_time_ms: 3669.2153, bright_stackoverflow_ndcg@10: 0.1666, bright_stackoverflow_recall@100: 0.4641, bright_stackoverflow_index_time_ms: 98543.1278, bright_stackoverflow_query_time_ms: 7586.5467, beir_trec-covid_ndcg@10: 0.6400, beir_trec-covid_recall@100: 0.1071, beir_trec-covid_index_time_ms: 131873.8413, beir_trec-covid_query_time_ms: 1421.4314, avg_ndcg@10: 0.2711, avg_recall@100: 0.4569, combined_score: 0.4197, total_index_time_ms: 368252.7193, total_query_time_ms: 45969.9148, total_time_ms: 414222.6341, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      # Core saturation / length norm\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Soft AND-pressure\n      coverage_gamma: float = 0.35\n  \n      # Damp overly common query terms\n      common_term_beta: float = 0.6\n  \n      # Prefer \"focused\" evidence but with diminishing penalty for long docs:\n      # use dl^p in the normalization (p<1). This was strong in prior best programs.\n      dl_p: float = 0.75\n  \n      # Pairwise synergy is expensive/noisy on long queries; only enable on short queries.\n      synergy_lambda: float = 0.10\n      synergy_cap: float = 1.0\n      synergy_max_terms: int = 6\nwith:\n  class Config:\n      # Core saturation / length norm\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Prefer \"focused\" evidence but with diminishing penalty for long docs.\n      dl_p: float = 0.75\n  \n      # Soft coordination / coverage pressure (bounded multiplicative boost).\n      coord_alpha: float = 0.2\n  \n      # Query-side clarity gating: downweight very common query terms smoothly.\n      # clarity = (idf/(idf+1))^p in [0,1]\n      q_clarity_power: float = 0.6\n  \n      # Sublinear query TF weighting: qtf^p (keeps repetition signal without verbosity blowups).\n      qtf_power: float = 0.5\nChange 2: Replace:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"\n          Deduplicate to represent a query as a *set* of lexical constraints.\n          Repeated query tokens often come from tokenization artifacts; set-style\n          matching tends to improve early precision across BEIR/BRIGHT mixes.\n          \"\"\"\n          seen = set()\n          uniq = []\n          for t in tokens:\n              if t not in seen:\n                  seen.add(t)\n                  uniq.append(t)\n          return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\nwith:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"\n          Represent query as unique constraints but keep a *sublinear* repetition signal.\n  \n          Motivation: repetition sometimes encodes emphasis (esp. noisy QA queries),\n          but linear qtf can over-weight verbosity. Use qtf^p with p\u22480.5.\n          \"\"\"\n          if not tokens:\n              return cls(terms=[], term_weights={})\n          c = Counter(tokens)\n          terms = list(c.keys())\n          w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n          return cls(terms=terms, term_weights=w)\nChange 3: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      CRSM with sublinear length normalization + guarded synergy.\n  \n      Why this change:\n      - The previous synergy term can be brittle and costly on long queries; gating it to short\n        queries tends to keep nDCG gains without harming recall/time too much.\n      - dl^p normalization was consistently helpful on long-document collections (StackOverflow,\n        scientific corpora) by not over-penalizing long docs.\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      beta = Config.common_term_beta\n      gamma = Config.coverage_gamma\n      lam = Config.synergy_lambda\n  \n      if not query_repr.terms:\n          return 0.0\n  \n      # Sublinear length normalization (must match vectorized path).\n      p_len = Config.dl_p\n      dl = float(doc_length) ** p_len\n      adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  ... (45 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 with sublinear length norm + soft coordination.\n  \n      Rationale:\n      - Term evidence is still BM25-like (saturated TF with length norm).\n      - Query-side clarity(t)=(idf/(idf+1))^p suppresses common query terms smoothly.\n      - Coordination prefers docs satisfying more distinct query constraints (helps nDCG@10).\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n  \n      # Sublinear length normalization (must match vectorized path).\n      p_len = Config.dl_p\n      dl = float(doc_length) ** p_len\n      adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      matched = 0\n      uq = 0\n  ... (25 more lines)\nChange 4: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          beta = Config.common_term_beta\n          gamma = Config.coverage_gamma\n          lam = Config.synergy_lambda\n  \n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          do_synergy = lam > 0.0 and len(query_term_ids) <= Config.synergy_max_terms\n  \n          # For synergy we need to know, per query term, which candidates contain it.\n          present_masks: list[NDArray[np.float64]] = []\n          idf_vals: list[float] = []\n  \n          total = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  ... (34 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          uq = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n              matched += present\n  ... (8 more lines)\nChange 5: Replace:\n  # Represent query as unique constraints (match QueryRepr behavior).\n          term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term in term_counts.keys():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(1.0)\nwith:\n  term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, cnt in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(cnt) ** Config.qtf_power)\nChange 6: Replace:\n  self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          # Precompute df-probability p(t) used for query-side commonness dampening.\n          self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\nwith:\n  self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3198, beir_nfcorpus_recall@100: 0.2558, beir_nfcorpus_index_time_ms: 3488.1835, beir_nfcorpus_query_time_ms: 91.9498, beir_scifact_ndcg@10: 0.6771, beir_scifact_recall@100: 0.9287, beir_scifact_index_time_ms: 4733.7805, beir_scifact_query_time_ms: 337.6735, bright_pony_ndcg@10: 0.1310, bright_pony_recall@100: 0.3323, bright_pony_index_time_ms: 1369.3383, bright_pony_query_time_ms: 427.4225, beir_arguana_ndcg@10: 0.2627, beir_arguana_recall@100: 0.8915, beir_arguana_index_time_ms: 5748.9837, beir_arguana_query_time_ms: 18016.7341, bright_theoremqa_theorems_ndcg@10: 0.0321, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10609.0558, bright_theoremqa_theorems_query_time_ms: 774.0265, beir_scidocs_ndcg@10: 0.1446, beir_scidocs_recall@100: 0.3428, beir_scidocs_index_time_ms: 19303.8815, beir_scidocs_query_time_ms: 3052.6253, bright_economics_ndcg@10: 0.1248, bright_economics_recall@100: 0.3518, bright_economics_index_time_ms: 13451.8813, bright_economics_query_time_ms: 2076.1998, bright_biology_ndcg@10: 0.2612, bright_biology_recall@100: 0.5562, bright_biology_index_time_ms: 14563.6615, bright_biology_query_time_ms: 2032.0730, beir_fiqa_ndcg@10: 0.2188, beir_fiqa_recall@100: 0.5155, beir_fiqa_index_time_ms: 33234.8825, beir_fiqa_query_time_ms: 5637.0935, bright_earth_science_ndcg@10: 0.2817, bright_earth_science_recall@100: 0.6476, bright_earth_science_index_time_ms: 31577.5927, bright_earth_science_query_time_ms: 3604.2909, bright_stackoverflow_ndcg@10: 0.1787, bright_stackoverflow_recall@100: 0.4416, bright_stackoverflow_index_time_ms: 96987.6919, bright_stackoverflow_query_time_ms: 7630.5595, beir_trec-covid_ndcg@10: 0.6539, beir_trec-covid_recall@100: 0.1074, beir_trec-covid_index_time_ms: 131516.3290, beir_trec-covid_query_time_ms: 1664.4579, avg_ndcg@10: 0.2739, avg_recall@100: 0.4604, combined_score: 0.4231, total_index_time_ms: 366585.2621, total_query_time_ms: 45345.1062, total_time_ms: 411930.3683, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      # Coverage-style scoring: per-term concavity + doc length damping\n      k1: float = 0.9   # kept for backwards compatibility (no longer used in core)\n      b: float = 0.4    # kept for backwards compatibility (no longer used in core)\n      epsilon: float = 1e-9\n      dl_alpha: float = 0.25   # how strongly long docs are downweighted\n      tf_log_base: float = 1.0 # log1p(tf/base); base>1 reduces tf impact\nwith:\n  class Config:\n      # Concave evidence + IDF-weighted coverage + query clarity gate (robust across BEIR/BRIGHT).\n      epsilon: float = 1e-9\n  \n      # Evidence aggregation: sum_t wt * log1p(tf/base), then log1p(total)\n      tf_log_base: float = 1.0\n  \n      # Mild verbosity prior (downweight very long docs, but keep it gentle for recall).\n      dl_alpha: float = 0.15\n  \n      # Soft-AND: reward covering more of the *informative* query mass.\n      coverage_gamma: float = 0.25\n  \n      # Downweight very common query terms smoothly: clarity=(idf/(idf+1))^p\n      q_clarity_power: float = 0.6\n  \n      # Sublinear query repetition weighting: count**p (keeps emphasis signal without verbosity blo...\n      qtf_power: float = 0.5\n  \n      # Compatibility leftovers (not used by scoring, but referenced in Corpus)\n      k1: float = 0.9\n      b: float = 0.4\nChange 2: Replace:\n  def idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n      \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n      return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\nwith:\n  def idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n      \"\"\"\n      Surprisal IDF: log1p(N/df).\n  \n      Interprets df/N as an occurrence probability; matching t contributes ~ -log p(t).\n      Smoother than BM25-odds IDF and usually more robust cross-domain.\n      \"\"\"\n      df = np.maximum(df, 1.0)\n      return np.log1p(N / df)\nChange 3: Replace:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n          return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\nwith:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"\n          Treat query as unique constraints but keep sublinear repetition as weight.\n          Helps early precision on verbose QA/scientific queries while staying recall-friendly.\n          \"\"\"\n          if not tokens:\n              return cls(terms=[], term_weights={})\n          c = Counter(tokens)\n          terms = list(c.keys())\n          w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n          return cls(terms=terms, term_weights=w)\nChange 4: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Soft coverage scoring (information-geometric intuition):\n      - Each term provides \"evidence mass\" ~ idf * log1p(tf) (concave TF utility).\n      - The query is satisfied when total evidence accumulates, but with diminishing\n        returns across terms: score = log1p(sum_evidence).\n      - Long documents get mild damping via log length ratio (verbosity prior).\n      \"\"\"\n      eps = Config.epsilon\n      base = Config.tf_log_base\n      sum_evidence = 0.0\n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0.0))\n          if tf <= 0.0:\n              continue\n          df = float(corpus_df.get(term, 1.0))\n          term_idf = float(idf(df, N))\n          w = query_repr.term_weights.get(term, 1.0)\n          sum_evidence += w * term_idf * math.log1p(tf / (base + eps))\n      if sum_evidence <= 0.0:\n          return 0.0\n      length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n      dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n  ... (1 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Concave evidence + IDF-weighted coverage + query clarity gating.\n  \n      Evidence: sum_t [ w_q(t)*idf(t)*clarity(t)*log1p(tf/base) ], then log1p(total).\n      Coverage: multiply by 1 + gamma * (matched_idf_mass / total_idf_mass).\n      Clarity: clarity(t)=(idf/(idf+1))^p suppresses very common query terms.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      eps = Config.epsilon\n      base = Config.tf_log_base\n  \n      sum_evidence = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n  \n      for term in query_repr.terms:\n          df = float(corpus_df.get(term, 1.0))\n          term_idf = float(idf(df, N))\n  \n          rarity = term_idf / (term_idf + 1.0)\n  ... (25 more lines)\nChange 5: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n          eps = Config.epsilon\n          base = Config.tf_log_base\n  \n          sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              sum_evidence += w * idf_val * np.log1p(tf_row / (base + eps))\n  \n          # log1p over total evidence encourages covering more query terms\n          scores = np.log1p(np.maximum(sum_evidence, 0.0))\n  \n          length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n          dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n          return scores / (dl_damp + eps)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          eps = Config.epsilon\n          base = Config.tf_log_base\n  \n          sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = wq * idf_val * clarity\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  ... (14 more lines)\nChange 6: Replace:\n  term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count))\nwith:\n  term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count) ** Config.qtf_power)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3297, beir_nfcorpus_recall@100: 0.2562, beir_nfcorpus_index_time_ms: 3516.4340, beir_nfcorpus_query_time_ms: 97.8597, beir_scifact_ndcg@10: 0.6814, beir_scifact_recall@100: 0.9142, beir_scifact_index_time_ms: 4753.2551, beir_scifact_query_time_ms: 377.3270, bright_pony_ndcg@10: 0.1003, bright_pony_recall@100: 0.2847, bright_pony_index_time_ms: 1403.5357, bright_pony_query_time_ms: 469.8152, beir_arguana_ndcg@10: 0.2899, beir_arguana_recall@100: 0.9193, beir_arguana_index_time_ms: 5718.6107, beir_arguana_query_time_ms: 20707.6621, bright_theoremqa_theorems_ndcg@10: 0.0289, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10574.4110, bright_theoremqa_theorems_query_time_ms: 884.8429, beir_scidocs_ndcg@10: 0.1520, beir_scidocs_recall@100: 0.3542, beir_scidocs_index_time_ms: 19088.1030, beir_scidocs_query_time_ms: 3440.5380, bright_economics_ndcg@10: 0.1493, bright_economics_recall@100: 0.3667, bright_economics_index_time_ms: 13561.9170, bright_economics_query_time_ms: 2392.9295, bright_biology_ndcg@10: 0.2412, bright_biology_recall@100: 0.5169, bright_biology_index_time_ms: 14722.4210, bright_biology_query_time_ms: 2298.1501, beir_fiqa_ndcg@10: 0.2285, beir_fiqa_recall@100: 0.5185, beir_fiqa_index_time_ms: 33409.9769, beir_fiqa_query_time_ms: 6230.5608, bright_earth_science_ndcg@10: 0.3009, bright_earth_science_recall@100: 0.6427, bright_earth_science_index_time_ms: 31559.1523, bright_earth_science_query_time_ms: 4019.3922, bright_stackoverflow_ndcg@10: 0.1936, bright_stackoverflow_recall@100: 0.5094, bright_stackoverflow_index_time_ms: 98262.8195, bright_stackoverflow_query_time_ms: 8716.0757, beir_trec-covid_ndcg@10: 0.7113, beir_trec-covid_recall@100: 0.1163, beir_trec-covid_index_time_ms: 130671.6043, beir_trec-covid_query_time_ms: 1469.6357, avg_ndcg@10: 0.2839, avg_recall@100: 0.4627, combined_score: 0.4270, total_index_time_ms: 367242.2406, total_query_time_ms: 51104.7890, total_time_ms: 418347.0296, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4270)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave-evidence scoring + explicit query-term coverage pressure.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence: idf * log1p(tf/base) then log1p(sum_evidence)\n    tf_log_base: float = 1.0\n\n    # Length prior: prefer focused docs, but keep it mild.\n    dl_alpha: float = 0.15\n\n    # Soft \"AND pressure\" to reward documents that cover more query constraints.\n    coverage_gamma: float = 0.25\n\n    # Query-side common-term gating (bounded in [0,1]).\n    q_clarity_power: float = 0.6\n\n    # New: normalize query-term repetition; verbose queries otherwise overcount easy terms.\n    # weight = count**qtf_power (0.5 ~ sqrt) and (optionally) treat query as unique constraints.\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Self-information IDF: log1p(N/df).\n\n    Rationale: interprets df/N as an occurrence probability; matching a term yields\n    surprisal. This tends to be smoother than BM25-odds IDF across heterogeneous corpora,\n    avoiding very negative/flat weights for frequent terms.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as a set of lexical constraints with sublinear qtf.\n        This usually helps early precision (nDCG@10) on verbose QA/scientific queries\n        while keeping recall robust.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        counts = Counter(tokens)\n        terms = list(counts.keys())  # unique constraints\n        weights = {t: float(c) ** Config.qtf_power for t, c in counts.items()}\n        return cls(terms=terms, term_weights=weights)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave evidence + coverage pressure + query-clarity gating.\n\n    Improvements vs current:\n    - Use unique query constraints (handled in QueryRepr) to stabilize denom/matched.\n    - Keep concavity but make the coverage multiplier depend on *IDF mass covered*,\n      not just a raw count. This rewards satisfying informative constraints.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        # query-side bounded clarity in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = query_repr.term_weights.get(term, 1.0)\n        wt = w * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)  # ~[0,1]\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Match QueryRepr: unique terms + sublinear qtf weights.\n        counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, c in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3297), Performs well on beir_nfcorpus_recall@100 (0.2562), Performs well on beir_nfcorpus_index_time_ms (3713.8193), Performs well on beir_nfcorpus_query_time_ms (101.7529), Performs well on beir_scifact_ndcg@10 (0.6814), Performs well on beir_scifact_recall@100 (0.9142), Performs well on beir_scifact_index_time_ms (5040.8995), Performs well on beir_scifact_query_time_ms (402.5420), Performs well on bright_pony_ndcg@10 (0.1003), Performs well on bright_pony_recall@100 (0.2847), Performs well on bright_pony_index_time_ms (1499.6320), Performs well on bright_pony_query_time_ms (489.7649), Performs well on beir_arguana_ndcg@10 (0.2899), Performs well on beir_arguana_recall@100 (0.9193), Performs well on beir_arguana_index_time_ms (6063.9265), Performs well on beir_arguana_query_time_ms (22360.2629), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0289), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11132.1687), Performs well on bright_theoremqa_theorems_query_time_ms (937.0960), Performs well on beir_scidocs_ndcg@10 (0.1520), Performs well on beir_scidocs_recall@100 (0.3542), Performs well on beir_scidocs_index_time_ms (20489.3998), Performs well on beir_scidocs_query_time_ms (3753.5755), Performs well on bright_economics_ndcg@10 (0.1493), Performs well on bright_economics_recall@100 (0.3667), Performs well on bright_economics_index_time_ms (15249.5355), Performs well on bright_economics_query_time_ms (2662.4329), Performs well on bright_biology_ndcg@10 (0.2412), Performs well on bright_biology_recall@100 (0.5169), Performs well on bright_biology_index_time_ms (17107.5830), Performs well on bright_biology_query_time_ms (2513.0199), Performs well on beir_fiqa_ndcg@10 (0.2285), Performs well on beir_fiqa_recall@100 (0.5185), Performs well on beir_fiqa_index_time_ms (37245.2325), Performs well on beir_fiqa_query_time_ms (6213.1188), Performs well on bright_earth_science_ndcg@10 (0.3009), Performs well on bright_earth_science_recall@100 (0.6427), Performs well on bright_earth_science_index_time_ms (34468.6744), Performs well on bright_earth_science_query_time_ms (4001.5292), Performs well on bright_stackoverflow_ndcg@10 (0.1936), Performs well on bright_stackoverflow_recall@100 (0.5094), Performs well on bright_stackoverflow_index_time_ms (103291.2602), Performs well on bright_stackoverflow_query_time_ms (8714.9653), Performs well on beir_trec-covid_ndcg@10 (0.7113), Performs well on beir_trec-covid_recall@100 (0.1163), Performs well on beir_trec-covid_index_time_ms (139683.6847), Performs well on beir_trec-covid_query_time_ms (1477.9168), Performs well on avg_ndcg@10 (0.2839), Performs well on avg_recall@100 (0.4627), Performs well on combined_score (0.4270), Performs well on total_index_time_ms (394985.8162), Performs well on total_query_time_ms (53627.9770), Performs well on total_time_ms (448613.7932), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4270)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + IDF-weighted coverage + query clarity gate (robust across BEIR/BRIGHT).\n    epsilon: float = 1e-9\n\n    # Evidence aggregation: sum_t wt * log1p(tf/base), then log1p(total)\n    tf_log_base: float = 1.0\n\n    # Mild verbosity prior (downweight very long docs, but keep it gentle for recall).\n    dl_alpha: float = 0.15\n\n    # Soft-AND: reward covering more of the *informative* query mass.\n    coverage_gamma: float = 0.25\n\n    # Downweight very common query terms smoothly: clarity=(idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting: count**p (keeps emphasis signal without verbosity blowups)\n    qtf_power: float = 0.5\n\n    # Compatibility leftovers (not used by scoring, but referenced in Corpus)\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Surprisal IDF: log1p(N/df).\n\n    Interprets df/N as an occurrence probability; matching t contributes ~ -log p(t).\n    Smoother than BM25-odds IDF and usually more robust cross-domain.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Treat query as unique constraints but keep sublinear repetition as weight.\n        Helps early precision on verbose QA/scientific queries while staying recall-friendly.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave evidence + IDF-weighted coverage + query clarity gating.\n\n    Evidence: sum_t [ w_q(t)*idf(t)*clarity(t)*log1p(tf/base) ], then log1p(total).\n    Coverage: multiply by 1 + gamma * (matched_idf_mass / total_idf_mass).\n    Clarity: clarity(t)=(idf/(idf+1))^p suppresses very common query terms.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3297), Performs well on beir_nfcorpus_recall@100 (0.2562), Performs well on beir_nfcorpus_index_time_ms (3516.4340), Performs well on beir_nfcorpus_query_time_ms (97.8597), Performs well on beir_scifact_ndcg@10 (0.6814), Performs well on beir_scifact_recall@100 (0.9142), Performs well on beir_scifact_index_time_ms (4753.2551), Performs well on beir_scifact_query_time_ms (377.3270), Performs well on bright_pony_ndcg@10 (0.1003), Performs well on bright_pony_recall@100 (0.2847), Performs well on bright_pony_index_time_ms (1403.5357), Performs well on bright_pony_query_time_ms (469.8152), Performs well on beir_arguana_ndcg@10 (0.2899), Performs well on beir_arguana_recall@100 (0.9193), Performs well on beir_arguana_index_time_ms (5718.6107), Performs well on beir_arguana_query_time_ms (20707.6621), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0289), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10574.4110), Performs well on bright_theoremqa_theorems_query_time_ms (884.8429), Performs well on beir_scidocs_ndcg@10 (0.1520), Performs well on beir_scidocs_recall@100 (0.3542), Performs well on beir_scidocs_index_time_ms (19088.1030), Performs well on beir_scidocs_query_time_ms (3440.5380), Performs well on bright_economics_ndcg@10 (0.1493), Performs well on bright_economics_recall@100 (0.3667), Performs well on bright_economics_index_time_ms (13561.9170), Performs well on bright_economics_query_time_ms (2392.9295), Performs well on bright_biology_ndcg@10 (0.2412), Performs well on bright_biology_recall@100 (0.5169), Performs well on bright_biology_index_time_ms (14722.4210), Performs well on bright_biology_query_time_ms (2298.1501), Performs well on beir_fiqa_ndcg@10 (0.2285), Performs well on beir_fiqa_recall@100 (0.5185), Performs well on beir_fiqa_index_time_ms (33409.9769), Performs well on beir_fiqa_query_time_ms (6230.5608), Performs well on bright_earth_science_ndcg@10 (0.3009), Performs well on bright_earth_science_recall@100 (0.6427), Performs well on bright_earth_science_index_time_ms (31559.1523), Performs well on bright_earth_science_query_time_ms (4019.3922), Performs well on bright_stackoverflow_ndcg@10 (0.1936), Performs well on bright_stackoverflow_recall@100 (0.5094), Performs well on bright_stackoverflow_index_time_ms (98262.8195), Performs well on bright_stackoverflow_query_time_ms (8716.0757), Performs well on beir_trec-covid_ndcg@10 (0.7113), Performs well on beir_trec-covid_recall@100 (0.1163), Performs well on beir_trec-covid_index_time_ms (130671.6043), Performs well on beir_trec-covid_query_time_ms (1469.6357), Performs well on avg_ndcg@10 (0.2839), Performs well on avg_recall@100 (0.4627), Performs well on combined_score (0.4270), Performs well on total_index_time_ms (367242.2406), Performs well on total_query_time_ms (51104.7890), Performs well on total_time_ms (418347.0296), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4231)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Prefer \"focused\" evidence but with diminishing penalty for long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    # clarity = (idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting: qtf^p (keeps repetition signal without verbosity blowups).\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a *sublinear* repetition signal.\n\n        Motivation: repetition sometimes encodes emphasis (esp. noisy QA queries),\n        but linear qtf can over-weight verbosity. Use qtf^p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 with sublinear length norm + soft coordination.\n\n    Rationale:\n    - Term evidence is still BM25-like (saturated TF with length norm).\n    - Query-side clarity(t)=(idf/(idf+1))^p suppresses common query terms smoothly.\n    - Coordination prefers docs satisfying more distinct query constraints (helps nDCG@10).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (must match vectorized path).\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n    uq = 0\n\n    for term in query_repr.terms:\n        uq += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += (wq * clarity) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (float(uq) + eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3198), Performs well on beir_nfcorpus_recall@100 (0.2558), Performs well on beir_nfcorpus_index_time_ms (3488.1835), Performs well on beir_nfcorpus_query_time_ms (91.9498), Performs well on beir_scifact_ndcg@10 (0.6771), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (4733.7805), Performs well on beir_scifact_query_time_ms (337.6735), Performs well on bright_pony_ndcg@10 (0.1310), Performs well on bright_pony_recall@100 (0.3323), Performs well on bright_pony_index_time_ms (1369.3383), Performs well on bright_pony_query_time_ms (427.4225), Performs well on beir_arguana_ndcg@10 (0.2627), Performs well on beir_arguana_recall@100 (0.8915), Performs well on beir_arguana_index_time_ms (5748.9837), Performs well on beir_arguana_query_time_ms (18016.7341), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0321), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10609.0558), Performs well on bright_theoremqa_theorems_query_time_ms (774.0265), Performs well on beir_scidocs_ndcg@10 (0.1446), Performs well on beir_scidocs_recall@100 (0.3428), Performs well on beir_scidocs_index_time_ms (19303.8815), Performs well on beir_scidocs_query_time_ms (3052.6253), Performs well on bright_economics_ndcg@10 (0.1248), Performs well on bright_economics_recall@100 (0.3518), Performs well on bright_economics_index_time_ms (13451.8813), Performs well on bright_economics_query_time_ms (2076.1998), Performs well on bright_biology_ndcg@10 (0.2612), Performs well on bright_biology_recall@100 (0.5562), Performs well on bright_biology_index_time_ms (14563.6615), Performs well on bright_biology_query_time_ms (2032.0730), Performs well on beir_fiqa_ndcg@10 (0.2188), Performs well on beir_fiqa_recall@100 (0.5155), Performs well on beir_fiqa_index_time_ms (33234.8825), Performs well on beir_fiqa_query_time_ms (5637.0935), Performs well on bright_earth_science_ndcg@10 (0.2817), Performs well on bright_earth_science_recall@100 (0.6476), Performs well on bright_earth_science_index_time_ms (31577.5927), Performs well on bright_earth_science_query_time_ms (3604.2909), Performs well on bright_stackoverflow_ndcg@10 (0.1787), Performs well on bright_stackoverflow_recall@100 (0.4416), Performs well on bright_stackoverflow_index_time_ms (96987.6919), Performs well on bright_stackoverflow_query_time_ms (7630.5595), Performs well on beir_trec-covid_ndcg@10 (0.6539), Performs well on beir_trec-covid_recall@100 (0.1074), Performs well on beir_trec-covid_index_time_ms (131516.3290), Performs well on beir_trec-covid_query_time_ms (1664.4579), Performs well on avg_ndcg@10 (0.2739), Performs well on avg_recall@100 (0.4604), Performs well on combined_score (0.4231), Performs well on total_index_time_ms (366585.2621), Performs well on total_query_time_ms (45345.1062), Performs well on total_time_ms (411930.3683), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4197)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: reduce over-penalizing long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination: prefer docs covering more distinct query terms.\n    coord_alpha: float = 0.18\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # NEW: mild \"soft-AND\" over (idf-weighted) coverage.\n    # Intuition: lexical match is a conjunction of constraints; encourage satisfying\n    # more informative constraints while keeping exponent small to preserve recall@100.\n    cov_and_power: float = 0.12\n\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Self-information (surprisal) IDF: log1p(N/df).\n\n    Interprets df/N as an occurrence probability; matching a term contributes\n    roughly its surprisal. This is smoother than BM25-odds IDF and tends to\n    generalize better across heterogeneous corpora.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 evidence + IDF-weighted coverage shaping.\n\n    Add a *very mild* soft-AND factor over the fraction of (query-weighted, idf-weighted)\n    constraints satisfied. This often improves nDCG@10 for multi-aspect queries without\n    collapsing recall@100 (power kept small).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = float(query_repr.term_weights.get(term, 1.0))\n        wt = w * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # Coverage in [0,1] (approximately): how much informative query mass is satisfied\n    coverage = cov_num / (cov_den + eps) if cov_den > 0.0 else 0.0\n\n    # Keep the old coordination idea but make it \"informativeness-weighted\"\n    score *= 1.0 + Config.coord_alpha * coverage\n\n    # Soft-AND pressure: small exponent to gently separate partial vs full matches\n    if Config.cov_and_power != 0.0:\n        score *= (coverage + eps) ** Config.cov_and_power\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score's sublinear length normalization.\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * clarity * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if cov_den > 0.0:\n            coverage = cov_num / (cov_den + eps)\n            if Config.coord_alpha != 0.0:\n                scores *= 1.0 + Config.coord_alpha * coverage\n            if Config.cov_and_power != 0.0:\n                scores *= np.power(coverage + eps, Config.cov_and_power)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3160), Performs well on beir_nfcorpus_recall@100 (0.2539), Performs well on beir_nfcorpus_index_time_ms (3494.0454), Performs well on beir_nfcorpus_query_time_ms (93.7513), Performs well on beir_scifact_ndcg@10 (0.6741), Performs well on beir_scifact_recall@100 (0.9280), Performs well on beir_scifact_index_time_ms (4717.8063), Performs well on beir_scifact_query_time_ms (337.3970), Performs well on bright_pony_ndcg@10 (0.0822), Performs well on bright_pony_recall@100 (0.2529), Performs well on bright_pony_index_time_ms (1357.4353), Performs well on bright_pony_query_time_ms (434.4529), Performs well on beir_arguana_ndcg@10 (0.2831), Performs well on beir_arguana_recall@100 (0.9201), Performs well on beir_arguana_index_time_ms (5683.7999), Performs well on beir_arguana_query_time_ms (18443.0112), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0226), Performs well on bright_theoremqa_theorems_recall@100 (0.1162), Performs well on bright_theoremqa_theorems_index_time_ms (10437.2885), Performs well on bright_theoremqa_theorems_query_time_ms (803.1504), Performs well on beir_scidocs_ndcg@10 (0.1441), Performs well on beir_scidocs_recall@100 (0.3411), Performs well on beir_scidocs_index_time_ms (19264.4555), Performs well on beir_scidocs_query_time_ms (3165.6904), Performs well on bright_economics_ndcg@10 (0.1376), Performs well on bright_economics_recall@100 (0.4107), Performs well on bright_economics_index_time_ms (13598.6383), Performs well on bright_economics_query_time_ms (2127.5046), Performs well on bright_biology_ndcg@10 (0.2683), Performs well on bright_biology_recall@100 (0.5292), Performs well on bright_biology_index_time_ms (14522.7426), Performs well on bright_biology_query_time_ms (2046.3627), Performs well on beir_fiqa_ndcg@10 (0.2177), Performs well on beir_fiqa_recall@100 (0.5078), Performs well on beir_fiqa_index_time_ms (33173.8061), Performs well on beir_fiqa_query_time_ms (5841.4010), Performs well on bright_earth_science_ndcg@10 (0.3005), Performs well on bright_earth_science_recall@100 (0.6514), Performs well on bright_earth_science_index_time_ms (31585.7322), Performs well on bright_earth_science_query_time_ms (3669.2153), Performs well on bright_stackoverflow_ndcg@10 (0.1666), Performs well on bright_stackoverflow_recall@100 (0.4641), Performs well on bright_stackoverflow_index_time_ms (98543.1278), Performs well on bright_stackoverflow_query_time_ms (7586.5467), Performs well on beir_trec-covid_ndcg@10 (0.6400), Performs well on beir_trec-covid_recall@100 (0.1071), Performs well on beir_trec-covid_index_time_ms (131873.8413), Performs well on beir_trec-covid_query_time_ms (1421.4314), Performs well on avg_ndcg@10 (0.2711), Performs well on avg_recall@100 (0.4569), Performs well on combined_score (0.4197), Performs well on total_index_time_ms (368252.7193), Performs well on total_query_time_ms (45969.9148), Performs well on total_time_ms (414222.6341), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4134)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave-evidence scoring + explicit query-term coverage pressure.\n    # Keep old params for compatibility with Corpus.norm_array etc.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence: idf * log1p(tf/base) then log1p(sum_evidence)\n    tf_log_base: float = 1.0\n\n    # Length prior: prefer focused docs, but keep it mild.\n    dl_alpha: float = 0.15\n\n    # NEW: soft \"AND pressure\" to reward documents that cover more distinct query terms.\n    # Multiplies by (1 + coverage_gamma * frac_matched) in [1, 1+coverage_gamma].\n    coverage_gamma: float = 0.25\n\n    # NEW: query-side common-term gating. Common query tokens are weak constraints.\n    # clarity(idf)= (idf/(idf+1))^q_clarity_power in [0,1].\n    q_clarity_power: float = 0.6\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave evidence + coverage pressure + query-clarity gating.\n\n    Base evidence (concave, robust):  sum_t [ w_q(t) * clarity(t) * idf(t) * log1p(tf/base) ]\n    Then compress across terms: log1p(sum_evidence)  (diminishing returns; encourages coverage)\n    Add explicit coverage factor: (1 + gamma * matched/|q|)  (soft AND; boosts nDCG@10)\n    Finally apply mild length prior: divide by (1 + alpha*log1p(dl/avgdl)).\n    \"\"\"\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    matched = 0\n    denom = 0\n\n    for term in query_repr.terms:\n        denom += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        # bounded \"clarity\" in [0,1]: downweights very common query terms\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = query_repr.term_weights.get(term, 1.0)\n        sum_evidence += (w * clarity) * term_idf * math.log1p(tf / (base + eps))\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if denom > 0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (matched / (float(denom) + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # clarity(idf)= (idf/(idf+1))^p\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            sum_evidence += (w * clarity) * idf_val * np.log1p(tf_row / (base + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        denom = float(len(query_term_ids))\n        if denom > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (matched / (denom + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4145)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm (kept close to BM25 but not tied to it)\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Novel additions:\n    # dl_p compresses document length in the normalization, reducing over-penalty for very long docs\n    # (helps domains like StackOverflow / long scientific docs where relevant evidence is sparse).\n    dl_p: float = 0.75\n\n    # coord_alpha adds a soft coordination factor: reward documents that cover more unique query terms.\n    # This often boosts nDCG@10 without harming recall@100 too much.\n    coord_alpha: float = 0.2\n\n    # idf_floor avoids negative/near-zero idf for extremely common terms when using alternative idf shapes.\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Discriminativity as self-information with a bounded tail.\n\n    Instead of the classic (N-df)/(df) odds, use: -log p(t in doc) = log(N/df).\n    Intuition: each matched term contributes \"surprisal\" bits; common terms carry little evidence.\n\n    The log1p variant is numerically stable and mild for ultra-rare terms.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence-accumulation scoring with two departures from BM25:\n\n    (1) Sublinear length normalization: use dl^p (p<1) to reduce harsh penalties on long docs.\n        This treats \"document size\" more like a growing opportunity for noise, but with diminishing impact.\n\n    (2) Soft coordination bonus: documents matching a larger fraction of unique query terms get a boost.\n        This tends to improve top-10 quality (nDCG) especially for multi-facet queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n    score = 0.0\n    matched = 0\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        matched += 1\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n\n    if not query_repr.terms:\n        return 0.0\n    # coordination factor in [1, 1+alpha]\n    score *= 1.0 + Config.coord_alpha * (matched / (len(set(query_repr.terms)) + eps))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Track how many unique query terms matched each doc for coordination bonus.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        # Soft coordination factor (same as retrieval_score)\n        denom = float(len(query_term_ids))\n        if denom > 0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (denom + eps))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        # Use unique terms only for coordination stability; keep counts as weights.\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4134)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core TF saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # IDF-weighted coordination (coverage of informative query terms).\n    coord_strength: float = 0.25\n\n    # Mild soft-AND pressure on (idf-weighted) coverage. Keep small to protect recall@100.\n    and_power: float = 0.35\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed discriminativity with a bounded dynamic range.\n\n    Intuition: extremely-rare terms can dominate BM25 (especially for noisy queries);\n    using log1p(N/df) keeps the \"information content\" story but avoids huge spikes.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Sublinear-length BM25 + IDF-weighted coverage shaping.\n\n    Key idea: ranking should reflect both (1) term-wise evidence and (2) how many\n    *informative* query constraints are satisfied. We implement (2) as an IDF-weighted\n    coverage ratio, used both as a small linear boost and a mild soft-AND exponent.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    # sublinear BM25 normalization\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    base = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        w = query_repr.term_weights.get(term, 1.0)\n\n        wt = w * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tf_part = tf / (tf + k1 * norm + eps)\n        base += wt * tf_part\n\n    if base <= 0.0:\n        return 0.0\n\n    coverage = cov_num / (cov_den + eps)  # in [0,1] (approximately)\n    base *= (1.0 + Config.coord_strength * coverage)\n    base *= (coverage + eps) ** Config.and_power\n    return base\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        base = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            base += wt * tf_part\n\n        if cov_den <= 0.0:\n            return base\n\n        coverage = cov_num / (cov_den + eps)\n        base *= (1.0 + Config.coord_strength * coverage)\n        base *= np.power(coverage + eps, Config.and_power)\n        return base\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4197)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Strong lexical baseline (BM25-like evidence) + robust cross-domain gates.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: reduce over-penalizing long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination: prefer docs covering more distinct query terms.\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Self-information (surprisal) IDF: log1p(N/df).\n\n    Interprets df/N as an occurrence probability; matching a term contributes\n    roughly its surprisal. This is smoother than BM25-odds IDF and tends to\n    generalize better across heterogeneous corpora.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 evidence + sublinear length norm + soft coordination.\n\n    Evidence (per term):\n      idf(t) * sat_tf(tf, dl)  with sat_tf = tf / (tf + k1*norm)\n    Query gate:\n      clarity(t) = (idf/(idf+1))^p  in [0,1] reduces noise from common query terms.\n    Coordination:\n      multiply by 1 + alpha * (matched/|q|) to prefer multi-constraint satisfaction.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    # sublinear doc-length normalization\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * clarity) * term_idf * tf_part\n\n    score *= 1.0 + Config.coord_alpha * (matched / (len(set(query_repr.terms)) + eps))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score's sublinear length normalization.\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        denom = float(len(query_term_ids))\n        if denom > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (denom + eps))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4145, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm (kept close to BM25 but not tied to it)\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Novel additions:\n    # dl_p compresses document length in the normalization, reducing over-penalty for very long docs\n    # (helps domains like StackOverflow / long scientific docs where relevant evidence is sparse).\n    dl_p: float = 0.75\n\n    # coord_alpha adds a soft coordination factor: reward documents that cover more unique query terms.\n    # This often boosts nDCG@10 without harming recall@100 too much.\n    coord_alpha: float = 0.2\n\n    # idf_floor avoids negative/near-zero idf for extremely common terms when using alternative idf shapes.\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Discriminativity as self-information with a bounded tail.\n\n    Instead of the classic (N-df)/(df) odds, use: -log p(t in doc) = log(N/df).\n    Intuition: each matched term contributes \"surprisal\" bits; common terms carry little evidence.\n\n    The log1p variant is numerically stable and mild for ultra-rare terms.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence-accumulation scoring with two departures from BM25:\n\n    (1) Sublinear length normalization: use dl^p (p<1) to reduce harsh penalties on long docs.\n        This treats \"document size\" more like a growing opportunity for noise, but with diminishing impact.\n\n    (2) Soft coordination bonus: documents matching a larger fraction of unique query terms get a boost.\n        This tends to improve top-10 quality (nDCG) especially for multi-facet queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n    score = 0.0\n    matched = 0\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        matched += 1\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n\n    if not query_repr.terms:\n        return 0.0\n    # coordination factor in [1, 1+alpha]\n    score *= 1.0 + Config.coord_alpha * (matched / (len(set(query_repr.terms)) + eps))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Track how many unique query terms matched each doc for coordination bonus.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        # Soft coordination factor (same as retrieval_score)\n        denom = float(len(query_term_ids))\n        if denom > 0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (denom + eps))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        # Use unique terms only for coordination stability; keep counts as weights.\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4105, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Soft AND-pressure (kept but slightly milder; synergy+coverage can over-boost)\n    coverage_gamma: float = 0.28\n\n    # Damp overly common query terms\n    common_term_beta: float = 0.6\n\n    # Prefer \"focused\" evidence but with diminishing penalty for long docs:\n    dl_p: float = 0.75\n\n    # NEW: self-information clarity gate (downweights very common query terms smoothly).\n    # Uses bounded rarity proxy r=idf/(idf+1) in [0,1], then r^power.\n    q_clarity_power: float = 0.7\n\n    # Pairwise synergy is expensive/noisy on long queries; keep but slightly weaker.\n    synergy_lambda: float = 0.06\n    synergy_cap: float = 1.0\n    synergy_max_terms: int = 6\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Deduplicate to represent a query as a *set* of lexical constraints.\n        Repeated query tokens often come from tokenization artifacts; set-style\n        matching tends to improve early precision across BEIR/BRIGHT mixes.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    CRSM with sublinear length normalization + (idf-based) query-clarity gating + guarded synergy.\n\n    Rationale:\n    - Common-term dampening via p(df) helps, but it is corpus-size sensitive; an IDF-derived\n      bounded \"clarity\" gate is a more stable cross-dataset notion of a query constraint.\n    - Keeping coverage pressure but making it effectively *informativeness-weighted* via clarity\n      tends to improve nDCG@10 without sacrificing recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n    lam = Config.synergy_lambda\n\n    if not query_repr.terms:\n        return 0.0\n\n    # Sublinear length normalization (must match vectorized path).\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    total = 0\n    matched_idf: list[float] = []\n\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0.0))\n\n        df = float(corpus_df.get(term, 1.0))\n        pdoc = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - pdoc) ** beta\n\n        term_idf = float(idf(df, N))\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wt = common_damp * clarity * term_idf\n        cov_den += wt\n\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n        matched_idf.append(term_idf)\n\n    if score <= 0.0:\n        return 0.0\n\n    # Soft AND-pressure but on informative (clarity-weighted) coverage\n    if cov_den > 0.0 and total > 0:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + gamma * coverage)\n\n    # Only apply synergy when the query is short enough (avoids long-query blowups + cost).\n    if (\n        lam > 0.0\n        and len(query_repr.terms) <= Config.synergy_max_terms\n        and len(matched_idf) >= 2\n    ):\n        s = 0.0\n        for i in range(len(matched_idf)):\n            ai = matched_idf[i]\n            for j in range(i + 1, len(matched_idf)):\n                aj = matched_idf[j]\n                s += (ai * aj) / (ai + aj + eps)\n        pairs = len(matched_idf) * (len(matched_idf) - 1) / 2.0\n        s = min(s / (pairs + eps), Config.synergy_cap)\n        score *= (1.0 + lam * s)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        # Precompute df-probability p(t) used for query-side commonness dampening.\n        self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n        lam = Config.synergy_lambda\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        do_synergy = lam > 0.0 and len(query_term_ids) <= Config.synergy_max_terms\n\n        # For synergy we need to know, per query term, which candidates contain it.\n        present_masks: list[NDArray[np.float64]] = []\n        idf_vals: list[float] = []\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            pdoc = float(self.corpus.p_df_array[term_id])\n            common_damp = (1.0 - pdoc) ** beta\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wt = w * common_damp * clarity * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n            if do_synergy:\n                present_masks.append(present)\n                idf_vals.append(idf_val)\n\n        if cov_den > 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + gamma * coverage)\n\n        # Synergy (gated): only for short queries to reduce brittleness + cost.\n        if do_synergy and len(present_masks) >= 2:\n            s = np.zeros(len(candidate_docs), dtype=np.float64)\n            denom = 0.0\n            for i in range(len(present_masks)):\n                ai = idf_vals[i]\n                for j in range(i + 1, len(present_masks)):\n                    aj = idf_vals[j]\n                    pair_w = (ai * aj) / (ai + aj + eps)\n                    s += pair_w * (present_masks[i] * present_masks[j])\n                    denom += pair_w\n            if denom > 0.0:\n                s = np.minimum(s / (denom + eps), Config.synergy_cap)\n                scores *= (1.0 + lam * s)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Represent query as unique constraints (match QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(1.0)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.3408, Type: Exploratory)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core BM25 knobs\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Soft-AND / coverage pressure (multiplicative, bounded)\n    # score *= (coverage) ** and_power\n    and_power: float = 0.6\n\n    # Query-adaptive length prior:\n    # apply a pivoted-length penalty only when the query is \"specific\" (high avg idf).\n    focus_lambda: float = 0.35   # strength of length prior\n    focus_idf_pivot: float = 2.5 # idf level at which focus begins to matter (in log space)\n\n    # Mild compression of extreme IDF (kept, but used on new IDF)\n    idf_power: float = 0.9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Bounded self-information: log(1 + N/df).\n\n    Rationale: keeps the \"surprisal\" interpretation (rare -> informative) but avoids\n    very large spikes for ultra-rare terms that can dominate rankings in noisy queries.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"Keep raw tokens; rank() already aggregates to unique ids for efficiency.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Query-Adaptive Soft-AND BM25 (QASA-BM25)\n\n    1) Base evidence: sum_t idf(t)^p * sat_tf(t,d)\n    2) Soft-AND: multiply by coverage^and_power to prefer satisfying more query constraints.\n    3) Query-adaptive focus prior: only when query is specific (high avg idf), apply a mild\n       pivoted length penalty to prefer focused docs; broad queries shouldn't punish length.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n    sum_idf = 0.0\n\n    seen: set[str] = set()\n    for term in query_repr.terms:\n        if term in seen:\n            continue\n        seen.add(term)\n\n        tf = float(doc_tf.get(term, 0))\n        df = float(corpus_df.get(term, 1))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        sum_idf += tidf\n\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += tidf * tf_part\n\n    uq = float(len(seen))\n    if score <= 0.0 or uq <= 0.0:\n        return 0.0\n\n    # (2) BOUNDED coordination (soft AND-pressure without killing recall):\n    # multiplier in [1, 1+and_power], where and_power acts like \"coord strength\"\n    coverage = matched / (uq + eps)\n    score *= 1.0 + Config.and_power * coverage\n\n    # (3) query-adaptive focus prior\n    avg_idf = sum_idf / (uq + eps)\n    # gate in [0,1], increasing with avg_idf\n    gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n    if gate > 0.0 and avgdl > 0.0:\n        # Use BM25's norm as the length signal (avoids double-penalizing long docs).\n        # Bound the effect so recall@100 doesn't collapse on long-document corpora.\n        pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (norm - 1.0))\n        score *= max(0.2, pivot)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        sum_idf = 0.0\n        uq = float(max(1, len(query_term_ids)))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            sum_idf += idf_val\n            if idf_val <= 0.0:\n                continue\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += idf_val * tf_part\n\n        # Bounded coordination (matches retrieval_score)\n        coverage = matched / (uq + eps)\n        scores *= 1.0 + Config.and_power * coverage\n\n        # Query-adaptive focus prior using BM25 norm (matches retrieval_score)\n        avg_idf = sum_idf / (uq + eps)\n        gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n        if gate > 0.0 and self.corpus.avgdl > 0.0:\n            pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (norms - 1.0))\n            scores *= np.maximum(0.2, pivot)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Use unique query terms (matches QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = None\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4197, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Strong lexical baseline (BM25-like evidence) + robust cross-domain gates.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: reduce over-penalizing long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination: prefer docs covering more distinct query terms.\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Self-information (surprisal) IDF: log1p(N/df).\n\n    Interprets df/N as an occurrence probability; matching a term contributes\n    roughly its surprisal. This is smoother than BM25-odds IDF and tends to\n    generalize better across heterogeneous corpora.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 evidence + sublinear length norm + soft coordination.\n\n    Evidence (per term):\n      idf(t) * sat_tf(tf, dl)  with sat_tf = tf / (tf + k1*norm)\n    Query gate:\n      clarity(t) = (idf/(idf+1))^p  in [0,1] reduces noise from common query terms.\n    Coordination:\n      multiply by 1 + alpha * (matched/|q|) to prefer multi-constraint satisfaction.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    # sublinear doc-length normalization\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * clarity) * term_idf * tf_part\n\n    score *= 1.0 + Config.coord_alpha * (matched / (len(set(query_repr.terms)) + eps))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score's sublinear length normalization.\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        denom = float(len(query_term_ids))\n        if denom > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (denom + eps))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: reduce over-penalizing long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination: prefer docs covering more distinct query terms.\n    coord_alpha: float = 0.18\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # NEW: mild \"soft-AND\" over (idf-weighted) coverage.\n    # Intuition: lexical match is a conjunction of constraints; encourage satisfying\n    # more informative constraints while keeping exponent small to preserve recall@100.\n    cov_and_power: float = 0.12\n\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Self-information (surprisal) IDF: log1p(N/df).\n\n    Interprets df/N as an occurrence probability; matching a term contributes\n    roughly its surprisal. This is smoother than BM25-odds IDF and tends to\n    generalize better across heterogeneous corpora.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 evidence + IDF-weighted coverage shaping.\n\n    Add a *very mild* soft-AND factor over the fraction of (query-weighted, idf-weighted)\n    constraints satisfied. This often improves nDCG@10 for multi-aspect queries without\n    collapsing recall@100 (power kept small).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = float(query_repr.term_weights.get(term, 1.0))\n        wt = w * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # Coverage in [0,1] (approximately): how much informative query mass is satisfied\n    coverage = cov_num / (cov_den + eps) if cov_den > 0.0 else 0.0\n\n    # Keep the old coordination idea but make it \"informativeness-weighted\"\n    score *= 1.0 + Config.coord_alpha * coverage\n\n    # Soft-AND pressure: small exponent to gently separate partial vs full matches\n    if Config.cov_and_power != 0.0:\n        score *= (coverage + eps) ** Config.cov_and_power\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score's sublinear length normalization.\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * clarity * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if cov_den > 0.0:\n            coverage = cov_num / (cov_den + eps)\n            if Config.coord_alpha != 0.0:\n                scores *= 1.0 + Config.coord_alpha * coverage\n            if Config.cov_and_power != 0.0:\n                scores *= np.power(coverage + eps, Config.cov_and_power)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # Core saturation/length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: reduce over-penalizing long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination: prefer docs covering more distinct query terms.\n    coord_alpha: float = 0.18\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # NEW: mild \"soft-AND\" over (idf-weighted) coverage.\n    # Intuition: lexical match is a conjunction of constraints; encourage satisfying\n    # more informative constraints while keeping exponent small to preserve recall@100.\n    cov_and_power: float = 0.12\n\n    idf_floor: float = 0.0\n=======\nclass Config:\n    # Core saturation/length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: reduce over-penalizing long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination: prefer docs covering more distinct query terms.\n    coord_alpha: float = 0.18\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # Mild \"soft-AND\" over (idf-weighted) coverage.\n    cov_and_power: float = 0.10\n\n    # NEW: query-adaptive focus prior.\n    # Intuition: when a query is \"specific\" (high avg IDF), relevance is more about\n    # concentrated evidence; for broad queries, length shouldn't be punished much.\n    focus_lambda: float = 0.25\n    focus_idf_pivot: float = 2.2\n\n    idf_floor: float = 0.0\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n=======\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent the query as unique constraints but keep sublinear repetition as weight.\n        Repetition often signals emphasis; sublinear weighting avoids verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        term_weights = {t: math.sqrt(float(cnt)) for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=term_weights)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 evidence + IDF-weighted coverage shaping.\n\n    Add a *very mild* soft-AND factor over the fraction of (query-weighted, idf-weighted)\n    constraints satisfied. This often improves nDCG@10 for multi-aspect queries without\n    collapsing recall@100 (power kept small).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = float(query_repr.term_weights.get(term, 1.0))\n        wt = w * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # Coverage in [0,1] (approximately): how much informative query mass is satisfied\n    coverage = cov_num / (cov_den + eps) if cov_den > 0.0 else 0.0\n\n    # Keep the old coordination idea but make it \"informativeness-weighted\"\n    score *= 1.0 + Config.coord_alpha * coverage\n\n    # Soft-AND pressure: small exponent to gently separate partial vs full matches\n    if Config.cov_and_power != 0.0:\n        score *= (coverage + eps) ** Config.cov_and_power\n\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 evidence + IDF-weighted coverage shaping + query-adaptive focus prior.\n\n    New piece: a *query-adaptive* length prior. Only when the query is specific\n    (high avg IDF across its constraints) do we penalize length more strongly\n    to favor focused documents (often improves nDCG@10 on scientific/QA).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    idf_mass = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        idf_mass += term_idf\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = float(query_repr.term_weights.get(term, 1.0))\n        wt = w * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    coverage = cov_num / (cov_den + eps) if cov_den > 0.0 else 0.0\n\n    score *= 1.0 + Config.coord_alpha * coverage\n    if Config.cov_and_power != 0.0:\n        score *= (coverage + eps) ** Config.cov_and_power\n\n    # Query-adaptive focus prior (bounded, recall-safe):\n    # if avg_idf is high => apply a mild penalty when norm>1 (longer than average).\n    uq = float(len(query_repr.terms))\n    avg_idf = idf_mass / (uq + eps)\n    gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n    if gate > 0.0:\n        score *= 1.0 / (1.0 + Config.focus_lambda * gate * max(0.0, norm - 1.0))\n\n    return score\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * clarity * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if cov_den > 0.0:\n            coverage = cov_num / (cov_den + eps)\n            if Config.coord_alpha != 0.0:\n                scores *= 1.0 + Config.coord_alpha * coverage\n            if Config.cov_and_power != 0.0:\n                scores *= np.power(coverage + eps, Config.cov_and_power)\n\n        return scores\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        idf_mass = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            idf_mass += idf_val\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * clarity * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if cov_den > 0.0:\n            coverage = cov_num / (cov_den + eps)\n            if Config.coord_alpha != 0.0:\n                scores *= 1.0 + Config.coord_alpha * coverage\n            if Config.cov_and_power != 0.0:\n                scores *= np.power(coverage + eps, Config.cov_and_power)\n\n        # Query-adaptive focus prior (matches retrieval_score)\n        uq = float(max(1, len(query_term_ids)))\n        avg_idf = idf_mass / (uq + eps)\n        gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n        if gate > 0.0:\n            scores *= 1.0 / (1.0 + Config.focus_lambda * gate * np.maximum(0.0, norms - 1.0))\n\n        return scores\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n=======\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(math.sqrt(float(count)))\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}