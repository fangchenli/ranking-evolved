{"id": "82f891d5-269a-481e-a1c0-df96670647cc", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core BM25 knobs\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization exponent (reduces over-penalizing long docs).\n    # This was consistently helpful in the best historical variants.\n    dl_p: float = 0.75\n\n    # Soft-AND / coverage pressure (multiplicative, bounded).\n    # Lower than current: the present 0.6 tends to hurt recall@100 on long/verbose corpora.\n    and_power: float = 0.25\n\n    # Add back a *bounded* coordination boost (helps nDCG@10 without hard AND-ing).\n    coord_alpha: float = 0.20\n\n    # Query-adaptive length prior:\n    # apply a pivoted-length penalty only when the query is \"specific\" (high avg idf).\n    focus_lambda: float = 0.20   # milder to protect recall\n    focus_idf_pivot: float = 2.2 # slightly easier to trigger for short/technical queries\n\n    # Mild compression of extreme IDF (kept, but used on new IDF)\n    idf_power: float = 0.9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Bounded self-information: log(1 + N/df).\n\n    Rationale: keeps the \"surprisal\" interpretation (rare -> informative) but avoids\n    very large spikes for ultra-rare terms that can dominate rankings in noisy queries.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique lexical constraints (set-like).\n        Repeated tokens are usually not meaningful and can destabilize coverage math.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Query-Adaptive Soft-AND BM25 (QASA-BM25), revised for recall safety.\n\n    Changes vs current:\n    - Sublinear length normalization dl^p (p<1): long docs are penalized less harshly.\n    - Weaken soft-AND (smaller and_power) and add a bounded coordination boost:\n        score *= (1 + coord_alpha * coverage)\n      This keeps the \"more query constraints is better\" intuition, but avoids\n      collapsing scores for partial matches (important for recall@100).\n    - Keep the query-adaptive focus prior, but milder.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear-length BM25 norm (matches vectorized path).\n    p = Config.dl_p\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    sum_idf = 0.0\n    uq = float(len(query_repr.terms))\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        sum_idf += tidf\n\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += tidf * tf_part\n\n    if score <= 0.0 or uq <= 0.0:\n        return 0.0\n\n    coverage = matched / (uq + eps)\n\n    # Soft-AND (kept mild) + bounded coordination (recall-safer than exponent alone).\n    if Config.and_power != 0.0:\n        score *= (coverage + eps) ** Config.and_power\n    if Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * coverage\n\n    # Query-adaptive focus prior (milder + uses same query specificity summary as before).\n    avg_idf = sum_idf / (uq + eps)\n    gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n    if gate > 0.0 and avgdl > 0.0:\n        pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (doc_length / (avgdl + eps)))\n        score *= pivot\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        sum_idf = 0.0\n        uq = float(max(1, len(query_term_ids)))\n\n        for term_id in query_term_ids:\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            sum_idf += idf_val\n            if idf_val <= 0.0:\n                continue\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += idf_val * tf_part\n\n        coverage = matched / (uq + eps)\n\n        if Config.and_power != 0.0:\n            scores *= np.power(coverage + eps, Config.and_power)\n        if Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * coverage\n\n        # query-adaptive focus prior (scalar gate shared across candidate docs)\n        avg_idf = sum_idf / (uq + eps)\n        gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n        if gate > 0.0 and self.corpus.avgdl > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (dl / (self.corpus.avgdl + eps)))\n            scores *= pivot\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Use unique query terms (matches QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = None\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "c976098a-5dac-4d68-ad45-e0ae3ad09b9b", "generation": 3, "timestamp": 1770052605.753349, "iteration_found": 21, "metrics": {"beir_nfcorpus_ndcg@10": 0.31173346690693243, "beir_nfcorpus_recall@100": 0.25291502769750507, "beir_nfcorpus_index_time_ms": 3515.3029580251314, "beir_nfcorpus_query_time_ms": 93.99637498427182, "beir_scifact_ndcg@10": 0.6730600712656678, "beir_scifact_recall@100": 0.9085555555555556, "beir_scifact_index_time_ms": 4774.066958983894, "beir_scifact_query_time_ms": 346.94145899266005, "bright_pony_ndcg@10": 0.13557600586326962, "bright_pony_recall@100": 0.31333275134476507, "bright_pony_index_time_ms": 1367.1152080059983, "bright_pony_query_time_ms": 429.0272499783896, "beir_arguana_ndcg@10": 0.22013508242226107, "beir_arguana_recall@100": 0.7930049964311207, "beir_arguana_index_time_ms": 5774.9068749835715, "beir_arguana_query_time_ms": 18189.922958961688, "bright_theoremqa_theorems_ndcg@10": 0.04705101805453375, "bright_theoremqa_theorems_recall@100": 0.1513157894736842, "bright_theoremqa_theorems_index_time_ms": 10496.920707984827, "bright_theoremqa_theorems_query_time_ms": 822.7124999975786, "beir_scidocs_ndcg@10": 0.1428800552286476, "beir_scidocs_recall@100": 0.3300833333333334, "beir_scidocs_index_time_ms": 19203.076665988192, "beir_scidocs_query_time_ms": 3165.827624965459, "bright_economics_ndcg@10": 0.11166649663117778, "bright_economics_recall@100": 0.30362144459832346, "bright_economics_index_time_ms": 13994.348250038456, "bright_economics_query_time_ms": 2156.2956249690615, "bright_biology_ndcg@10": 0.0683576729073087, "bright_biology_recall@100": 0.24044507272662613, "bright_biology_index_time_ms": 14688.307584030554, "bright_biology_query_time_ms": 2066.790500015486, "beir_fiqa_ndcg@10": 0.21305302995585354, "beir_fiqa_recall@100": 0.4944096241549945, "beir_fiqa_index_time_ms": 32773.92750000581, "beir_fiqa_query_time_ms": 5537.175624980591, "bright_earth_science_ndcg@10": 0.09148239511196483, "bright_earth_science_recall@100": 0.27102252267293103, "bright_earth_science_index_time_ms": 31328.61112500541, "bright_earth_science_query_time_ms": 3509.444875002373, "bright_stackoverflow_ndcg@10": 0.1380453360223271, "bright_stackoverflow_recall@100": 0.3720855411539095, "bright_stackoverflow_index_time_ms": 96909.84970796853, "bright_stackoverflow_query_time_ms": 7485.903249995317, "beir_trec-covid_ndcg@10": 0.6261607615201968, "beir_trec-covid_recall@100": 0.09885163048123387, "beir_trec-covid_index_time_ms": 131544.32854201877, "beir_trec-covid_query_time_ms": 1386.6043330053799, "avg_ndcg@10": 0.2316001159908451, "avg_recall@100": 0.3774702741353319, "combined_score": 0.3482962425064346, "total_index_time_ms": 366370.76208303915, "total_query_time_ms": 45190.642375848256, "total_time_ms": 411561.4044588874, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Core BM25 knobs\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Soft-AND / coverage pressure (multiplicative, bounded)\n      # score *= (coverage) ** and_power\n      and_power: float = 0.6\n  \n      # Query-adaptive length prior:\n      # apply a pivoted-length penalty only when the query is \"specific\" (high avg idf).\n      focus_lambda: float = 0.35   # strength of length prior\n      focus_idf_pivot: float = 2.5 # idf level at which focus begins to matter (in log space)\n  \n      # Mild compression of extreme IDF (kept, but used on new IDF)\n      idf_power: float = 0.9\nwith:\n  class Config:\n      # Core BM25 knobs\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization exponent (reduces over-penalizing long docs).\n      # This was consistently helpful in the best historical variants.\n      dl_p: float = 0.75\n  \n      # Soft-AND / coverage pressure (multiplicative, bounded).\n      # Lower than current: the present 0.6 tends to hurt recall@100 on long/verbose corpora.\n      and_power: float = 0.25\n  \n      # Add back a *bounded* coordination boost (helps nDCG@10 without hard AND-ing).\n      coord_alpha: float = 0.20\n  \n      # Query-adaptive length prior:\n      # apply a pivoted-length penalty only when the query is \"specific\" (high avg idf).\n      focus_lambda: float = 0.20   # milder to protect recall\n      focus_idf_pivot: float = 2.2 # slightly easier to trigger for short/technical queries\n  \n      # Mild compression of extreme IDF (kept, but used on new IDF)\n      idf_power: float = 0.9\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Query-Adaptive Soft-AND BM25 (QASA-BM25)\n  \n      1) Base evidence: sum_t idf(t)^p * sat_tf(t,d)\n      2) Soft-AND: multiply by coverage^and_power to prefer satisfying more query constraints.\n      3) Query-adaptive focus prior: only when query is specific (high avg idf), apply a mild\n         pivoted length penalty to prefer focused docs; broad queries shouldn't punish length.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n  \n      score = 0.0\n      matched = 0.0\n      sum_idf = 0.0\n  \n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0))\n          df = float(corpus_df.get(term, 1))\n          tidf = float(idf(df, N))\n  ... (26 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Query-Adaptive Soft-AND BM25 (QASA-BM25), revised for recall safety.\n  \n      Changes vs current:\n      - Sublinear length normalization dl^p (p<1): long docs are penalized less harshly.\n      - Weaken soft-AND (smaller and_power) and add a bounded coordination boost:\n          score *= (1 + coord_alpha * coverage)\n        This keeps the \"more query constraints is better\" intuition, but avoids\n        collapsing scores for partial matches (important for recall@100).\n      - Keep the query-adaptive focus prior, but milder.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n  \n      # Sublinear-length BM25 norm (matches vectorized path).\n      p = Config.dl_p\n      dl = float(doc_length) ** p\n      adl = float(avgdl) ** p if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n  ... (37 more lines)\nChange 3: Replace:\n  self.tf_matrix = csr_matrix(tf_matrix_lil)\n          self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          b = Config.b\n          self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n          self.document_frequency = Counter(\n              {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n          )\nwith:\n  self.tf_matrix = csr_matrix(tf_matrix_lil)\n          self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n  \n          # Must match retrieval_score(): sublinear length normalization (dl^p).\n          b = Config.b\n          p = Config.dl_p\n          dl = np.power(self.doc_lengths, p, dtype=np.float64)\n          adl = float(max(self.avgdl, 1.0)) ** p\n          self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n  \n          self.document_frequency = Counter(\n              {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n          )\nChange 4: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          sum_idf = 0.0\n          uq = float(max(1, len(query_term_ids)))\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              idf_val = max(0.0, idf_val) ** Config.idf_power\n              sum_idf += idf_val\n              if idf_val <= 0.0:\n                  continue\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0).astype(np.float64)\n              matched += present\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n              scores += idf_val * tf_part\n  ... (14 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          sum_idf = 0.0\n          uq = float(max(1, len(query_term_ids)))\n  \n          for term_id in query_term_ids:\n              idf_val = float(self.corpus.idf_array[term_id])\n              idf_val = max(0.0, idf_val) ** Config.idf_power\n              sum_idf += idf_val\n              if idf_val <= 0.0:\n                  continue\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n              matched += present\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n              scores += idf_val * tf_part\n  ... (17 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.30832129164546124, "beir_nfcorpus_recall@100": 0.24915591889559013, "beir_nfcorpus_index_time_ms": 3499.367083015386, "beir_nfcorpus_query_time_ms": 95.9312500199303, "beir_scifact_ndcg@10": 0.6600223653331517, "beir_scifact_recall@100": 0.907888888888889, "beir_scifact_index_time_ms": 4734.822792001069, "beir_scifact_query_time_ms": 337.84929098328575, "bright_pony_ndcg@10": 0.11399542661291143, "bright_pony_recall@100": 0.28217517973192824, "bright_pony_index_time_ms": 1395.4432499594986, "bright_pony_query_time_ms": 430.7205420336686, "beir_arguana_ndcg@10": 0.21607519568968567, "beir_arguana_recall@100": 0.7765881513204854, "beir_arguana_index_time_ms": 5641.656125022564, "beir_arguana_query_time_ms": 18465.33008298138, "bright_theoremqa_theorems_ndcg@10": 0.03277721673426055, "bright_theoremqa_theorems_recall@100": 0.1513157894736842, "bright_theoremqa_theorems_index_time_ms": 11019.155625021085, "bright_theoremqa_theorems_query_time_ms": 790.9339999896474, "beir_scidocs_ndcg@10": 0.1395100507678652, "beir_scidocs_recall@100": 0.32123333333333337, "beir_scidocs_index_time_ms": 19809.477834030986, "beir_scidocs_query_time_ms": 3173.8468339899555, "bright_economics_ndcg@10": 0.0908636413978005, "bright_economics_recall@100": 0.2858569297995543, "bright_economics_index_time_ms": 14187.861707992852, "bright_economics_query_time_ms": 2138.602291990537, "bright_biology_ndcg@10": 0.049084623889056225, "bright_biology_recall@100": 0.19918471496141402, "bright_biology_index_time_ms": 14868.150874972343, "bright_biology_query_time_ms": 2091.536665975582, "beir_fiqa_ndcg@10": 0.1999176143668315, "beir_fiqa_recall@100": 0.4810026821600895, "beir_fiqa_index_time_ms": 33279.84154195292, "beir_fiqa_query_time_ms": 5460.539833002258, "bright_earth_science_ndcg@10": 0.05966736697433172, "bright_earth_science_recall@100": 0.18627899751992308, "bright_earth_science_index_time_ms": 31527.274708030745, "bright_earth_science_query_time_ms": 3567.1538750175387, "bright_stackoverflow_ndcg@10": 0.13042823895385752, "bright_stackoverflow_recall@100": 0.3716509474989769, "bright_stackoverflow_index_time_ms": 97599.98341702158, "bright_stackoverflow_query_time_ms": 7446.274042013101, "beir_trec-covid_ndcg@10": 0.6268518637003169, "beir_trec-covid_recall@100": 0.0950862234105348, "beir_trec-covid_index_time_ms": 129969.70775001682, "beir_trec-covid_query_time_ms": 1380.0201659905724, "avg_ndcg@10": 0.21895957467212754, "avg_recall@100": 0.3589514797495336, "combined_score": 0.33095309873405243, "total_index_time_ms": 367532.74270903785, "total_query_time_ms": 45378.73887398746, "total_time_ms": 412911.4815830253, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.3310\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4134 \u2192 0.3310. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      # Core BM25 knobs (kept)\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Novel add-ons (lightweight, lexical):\n      # 1) pivoted length penalty: softly prefers \"focused\" docs (helps nDCG@10 on verbose corpora)\n      length_pivot: float = 0.35  # 0=no extra penalty\n      # 2) coordination factor: rewards covering more unique query terms (helps early precision)\n      coord_strength: float = 0.6\n      # 3) rarer-term gating: emphasizes rare terms more when query is short (helps theorem/keyword...\n      rare_boost: float = 0.25\nwith:\n  class Config:\n      # Core TF saturation / length norm\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization (reduces over-penalizing long docs).\n      dl_p: float = 0.75\n  \n      # IDF-weighted coordination (coverage of informative query terms).\n      coord_strength: float = 0.25\n  \n      # Mild soft-AND pressure on (idf-weighted) coverage. Keep small to protect recall@100.\n      and_power: float = 0.35\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      BM25 + (A) coordination + (B) pivoted length prior + (C) rare-term gating.\n  \n      A) Coordination: lexical matching behaves more like set-coverage for short queries;\n         rewarding fraction of unique query terms matched tends to improve nDCG@10.\n      B) Pivoted length prior: independent of TF saturation, softly prefers concise docs.\n      C) Rare-term gating: for short queries, rare terms should steer ranking more.\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n  \n      base = 0.0\n      matched = 0\n      uq = 0\n      for term in query_repr.terms:\n          uq += 1\n          tf = float(doc_tf.get(term, 0))\n          if tf <= 0:\n              continue\n          matched += 1\n          df = float(corpus_df.get(term, 1))\n          term_idf = float(idf(df, N))\n  ... (21 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Sublinear-length BM25 + IDF-weighted coverage shaping.\n  \n      Key idea: ranking should reflect both (1) term-wise evidence and (2) how many\n      *informative* query constraints are satisfied. We implement (2) as an IDF-weighted\n      coverage ratio, used both as a small linear boost and a mild soft-AND exponent.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      p = Config.dl_p\n  \n      # sublinear BM25 normalization\n      dl = float(doc_length) ** p\n      adl = float(avgdl) ** p if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      base = 0.0\n      cov_num = 0.0\n      cov_den = 0.0\n  \n  ... (23 more lines)\nChange 3: Replace:\n  self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          b = Config.b\n          self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\nwith:\n  self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n  \n          # Must match retrieval_score(): sublinear length normalization (dl^p).\n          b = Config.b\n          p = Config.dl_p\n          dl = np.power(self.doc_lengths, p, dtype=np.float64)\n          adl = float(max(self.avgdl, 1.0)) ** p\n          self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\nChange 4: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          base = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0:\n                  continue\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0).astype(np.float64)\n              matched += present\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n  \n              # approximate rare-term gating with df via idf_val (monotone proxy)\n              rare = 1.0 + Config.rare_boost * (idf_val / (1.0 + idf_val))\n              base += w * idf_val * tf_part * rare\n  \n          uq = max(1.0, float(len(query_term_ids)))\n  ... (10 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          base = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = w * idf_val\n              cov_den += wt\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n              cov_num += wt * present\n  \n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n  ... (9 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3133, beir_nfcorpus_recall@100: 0.2514, beir_nfcorpus_index_time_ms: 3493.2196, beir_nfcorpus_query_time_ms: 92.8338, beir_scifact_ndcg@10: 0.6725, beir_scifact_recall@100: 0.9247, beir_scifact_index_time_ms: 4810.5135, beir_scifact_query_time_ms: 344.9403, bright_pony_ndcg@10: 0.0978, bright_pony_recall@100: 0.2651, bright_pony_index_time_ms: 1332.8438, bright_pony_query_time_ms: 432.1197, beir_arguana_ndcg@10: 0.2698, beir_arguana_recall@100: 0.9058, beir_arguana_index_time_ms: 5718.4992, beir_arguana_query_time_ms: 18468.7867, bright_theoremqa_theorems_ndcg@10: 0.0181, bright_theoremqa_theorems_recall@100: 0.1162, bright_theoremqa_theorems_index_time_ms: 10439.6349, bright_theoremqa_theorems_query_time_ms: 788.4872, beir_scidocs_ndcg@10: 0.1401, beir_scidocs_recall@100: 0.3337, beir_scidocs_index_time_ms: 19133.2898, beir_scidocs_query_time_ms: 3170.7512, bright_economics_ndcg@10: 0.1285, bright_economics_recall@100: 0.3878, bright_economics_index_time_ms: 13702.9996, bright_economics_query_time_ms: 2150.1150, bright_biology_ndcg@10: 0.2545, bright_biology_recall@100: 0.5425, bright_biology_index_time_ms: 15011.8400, bright_biology_query_time_ms: 2111.7034, beir_fiqa_ndcg@10: 0.2038, beir_fiqa_recall@100: 0.4974, beir_fiqa_index_time_ms: 32265.8082, beir_fiqa_query_time_ms: 5497.3578, bright_earth_science_ndcg@10: 0.2684, bright_earth_science_recall@100: 0.6445, bright_earth_science_index_time_ms: 30932.1339, bright_earth_science_query_time_ms: 3451.3252, bright_stackoverflow_ndcg@10: 0.1657, bright_stackoverflow_recall@100: 0.4440, bright_stackoverflow_index_time_ms: 97133.6350, bright_stackoverflow_query_time_ms: 7477.7710, beir_trec-covid_ndcg@10: 0.6101, beir_trec-covid_recall@100: 0.1026, beir_trec-covid_index_time_ms: 129824.7455, beir_trec-covid_query_time_ms: 1385.6912, avg_ndcg@10: 0.2619, avg_recall@100: 0.4513, combined_score: 0.4134, total_index_time_ms: 363799.1629, total_query_time_ms: 45371.8825, total_time_ms: 409171.0454, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\nwith:\n  class Config:\n      # Core saturation/length norm (kept close to BM25 but not tied to it)\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Novel additions:\n      # dl_p compresses document length in the normalization, reducing over-penalty for very long docs\n      # (helps domains like StackOverflow / long scientific docs where relevant evidence is sparse).\n      dl_p: float = 0.75\n  \n      # coord_alpha adds a soft coordination factor: reward documents that cover more unique query ...\n      # This often boosts nDCG@10 without harming recall@100 too much.\n      coord_alpha: float = 0.2\n  \n      # idf_floor avoids negative/near-zero idf for extremely common terms when using alternative i...\n      idf_floor: float = 0.0\nChange 2: Replace:\n  def idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n      \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n      return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\nwith:\n  def idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n      \"\"\"\n      Discriminativity as self-information with a bounded tail.\n  \n      Instead of the classic (N-df)/(df) odds, use: -log p(t in doc) = log(N/df).\n      Intuition: each matched term contributes \"surprisal\" bits; common terms carry little evidence.\n  \n      The log1p variant is numerically stable and mild for ultra-rare terms.\n      \"\"\"\n      df = np.maximum(df, 1.0)\n      val = np.log1p(N / df)\n      return np.maximum(val, Config.idf_floor)\nChange 3: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Score one document for one query. This is the lexical retrieval method.\n      EVOLVE: design a formulation with deep, fundamental, intuitive justification.\n      Default: Lucene BM25 (IDF \u00d7 saturated TF, length-normalized).\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      score = 0.0\n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0))\n          if tf <= 0:\n              continue\n          df = float(corpus_df.get(term, 1))\n          term_idf = float(idf(df, N))\n          norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n          tf_part = tf / (tf + k1 * norm + eps)\n          w = query_repr.term_weights.get(term, 1.0)\n          score += w * term_idf * tf_part\n      return score\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Evidence-accumulation scoring with two departures from BM25:\n  \n      (1) Sublinear length normalization: use dl^p (p<1) to reduce harsh penalties on long docs.\n          This treats \"document size\" more like a growing opportunity for noise, but with diminishi...\n  \n      (2) Soft coordination bonus: documents matching a larger fraction of unique query terms get a...\n          This tends to improve top-10 quality (nDCG) especially for multi-facet queries.\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      p = Config.dl_p\n      score = 0.0\n      matched = 0\n  \n      dl = float(doc_length) ** p\n      adl = float(avgdl) ** p if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0))\n          if tf <= 0:\n              continue\n  ... (12 more lines)\nChange 4: Replace:\n  self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          b = Config.b\n          self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\nwith:\n  self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          b = Config.b\n          p = Config.dl_p\n          dl = np.power(self.doc_lengths, p, dtype=np.float64)\n          adl = float(max(self.avgdl, 1.0)) ** p\n          self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\nChange 5: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = self.corpus.idf_array[term_id]\n              if idf_val <= 0:\n                  continue\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n              scores += w * idf_val * tf_part\n          return scores\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          # Track how many unique query terms matched each doc for coordination bonus.\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = self.corpus.idf_array[term_id]\n              if idf_val <= 0:\n                  continue\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0).astype(np.float64)\n              matched += present\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n              scores += w * idf_val * tf_part\n  \n          # Soft coordination factor (same as retrieval_score)\n          denom = float(len(query_term_ids))\n          if denom > 0 and Config.coord_alpha != 0.0:\n  ... (2 more lines)\nChange 6: Replace:\n  term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count))\nwith:\n  term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          # Use unique terms only for coordination stability; keep counts as weights.\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count))\n- Metrics: beir_nfcorpus_ndcg@10: 0.3177, beir_nfcorpus_recall@100: 0.2524, beir_nfcorpus_index_time_ms: 3492.1345, beir_nfcorpus_query_time_ms: 92.2577, beir_scifact_ndcg@10: 0.6731, beir_scifact_recall@100: 0.9186, beir_scifact_index_time_ms: 4687.6085, beir_scifact_query_time_ms: 336.0557, bright_pony_ndcg@10: 0.0755, bright_pony_recall@100: 0.2431, bright_pony_index_time_ms: 1337.4504, bright_pony_query_time_ms: 424.8436, beir_arguana_ndcg@10: 0.2735, beir_arguana_recall@100: 0.9143, beir_arguana_index_time_ms: 5695.7273, beir_arguana_query_time_ms: 18194.5031, bright_theoremqa_theorems_ndcg@10: 0.0205, bright_theoremqa_theorems_recall@100: 0.1162, bright_theoremqa_theorems_index_time_ms: 10543.3417, bright_theoremqa_theorems_query_time_ms: 766.3841, beir_scidocs_ndcg@10: 0.1434, beir_scidocs_recall@100: 0.3395, beir_scidocs_index_time_ms: 19196.4587, beir_scidocs_query_time_ms: 3062.7096, bright_economics_ndcg@10: 0.1413, bright_economics_recall@100: 0.3911, bright_economics_index_time_ms: 13557.0267, bright_economics_query_time_ms: 2118.7635, bright_biology_ndcg@10: 0.2702, bright_biology_recall@100: 0.5209, bright_biology_index_time_ms: 14394.6182, bright_biology_query_time_ms: 2018.1063, beir_fiqa_ndcg@10: 0.2098, beir_fiqa_recall@100: 0.4955, beir_fiqa_index_time_ms: 32379.5092, beir_fiqa_query_time_ms: 5402.7507, bright_earth_science_ndcg@10: 0.3006, bright_earth_science_recall@100: 0.6528, bright_earth_science_index_time_ms: 30585.2701, bright_earth_science_query_time_ms: 3474.5707, bright_stackoverflow_ndcg@10: 0.1691, bright_stackoverflow_recall@100: 0.4532, bright_stackoverflow_index_time_ms: 97438.6286, bright_stackoverflow_query_time_ms: 7382.8049, beir_trec-covid_ndcg@10: 0.6568, beir_trec-covid_recall@100: 0.1065, beir_trec-covid_index_time_ms: 129600.8726, beir_trec-covid_query_time_ms: 1311.8423, avg_ndcg@10: 0.2710, avg_recall@100: 0.4503, combined_score: 0.4145, total_index_time_ms: 362908.6465, total_query_time_ms: 44585.5922, total_time_ms: 407494.2387, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      # Core BM25 knobs (kept)\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # New: query-side discriminativity shaping.\n      # Intuition: very rare terms are often brittle (typos, entity variants); very common terms ar...\n      # We softly \"center\" usefulness around mid-rarity by compressing extreme IDFs.\n      idf_power: float = 0.85  # <1 compresses extremes, often helps nDCG across heterogeneous corpora\n  \n      # New: coordination / coverage bonus.\n      # Intuition: lexical relevance is not only sum of independent term matches; matching more dis...\n      # is evidence of topical alignment. This helps short-answer / factoid datasets and reduces sp...\n      coord_weight: float = 0.25  # additive bonus scaled by fraction of unique query terms matched\n  \n      # New: mild extra brevity preference (focus) beyond BM25's b.\n      # Intuition: many collections have long docs; rewarding focused docs can improve precision@n ...\n      focus_power: float = 0.15  # divides by norm^focus_power (very mild)\nwith:\n  class Config:\n      # Core saturation/length norm\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization: penalize long docs less harshly than BM25.\n      # Helps long-document corpora (StackOverflow, SciDocs, TREC-COVID) without giving up much pre...\n      dl_p: float = 0.75\n  \n      # Soft coordination: multiplicatively reward covering more unique query terms.\n      # Multiplicative (not additive) avoids overwhelming lexical evidence for very short queries.\n      coord_alpha: float = 0.2\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      BM25-style evidence + (1) IDF compression + (2) coordination bonus + (3) mild focus preference.\n  \n      Rationale (lexical, non-learned):\n      - IDF compression (idf**p, p<1): reduces over-commitment to ultra-rare terms (often noisy) wh...\n      - Coordination: matching more *distinct* query terms is additional evidence beyond independen...\n      - Focus: slightly prefer concentrated matches in shorter/focused docs (helps precision-heavy ...\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n  \n      score = 0.0\n      matched = 0\n      uq = 0\n      seen = set()\n      for term in query_repr.terms:\n          if term in seen:\n              continue\n          seen.add(term)\n          uq += 1\n          tf = float(doc_tf.get(term, 0))\n          if tf <= 0:\n  ... (15 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Sublinear-Length BM25 + Soft Coordination (SLC-BM25)\n  \n      Two principled departures from standard BM25:\n      (1) Sublinear length normalization: use dl^p (p<1) to reduce over-penalty for long docs.\n          Interpretable as a \"verbosity prior\" with diminishing growth of expected noise.\n      (2) Multiplicative coordination: reward satisfying more unique query constraints without\n          overpowering term evidence (bounded multiplier in [1, 1+coord_alpha]).\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      p = Config.dl_p\n  \n      if not query_repr.terms:\n          return 0.0\n  \n      dl = float(doc_length) ** p\n      adl = float(avgdl) ** p if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      matched = 0\n      uq = 0\n  ... (22 more lines)\nChange 3: Replace:\n  self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          b = Config.b\n          self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\nwith:\n  self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          b = Config.b\n          p = Config.dl_p\n          dl = np.power(self.doc_lengths, p, dtype=np.float64)\n          adl = float(max(self.avgdl, 1.0)) ** p\n          self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\nChange 4: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          # track coordination (distinct term coverage)\n          matched_any = np.zeros(len(candidate_docs), dtype=np.float64)\n          uq = max(1.0, float(len(query_term_ids)))\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              idf_val = max(0.0, idf_val) ** Config.idf_power\n              if idf_val <= 0:\n                  continue\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              has = (tf_row > 0).astype(np.float64)\n              matched_any += has\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n              scores += w * idf_val * tf_part\n  \n          # coordination bonus (bounded [0, coord_weight])\n  ... (5 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n          uq = float(max(1, len(query_term_ids)))\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0.0).astype(np.float64)\n              matched += present\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n              scores += w * idf_val * tf_part\n  \n          if Config.coord_alpha != 0.0:\n              scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n  ... (1 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3180, beir_nfcorpus_recall@100: 0.2537, beir_nfcorpus_index_time_ms: 3522.3415, beir_nfcorpus_query_time_ms: 92.3957, beir_scifact_ndcg@10: 0.6763, beir_scifact_recall@100: 0.9247, beir_scifact_index_time_ms: 4761.3125, beir_scifact_query_time_ms: 337.8489, bright_pony_ndcg@10: 0.0777, bright_pony_recall@100: 0.2446, bright_pony_index_time_ms: 1378.2990, bright_pony_query_time_ms: 427.5426, beir_arguana_ndcg@10: 0.2804, beir_arguana_recall@100: 0.9222, beir_arguana_index_time_ms: 5765.7077, beir_arguana_query_time_ms: 18017.8596, bright_theoremqa_theorems_ndcg@10: 0.0168, bright_theoremqa_theorems_recall@100: 0.1162, bright_theoremqa_theorems_index_time_ms: 10523.8529, bright_theoremqa_theorems_query_time_ms: 764.8276, beir_scidocs_ndcg@10: 0.1438, beir_scidocs_recall@100: 0.3411, beir_scidocs_index_time_ms: 19213.9950, beir_scidocs_query_time_ms: 3054.4751, bright_economics_ndcg@10: 0.1414, bright_economics_recall@100: 0.4033, bright_economics_index_time_ms: 14008.1680, bright_economics_query_time_ms: 2142.3670, bright_biology_ndcg@10: 0.2710, bright_biology_recall@100: 0.5209, bright_biology_index_time_ms: 14881.9058, bright_biology_query_time_ms: 2016.6417, beir_fiqa_ndcg@10: 0.2153, beir_fiqa_recall@100: 0.5112, beir_fiqa_index_time_ms: 32480.7967, beir_fiqa_query_time_ms: 5442.0669, bright_earth_science_ndcg@10: 0.3033, bright_earth_science_recall@100: 0.6528, bright_earth_science_index_time_ms: 30781.1194, bright_earth_science_query_time_ms: 3458.2928, bright_stackoverflow_ndcg@10: 0.1681, bright_stackoverflow_recall@100: 0.4604, bright_stackoverflow_index_time_ms: 97177.1555, bright_stackoverflow_query_time_ms: 7386.3500, beir_trec-covid_ndcg@10: 0.6549, beir_trec-covid_recall@100: 0.1065, beir_trec-covid_index_time_ms: 131178.3508, beir_trec-covid_query_time_ms: 1297.9409, avg_ndcg@10: 0.2722, avg_recall@100: 0.4548, combined_score: 0.4183, total_index_time_ms: 365673.0050, total_query_time_ms: 44438.6089, total_time_ms: 410111.6138, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4197)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # dl_p compresses document length in the normalization.\n    dl_p: float = 0.75\n\n    # coord_alpha: soft coordination (fraction of query terms matched).\n    coord_alpha: float = 0.2\n\n    # New: query-side \"clarity\" weighting.\n    # Idea: treat each query term as a constraint whose utility depends on how specific it is.\n    # Use a bounded rarity proxy r(t)=idf/(idf+1) in [0,1] and raise to a small power.\n    # This reduces noise from very common query words (helps mixed BEIR/BRIGHT).\n    q_clarity_power: float = 0.6\n\n    # Keep as safety for alternative idf shapes.\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Discriminativity as self-information with a bounded tail.\n\n    Instead of the classic (N-df)/(df) odds, use: -log p(t in doc) = log(N/df).\n    Intuition: each matched term contributes \"surprisal\" bits; common terms carry little evidence.\n\n    The log1p variant is numerically stable and mild for ultra-rare terms.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation + *query-clarity weighting*.\n\n    Keep the strong BM25-like core, but weight each query term by a bounded\n    \"constraint clarity\" factor derived from IDF:\n        clarity(t) = (idf(t)/(idf(t)+1))^p  in [0,1]\n\n    Intuition:\n    - Very common query terms are weak constraints and create cross-domain noise.\n    - Very rare terms still get high IDF in the main evidence; clarity is bounded\n      so we don't explode scores, but we do gate common terms down.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n    score = 0.0\n    matched = 0\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        # bounded rarity proxy in [0,1], then a mild power to adjust sharpness\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * clarity) * term_idf * tf_part\n\n    if not query_repr.terms:\n        return 0.0\n\n    # coordination factor in [1, 1+alpha]\n    score *= 1.0 + Config.coord_alpha * (matched / (len(set(query_repr.terms)) + eps))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # clarity(t) = (idf/(idf+1))^p\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        denom = float(len(query_term_ids))\n        if denom > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (denom + eps))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        # Use unique terms only for coordination stability; keep counts as weights.\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3182), Performs well on beir_nfcorpus_recall@100 (0.2539), Performs well on beir_nfcorpus_index_time_ms (3501.7539), Performs well on beir_nfcorpus_query_time_ms (92.0034), Performs well on beir_scifact_ndcg@10 (0.6773), Performs well on beir_scifact_recall@100 (0.9280), Performs well on beir_scifact_index_time_ms (4762.4463), Performs well on beir_scifact_query_time_ms (335.5325), Performs well on bright_pony_ndcg@10 (0.0779), Performs well on bright_pony_recall@100 (0.2457), Performs well on bright_pony_index_time_ms (1354.9719), Performs well on bright_pony_query_time_ms (426.9018), Performs well on beir_arguana_ndcg@10 (0.2815), Performs well on beir_arguana_recall@100 (0.9201), Performs well on beir_arguana_index_time_ms (5737.4876), Performs well on beir_arguana_query_time_ms (18191.9293), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0209), Performs well on bright_theoremqa_theorems_recall@100 (0.1162), Performs well on bright_theoremqa_theorems_index_time_ms (10438.3994), Performs well on bright_theoremqa_theorems_query_time_ms (736.4401), Performs well on beir_scidocs_ndcg@10 (0.1449), Performs well on beir_scidocs_recall@100 (0.3427), Performs well on beir_scidocs_index_time_ms (19214.3510), Performs well on beir_scidocs_query_time_ms (3071.1973), Performs well on bright_economics_ndcg@10 (0.1433), Performs well on bright_economics_recall@100 (0.4131), Performs well on bright_economics_index_time_ms (13571.0581), Performs well on bright_economics_query_time_ms (2125.4366), Performs well on bright_biology_ndcg@10 (0.2682), Performs well on bright_biology_recall@100 (0.5181), Performs well on bright_biology_index_time_ms (14707.7685), Performs well on bright_biology_query_time_ms (2060.0544), Performs well on beir_fiqa_ndcg@10 (0.2164), Performs well on beir_fiqa_recall@100 (0.5111), Performs well on beir_fiqa_index_time_ms (32048.1327), Performs well on beir_fiqa_query_time_ms (5557.0825), Performs well on bright_earth_science_ndcg@10 (0.3071), Performs well on bright_earth_science_recall@100 (0.6485), Performs well on bright_earth_science_index_time_ms (30166.8769), Performs well on bright_earth_science_query_time_ms (3480.8247), Performs well on bright_stackoverflow_ndcg@10 (0.1703), Performs well on bright_stackoverflow_recall@100 (0.4682), Performs well on bright_stackoverflow_index_time_ms (97049.3410), Performs well on bright_stackoverflow_query_time_ms (7312.7749), Performs well on beir_trec-covid_ndcg@10 (0.6579), Performs well on beir_trec-covid_recall@100 (0.1084), Performs well on beir_trec-covid_index_time_ms (130071.3926), Performs well on beir_trec-covid_query_time_ms (1284.2068), Performs well on avg_ndcg@10 (0.2737), Performs well on avg_recall@100 (0.4562), Performs well on combined_score (0.4197), Performs well on total_index_time_ms (362623.9800), Performs well on total_query_time_ms (44674.3844), Performs well on total_time_ms (407298.3644), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4183)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: penalize long docs less harshly than BM25.\n    # Helps long-document corpora (StackOverflow, SciDocs, TREC-COVID) without giving up much precision.\n    dl_p: float = 0.75\n\n    # Soft coordination: multiplicatively reward covering more unique query terms.\n    # Multiplicative (not additive) avoids overwhelming lexical evidence for very short queries.\n    coord_alpha: float = 0.2\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Sublinear-Length BM25 + Soft Coordination (SLC-BM25)\n\n    Two principled departures from standard BM25:\n    (1) Sublinear length normalization: use dl^p (p<1) to reduce over-penalty for long docs.\n        Interpretable as a \"verbosity prior\" with diminishing growth of expected noise.\n    (2) Multiplicative coordination: reward satisfying more unique query constraints without\n        overpowering term evidence (bounded multiplier in [1, 1+coord_alpha]).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    if not query_repr.terms:\n        return 0.0\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n    uq = 0\n    seen: set[str] = set()\n\n    for term in query_repr.terms:\n        if term in seen:\n            continue\n        seen.add(term)\n        uq += 1\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n\n    if uq > 0 and matched > 0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (float(uq) + eps))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        uq = float(max(1, len(query_term_ids)))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3180), Performs well on beir_nfcorpus_recall@100 (0.2537), Performs well on beir_nfcorpus_index_time_ms (3522.3415), Performs well on beir_nfcorpus_query_time_ms (92.3957), Performs well on beir_scifact_ndcg@10 (0.6763), Performs well on beir_scifact_recall@100 (0.9247), Performs well on beir_scifact_index_time_ms (4761.3125), Performs well on beir_scifact_query_time_ms (337.8489), Performs well on bright_pony_ndcg@10 (0.0777), Performs well on bright_pony_recall@100 (0.2446), Performs well on bright_pony_index_time_ms (1378.2990), Performs well on bright_pony_query_time_ms (427.5426), Performs well on beir_arguana_ndcg@10 (0.2804), Performs well on beir_arguana_recall@100 (0.9222), Performs well on beir_arguana_index_time_ms (5765.7077), Performs well on beir_arguana_query_time_ms (18017.8596), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0168), Performs well on bright_theoremqa_theorems_recall@100 (0.1162), Performs well on bright_theoremqa_theorems_index_time_ms (10523.8529), Performs well on bright_theoremqa_theorems_query_time_ms (764.8276), Performs well on beir_scidocs_ndcg@10 (0.1438), Performs well on beir_scidocs_recall@100 (0.3411), Performs well on beir_scidocs_index_time_ms (19213.9950), Performs well on beir_scidocs_query_time_ms (3054.4751), Performs well on bright_economics_ndcg@10 (0.1414), Performs well on bright_economics_recall@100 (0.4033), Performs well on bright_economics_index_time_ms (14008.1680), Performs well on bright_economics_query_time_ms (2142.3670), Performs well on bright_biology_ndcg@10 (0.2710), Performs well on bright_biology_recall@100 (0.5209), Performs well on bright_biology_index_time_ms (14881.9058), Performs well on bright_biology_query_time_ms (2016.6417), Performs well on beir_fiqa_ndcg@10 (0.2153), Performs well on beir_fiqa_recall@100 (0.5112), Performs well on beir_fiqa_index_time_ms (32480.7967), Performs well on beir_fiqa_query_time_ms (5442.0669), Performs well on bright_earth_science_ndcg@10 (0.3033), Performs well on bright_earth_science_recall@100 (0.6528), Performs well on bright_earth_science_index_time_ms (30781.1194), Performs well on bright_earth_science_query_time_ms (3458.2928), Performs well on bright_stackoverflow_ndcg@10 (0.1681), Performs well on bright_stackoverflow_recall@100 (0.4604), Performs well on bright_stackoverflow_index_time_ms (97177.1555), Performs well on bright_stackoverflow_query_time_ms (7386.3500), Performs well on beir_trec-covid_ndcg@10 (0.6549), Performs well on beir_trec-covid_recall@100 (0.1065), Performs well on beir_trec-covid_index_time_ms (131178.3508), Performs well on beir_trec-covid_query_time_ms (1297.9409), Performs well on avg_ndcg@10 (0.2722), Performs well on avg_recall@100 (0.4548), Performs well on combined_score (0.4183), Performs well on total_index_time_ms (365673.0050), Performs well on total_query_time_ms (44438.6089), Performs well on total_time_ms (410111.6138), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4145)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm (kept close to BM25 but not tied to it)\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Novel additions:\n    # dl_p compresses document length in the normalization, reducing over-penalty for very long docs\n    # (helps domains like StackOverflow / long scientific docs where relevant evidence is sparse).\n    dl_p: float = 0.75\n\n    # coord_alpha adds a soft coordination factor: reward documents that cover more unique query terms.\n    # This often boosts nDCG@10 without harming recall@100 too much.\n    coord_alpha: float = 0.2\n\n    # idf_floor avoids negative/near-zero idf for extremely common terms when using alternative idf shapes.\n    idf_floor: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Discriminativity as self-information with a bounded tail.\n\n    Instead of the classic (N-df)/(df) odds, use: -log p(t in doc) = log(N/df).\n    Intuition: each matched term contributes \"surprisal\" bits; common terms carry little evidence.\n\n    The log1p variant is numerically stable and mild for ultra-rare terms.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    val = np.log1p(N / df)\n    return np.maximum(val, Config.idf_floor)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence-accumulation scoring with two departures from BM25:\n\n    (1) Sublinear length normalization: use dl^p (p<1) to reduce harsh penalties on long docs.\n        This treats \"document size\" more like a growing opportunity for noise, but with diminishing impact.\n\n    (2) Soft coordination bonus: documents matching a larger fraction of unique query terms get a boost.\n        This tends to improve top-10 quality (nDCG) especially for multi-facet queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n    score = 0.0\n    matched = 0\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        matched += 1\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n\n    if not query_repr.terms:\n        return 0.0\n    # coordination factor in [1, 1+alpha]\n    score *= 1.0 + Config.coord_alpha * (matched / (len(set(query_repr.terms)) + eps))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Track how many unique query terms matched each doc for coordination bonus.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        # Soft coordination factor (same as retrieval_score)\n        denom = float(len(query_term_ids))\n        if denom > 0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (denom + eps))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        # Use unique terms only for coordination stability; keep counts as weights.\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3177), Performs well on beir_nfcorpus_recall@100 (0.2524), Performs well on beir_nfcorpus_index_time_ms (3492.1345), Performs well on beir_nfcorpus_query_time_ms (92.2577), Performs well on beir_scifact_ndcg@10 (0.6731), Performs well on beir_scifact_recall@100 (0.9186), Performs well on beir_scifact_index_time_ms (4687.6085), Performs well on beir_scifact_query_time_ms (336.0557), Performs well on bright_pony_ndcg@10 (0.0755), Performs well on bright_pony_recall@100 (0.2431), Performs well on bright_pony_index_time_ms (1337.4504), Performs well on bright_pony_query_time_ms (424.8436), Performs well on beir_arguana_ndcg@10 (0.2735), Performs well on beir_arguana_recall@100 (0.9143), Performs well on beir_arguana_index_time_ms (5695.7273), Performs well on beir_arguana_query_time_ms (18194.5031), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0205), Performs well on bright_theoremqa_theorems_recall@100 (0.1162), Performs well on bright_theoremqa_theorems_index_time_ms (10543.3417), Performs well on bright_theoremqa_theorems_query_time_ms (766.3841), Performs well on beir_scidocs_ndcg@10 (0.1434), Performs well on beir_scidocs_recall@100 (0.3395), Performs well on beir_scidocs_index_time_ms (19196.4587), Performs well on beir_scidocs_query_time_ms (3062.7096), Performs well on bright_economics_ndcg@10 (0.1413), Performs well on bright_economics_recall@100 (0.3911), Performs well on bright_economics_index_time_ms (13557.0267), Performs well on bright_economics_query_time_ms (2118.7635), Performs well on bright_biology_ndcg@10 (0.2702), Performs well on bright_biology_recall@100 (0.5209), Performs well on bright_biology_index_time_ms (14394.6182), Performs well on bright_biology_query_time_ms (2018.1063), Performs well on beir_fiqa_ndcg@10 (0.2098), Performs well on beir_fiqa_recall@100 (0.4955), Performs well on beir_fiqa_index_time_ms (32379.5092), Performs well on beir_fiqa_query_time_ms (5402.7507), Performs well on bright_earth_science_ndcg@10 (0.3006), Performs well on bright_earth_science_recall@100 (0.6528), Performs well on bright_earth_science_index_time_ms (30585.2701), Performs well on bright_earth_science_query_time_ms (3474.5707), Performs well on bright_stackoverflow_ndcg@10 (0.1691), Performs well on bright_stackoverflow_recall@100 (0.4532), Performs well on bright_stackoverflow_index_time_ms (97438.6286), Performs well on bright_stackoverflow_query_time_ms (7382.8049), Performs well on beir_trec-covid_ndcg@10 (0.6568), Performs well on beir_trec-covid_recall@100 (0.1065), Performs well on beir_trec-covid_index_time_ms (129600.8726), Performs well on beir_trec-covid_query_time_ms (1311.8423), Performs well on avg_ndcg@10 (0.2710), Performs well on avg_recall@100 (0.4503), Performs well on combined_score (0.4145), Performs well on total_index_time_ms (362908.6465), Performs well on total_query_time_ms (44585.5922), Performs well on total_time_ms (407494.2387), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4134)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core TF saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # IDF-weighted coordination (coverage of informative query terms).\n    coord_strength: float = 0.25\n\n    # Mild soft-AND pressure on (idf-weighted) coverage. Keep small to protect recall@100.\n    and_power: float = 0.35\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed discriminativity with a bounded dynamic range.\n\n    Intuition: extremely-rare terms can dominate BM25 (especially for noisy queries);\n    using log1p(N/df) keeps the \"information content\" story but avoids huge spikes.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Sublinear-length BM25 + IDF-weighted coverage shaping.\n\n    Key idea: ranking should reflect both (1) term-wise evidence and (2) how many\n    *informative* query constraints are satisfied. We implement (2) as an IDF-weighted\n    coverage ratio, used both as a small linear boost and a mild soft-AND exponent.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    # sublinear BM25 normalization\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    base = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        w = query_repr.term_weights.get(term, 1.0)\n\n        wt = w * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tf_part = tf / (tf + k1 * norm + eps)\n        base += wt * tf_part\n\n    if base <= 0.0:\n        return 0.0\n\n    coverage = cov_num / (cov_den + eps)  # in [0,1] (approximately)\n    base *= (1.0 + Config.coord_strength * coverage)\n    base *= (coverage + eps) ** Config.and_power\n    return base\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        base = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            base += wt * tf_part\n\n        if cov_den <= 0.0:\n            return base\n\n        coverage = cov_num / (cov_den + eps)\n        base *= (1.0 + Config.coord_strength * coverage)\n        base *= np.power(coverage + eps, Config.and_power)\n        return base\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3133), Performs well on beir_nfcorpus_recall@100 (0.2514), Performs well on beir_nfcorpus_index_time_ms (3493.2196), Performs well on beir_nfcorpus_query_time_ms (92.8338), Performs well on beir_scifact_ndcg@10 (0.6725), Performs well on beir_scifact_recall@100 (0.9247), Performs well on beir_scifact_index_time_ms (4810.5135), Performs well on beir_scifact_query_time_ms (344.9403), Performs well on bright_pony_ndcg@10 (0.0978), Performs well on bright_pony_recall@100 (0.2651), Performs well on bright_pony_index_time_ms (1332.8438), Performs well on bright_pony_query_time_ms (432.1197), Performs well on beir_arguana_ndcg@10 (0.2698), Performs well on beir_arguana_recall@100 (0.9058), Performs well on beir_arguana_index_time_ms (5718.4992), Performs well on beir_arguana_query_time_ms (18468.7867), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0181), Performs well on bright_theoremqa_theorems_recall@100 (0.1162), Performs well on bright_theoremqa_theorems_index_time_ms (10439.6349), Performs well on bright_theoremqa_theorems_query_time_ms (788.4872), Performs well on beir_scidocs_ndcg@10 (0.1401), Performs well on beir_scidocs_recall@100 (0.3337), Performs well on beir_scidocs_index_time_ms (19133.2898), Performs well on beir_scidocs_query_time_ms (3170.7512), Performs well on bright_economics_ndcg@10 (0.1285), Performs well on bright_economics_recall@100 (0.3878), Performs well on bright_economics_index_time_ms (13702.9996), Performs well on bright_economics_query_time_ms (2150.1150), Performs well on bright_biology_ndcg@10 (0.2545), Performs well on bright_biology_recall@100 (0.5425), Performs well on bright_biology_index_time_ms (15011.8400), Performs well on bright_biology_query_time_ms (2111.7034), Performs well on beir_fiqa_ndcg@10 (0.2038), Performs well on beir_fiqa_recall@100 (0.4974), Performs well on beir_fiqa_index_time_ms (32265.8082), Performs well on beir_fiqa_query_time_ms (5497.3578), Performs well on bright_earth_science_ndcg@10 (0.2684), Performs well on bright_earth_science_recall@100 (0.6445), Performs well on bright_earth_science_index_time_ms (30932.1339), Performs well on bright_earth_science_query_time_ms (3451.3252), Performs well on bright_stackoverflow_ndcg@10 (0.1657), Performs well on bright_stackoverflow_recall@100 (0.4440), Performs well on bright_stackoverflow_index_time_ms (97133.6350), Performs well on bright_stackoverflow_query_time_ms (7477.7710), Performs well on beir_trec-covid_ndcg@10 (0.6101), Performs well on beir_trec-covid_recall@100 (0.1026), Performs well on beir_trec-covid_index_time_ms (129824.7455), Performs well on beir_trec-covid_query_time_ms (1385.6912), Performs well on avg_ndcg@10 (0.2619), Performs well on avg_recall@100 (0.4513), Performs well on combined_score (0.4134), Performs well on total_index_time_ms (363799.1629), Performs well on total_query_time_ms (45371.8825), Performs well on total_time_ms (409171.0454), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4081)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Soft \"AND pressure\": prefer docs covering more distinct query terms.\n    # Kept tiny + bounded to avoid hurting recall@100.\n    coord_gamma: float = 0.35\n\n    # Query-side common-term dampening: downweight terms that appear in many docs.\n    # Interpretable as \"weak constraints contribute less evidence\".\n    common_beta: float = 0.6\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Treat queries as a set of lexical constraints: deduplicate tokens.\n        This typically improves early precision (nDCG@10) when tokenization\n        yields repeats (especially in QA-style queries).\n        \"\"\"\n        seen = set()\n        uniq: list[str] = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Coverage-Regularized Surprisal Matching (CRSM)\n\n    Base evidence: sum_t  w_q(t) * surprisal_idf(t) * sat_tf(t,d)\n    Then apply:\n      (1) query-side common-term dampening  (1 - p(t))^beta, p\u2248df/N\n      (2) soft coordination boost          1 + gamma * (matched/|q|)\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_beta\n    gamma = Config.coord_gamma\n\n    if not query_repr.terms:\n        return 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    total = 0\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0.0))\n        df = float(corpus_df.get(term, 1.0))\n\n        # query-side: common terms are weak constraints, so contribute less\n        p = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p) ** beta\n\n        if tf <= 0.0:\n            continue\n        matched += 1\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * common_damp) * term_idf * tf_part\n\n    if total > 0 and matched > 0:\n        score *= (1.0 + gamma * (matched / float(total)))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        # Cached df-probabilities for query-side commonness dampening.\n        self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_beta\n        gamma = Config.coord_gamma\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            p = float(self.corpus.p_df_array[term_id])\n            common_damp = (1.0 - p) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n        if total > 0.0:\n            scores *= (1.0 + gamma * (matched / total))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Use unique query terms to match QueryRepr behavior (set of constraints).\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(1.0)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4088)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep BM25 params (strong baseline), but add a principled \"coverage prior\".\n    # Intuition: queries are often multi-aspect; docs matching more distinct query terms\n    # should be preferred even if one term is very frequent.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Coverage prior strength. 0 => pure BM25. Small >0 tends to help nDCG@10.\n    coverage_lambda: float = 0.25\n\n    # How much we de-emphasize repeated query terms (robust across verbose queries).\n    # weight = (qtf)^qtf_power, so 0.5 ~ sqrt.\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query weighting:\n        - Dedup terms for scoring stability (prevents long queries over-counting).\n        - Use sublinear qtf weighting (power law) rather than linear repetition.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        counts = Counter(tokens)\n        terms = list(counts.keys())\n        w = {t: float(c) ** Config.qtf_power for t, c in counts.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    BM25 + probabilistic coverage prior (novel additive log-prob term).\n\n    View BM25 as an additive evidence score. Add a *coverage prior* that prefers\n    documents matching many distinct query terms:\n        coverage = sum_t idf(t) * 1[tf(t)>0]\n        score = BM25 + \u03bb * log1p(coverage)\n\n    Why log1p? Diminishing returns: going from 0->1 matched aspects matters more\n    than 8->9. Why IDF-weighted coverage? Matching rare terms is stronger evidence\n    of covering a query aspect than matching common terms.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    bm25 = 0.0\n    coverage = 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        w = query_repr.term_weights.get(term, 1.0)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        bm25 += w * term_idf * tf_part\n\n        # IDF-weighted binary match mass (query weight included).\n        coverage += w * term_idf\n\n    if coverage > 0.0 and Config.coverage_lambda > 0.0:\n        bm25 += Config.coverage_lambda * math.log1p(coverage)\n\n    return bm25\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4081)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm (kept close to BM25 but not identical usage below)\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Novel: \"coverage-pressure\" encourages matching more distinct query terms\n    # (helps nDCG@10 on verbose scientific/QA queries where partial matches are common).\n    coverage_gamma: float = 0.35\n\n    # Novel: damp overly common query terms (approx. \"informativeness\" prior).\n    # When df/N is high, weight is reduced smoothly.\n    common_term_beta: float = 0.6\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Deduplicate to represent a query as a *set* of lexical constraints.\n        Repeated query tokens often come from tokenization artifacts; set-style\n        matching tends to improve early precision across BEIR/BRIGHT mixes.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Coverage-Regularized Surprisal Matching (CRSM)\n\n    Base evidence: sum_t  w_q(t) * IDF_surprisal(t) * sat_tf(t,d)\n    Then apply a *soft coverage* boost encouraging documents that satisfy\n    more distinct query constraints.\n\n    Intuition:\n    - The term-wise sum captures independent lexical evidence.\n    - The coverage factor acts like an \"AND-pressure\": among similar scores,\n      prefer docs that hit more query terms (often boosts nDCG@10).\n    - Additionally, downweight extremely common terms *in the query* because\n      they are weak constraints and tend to add noise across diverse datasets.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n\n    if not query_repr.terms:\n        return 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    total = 0\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0))\n        df = float(corpus_df.get(term, 1))\n        # Query-side commonness dampening: (1 - p)^beta where p\u2248df/N.\n        p = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p) ** beta\n\n        if tf <= 0:\n            continue\n        matched += 1\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * common_damp) * term_idf * tf_part\n\n    # Soft coverage: 1 + gamma * (matched/total)\n    if total > 0 and matched > 0:\n        score *= (1.0 + gamma * (matched / float(total)))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        # Precompute df-probability p(t) used for query-side commonness dampening.\n        self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            p = self.corpus.p_df_array[term_id]\n            common_damp = (1.0 - p) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n        if total > 0:\n            scores *= (1.0 + gamma * (matched / total))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Represent query as unique constraints (match QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(1.0)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4088)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Score one document for one query. This is the lexical retrieval method.\n    EVOLVE: design a formulation with deep, fundamental, intuitive justification.\n    Default: Lucene BM25 (IDF \u00d7 saturated TF, length-normalized).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3243, Type: Exploratory)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core BM25 knobs (kept)\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Novel add-ons (lightweight, lexical):\n    # 1) pivoted length penalty: softly prefers \"focused\" docs (helps nDCG@10 on verbose corpora)\n    length_pivot: float = 0.35  # 0=no extra penalty\n    # 2) coordination factor: rewards covering more unique query terms (helps early precision)\n    coord_strength: float = 0.6\n    # 3) rarer-term gating: emphasizes rare terms more when query is short (helps theorem/keyword queries)\n    rare_boost: float = 0.25\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed discriminativity with a bounded dynamic range.\n\n    Intuition: extremely-rare terms can dominate BM25 (especially for noisy queries);\n    using log1p(N/df) keeps the \"information content\" story but avoids huge spikes.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    BM25 + (A) coordination + (B) pivoted length prior + (C) rare-term gating.\n\n    A) Coordination: lexical matching behaves more like set-coverage for short queries;\n       rewarding fraction of unique query terms matched tends to improve nDCG@10.\n    B) Pivoted length prior: independent of TF saturation, softly prefers concise docs.\n    C) Rare-term gating: for short queries, rare terms should steer ranking more.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    base = 0.0\n    matched = 0\n    uq = 0\n    for term in query_repr.terms:\n        uq += 1\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        matched += 1\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n\n        # rare-term gating: boost decreases as df grows; stronger effect on short queries\n        rare = 1.0 + Config.rare_boost * (1.0 / (1.0 + math.log1p(df)))\n        base += w * term_idf * tf_part * rare\n\n    if uq == 0:\n        return 0.0\n\n    # coordination factor (bounded, smooth)\n    coord = (matched / uq) if uq > 0 else 0.0\n    coord_factor = 1.0 + Config.coord_strength * coord\n\n    # pivoted length prior (bounded): penalize long docs even if TF is high\n    if avgdl > 0:\n        pivot = 1.0 / (1.0 + Config.length_pivot * (doc_length / (avgdl + eps)))\n    else:\n        pivot = 1.0\n\n    return base * coord_factor * pivot\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        base = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n\n            # approximate rare-term gating with df via idf_val (monotone proxy)\n            rare = 1.0 + Config.rare_boost * (idf_val / (1.0 + idf_val))\n            base += w * idf_val * tf_part * rare\n\n        uq = max(1.0, float(len(query_term_ids)))\n        coord = matched / uq\n        coord_factor = 1.0 + Config.coord_strength * coord\n\n        if self.corpus.avgdl > 0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            pivot = 1.0 / (1.0 + Config.length_pivot * (dl / (self.corpus.avgdl + eps)))\n        else:\n            pivot = 1.0\n\n        return base * coord_factor * pivot\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.3964, Type: Exploratory)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Coverage-pressure encourages satisfying more distinct query constraints.\n    coverage_gamma: float = 0.35\n\n    # Damp overly common query terms.\n    common_term_beta: float = 0.6\n\n    # NEW: sublinear length normalization exponent (reduces over-penalizing long docs).\n    # This helped the best known program on this benchmark mix.\n    dl_p: float = 0.80\n\n    # NEW: rare-term burstiness discount.\n    # Many corpora contain boilerplate where rare terms repeat; we reward presence more than repetition.\n    # Implemented as tf' = 1 + log1p(tf-1), then BM25 saturation on tf'.\n    burst_log: float = 0.7\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Deduplicate to represent a query as a *set* of lexical constraints.\n        Repeated query tokens often come from tokenization artifacts; set-style\n        matching tends to improve early precision across BEIR/BRIGHT mixes.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    CRSM with (1) sublinear length norm and (2) burstiness-discounted TF.\n\n    Why this helps:\n    - Sublinear length norm (dl^p) avoids over-penalizing long docs where relevant evidence is sparse\n      (common in SciDocs / StackOverflow / long scientific corpora), improving recall@100.\n    - Burstiness discount treats repeated occurrences as diminishing evidence (boilerplate/metadata),\n      improving top-rank quality without requiring positions.\n    - Keeps CRSM's query-side commonness damp + soft coverage, which helps nDCG@10 on multi-aspect queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n    p_len = Config.dl_p\n    burst_log = Config.burst_log\n\n    if not query_repr.terms:\n        return 0.0\n\n    # Sublinear length normalization (like best-known program): norm uses dl^p.\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n    total = 0\n\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        df = float(corpus_df.get(term, 1.0))\n        # Query-side commonness dampening: (1 - p)^beta where p\u2248df/N.\n        p_df = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p_df) ** beta\n\n        term_idf = float(idf(df, N))\n\n        # Burstiness discount: presence matters most; repetition adds log utility.\n        # Ensures tf_eff=1 when tf=1, grows slowly afterwards.\n        if tf > 1.0 and burst_log > 0.0:\n            tf_eff = 1.0 + burst_log * math.log1p(tf - 1.0)\n        else:\n            tf_eff = tf\n\n        tf_part = tf_eff / (tf_eff + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * common_damp) * term_idf * tf_part\n\n        matched += 1\n\n    if total > 0 and matched > 0:\n        score *= 1.0 + gamma * (matched / float(total))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        # Precompute df-probability p(t) used for query-side commonness dampening.\n        self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n\n        # Match retrieval_score: sublinear length normalization uses dl^p.\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n        burst_log = Config.burst_log\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            p_df = float(self.corpus.p_df_array[term_id])\n            common_damp = (1.0 - p_df) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            # Burstiness discount in vectorized form (safe for tf=0).\n            if burst_log > 0.0:\n                tf_eff = np.where(tf_row > 1.0, 1.0 + burst_log * np.log1p(tf_row - 1.0), tf_row)\n            else:\n                tf_eff = tf_row\n\n            tf_part = tf_eff / (tf_eff + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n        if total > 0.0:\n            scores *= 1.0 + gamma * (matched / total)\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Represent query as unique constraints (match QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(1.0)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4088, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep BM25 params (strong baseline), but add a principled \"coverage prior\".\n    # Intuition: queries are often multi-aspect; docs matching more distinct query terms\n    # should be preferred even if one term is very frequent.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Coverage prior strength. 0 => pure BM25. Small >0 tends to help nDCG@10.\n    coverage_lambda: float = 0.25\n\n    # How much we de-emphasize repeated query terms (robust across verbose queries).\n    # weight = (qtf)^qtf_power, so 0.5 ~ sqrt.\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Slightly more robust query weighting:\n        - Dedup terms for scoring stability (prevents long queries over-counting).\n        - Use sublinear qtf weighting (power law) rather than linear repetition.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        counts = Counter(tokens)\n        terms = list(counts.keys())\n        w = {t: float(c) ** Config.qtf_power for t, c in counts.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    BM25 + probabilistic coverage prior (novel additive log-prob term).\n\n    View BM25 as an additive evidence score. Add a *coverage prior* that prefers\n    documents matching many distinct query terms:\n        coverage = sum_t idf(t) * 1[tf(t)>0]\n        score = BM25 + \u03bb * log1p(coverage)\n\n    Why log1p? Diminishing returns: going from 0->1 matched aspects matters more\n    than 8->9. Why IDF-weighted coverage? Matching rare terms is stronger evidence\n    of covering a query aspect than matching common terms.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    bm25 = 0.0\n    coverage = 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        w = query_repr.term_weights.get(term, 1.0)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        bm25 += w * term_idf * tf_part\n\n        # IDF-weighted binary match mass (query weight included).\n        coverage += w * term_idf\n\n    if coverage > 0.0 and Config.coverage_lambda > 0.0:\n        bm25 += Config.coverage_lambda * math.log1p(coverage)\n\n    return bm25\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4183, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation/length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization: penalize long docs less harshly than BM25.\n    # Helps long-document corpora (StackOverflow, SciDocs, TREC-COVID) without giving up much precision.\n    dl_p: float = 0.75\n\n    # Soft coordination: multiplicatively reward covering more unique query terms.\n    # Multiplicative (not additive) avoids overwhelming lexical evidence for very short queries.\n    coord_alpha: float = 0.2\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Sublinear-Length BM25 + Soft Coordination (SLC-BM25)\n\n    Two principled departures from standard BM25:\n    (1) Sublinear length normalization: use dl^p (p<1) to reduce over-penalty for long docs.\n        Interpretable as a \"verbosity prior\" with diminishing growth of expected noise.\n    (2) Multiplicative coordination: reward satisfying more unique query constraints without\n        overpowering term evidence (bounded multiplier in [1, 1+coord_alpha]).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    p = Config.dl_p\n\n    if not query_repr.terms:\n        return 0.0\n\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n    uq = 0\n    seen: set[str] = set()\n\n    for term in query_repr.terms:\n        if term in seen:\n            continue\n        seen.add(term)\n        uq += 1\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n\n    if uq > 0 and matched > 0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (float(uq) + eps))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        uq = float(max(1, len(query_term_ids)))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core BM25 knobs\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Soft-AND / coverage pressure (multiplicative, bounded)\n    # score *= (coverage) ** and_power\n    and_power: float = 0.6\n\n    # Query-adaptive length prior:\n    # apply a pivoted-length penalty only when the query is \"specific\" (high avg idf).\n    focus_lambda: float = 0.35   # strength of length prior\n    focus_idf_pivot: float = 2.5 # idf level at which focus begins to matter (in log space)\n\n    # Mild compression of extreme IDF (kept, but used on new IDF)\n    idf_power: float = 0.9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Bounded self-information: log(1 + N/df).\n\n    Rationale: keeps the \"surprisal\" interpretation (rare -> informative) but avoids\n    very large spikes for ultra-rare terms that can dominate rankings in noisy queries.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique lexical constraints (set-like).\n        Repeated tokens are usually not meaningful and can destabilize coverage math.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Query-Adaptive Soft-AND BM25 (QASA-BM25)\n\n    1) Base evidence: sum_t idf(t)^p * sat_tf(t,d)\n    2) Soft-AND: multiply by coverage^and_power to prefer satisfying more query constraints.\n    3) Query-adaptive focus prior: only when query is specific (high avg idf), apply a mild\n       pivoted length penalty to prefer focused docs; broad queries shouldn't punish length.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n    sum_idf = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        df = float(corpus_df.get(term, 1))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        sum_idf += tidf\n\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += tidf * tf_part\n\n    uq = float(len(query_repr.terms))\n    if score <= 0.0 or uq <= 0.0:\n        return 0.0\n\n    # (2) soft-AND coverage pressure (bounded, helps early precision)\n    coverage = matched / (uq + eps)\n    score *= (coverage + eps) ** Config.and_power\n\n    # (3) query-adaptive focus prior\n    avg_idf = sum_idf / (uq + eps)\n    # gate in [0,1], increasing with avg_idf\n    gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n    if gate > 0.0 and avgdl > 0.0:\n        pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (doc_length / (avgdl + eps)))\n        score *= pivot\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        sum_idf = 0.0\n        uq = float(max(1, len(query_term_ids)))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            sum_idf += idf_val\n            if idf_val <= 0.0:\n                continue\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += idf_val * tf_part\n\n        # soft-AND coverage\n        coverage = matched / (uq + eps)\n        scores *= np.power(coverage + eps, Config.and_power)\n\n        # query-adaptive focus prior (scalar gate shared across candidate docs)\n        avg_idf = sum_idf / (uq + eps)\n        gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n        if gate > 0.0 and self.corpus.avgdl > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (dl / (self.corpus.avgdl + eps)))\n            scores *= pivot\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Use unique query terms (matches QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = None\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # Core BM25 knobs\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Soft-AND / coverage pressure (multiplicative, bounded)\n    # score *= (coverage) ** and_power\n    and_power: float = 0.6\n\n    # Query-adaptive length prior:\n    # apply a pivoted-length penalty only when the query is \"specific\" (high avg idf).\n    focus_lambda: float = 0.35   # strength of length prior\n    focus_idf_pivot: float = 2.5 # idf level at which focus begins to matter (in log space)\n\n    # Mild compression of extreme IDF (kept, but used on new IDF)\n    idf_power: float = 0.9\n=======\nclass Config:\n    # Core BM25 knobs\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization exponent (reduces over-penalizing long docs).\n    # This was consistently helpful in the best historical variants.\n    dl_p: float = 0.75\n\n    # Soft-AND / coverage pressure (multiplicative, bounded).\n    # Lower than current: the present 0.6 tends to hurt recall@100 on long/verbose corpora.\n    and_power: float = 0.25\n\n    # Add back a *bounded* coordination boost (helps nDCG@10 without hard AND-ing).\n    coord_alpha: float = 0.20\n\n    # Query-adaptive length prior:\n    # apply a pivoted-length penalty only when the query is \"specific\" (high avg idf).\n    focus_lambda: float = 0.20   # milder to protect recall\n    focus_idf_pivot: float = 2.2 # slightly easier to trigger for short/technical queries\n\n    # Mild compression of extreme IDF (kept, but used on new IDF)\n    idf_power: float = 0.9\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Query-Adaptive Soft-AND BM25 (QASA-BM25)\n\n    1) Base evidence: sum_t idf(t)^p * sat_tf(t,d)\n    2) Soft-AND: multiply by coverage^and_power to prefer satisfying more query constraints.\n    3) Query-adaptive focus prior: only when query is specific (high avg idf), apply a mild\n       pivoted length penalty to prefer focused docs; broad queries shouldn't punish length.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n    sum_idf = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        df = float(corpus_df.get(term, 1))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        sum_idf += tidf\n\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += tidf * tf_part\n\n    uq = float(len(query_repr.terms))\n    if score <= 0.0 or uq <= 0.0:\n        return 0.0\n\n    # (2) soft-AND coverage pressure (bounded, helps early precision)\n    coverage = matched / (uq + eps)\n    score *= (coverage + eps) ** Config.and_power\n\n    # (3) query-adaptive focus prior\n    avg_idf = sum_idf / (uq + eps)\n    # gate in [0,1], increasing with avg_idf\n    gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n    if gate > 0.0 and avgdl > 0.0:\n        pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (doc_length / (avgdl + eps)))\n        score *= pivot\n\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Query-Adaptive Soft-AND BM25 (QASA-BM25), revised for recall safety.\n\n    Changes vs current:\n    - Sublinear length normalization dl^p (p<1): long docs are penalized less harshly.\n    - Weaken soft-AND (smaller and_power) and add a bounded coordination boost:\n        score *= (1 + coord_alpha * coverage)\n      This keeps the \"more query constraints is better\" intuition, but avoids\n      collapsing scores for partial matches (important for recall@100).\n    - Keep the query-adaptive focus prior, but milder.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear-length BM25 norm (matches vectorized path).\n    p = Config.dl_p\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    sum_idf = 0.0\n    uq = float(len(query_repr.terms))\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        sum_idf += tidf\n\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += tidf * tf_part\n\n    if score <= 0.0 or uq <= 0.0:\n        return 0.0\n\n    coverage = matched / (uq + eps)\n\n    # Soft-AND (kept mild) + bounded coordination (recall-safer than exponent alone).\n    if Config.and_power != 0.0:\n        score *= (coverage + eps) ** Config.and_power\n    if Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * coverage\n\n    # Query-adaptive focus prior (milder + uses same query specificity summary as before).\n    avg_idf = sum_idf / (uq + eps)\n    gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n    if gate > 0.0 and avgdl > 0.0:\n        pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (doc_length / (avgdl + eps)))\n        score *= pivot\n\n    return score\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n=======\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        sum_idf = 0.0\n        uq = float(max(1, len(query_term_ids)))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            sum_idf += idf_val\n            if idf_val <= 0.0:\n                continue\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += idf_val * tf_part\n\n        # soft-AND coverage\n        coverage = matched / (uq + eps)\n        scores *= np.power(coverage + eps, Config.and_power)\n\n        # query-adaptive focus prior (scalar gate shared across candidate docs)\n        avg_idf = sum_idf / (uq + eps)\n        gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n        if gate > 0.0 and self.corpus.avgdl > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (dl / (self.corpus.avgdl + eps)))\n            scores *= pivot\n\n        return scores\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        sum_idf = 0.0\n        uq = float(max(1, len(query_term_ids)))\n\n        for term_id in query_term_ids:\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            sum_idf += idf_val\n            if idf_val <= 0.0:\n                continue\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += idf_val * tf_part\n\n        coverage = matched / (uq + eps)\n\n        if Config.and_power != 0.0:\n            scores *= np.power(coverage + eps, Config.and_power)\n        if Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * coverage\n\n        # query-adaptive focus prior (scalar gate shared across candidate docs)\n        avg_idf = sum_idf / (uq + eps)\n        gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n        if gate > 0.0 and self.corpus.avgdl > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (dl / (self.corpus.avgdl + eps)))\n            scores *= pivot\n\n        return scores\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}