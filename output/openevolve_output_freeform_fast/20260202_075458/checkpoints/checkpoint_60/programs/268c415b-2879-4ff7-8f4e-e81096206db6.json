{"id": "268c415b-2879-4ff7-8f4e-e81096206db6", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Soft AND-pressure (kept from current best)\n    coverage_gamma: float = 0.35\n\n    # Damp overly common query terms\n    common_term_beta: float = 0.6\n\n    # NEW: reward *discriminative* co-occurrence among matched query terms.\n    # Intuition: in many corpora (BRIGHT, StackOverflow, SciDocs), relevant docs\n    # tend to contain multiple query terms *together*; PMI-like synergy captures\n    # this without needing positions or learning.\n    synergy_lambda: float = 0.15   # strength of pairwise synergy\n    synergy_cap: float = 1.0       # prevents long queries from exploding\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Deduplicate to represent a query as a *set* of lexical constraints.\n        Repeated query tokens often come from tokenization artifacts; set-style\n        matching tends to improve early precision across BEIR/BRIGHT mixes.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    CRSM + Pairwise Synergy (PMI-style)\n\n    Keep the strong parts of CRSM:\n      - surprisal-ish IDF\n      - BM25-style TF saturation\n      - soft coverage boost\n      - query common-term dampening\n\n    Add a *synergy* bonus for documents that match multiple query terms:\n    - For matched terms i,j, add ~ idf(i)*idf(j) / (idf(i)+idf(j)).\n      This is a smooth \"both rare => strong conjunction\" signal.\n    - Acts like an inexpensive approximation to discriminative phrase/proximity:\n      without positions, co-presence is the simplest lexical interaction.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n    lam = Config.synergy_lambda\n\n    if not query_repr.terms:\n        return 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    total = 0\n\n    matched_idf: list[float] = []\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        df = float(corpus_df.get(term, 1.0))\n        # Query-side commonness dampening: (1 - p)^beta where p\u2248df/N.\n        p = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p) ** beta\n\n        matched += 1\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n\n        score += (w * common_damp) * term_idf * tf_part\n        matched_idf.append(term_idf)\n\n    if score <= 0.0:\n        return 0.0\n\n    # Soft coverage: 1 + gamma * (matched/total)\n    if total > 0 and matched > 0:\n        score *= (1.0 + gamma * (matched / float(total)))\n\n    # Pairwise synergy among matched terms (cap to avoid long-query blowups).\n    if lam > 0.0 and len(matched_idf) >= 2:\n        # harmonic-ish mean emphasizes \"both must be informative\"\n        s = 0.0\n        for i in range(len(matched_idf)):\n            ai = matched_idf[i]\n            for j in range(i + 1, len(matched_idf)):\n                aj = matched_idf[j]\n                s += (ai * aj) / (ai + aj + eps)\n        # normalize by number of pairs to be query-length robust\n        pairs = len(matched_idf) * (len(matched_idf) - 1) / 2.0\n        s = s / (pairs + eps)\n        s = min(s, Config.synergy_cap)\n        score *= (1.0 + lam * s)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        # Precompute df-probability p(t) used for query-side commonness dampening.\n        self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n        lam = Config.synergy_lambda\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For synergy we need to know, per query term, which candidates contain it.\n        present_masks: list[NDArray[np.float64]] = []\n        idf_vals: list[float] = []\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            p = float(self.corpus.p_df_array[term_id])\n            common_damp = (1.0 - p) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n            present_masks.append(present)\n            idf_vals.append(idf_val)\n\n        if total > 0.0:\n            scores *= (1.0 + gamma * (matched / total))\n\n        # Synergy: for each pair of query terms, add a bonus if both are present.\n        if lam > 0.0 and len(present_masks) >= 2:\n            s = np.zeros(len(candidate_docs), dtype=np.float64)\n            denom = 0.0\n            for i in range(len(present_masks)):\n                ai = idf_vals[i]\n                for j in range(i + 1, len(present_masks)):\n                    aj = idf_vals[j]\n                    pair_w = (ai * aj) / (ai + aj + eps)\n                    s += pair_w * (present_masks[i] * present_masks[j])\n                    denom += pair_w\n            if denom > 0.0:\n                s = s / (denom + eps)\n                s = np.minimum(s, Config.synergy_cap)\n                scores *= (1.0 + lam * s)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Represent query as unique constraints (match QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(1.0)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "6403dd84-efda-4a92-b6df-db70258451f2", "generation": 2, "timestamp": 1770049483.082879, "iteration_found": 7, "metrics": {"beir_nfcorpus_ndcg@10": 0.3170672628789085, "beir_nfcorpus_recall@100": 0.2558642228641034, "beir_nfcorpus_index_time_ms": 3530.9360829996876, "beir_nfcorpus_query_time_ms": 96.0954999900423, "beir_scifact_ndcg@10": 0.6782239538636547, "beir_scifact_recall@100": 0.9279999999999999, "beir_scifact_index_time_ms": 4762.4884999822825, "beir_scifact_query_time_ms": 383.7183339637704, "bright_pony_ndcg@10": 0.14972212542817523, "bright_pony_recall@100": 0.34917785036895654, "bright_pony_index_time_ms": 1382.156917010434, "bright_pony_query_time_ms": 724.139291967731, "beir_arguana_ndcg@10": 0.2377366828500651, "beir_arguana_recall@100": 0.8365453247680228, "beir_arguana_index_time_ms": 5740.304499980994, "beir_arguana_query_time_ms": 92481.1708329944, "bright_theoremqa_theorems_ndcg@10": 0.05226012528106348, "bright_theoremqa_theorems_recall@100": 0.1513157894736842, "bright_theoremqa_theorems_index_time_ms": 10410.83745798096, "bright_theoremqa_theorems_query_time_ms": 1529.631084005814, "beir_scidocs_ndcg@10": 0.14577497161782674, "beir_scidocs_recall@100": 0.3411333333333334, "beir_scidocs_index_time_ms": 19130.59637497645, "beir_scidocs_query_time_ms": 3388.663457997609, "bright_economics_ndcg@10": 0.13279739018677764, "bright_economics_recall@100": 0.3328158722963522, "bright_economics_index_time_ms": 13822.957666998263, "bright_economics_query_time_ms": 6796.049750002567, "bright_biology_ndcg@10": 0.2112891246821944, "bright_biology_recall@100": 0.471633121888615, "bright_biology_index_time_ms": 14729.045541025698, "bright_biology_query_time_ms": 6022.770749987103, "beir_fiqa_ndcg@10": 0.22326954915623207, "beir_fiqa_recall@100": 0.5261078898810381, "beir_fiqa_index_time_ms": 32731.52183298953, "beir_fiqa_query_time_ms": 6227.628999971785, "bright_earth_science_ndcg@10": 0.30836605733295874, "bright_earth_science_recall@100": 0.6037643007073589, "bright_earth_science_index_time_ms": 31655.81166697666, "bright_earth_science_query_time_ms": 8969.96170800412, "bright_stackoverflow_ndcg@10": 0.16763546920141278, "bright_stackoverflow_recall@100": 0.3962714828539755, "bright_stackoverflow_index_time_ms": 97884.9024589872, "bright_stackoverflow_query_time_ms": 23678.663290978875, "beir_trec-covid_ndcg@10": 0.6478873959331812, "beir_trec-covid_recall@100": 0.10370625772667044, "beir_trec-covid_index_time_ms": 130853.99366705678, "beir_trec-covid_query_time_ms": 1439.8043749970384, "avg_ndcg@10": 0.27266917570103755, "avg_recall@100": 0.4413612871801758, "combined_score": 0.4076228648843482, "total_index_time_ms": 366635.55266696494, "total_query_time_ms": 151738.29737486085, "total_time_ms": 518373.8500418258, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace:\n  class Config:\n      # Core saturation / length norm (kept close to BM25 but not identical usage below)\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Novel: \"coverage-pressure\" encourages matching more distinct query terms\n      # (helps nDCG@10 on verbose scientific/QA queries where partial matches are common).\n      coverage_gamma: float = 0.35\n  \n      # Novel: damp overly common query terms (approx. \"informativeness\" prior).\n      # When df/N is high, weight is reduced smoothly.\n      common_term_beta: float = 0.6\nwith:\n  class Config:\n      # Core saturation / length norm\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Soft AND-pressure (kept from current best)\n      coverage_gamma: float = 0.35\n  \n      # Damp overly common query terms\n      common_term_beta: float = 0.6\n  \n      # NEW: reward *discriminative* co-occurrence among matched query terms.\n      # Intuition: in many corpora (BRIGHT, StackOverflow, SciDocs), relevant docs\n      # tend to contain multiple query terms *together*; PMI-like synergy captures\n      # this without needing positions or learning.\n      synergy_lambda: float = 0.15   # strength of pairwise synergy\n      synergy_cap: float = 1.0       # prevents long queries from exploding\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Coverage-Regularized Surprisal Matching (CRSM)\n  \n      Base evidence: sum_t  w_q(t) * IDF_surprisal(t) * sat_tf(t,d)\n      Then apply a *soft coverage* boost encouraging documents that satisfy\n      more distinct query constraints.\n  \n      Intuition:\n      - The term-wise sum captures independent lexical evidence.\n      - The coverage factor acts like an \"AND-pressure\": among similar scores,\n        prefer docs that hit more query terms (often boosts nDCG@10).\n      - Additionally, downweight extremely common terms *in the query* because\n        they are weak constraints and tend to add noise across diverse datasets.\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      beta = Config.common_term_beta\n      gamma = Config.coverage_gamma\n  \n      if not query_repr.terms:\n          return 0.0\n  \n      norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n  ... (24 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      CRSM + Pairwise Synergy (PMI-style)\n  \n      Keep the strong parts of CRSM:\n        - surprisal-ish IDF\n        - BM25-style TF saturation\n        - soft coverage boost\n        - query common-term dampening\n  \n      Add a *synergy* bonus for documents that match multiple query terms:\n      - For matched terms i,j, add ~ idf(i)*idf(j) / (idf(i)+idf(j)).\n        This is a smooth \"both rare => strong conjunction\" signal.\n      - Acts like an inexpensive approximation to discriminative phrase/proximity:\n        without positions, co-presence is the simplest lexical interaction.\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      beta = Config.common_term_beta\n      gamma = Config.coverage_gamma\n      lam = Config.synergy_lambda\n  \n      if not query_repr.terms:\n          return 0.0\n  ... (50 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          beta = Config.common_term_beta\n          gamma = Config.coverage_gamma\n  \n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          total = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = self.corpus.idf_array[term_id]\n              if idf_val <= 0:\n                  continue\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              p = self.corpus.p_df_array[term_id]\n              common_damp = (1.0 - p) ** beta\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0).astype(np.float64)\n              matched += present\n  ... (7 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          beta = Config.common_term_beta\n          gamma = Config.coverage_gamma\n          lam = Config.synergy_lambda\n  \n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          # For synergy we need to know, per query term, which candidates contain it.\n          present_masks: list[NDArray[np.float64]] = []\n          idf_vals: list[float] = []\n  \n          total = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              p = float(self.corpus.p_df_array[term_id])\n  ... (32 more lines)", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3185121969431066, "beir_nfcorpus_recall@100": 0.2559581290395, "beir_nfcorpus_index_time_ms": 3527.7837910107337, "beir_nfcorpus_query_time_ms": 93.21249998174608, "beir_scifact_ndcg@10": 0.6800943744239561, "beir_scifact_recall@100": 0.9279999999999999, "beir_scifact_index_time_ms": 4752.117583004292, "beir_scifact_query_time_ms": 337.1330000227317, "bright_pony_ndcg@10": 0.14695898049125194, "bright_pony_recall@100": 0.34782864401975017, "bright_pony_index_time_ms": 1353.2858749968, "bright_pony_query_time_ms": 431.2353329733014, "beir_arguana_ndcg@10": 0.23876331881547905, "beir_arguana_recall@100": 0.8372591006423983, "beir_arguana_index_time_ms": 5734.7537919995375, "beir_arguana_query_time_ms": 18152.451917005237, "bright_theoremqa_theorems_ndcg@10": 0.05226012528106348, "bright_theoremqa_theorems_recall@100": 0.16447368421052633, "bright_theoremqa_theorems_index_time_ms": 10555.255667015444, "bright_theoremqa_theorems_query_time_ms": 746.1447499808855, "beir_scidocs_ndcg@10": 0.14697560774132246, "beir_scidocs_recall@100": 0.3426666666666667, "beir_scidocs_index_time_ms": 19184.54412498977, "beir_scidocs_query_time_ms": 3070.1044169836678, "bright_economics_ndcg@10": 0.13406016540763344, "bright_economics_recall@100": 0.3328158722963522, "bright_economics_index_time_ms": 13712.460208975244, "bright_economics_query_time_ms": 2093.634499993641, "bright_biology_ndcg@10": 0.20266838774986767, "bright_biology_recall@100": 0.46677875295657617, "bright_biology_index_time_ms": 14339.945833024103, "bright_biology_query_time_ms": 2036.8796249968, "beir_fiqa_ndcg@10": 0.22852931642860927, "beir_fiqa_recall@100": 0.5312433494146457, "beir_fiqa_index_time_ms": 32767.950499954168, "beir_fiqa_query_time_ms": 5614.811000006739, "bright_earth_science_ndcg@10": 0.3028809392406599, "bright_earth_science_recall@100": 0.5917569115447973, "bright_earth_science_index_time_ms": 31313.309208024293, "bright_earth_science_query_time_ms": 3446.539083030075, "bright_stackoverflow_ndcg@10": 0.17154104895490446, "bright_stackoverflow_recall@100": 0.396698833281326, "bright_stackoverflow_index_time_ms": 97674.23091694945, "bright_stackoverflow_query_time_ms": 7457.58870796999, "beir_trec-covid_ndcg@10": 0.6565474279487266, "beir_trec-covid_recall@100": 0.10539614830724343, "beir_trec-covid_index_time_ms": 131780.16274998663, "beir_trec-covid_query_time_ms": 1295.631584012881, "avg_ndcg@10": 0.2733159907855484, "avg_recall@100": 0.4417396743649819, "combined_score": 0.40805493764909523, "total_index_time_ms": 366695.8002499305, "total_query_time_ms": 44775.366416957695, "total_time_ms": 411471.16666688817, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4081\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.3500 \u2192 0.4081\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\nwith:\n  class Config:\n      # Coverage-style scoring: per-term concavity + doc length damping\n      k1: float = 0.9   # kept for backwards compatibility (no longer used in core)\n      b: float = 0.4    # kept for backwards compatibility (no longer used in core)\n      epsilon: float = 1e-9\n      dl_alpha: float = 0.25   # how strongly long docs are downweighted\n      tf_log_base: float = 1.0 # log1p(tf/base); base>1 reduces tf impact\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Score one document for one query. This is the lexical retrieval method.\n      EVOLVE: design a formulation with deep, fundamental, intuitive justification.\n      Default: Lucene BM25 (IDF \u00d7 saturated TF, length-normalized).\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      score = 0.0\n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0))\n          if tf <= 0:\n              continue\n          df = float(corpus_df.get(term, 1))\n          term_idf = float(idf(df, N))\n          norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n          tf_part = tf / (tf + k1 * norm + eps)\n          w = query_repr.term_weights.get(term, 1.0)\n          score += w * term_idf * tf_part\n      return score\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Soft coverage scoring (information-geometric intuition):\n      - Each term provides \"evidence mass\" ~ idf * log1p(tf) (concave TF utility).\n      - The query is satisfied when total evidence accumulates, but with diminishing\n        returns across terms: score = log1p(sum_evidence).\n      - Long documents get mild damping via log length ratio (verbosity prior).\n      \"\"\"\n      eps = Config.epsilon\n      base = Config.tf_log_base\n      sum_evidence = 0.0\n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0.0))\n          if tf <= 0.0:\n              continue\n          df = float(corpus_df.get(term, 1.0))\n          term_idf = float(idf(df, N))\n          w = query_repr.term_weights.get(term, 1.0)\n          sum_evidence += w * term_idf * math.log1p(tf / (base + eps))\n      if sum_evidence <= 0.0:\n          return 0.0\n      length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n      dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n  ... (1 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = self.corpus.idf_array[term_id]\n              if idf_val <= 0:\n                  continue\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n              scores += w * idf_val * tf_part\n          return scores\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n          eps = Config.epsilon\n          base = Config.tf_log_base\n  \n          sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              sum_evidence += w * idf_val * np.log1p(tf_row / (base + eps))\n  \n          # log1p over total evidence encourages covering more query terms\n          scores = np.log1p(np.maximum(sum_evidence, 0.0))\n  \n          length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n          dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n          return scores / (dl_damp + eps)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3288, beir_nfcorpus_recall@100: 0.2552, beir_nfcorpus_index_time_ms: 3492.3412, beir_nfcorpus_query_time_ms: 93.9525, beir_scifact_ndcg@10: 0.6755, beir_scifact_recall@100: 0.9116, beir_scifact_index_time_ms: 4753.1859, beir_scifact_query_time_ms: 352.5971, bright_pony_ndcg@10: 0.0193, bright_pony_recall@100: 0.1023, bright_pony_index_time_ms: 1330.0643, bright_pony_query_time_ms: 439.3835, beir_arguana_ndcg@10: 0.3704, beir_arguana_recall@100: 0.9636, beir_arguana_index_time_ms: 5719.0079, beir_arguana_query_time_ms: 19243.6874, bright_theoremqa_theorems_ndcg@10: 0.0035, bright_theoremqa_theorems_recall@100: 0.0899, bright_theoremqa_theorems_index_time_ms: 10597.0123, bright_theoremqa_theorems_query_time_ms: 806.9539, beir_scidocs_ndcg@10: 0.1552, beir_scidocs_recall@100: 0.3574, beir_scidocs_index_time_ms: 19288.4210, beir_scidocs_query_time_ms: 3311.6373, bright_economics_ndcg@10: 0.0683, bright_economics_recall@100: 0.2752, bright_economics_index_time_ms: 13903.2626, bright_economics_query_time_ms: 2217.4045, bright_biology_ndcg@10: 0.0290, bright_biology_recall@100: 0.2525, bright_biology_index_time_ms: 14736.0464, bright_biology_query_time_ms: 2131.2299, beir_fiqa_ndcg@10: 0.2317, beir_fiqa_recall@100: 0.5433, beir_fiqa_index_time_ms: 33332.8606, beir_fiqa_query_time_ms: 5906.3242, bright_earth_science_ndcg@10: 0.0486, bright_earth_science_recall@100: 0.2691, bright_earth_science_index_time_ms: 31177.7197, bright_earth_science_query_time_ms: 3604.6386, bright_stackoverflow_ndcg@10: 0.0991, bright_stackoverflow_recall@100: 0.4251, bright_stackoverflow_index_time_ms: 96559.0278, bright_stackoverflow_query_time_ms: 7758.8505, beir_trec-covid_ndcg@10: 0.7222, beir_trec-covid_recall@100: 0.1164, beir_trec-covid_index_time_ms: 128399.5393, beir_trec-covid_query_time_ms: 1458.1356, avg_ndcg@10: 0.2293, avg_recall@100: 0.3801, combined_score: 0.3500, total_index_time_ms: 363288.4890, total_query_time_ms: 47324.7951, total_time_ms: 410613.2841, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\nwith:\n  class Config:\n      # Core saturation / length norm (kept close to BM25 but not identical usage below)\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Novel: \"coverage-pressure\" encourages matching more distinct query terms\n      # (helps nDCG@10 on verbose scientific/QA queries where partial matches are common).\n      coverage_gamma: float = 0.35\n  \n      # Novel: damp overly common query terms (approx. \"informativeness\" prior).\n      # When df/N is high, weight is reduced smoothly.\n      common_term_beta: float = 0.6\nChange 2: Replace:\n  def idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n      \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n      return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\nwith:\n  def idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n      \"\"\"\n      Smoothed \"surprisal\" IDF:\n      - Interprets df/N as an empirical occurrence probability p(t in doc).\n      - Uses -log(p) with add-one style smoothing to avoid infinities.\n      This tends to behave better than classic BM25 IDF on very spiky corpora.\n      \"\"\"\n      df = np.asarray(df, dtype=np.float64)\n      p = (df + 1.0) / (N + 2.0)\n      return -np.log(p)\nChange 3: Replace:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n          return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\nwith:\n  class QueryRepr:\n      def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n          self.terms = terms\n          self.term_weights = term_weights or {t: 1.0 for t in terms}\n  \n      @classmethod\n      def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n          \"\"\"\n          Deduplicate to represent a query as a *set* of lexical constraints.\n          Repeated query tokens often come from tokenization artifacts; set-style\n          matching tends to improve early precision across BEIR/BRIGHT mixes.\n          \"\"\"\n          seen = set()\n          uniq = []\n          for t in tokens:\n              if t not in seen:\n                  seen.add(t)\n                  uniq.append(t)\n          return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\nChange 4: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Score one document for one query. This is the lexical retrieval method.\n      EVOLVE: design a formulation with deep, fundamental, intuitive justification.\n      Default: Lucene BM25 (IDF \u00d7 saturated TF, length-normalized).\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      score = 0.0\n      for term in query_repr.terms:\n          tf = float(doc_tf.get(term, 0))\n          if tf <= 0:\n              continue\n          df = float(corpus_df.get(term, 1))\n          term_idf = float(idf(df, N))\n          norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n          tf_part = tf / (tf + k1 * norm + eps)\n          w = query_repr.term_weights.get(term, 1.0)\n          score += w * term_idf * tf_part\n      return score\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Coverage-Regularized Surprisal Matching (CRSM)\n  \n      Base evidence: sum_t  w_q(t) * IDF_surprisal(t) * sat_tf(t,d)\n      Then apply a *soft coverage* boost encouraging documents that satisfy\n      more distinct query constraints.\n  \n      Intuition:\n      - The term-wise sum captures independent lexical evidence.\n      - The coverage factor acts like an \"AND-pressure\": among similar scores,\n        prefer docs that hit more query terms (often boosts nDCG@10).\n      - Additionally, downweight extremely common terms *in the query* because\n        they are weak constraints and tend to add noise across diverse datasets.\n      \"\"\"\n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n      beta = Config.common_term_beta\n      gamma = Config.coverage_gamma\n  \n      if not query_repr.terms:\n          return 0.0\n  \n      norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n  ... (24 more lines)\nChange 5: Replace:\n  self.tf_matrix = csr_matrix(tf_matrix_lil)\n          self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          b = Config.b\n          self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\nwith:\n  self.tf_matrix = csr_matrix(tf_matrix_lil)\n          self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n          # Precompute df-probability p(t) used for query-side commonness dampening.\n          self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n  \n          b = Config.b\n          self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\nChange 6: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = self.corpus.idf_array[term_id]\n              if idf_val <= 0:\n                  continue\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n              tf_part = tf_row / (tf_row + k1 * norms + eps)\n              scores += w * idf_val * tf_part\n          return scores\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          beta = Config.common_term_beta\n          gamma = Config.coverage_gamma\n  \n          norms = self.corpus.norm_array[candidate_docs]\n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          total = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = self.corpus.idf_array[term_id]\n              if idf_val <= 0:\n                  continue\n              w = query_term_weights[i] if query_term_weights is not None else 1.0\n              p = self.corpus.p_df_array[term_id]\n              common_damp = (1.0 - p) ** beta\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = (tf_row > 0).astype(np.float64)\n              matched += present\n  ... (7 more lines)\nChange 7: Replace:\n  term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term, count in term_counts.items():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(float(count))\n          if not query_term_ids:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n          qtf = np.array(query_term_weights, dtype=np.float64)\nwith:\n  # Represent query as unique constraints (match QueryRepr behavior).\n          term_counts = Counter(query)\n          query_term_ids = []\n          query_term_weights = []\n          for term in term_counts.keys():\n              tid = self.corpus.get_term_id(term)\n              if tid is not None:\n                  query_term_ids.append(tid)\n                  query_term_weights.append(1.0)\n          if not query_term_ids:\n              return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.flo...\n          qtf = np.array(query_term_weights, dtype=np.float64)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3185, beir_nfcorpus_recall@100: 0.2560, beir_nfcorpus_index_time_ms: 3527.7838, beir_nfcorpus_query_time_ms: 93.2125, beir_scifact_ndcg@10: 0.6801, beir_scifact_recall@100: 0.9280, beir_scifact_index_time_ms: 4752.1176, beir_scifact_query_time_ms: 337.1330, bright_pony_ndcg@10: 0.1470, bright_pony_recall@100: 0.3478, bright_pony_index_time_ms: 1353.2859, bright_pony_query_time_ms: 431.2353, beir_arguana_ndcg@10: 0.2388, beir_arguana_recall@100: 0.8373, beir_arguana_index_time_ms: 5734.7538, beir_arguana_query_time_ms: 18152.4519, bright_theoremqa_theorems_ndcg@10: 0.0523, bright_theoremqa_theorems_recall@100: 0.1645, bright_theoremqa_theorems_index_time_ms: 10555.2557, bright_theoremqa_theorems_query_time_ms: 746.1447, beir_scidocs_ndcg@10: 0.1470, beir_scidocs_recall@100: 0.3427, beir_scidocs_index_time_ms: 19184.5441, beir_scidocs_query_time_ms: 3070.1044, bright_economics_ndcg@10: 0.1341, bright_economics_recall@100: 0.3328, bright_economics_index_time_ms: 13712.4602, bright_economics_query_time_ms: 2093.6345, bright_biology_ndcg@10: 0.2027, bright_biology_recall@100: 0.4668, bright_biology_index_time_ms: 14339.9458, bright_biology_query_time_ms: 2036.8796, beir_fiqa_ndcg@10: 0.2285, beir_fiqa_recall@100: 0.5312, beir_fiqa_index_time_ms: 32767.9505, beir_fiqa_query_time_ms: 5614.8110, bright_earth_science_ndcg@10: 0.3029, bright_earth_science_recall@100: 0.5918, bright_earth_science_index_time_ms: 31313.3092, bright_earth_science_query_time_ms: 3446.5391, bright_stackoverflow_ndcg@10: 0.1715, bright_stackoverflow_recall@100: 0.3967, bright_stackoverflow_index_time_ms: 97674.2309, bright_stackoverflow_query_time_ms: 7457.5887, beir_trec-covid_ndcg@10: 0.6565, beir_trec-covid_recall@100: 0.1054, beir_trec-covid_index_time_ms: 131780.1627, beir_trec-covid_query_time_ms: 1295.6316, avg_ndcg@10: 0.2733, avg_recall@100: 0.4417, combined_score: 0.4081, total_index_time_ms: 366695.8002, total_query_time_ms: 44775.3664, total_time_ms: 411471.1667, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: beir_nfcorpus_ndcg@10: 0.3199, beir_nfcorpus_recall@100: 0.2547, beir_nfcorpus_index_time_ms: 3508.6520, beir_nfcorpus_query_time_ms: 89.5200, beir_scifact_ndcg@10: 0.6769, beir_scifact_recall@100: 0.9287, beir_scifact_index_time_ms: 4794.3557, beir_scifact_query_time_ms: 327.0705, bright_pony_ndcg@10: 0.0528, bright_pony_recall@100: 0.1991, bright_pony_index_time_ms: 1383.9245, bright_pony_query_time_ms: 408.2655, beir_arguana_ndcg@10: 0.3025, beir_arguana_recall@100: 0.9343, beir_arguana_index_time_ms: 5761.0583, beir_arguana_query_time_ms: 17320.1148, bright_theoremqa_theorems_ndcg@10: 0.0144, bright_theoremqa_theorems_recall@100: 0.1096, bright_theoremqa_theorems_index_time_ms: 10451.4483, bright_theoremqa_theorems_query_time_ms: 720.2516, beir_scidocs_ndcg@10: 0.1498, beir_scidocs_recall@100: 0.3469, beir_scidocs_index_time_ms: 19156.9070, beir_scidocs_query_time_ms: 2952.7330, bright_economics_ndcg@10: 0.1566, bright_economics_recall@100: 0.4036, bright_economics_index_time_ms: 13919.7775, bright_economics_query_time_ms: 1982.1463, bright_biology_ndcg@10: 0.1789, bright_biology_recall@100: 0.4283, bright_biology_index_time_ms: 14951.5650, bright_biology_query_time_ms: 1890.4380, beir_fiqa_ndcg@10: 0.2363, beir_fiqa_recall@100: 0.5372, beir_fiqa_index_time_ms: 32796.1323, beir_fiqa_query_time_ms: 5318.4477, bright_earth_science_ndcg@10: 0.2855, bright_earth_science_recall@100: 0.6105, bright_earth_science_index_time_ms: 30455.3227, bright_earth_science_query_time_ms: 3238.6602, bright_stackoverflow_ndcg@10: 0.1679, bright_stackoverflow_recall@100: 0.4678, bright_stackoverflow_index_time_ms: 98593.1018, bright_stackoverflow_query_time_ms: 6939.6527, beir_trec-covid_ndcg@10: 0.6690, beir_trec-covid_recall@100: 0.1091, beir_trec-covid_index_time_ms: 131270.7752, beir_trec-covid_query_time_ms: 1265.0325, avg_ndcg@10: 0.2675, avg_recall@100: 0.4441, combined_score: 0.4088, total_index_time_ms: 367043.0202, total_query_time_ms: 42452.3327, total_time_ms: 409495.3530, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4088)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Score one document for one query. This is the lexical retrieval method.\n    EVOLVE: design a formulation with deep, fundamental, intuitive justification.\n    Default: Lucene BM25 (IDF \u00d7 saturated TF, length-normalized).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3199), Performs well on beir_nfcorpus_recall@100 (0.2547), Performs well on beir_nfcorpus_index_time_ms (3508.6520), Performs well on beir_nfcorpus_query_time_ms (89.5200), Performs well on beir_scifact_ndcg@10 (0.6769), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (4794.3557), Performs well on beir_scifact_query_time_ms (327.0705), Performs well on bright_pony_ndcg@10 (0.0528), Performs well on bright_pony_recall@100 (0.1991), Performs well on bright_pony_index_time_ms (1383.9245), Performs well on bright_pony_query_time_ms (408.2655), Performs well on beir_arguana_ndcg@10 (0.3025), Performs well on beir_arguana_recall@100 (0.9343), Performs well on beir_arguana_index_time_ms (5761.0583), Performs well on beir_arguana_query_time_ms (17320.1148), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0144), Performs well on bright_theoremqa_theorems_recall@100 (0.1096), Performs well on bright_theoremqa_theorems_index_time_ms (10451.4483), Performs well on bright_theoremqa_theorems_query_time_ms (720.2516), Performs well on beir_scidocs_ndcg@10 (0.1498), Performs well on beir_scidocs_recall@100 (0.3469), Performs well on beir_scidocs_index_time_ms (19156.9070), Performs well on beir_scidocs_query_time_ms (2952.7330), Performs well on bright_economics_ndcg@10 (0.1566), Performs well on bright_economics_recall@100 (0.4036), Performs well on bright_economics_index_time_ms (13919.7775), Performs well on bright_economics_query_time_ms (1982.1463), Performs well on bright_biology_ndcg@10 (0.1789), Performs well on bright_biology_recall@100 (0.4283), Performs well on bright_biology_index_time_ms (14951.5650), Performs well on bright_biology_query_time_ms (1890.4380), Performs well on beir_fiqa_ndcg@10 (0.2363), Performs well on beir_fiqa_recall@100 (0.5372), Performs well on beir_fiqa_index_time_ms (32796.1323), Performs well on beir_fiqa_query_time_ms (5318.4477), Performs well on bright_earth_science_ndcg@10 (0.2855), Performs well on bright_earth_science_recall@100 (0.6105), Performs well on bright_earth_science_index_time_ms (30455.3227), Performs well on bright_earth_science_query_time_ms (3238.6602), Performs well on bright_stackoverflow_ndcg@10 (0.1679), Performs well on bright_stackoverflow_recall@100 (0.4678), Performs well on bright_stackoverflow_index_time_ms (98593.1018), Performs well on bright_stackoverflow_query_time_ms (6939.6527), Performs well on beir_trec-covid_ndcg@10 (0.6690), Performs well on beir_trec-covid_recall@100 (0.1091), Performs well on beir_trec-covid_index_time_ms (131270.7752), Performs well on beir_trec-covid_query_time_ms (1265.0325), Performs well on avg_ndcg@10 (0.2675), Performs well on avg_recall@100 (0.4441), Performs well on combined_score (0.4088), Performs well on total_index_time_ms (367043.0202), Performs well on total_query_time_ms (42452.3327), Performs well on total_time_ms (409495.3530), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4081)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm (kept close to BM25 but not identical usage below)\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Novel: \"coverage-pressure\" encourages matching more distinct query terms\n    # (helps nDCG@10 on verbose scientific/QA queries where partial matches are common).\n    coverage_gamma: float = 0.35\n\n    # Novel: damp overly common query terms (approx. \"informativeness\" prior).\n    # When df/N is high, weight is reduced smoothly.\n    common_term_beta: float = 0.6\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Deduplicate to represent a query as a *set* of lexical constraints.\n        Repeated query tokens often come from tokenization artifacts; set-style\n        matching tends to improve early precision across BEIR/BRIGHT mixes.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Coverage-Regularized Surprisal Matching (CRSM)\n\n    Base evidence: sum_t  w_q(t) * IDF_surprisal(t) * sat_tf(t,d)\n    Then apply a *soft coverage* boost encouraging documents that satisfy\n    more distinct query constraints.\n\n    Intuition:\n    - The term-wise sum captures independent lexical evidence.\n    - The coverage factor acts like an \"AND-pressure\": among similar scores,\n      prefer docs that hit more query terms (often boosts nDCG@10).\n    - Additionally, downweight extremely common terms *in the query* because\n      they are weak constraints and tend to add noise across diverse datasets.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n\n    if not query_repr.terms:\n        return 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    total = 0\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0))\n        df = float(corpus_df.get(term, 1))\n        # Query-side commonness dampening: (1 - p)^beta where p\u2248df/N.\n        p = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p) ** beta\n\n        if tf <= 0:\n            continue\n        matched += 1\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * common_damp) * term_idf * tf_part\n\n    # Soft coverage: 1 + gamma * (matched/total)\n    if total > 0 and matched > 0:\n        score *= (1.0 + gamma * (matched / float(total)))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        # Precompute df-probability p(t) used for query-side commonness dampening.\n        self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            p = self.corpus.p_df_array[term_id]\n            common_damp = (1.0 - p) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n        if total > 0:\n            scores *= (1.0 + gamma * (matched / total))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Represent query as unique constraints (match QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(1.0)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3185), Performs well on beir_nfcorpus_recall@100 (0.2560), Performs well on beir_nfcorpus_index_time_ms (3527.7838), Performs well on beir_nfcorpus_query_time_ms (93.2125), Performs well on beir_scifact_ndcg@10 (0.6801), Performs well on beir_scifact_recall@100 (0.9280), Performs well on beir_scifact_index_time_ms (4752.1176), Performs well on beir_scifact_query_time_ms (337.1330), Performs well on bright_pony_ndcg@10 (0.1470), Performs well on bright_pony_recall@100 (0.3478), Performs well on bright_pony_index_time_ms (1353.2859), Performs well on bright_pony_query_time_ms (431.2353), Performs well on beir_arguana_ndcg@10 (0.2388), Performs well on beir_arguana_recall@100 (0.8373), Performs well on beir_arguana_index_time_ms (5734.7538), Performs well on beir_arguana_query_time_ms (18152.4519), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0523), Performs well on bright_theoremqa_theorems_recall@100 (0.1645), Performs well on bright_theoremqa_theorems_index_time_ms (10555.2557), Performs well on bright_theoremqa_theorems_query_time_ms (746.1447), Performs well on beir_scidocs_ndcg@10 (0.1470), Performs well on beir_scidocs_recall@100 (0.3427), Performs well on beir_scidocs_index_time_ms (19184.5441), Performs well on beir_scidocs_query_time_ms (3070.1044), Performs well on bright_economics_ndcg@10 (0.1341), Performs well on bright_economics_recall@100 (0.3328), Performs well on bright_economics_index_time_ms (13712.4602), Performs well on bright_economics_query_time_ms (2093.6345), Performs well on bright_biology_ndcg@10 (0.2027), Performs well on bright_biology_recall@100 (0.4668), Performs well on bright_biology_index_time_ms (14339.9458), Performs well on bright_biology_query_time_ms (2036.8796), Performs well on beir_fiqa_ndcg@10 (0.2285), Performs well on beir_fiqa_recall@100 (0.5312), Performs well on beir_fiqa_index_time_ms (32767.9505), Performs well on beir_fiqa_query_time_ms (5614.8110), Performs well on bright_earth_science_ndcg@10 (0.3029), Performs well on bright_earth_science_recall@100 (0.5918), Performs well on bright_earth_science_index_time_ms (31313.3092), Performs well on bright_earth_science_query_time_ms (3446.5391), Performs well on bright_stackoverflow_ndcg@10 (0.1715), Performs well on bright_stackoverflow_recall@100 (0.3967), Performs well on bright_stackoverflow_index_time_ms (97674.2309), Performs well on bright_stackoverflow_query_time_ms (7457.5887), Performs well on beir_trec-covid_ndcg@10 (0.6565), Performs well on beir_trec-covid_recall@100 (0.1054), Performs well on beir_trec-covid_index_time_ms (131780.1627), Performs well on beir_trec-covid_query_time_ms (1295.6316), Performs well on avg_ndcg@10 (0.2733), Performs well on avg_recall@100 (0.4417), Performs well on combined_score (0.4081), Performs well on total_index_time_ms (366695.8002), Performs well on total_query_time_ms (44775.3664), Performs well on total_time_ms (411471.1667), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.3500)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Coverage-style scoring: per-term concavity + doc length damping\n    k1: float = 0.9   # kept for backwards compatibility (no longer used in core)\n    b: float = 0.4    # kept for backwards compatibility (no longer used in core)\n    epsilon: float = 1e-9\n    dl_alpha: float = 0.25   # how strongly long docs are downweighted\n    tf_log_base: float = 1.0 # log1p(tf/base); base>1 reduces tf impact\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Soft coverage scoring (information-geometric intuition):\n    - Each term provides \"evidence mass\" ~ idf * log1p(tf) (concave TF utility).\n    - The query is satisfied when total evidence accumulates, but with diminishing\n      returns across terms: score = log1p(sum_evidence).\n    - Long documents get mild damping via log length ratio (verbosity prior).\n    \"\"\"\n    eps = Config.epsilon\n    base = Config.tf_log_base\n    sum_evidence = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        w = query_repr.term_weights.get(term, 1.0)\n        sum_evidence += w * term_idf * math.log1p(tf / (base + eps))\n    if sum_evidence <= 0.0:\n        return 0.0\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return math.log1p(sum_evidence) / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            sum_evidence += w * idf_val * np.log1p(tf_row / (base + eps))\n\n        # log1p over total evidence encourages covering more query terms\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3288), Performs well on beir_nfcorpus_recall@100 (0.2552), Performs well on beir_nfcorpus_index_time_ms (3492.3412), Performs well on beir_nfcorpus_query_time_ms (93.9525), Performs well on beir_scifact_ndcg@10 (0.6755), Performs well on beir_scifact_recall@100 (0.9116), Performs well on beir_scifact_index_time_ms (4753.1859), Performs well on beir_scifact_query_time_ms (352.5971), Performs well on bright_pony_ndcg@10 (0.0193), Performs well on bright_pony_recall@100 (0.1023), Performs well on bright_pony_index_time_ms (1330.0643), Performs well on bright_pony_query_time_ms (439.3835), Performs well on beir_arguana_ndcg@10 (0.3704), Performs well on beir_arguana_recall@100 (0.9636), Performs well on beir_arguana_index_time_ms (5719.0079), Performs well on beir_arguana_query_time_ms (19243.6874), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0035), Performs well on bright_theoremqa_theorems_recall@100 (0.0899), Performs well on bright_theoremqa_theorems_index_time_ms (10597.0123), Performs well on bright_theoremqa_theorems_query_time_ms (806.9539), Performs well on beir_scidocs_ndcg@10 (0.1552), Performs well on beir_scidocs_recall@100 (0.3574), Performs well on beir_scidocs_index_time_ms (19288.4210), Performs well on beir_scidocs_query_time_ms (3311.6373), Performs well on bright_economics_ndcg@10 (0.0683), Performs well on bright_economics_recall@100 (0.2752), Performs well on bright_economics_index_time_ms (13903.2626), Performs well on bright_economics_query_time_ms (2217.4045), Performs well on bright_biology_ndcg@10 (0.0290), Performs well on bright_biology_recall@100 (0.2525), Performs well on bright_biology_index_time_ms (14736.0464), Performs well on bright_biology_query_time_ms (2131.2299), Performs well on beir_fiqa_ndcg@10 (0.2317), Performs well on beir_fiqa_recall@100 (0.5433), Performs well on beir_fiqa_index_time_ms (33332.8606), Performs well on beir_fiqa_query_time_ms (5906.3242), Performs well on bright_earth_science_ndcg@10 (0.0486), Performs well on bright_earth_science_recall@100 (0.2691), Performs well on bright_earth_science_index_time_ms (31177.7197), Performs well on bright_earth_science_query_time_ms (3604.6386), Performs well on bright_stackoverflow_ndcg@10 (0.0991), Performs well on bright_stackoverflow_recall@100 (0.4251), Performs well on bright_stackoverflow_index_time_ms (96559.0278), Performs well on bright_stackoverflow_query_time_ms (7758.8505), Performs well on beir_trec-covid_ndcg@10 (0.7222), Performs well on beir_trec-covid_recall@100 (0.1164), Performs well on beir_trec-covid_index_time_ms (128399.5393), Performs well on beir_trec-covid_query_time_ms (1458.1356), Performs well on avg_ndcg@10 (0.2293), Performs well on avg_recall@100 (0.3801), Performs well on combined_score (0.3500), Performs well on total_index_time_ms (363288.4890), Performs well on total_query_time_ms (47324.7951), Performs well on total_time_ms (410613.2841), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4088, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Score one document for one query. This is the lexical retrieval method.\n    EVOLVE: design a formulation with deep, fundamental, intuitive justification.\n    Default: Lucene BM25 (IDF \u00d7 saturated TF, length-normalized).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    score = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm (kept close to BM25 but not identical usage below)\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Novel: \"coverage-pressure\" encourages matching more distinct query terms\n    # (helps nDCG@10 on verbose scientific/QA queries where partial matches are common).\n    coverage_gamma: float = 0.35\n\n    # Novel: damp overly common query terms (approx. \"informativeness\" prior).\n    # When df/N is high, weight is reduced smoothly.\n    common_term_beta: float = 0.6\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Deduplicate to represent a query as a *set* of lexical constraints.\n        Repeated query tokens often come from tokenization artifacts; set-style\n        matching tends to improve early precision across BEIR/BRIGHT mixes.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Coverage-Regularized Surprisal Matching (CRSM)\n\n    Base evidence: sum_t  w_q(t) * IDF_surprisal(t) * sat_tf(t,d)\n    Then apply a *soft coverage* boost encouraging documents that satisfy\n    more distinct query constraints.\n\n    Intuition:\n    - The term-wise sum captures independent lexical evidence.\n    - The coverage factor acts like an \"AND-pressure\": among similar scores,\n      prefer docs that hit more query terms (often boosts nDCG@10).\n    - Additionally, downweight extremely common terms *in the query* because\n      they are weak constraints and tend to add noise across diverse datasets.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n\n    if not query_repr.terms:\n        return 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    total = 0\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0))\n        df = float(corpus_df.get(term, 1))\n        # Query-side commonness dampening: (1 - p)^beta where p\u2248df/N.\n        p = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p) ** beta\n\n        if tf <= 0:\n            continue\n        matched += 1\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * common_damp) * term_idf * tf_part\n\n    # Soft coverage: 1 + gamma * (matched/total)\n    if total > 0 and matched > 0:\n        score *= (1.0 + gamma * (matched / float(total)))\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        # Precompute df-probability p(t) used for query-side commonness dampening.\n        self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            p = self.corpus.p_df_array[term_id]\n            common_damp = (1.0 - p) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n        if total > 0:\n            scores *= (1.0 + gamma * (matched / total))\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Represent query as unique constraints (match QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(1.0)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # Core saturation / length norm (kept close to BM25 but not identical usage below)\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Novel: \"coverage-pressure\" encourages matching more distinct query terms\n    # (helps nDCG@10 on verbose scientific/QA queries where partial matches are common).\n    coverage_gamma: float = 0.35\n\n    # Novel: damp overly common query terms (approx. \"informativeness\" prior).\n    # When df/N is high, weight is reduced smoothly.\n    common_term_beta: float = 0.6\n=======\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Soft AND-pressure (kept from current best)\n    coverage_gamma: float = 0.35\n\n    # Damp overly common query terms\n    common_term_beta: float = 0.6\n\n    # NEW: reward *discriminative* co-occurrence among matched query terms.\n    # Intuition: in many corpora (BRIGHT, StackOverflow, SciDocs), relevant docs\n    # tend to contain multiple query terms *together*; PMI-like synergy captures\n    # this without needing positions or learning.\n    synergy_lambda: float = 0.15   # strength of pairwise synergy\n    synergy_cap: float = 1.0       # prevents long queries from exploding\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Coverage-Regularized Surprisal Matching (CRSM)\n\n    Base evidence: sum_t  w_q(t) * IDF_surprisal(t) * sat_tf(t,d)\n    Then apply a *soft coverage* boost encouraging documents that satisfy\n    more distinct query constraints.\n\n    Intuition:\n    - The term-wise sum captures independent lexical evidence.\n    - The coverage factor acts like an \"AND-pressure\": among similar scores,\n      prefer docs that hit more query terms (often boosts nDCG@10).\n    - Additionally, downweight extremely common terms *in the query* because\n      they are weak constraints and tend to add noise across diverse datasets.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n\n    if not query_repr.terms:\n        return 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    total = 0\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0))\n        df = float(corpus_df.get(term, 1))\n        # Query-side commonness dampening: (1 - p)^beta where p\u2248df/N.\n        p = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p) ** beta\n\n        if tf <= 0:\n            continue\n        matched += 1\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += (w * common_damp) * term_idf * tf_part\n\n    # Soft coverage: 1 + gamma * (matched/total)\n    if total > 0 and matched > 0:\n        score *= (1.0 + gamma * (matched / float(total)))\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    CRSM + Pairwise Synergy (PMI-style)\n\n    Keep the strong parts of CRSM:\n      - surprisal-ish IDF\n      - BM25-style TF saturation\n      - soft coverage boost\n      - query common-term dampening\n\n    Add a *synergy* bonus for documents that match multiple query terms:\n    - For matched terms i,j, add ~ idf(i)*idf(j) / (idf(i)+idf(j)).\n      This is a smooth \"both rare => strong conjunction\" signal.\n    - Acts like an inexpensive approximation to discriminative phrase/proximity:\n      without positions, co-presence is the simplest lexical interaction.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n    lam = Config.synergy_lambda\n\n    if not query_repr.terms:\n        return 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    total = 0\n\n    matched_idf: list[float] = []\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        df = float(corpus_df.get(term, 1.0))\n        # Query-side commonness dampening: (1 - p)^beta where p\u2248df/N.\n        p = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p) ** beta\n\n        matched += 1\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n\n        score += (w * common_damp) * term_idf * tf_part\n        matched_idf.append(term_idf)\n\n    if score <= 0.0:\n        return 0.0\n\n    # Soft coverage: 1 + gamma * (matched/total)\n    if total > 0 and matched > 0:\n        score *= (1.0 + gamma * (matched / float(total)))\n\n    # Pairwise synergy among matched terms (cap to avoid long-query blowups).\n    if lam > 0.0 and len(matched_idf) >= 2:\n        # harmonic-ish mean emphasizes \"both must be informative\"\n        s = 0.0\n        for i in range(len(matched_idf)):\n            ai = matched_idf[i]\n            for j in range(i + 1, len(matched_idf)):\n                aj = matched_idf[j]\n                s += (ai * aj) / (ai + aj + eps)\n        # normalize by number of pairs to be query-length robust\n        pairs = len(matched_idf) * (len(matched_idf) - 1) / 2.0\n        s = s / (pairs + eps)\n        s = min(s, Config.synergy_cap)\n        score *= (1.0 + lam * s)\n\n    return score\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            p = self.corpus.p_df_array[term_id]\n            common_damp = (1.0 - p) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n        if total > 0:\n            scores *= (1.0 + gamma * (matched / total))\n        return scores\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n        lam = Config.synergy_lambda\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For synergy we need to know, per query term, which candidates contain it.\n        present_masks: list[NDArray[np.float64]] = []\n        idf_vals: list[float] = []\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            p = float(self.corpus.p_df_array[term_id])\n            common_damp = (1.0 - p) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n            present_masks.append(present)\n            idf_vals.append(idf_val)\n\n        if total > 0.0:\n            scores *= (1.0 + gamma * (matched / total))\n\n        # Synergy: for each pair of query terms, add a bonus if both are present.\n        if lam > 0.0 and len(present_masks) >= 2:\n            s = np.zeros(len(candidate_docs), dtype=np.float64)\n            denom = 0.0\n            for i in range(len(present_masks)):\n                ai = idf_vals[i]\n                for j in range(i + 1, len(present_masks)):\n                    aj = idf_vals[j]\n                    pair_w = (ai * aj) / (ai + aj + eps)\n                    s += pair_w * (present_masks[i] * present_masks[j])\n                    denom += pair_w\n            if denom > 0.0:\n                s = s / (denom + eps)\n                s = np.minimum(s, Config.synergy_cap)\n                scores *= (1.0 + lam * s)\n\n        return scores\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}