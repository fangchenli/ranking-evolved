{"id": "17590370-7947-49a4-8982-e30cb95bb962", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence accumulation + informative coverage.\n    epsilon: float = 1e-9\n\n    # Evidence: wt * log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Mild verbosity prior (keep recall-friendly).\n    dl_alpha: float = 0.15\n\n    # Soft-AND on informative query mass covered.\n    coverage_gamma: float = 0.25\n\n    # Query-side common-term gating (bounded in [0,1]).\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting.\n    qtf_power: float = 0.5\n\n    # Verbosity-aware discriminativity (residual-IDF).\n    resid_rho: float = 0.35\n    resid_floor: float = 0.0\n    ridf_tau: float = 0.35  # soft hinge temperature (softplus)\n\n    # Burstiness reward (keyword-like concentration).\n    burst_beta: float = 0.12\n\n    # NEW: query-facet sharpening (query-only). Reallocates weight toward high-idf constraints\n    # to improve early precision without hard AND.\n    facet_mix: float = 0.12\n    facet_power: float = 1.6\n\n    # NEW: bounded rare-key presence boost (indicator-like). Helps BRIGHT-style identifier queries.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # --- Compatibility: Corpus expects BM25's b for norm_array construction ---\n    b: float = 0.4\n\n    # Keep the old discrete hit bonus but default to 0; residual/coverage handle coordination more smoothly.\n    hit_bonus: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Interprets df/N as an empirical occurrence probability but avoids infinities/extremes.\n    This was consistently robust across heterogeneous BEIR+BRIGHT mixes in prior attempts.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as a set of lexical constraints with sublinear qtf.\n        This usually helps early precision (nDCG@10) on verbose QA/scientific queries\n        while keeping recall robust.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        counts = Counter(tokens)\n        terms = list(counts.keys())  # unique constraints\n        weights = {t: float(c) ** Config.qtf_power for t, c in counts.items()}\n        return cls(terms=terms, term_weights=weights)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Smooth residual-IDF concave evidence + informative coverage + mild verbosity prior,\n    with two query-only shaping signals:\n\n      (1) Facet prior: queries often contain a few decisive, high-IDF constraints and\n          many weak hints. Reweight IDF inside the query to sharpen early precision.\n      (2) Rare-key presence: a bounded multiplier when ultra-rare query tokens appear\n          at least once (identifiers/symbols), improving BRIGHT-style retrieval.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    v = math.log1p((doc_length + 1.0) / (avgdl + 1.0))\n    tau = max(Config.ridf_tau, 1e-6)\n\n    # Query-only facet scale.\n    idf_pows = []\n    for term in query_repr.terms:\n        tidf0 = float(idf(float(corpus_df.get(term, 1.0)), N))\n        if tidf0 > 0.0:\n            idf_pows.append(tidf0 ** Config.facet_power)\n    mean_idf_pow = (sum(idf_pows) / len(idf_pows)) if idf_pows else 1.0\n    mean_idf_pow = max(mean_idf_pow, eps)\n    mix = max(0.0, min(1.0, Config.facet_mix))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # residualize by verbosity\n        x = (tidf - Config.resid_rho * v) - Config.resid_floor\n        ridf = tau * math.log1p(math.exp(x / tau))\n        if ridf <= 0.0:\n            continue\n\n        # facet-reweighted idf (query-only) used for evidence, but keep coverage on raw IDF mass\n        facet = (tidf ** Config.facet_power) / mean_idf_pow\n        idf_used = (1.0 - mix) * tidf + mix * facet\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        cov_wt = wq * tidf * clarity\n        cov_den += cov_wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += cov_wt\n\n        tfu = math.log1p(tf / (base + eps))\n        burst = Config.burst_beta * tfu * math.log1p(N / (df + eps))\n        sum_evidence += (wq * clarity * ridf * idf_used / (tidf + eps)) * (tfu + burst)\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    qmass = cov_den if cov_den > 0.0 else 1.0\n    score = math.log1p(sum_evidence / (qmass + eps))\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    dl_damp = 1.0 + Config.dl_alpha * v\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        v = np.log1p((self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0))\n        tau = max(Config.ridf_tau, 1e-6)\n\n        # Query-only facet scale (computed from idf_array; query-only).\n        idf_vals = np.array([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        idf_vals = np.maximum(idf_vals, 0.0)\n        idf_pow = np.power(idf_vals, Config.facet_power, dtype=np.float64)\n        mean_idf_pow = float(np.mean(idf_pow)) if idf_pow.size > 0 else 1.0\n        mean_idf_pow = max(mean_idf_pow, eps)\n        mix = max(0.0, min(1.0, Config.facet_mix))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            facet = (idf_val ** Config.facet_power) / mean_idf_pow\n            idf_used = (1.0 - mix) * idf_val + mix * facet\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            cov_wt = wq * idf_val * clarity\n            cov_den += cov_wt\n\n            x = (idf_val - Config.resid_rho * v) - Config.resid_floor\n            ridf = tau * np.log1p(np.exp(x / tau))\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += cov_wt * present\n\n            tfu = np.log1p(tf_row / (base + eps))\n            burst = Config.burst_beta * tfu * math.log1p(self.corpus.N / (float(self.corpus._df[term_id]) + eps))\n\n            sum_evidence += (wq * clarity) * ridf * (idf_used / (idf_val + eps)) * (tfu + burst)\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        qmass = max(cov_den, eps)\n        scores = np.log1p(np.maximum(sum_evidence, 0.0) / qmass)\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        dl_damp = 1.0 + Config.dl_alpha * v\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Match QueryRepr: unique terms + sublinear qtf weights.\n        counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, c in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "eac0f7a4-d846-401f-acd9-aed0e12fd36e", "generation": 6, "timestamp": 1770204383.1111321, "iteration_found": 196, "metrics": {"bright_pony_ndcg@10": 0.0944027006354177, "bright_pony_recall@100": 0.30386761580288874, "bright_pony_index_time_ms": 7653.952438849956, "bright_pony_query_time_ms": 955.6831386871636, "beir_nfcorpus_ndcg@10": 0.3295926997627994, "beir_nfcorpus_recall@100": 0.2488836760170155, "beir_nfcorpus_index_time_ms": 17770.34447574988, "beir_nfcorpus_query_time_ms": 218.99716835469007, "beir_scifact_ndcg@10": 0.6652829349374402, "beir_scifact_recall@100": 0.9182222222222223, "beir_scifact_index_time_ms": 24316.346514970064, "beir_scifact_query_time_ms": 758.5587901994586, "beir_arguana_ndcg@10": 0.2615520475390758, "beir_arguana_recall@100": 0.9200571020699501, "beir_arguana_index_time_ms": 26913.354430813342, "beir_arguana_query_time_ms": 40966.9697675854, "bright_theoremqa_theorems_ndcg@10": 0.03857251777441084, "bright_theoremqa_theorems_recall@100": 0.16885964912280702, "bright_theoremqa_theorems_index_time_ms": 52943.48915386945, "bright_theoremqa_theorems_query_time_ms": 1484.3048881739378, "beir_scidocs_ndcg@10": 0.1455743869533947, "beir_scidocs_recall@100": 0.3539333333333334, "beir_scidocs_index_time_ms": 92654.16937414557, "beir_scidocs_query_time_ms": 5054.0757440030575, "bright_economics_ndcg@10": 0.13195944591170233, "bright_economics_recall@100": 0.37727837350684446, "bright_economics_index_time_ms": 61103.03622903302, "bright_economics_query_time_ms": 4102.328614331782, "bright_biology_ndcg@10": 0.25339637249804675, "bright_biology_recall@100": 0.5902103091934466, "bright_biology_index_time_ms": 67270.52351459861, "bright_biology_query_time_ms": 3697.975868359208, "bright_earth_science_ndcg@10": 0.24999582682730515, "bright_earth_science_recall@100": 0.6589155670655217, "bright_earth_science_index_time_ms": 137061.12875184044, "bright_earth_science_query_time_ms": 6936.65807088837, "beir_fiqa_ndcg@10": 0.2193785747362115, "beir_fiqa_recall@100": 0.5312052873395465, "beir_fiqa_index_time_ms": 147582.4930849485, "beir_fiqa_query_time_ms": 9011.240383144468, "bright_stackoverflow_ndcg@10": 0.21322894429670405, "bright_stackoverflow_recall@100": 0.5043868859143683, "bright_stackoverflow_index_time_ms": 469936.0579890199, "bright_stackoverflow_query_time_ms": 15176.408494822681, "beir_trec-covid_ndcg@10": 0.6697422106191432, "beir_trec-covid_recall@100": 0.1174516129991351, "beir_trec-covid_index_time_ms": 586984.408733435, "beir_trec-covid_query_time_ms": 2119.4929108023643, "avg_ndcg@10": 0.27272322187430426, "avg_recall@100": 0.4744393028822567, "combined_score": 0.43409608668066624, "total_index_time_ms": 1692189.3046912737, "total_query_time_ms": 90482.69383935258, "total_time_ms": 1782671.9985306263, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 33 lines with 41 lines\nChange 2: Replace 78 lines with 91 lines\nChange 3: Replace 54 lines with 70 lines", "parent_metrics": {"bright_pony_ndcg@10": 0.09714024967586911, "bright_pony_recall@100": 0.30020065840728233, "bright_pony_index_time_ms": 7214.189858175814, "bright_pony_query_time_ms": 1013.2884806953371, "beir_nfcorpus_ndcg@10": 0.32970717518983234, "beir_nfcorpus_recall@100": 0.24881424956331358, "beir_nfcorpus_index_time_ms": 17522.383871022612, "beir_nfcorpus_query_time_ms": 215.05521517246962, "beir_scifact_ndcg@10": 0.6686954020736469, "beir_scifact_recall@100": 0.9182222222222223, "beir_scifact_index_time_ms": 23489.529308862984, "beir_scifact_query_time_ms": 726.9460894167423, "beir_arguana_ndcg@10": 0.26029503598794956, "beir_arguana_recall@100": 0.9186295503211992, "beir_arguana_index_time_ms": 28479.093099944293, "beir_arguana_query_time_ms": 42867.045055143535, "bright_theoremqa_theorems_ndcg@10": 0.03881975824745205, "bright_theoremqa_theorems_recall@100": 0.16885964912280702, "bright_theoremqa_theorems_index_time_ms": 53782.90276695043, "bright_theoremqa_theorems_query_time_ms": 1533.278705086559, "beir_scidocs_ndcg@10": 0.1460069188527766, "beir_scidocs_recall@100": 0.35478333333333334, "beir_scidocs_index_time_ms": 89887.4653945677, "beir_scidocs_query_time_ms": 5506.5374108962715, "bright_economics_ndcg@10": 0.13088048662331905, "bright_economics_recall@100": 0.3759491746126059, "bright_economics_index_time_ms": 39009.70445480198, "bright_economics_query_time_ms": 4077.887760940939, "bright_biology_ndcg@10": 0.24963089921582876, "bright_biology_recall@100": 0.5853559402614078, "bright_biology_index_time_ms": 61822.49208493158, "bright_biology_query_time_ms": 3838.1234239786863, "bright_earth_science_ndcg@10": 0.25817243532343187, "bright_earth_science_recall@100": 0.6564503523044812, "bright_earth_science_index_time_ms": 105529.55313399434, "bright_earth_science_query_time_ms": 6698.879445903003, "beir_fiqa_ndcg@10": 0.219070683761601, "beir_fiqa_recall@100": 0.5295849169691763, "beir_fiqa_index_time_ms": 107281.17302199826, "beir_fiqa_query_time_ms": 8836.03419503197, "bright_stackoverflow_ndcg@10": 0.21223707036566883, "bright_stackoverflow_recall@100": 0.5123640938915762, "bright_stackoverflow_index_time_ms": 421211.49624790996, "bright_stackoverflow_query_time_ms": 15647.630032151937, "beir_trec-covid_ndcg@10": 0.6800265566437084, "beir_trec-covid_recall@100": 0.11744768989833355, "beir_trec-covid_index_time_ms": 534982.1955249645, "beir_trec-covid_query_time_ms": 2286.3832358270884, "avg_ndcg@10": 0.274223555996757, "avg_recall@100": 0.4738884859089782, "combined_score": 0.433955499926534, "total_index_time_ms": 1490212.1787681244, "total_query_time_ms": 93247.08905024454, "total_time_ms": 1583459.267818369, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4340\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4379 \u2192 0.4340. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: 'dl_p: float = 0.75' to ''\nChange 2: Replace 6 lines with 3 lines\nChange 3: Replace 4 lines with 11 lines\nChange 4: Replace 22 lines with 42 lines\nChange 5: Replace 9 lines with 10 lines\nChange 6: Replace 2 lines with 6 lines\nChange 7: Replace 4 lines with 7 lines\nChange 8: Replace 8 lines with 9 lines\nChange 9: Replace sum_evidence += wt * np.log1p(tf_row / (base + eps)) with 4 lines\nChange 10: Replace 7 lines with 10 lines\nChange 11: Replace 7 lines with 18 lines\nChange 12: Replace 10 lines with 25 lines\n- Metrics: bright_pony_ndcg@10: 0.1077, bright_pony_recall@100: 0.2917, bright_pony_index_time_ms: 7555.1467, bright_pony_query_time_ms: 1137.3704, beir_nfcorpus_ndcg@10: 0.3336, beir_nfcorpus_recall@100: 0.2553, beir_nfcorpus_index_time_ms: 17450.9895, beir_nfcorpus_query_time_ms: 277.8385, beir_scifact_ndcg@10: 0.6840, beir_scifact_recall@100: 0.9187, beir_scifact_index_time_ms: 24339.7861, beir_scifact_query_time_ms: 930.2466, beir_arguana_ndcg@10: 0.3002, beir_arguana_recall@100: 0.9336, beir_arguana_index_time_ms: 30392.3920, beir_arguana_query_time_ms: 52064.5465, bright_theoremqa_theorems_ndcg@10: 0.0284, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 55095.1949, bright_theoremqa_theorems_query_time_ms: 1639.9845, beir_scidocs_ndcg@10: 0.1519, beir_scidocs_recall@100: 0.3563, beir_scidocs_index_time_ms: 99729.6748, beir_scidocs_query_time_ms: 7263.2105, bright_economics_ndcg@10: 0.1394, bright_economics_recall@100: 0.3855, bright_economics_index_time_ms: 55509.1828, bright_economics_query_time_ms: 4439.6979, bright_biology_ndcg@10: 0.2602, bright_biology_recall@100: 0.5494, bright_biology_index_time_ms: 70518.6986, bright_biology_query_time_ms: 4190.5752, bright_earth_science_ndcg@10: 0.2809, bright_earth_science_recall@100: 0.6551, bright_earth_science_index_time_ms: 133077.5971, bright_earth_science_query_time_ms: 7543.9347, beir_fiqa_ndcg@10: 0.2353, beir_fiqa_recall@100: 0.5303, beir_fiqa_index_time_ms: 153522.3755, beir_fiqa_query_time_ms: 9892.9281, bright_stackoverflow_ndcg@10: 0.1960, bright_stackoverflow_recall@100: 0.5224, bright_stackoverflow_index_time_ms: 482095.8043, bright_stackoverflow_query_time_ms: 16319.4399, beir_trec-covid_ndcg@10: 0.7080, beir_trec-covid_recall@100: 0.1209, beir_trec-covid_index_time_ms: 626876.5627, beir_trec-covid_query_time_ms: 2594.5513, avg_ndcg@10: 0.2855, avg_recall@100: 0.4760, combined_score: 0.4379, total_index_time_ms: 1756163.4049, total_query_time_ms: 108294.3241, total_time_ms: 1864457.7290, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace 31 lines with 29 lines\nChange 2: Replace 90 lines with 85 lines\nChange 3: Replace 61 lines with 52 lines\nChange 4: Replace 9 lines with 9 lines\n- Metrics: bright_pony_ndcg@10: 0.1127, bright_pony_recall@100: 0.3003, bright_pony_index_time_ms: 7786.9326, bright_pony_query_time_ms: 1245.3978, beir_nfcorpus_ndcg@10: 0.3330, beir_nfcorpus_recall@100: 0.2555, beir_nfcorpus_index_time_ms: 12821.6745, beir_nfcorpus_query_time_ms: 291.8281, beir_scifact_ndcg@10: 0.6808, beir_scifact_recall@100: 0.9187, beir_scifact_index_time_ms: 19246.7742, beir_scifact_query_time_ms: 1095.3024, beir_arguana_ndcg@10: 0.2954, beir_arguana_recall@100: 0.9279, beir_arguana_index_time_ms: 28904.2979, beir_arguana_query_time_ms: 55437.6446, bright_theoremqa_theorems_ndcg@10: 0.0310, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 59916.5198, bright_theoremqa_theorems_query_time_ms: 1679.0689, beir_scidocs_ndcg@10: 0.1511, beir_scidocs_recall@100: 0.3558, beir_scidocs_index_time_ms: 104119.6346, beir_scidocs_query_time_ms: 7257.6871, bright_economics_ndcg@10: 0.1371, bright_economics_recall@100: 0.3723, bright_economics_index_time_ms: 62405.5141, bright_economics_query_time_ms: 4796.7845, bright_biology_ndcg@10: 0.2592, bright_biology_recall@100: 0.5629, bright_biology_index_time_ms: 68112.6566, bright_biology_query_time_ms: 4353.8275, bright_earth_science_ndcg@10: 0.2720, bright_earth_science_recall@100: 0.6611, bright_earth_science_index_time_ms: 146198.1413, bright_earth_science_query_time_ms: 7256.5305, beir_fiqa_ndcg@10: 0.2347, beir_fiqa_recall@100: 0.5286, beir_fiqa_index_time_ms: 160862.4540, beir_fiqa_query_time_ms: 9429.7614, bright_stackoverflow_ndcg@10: 0.1973, bright_stackoverflow_recall@100: 0.5224, bright_stackoverflow_index_time_ms: 479393.2771, bright_stackoverflow_query_time_ms: 15898.8657, beir_trec-covid_ndcg@10: 0.7053, beir_trec-covid_recall@100: 0.1200, beir_trec-covid_index_time_ms: 620428.5713, beir_trec-covid_query_time_ms: 2649.8167, avg_ndcg@10: 0.2841, avg_recall@100: 0.4765, combined_score: 0.4381, total_index_time_ms: 1770196.4477, total_query_time_ms: 111392.5154, total_time_ms: 1881588.9632, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 31 lines with 40 lines\nChange 2: Replace 25 lines with 42 lines\nChange 3: Replace 70 lines with 87 lines\nChange 4: Replace 8 lines with 8 lines\nChange 5: Replace 6 lines with 12 lines\nChange 6: Replace 57 lines with 63 lines\nChange 7: Replace 43 lines with 55 lines\n- Metrics: bright_pony_ndcg@10: 0.1290, bright_pony_recall@100: 0.3196, bright_pony_index_time_ms: 8700.5941, bright_pony_query_time_ms: 1604.7472, beir_nfcorpus_ndcg@10: 0.3372, beir_nfcorpus_recall@100: 0.2551, beir_nfcorpus_index_time_ms: 22372.0019, beir_nfcorpus_query_time_ms: 327.6606, beir_scifact_ndcg@10: 0.6926, beir_scifact_recall@100: 0.9220, beir_scifact_index_time_ms: 28891.3341, beir_scifact_query_time_ms: 1305.9961, beir_arguana_ndcg@10: 0.3077, beir_arguana_recall@100: 0.9358, beir_arguana_index_time_ms: 35495.2623, beir_arguana_query_time_ms: 94331.3494, bright_theoremqa_theorems_ndcg@10: 0.0318, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 59085.4594, bright_theoremqa_theorems_query_time_ms: 2380.3728, beir_scidocs_ndcg@10: 0.1562, beir_scidocs_recall@100: 0.3606, beir_scidocs_index_time_ms: 119007.0439, beir_scidocs_query_time_ms: 9902.6080, bright_economics_ndcg@10: 0.1423, bright_economics_recall@100: 0.3869, bright_economics_index_time_ms: 76000.0366, bright_economics_query_time_ms: 7838.5754, bright_biology_ndcg@10: 0.2553, bright_biology_recall@100: 0.5453, bright_biology_index_time_ms: 81238.9485, bright_biology_query_time_ms: 6800.6634, bright_earth_science_ndcg@10: 0.2945, bright_earth_science_recall@100: 0.6664, bright_earth_science_index_time_ms: 166467.9731, bright_earth_science_query_time_ms: 11754.1740, beir_fiqa_ndcg@10: 0.2437, beir_fiqa_recall@100: 0.5317, beir_fiqa_index_time_ms: 183284.1013, beir_fiqa_query_time_ms: 13699.4116, bright_stackoverflow_ndcg@10: 0.1988, bright_stackoverflow_recall@100: 0.5173, bright_stackoverflow_index_time_ms: 540675.0104, bright_stackoverflow_query_time_ms: 30021.7301, beir_trec-covid_ndcg@10: 0.7023, beir_trec-covid_recall@100: 0.1197, beir_trec-covid_index_time_ms: 778077.4931, beir_trec-covid_query_time_ms: 4007.0410, avg_ndcg@10: 0.2909, avg_recall@100: 0.4794, combined_score: 0.4417, total_index_time_ms: 2099295.2587, total_query_time_ms: 183974.3297, total_time_ms: 2283269.5884, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4424)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave surprisal evidence + informative-coverage boost + mild verbosity prior.\n    k1: float = 0.9   # kept for backwards compatibility (Corpus.norm_array)\n    b: float = 0.4    # kept for backwards compatibility (Corpus.norm_array)\n    epsilon: float = 1e-9\n\n    tf_log_base: float = 1.0\n    dl_alpha: float = 0.15\n    q_clarity_power: float = 0.6\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n\n    # \"Facet prior\": reallocates query mass toward discriminative constraints (query-only).\n    facet_mix: float = 0.12\n    facet_power: float = 1.6\n\n    # Soft-AND / coordination pressure based on informative mass covered.\n    coord_beta: float = 0.08\n\n    # --- Robust lexical matching (secondary channel) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: character n-gram channel for tokenization mismatch (URLs, code, hyphenation).\n    # We keep it tiny + cheap: only query ngrams, treated as pseudo-terms.\n    ngram_n: int = 4\n    ngram_max_per_token: int = 2\n    ngram_weight: float = 0.10\n\n    # --- Rare-key presence (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Surprisal IDF: log1p(N/df).\n\n    Interprets df/N as an occurrence probability; matching a term yields self-information.\n    This is typically smoother and more robust cross-domain than BM25-odds IDF.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent the query as unique lexical constraints but keep sublinear repetition.\n\n        Motivation: repeated tokens can encode emphasis, but linear qtf tends to\n        over-weight verbosity/noisy tokenization.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + clarity gating + coverage, plus:\n      (1) query-internal \"facet prior\" (reallocates weight toward high-IDF constraints)\n      (2) saturating coordination bonus based on informative mass covered\n      (3) bounded rare-key presence multiplier\n\n    Facet prior story:\n      A query is often a mixture of a few decisive facets and many background hints.\n      We approximate this by transforming per-term IDF inside the query:\n        idf_used = (1-m)*idf + m*(idf^p / mean(idf^p))\n      This sharpens ranking (nDCG@10) while keeping the evidence still additive/recall-friendly.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # Precompute query-level scale for facet prior (query-only; stable across documents).\n    idf_pows = []\n    for term in query_repr.terms:\n        tidf = float(idf(float(corpus_df.get(term, 1.0)), N))\n        if tidf > 0.0:\n            idf_pows.append(tidf ** Config.facet_power)\n    mean_idf_pow = (sum(idf_pows) / len(idf_pows)) if idf_pows else 1.0\n    mean_idf_pow = max(mean_idf_pow, eps)\n    mix = max(0.0, min(1.0, Config.facet_mix))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        # bounded query clarity in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # facet-reweighted idf (query-dependent only)\n        facet = (term_idf ** Config.facet_power) / mean_idf_pow\n        idf_used = (1.0 - mix) * term_idf + mix * facet\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # coverage uses plain IDF mass (keeps recall stable across corpora)\n        cov_wt = wq * term_idf * clarity\n        cov_den += cov_wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += cov_wt\n        sum_evidence += (wq * clarity * idf_used) * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and term_idf > Config.rare_idf_pivot:\n            rare_hits += (term_idf - Config.rare_idf_pivot) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    # soft-AND via informative coverage\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n        if Config.coord_beta != 0.0:\n            # saturating coordination: emphasizes \"more constraints satisfied\" without hard AND\n            score *= 1.0 + Config.coord_beta * (1.0 - math.exp(-3.0 * coverage))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel (exact tokens).\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel (prefixes) for robust lexical matching.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # NEW: tiny character n-gram channel (query-only extraction) to survive tokenization gaps.\n    # This is still lexical and uses the same retrieval_score machinery via pseudo-terms.\n    if Config.ngram_weight != 0.0 and Config.ngram_n > 1:\n        n = int(Config.ngram_n)\n        cap = max(1, int(Config.ngram_max_per_token))\n        grams: list[str] = []\n        for t in query:\n            if len(t) < n:\n                continue\n            step = max(1, (len(t) - n) // cap)  # take a couple spaced grams, not all\n            for j in range(0, len(t) - n + 1, step):\n                grams.append(\"G:\" + t[j : j + n])\n                if len(grams) >= cap * max(1, len(query)):\n                    break\n        if grams:\n            gq = QueryRepr.from_tokens(grams)\n            s += Config.ngram_weight * retrieval_score(\n                gq,\n                corpus.ngram_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: tiny character n-gram view (disjoint channel) for tokenization mismatch.\n        n = int(Config.ngram_n) if getattr(Config, \"ngram_n\", 0) else 0\n        cap = max(1, int(Config.ngram_max_per_token)) if getattr(Config, \"ngram_max_per_token\", 0) else 1\n        if n > 1:\n            docs_ngrams: list[list[str]] = []\n            for doc in documents:\n                gs: list[str] = []\n                for t in doc:\n                    if len(t) < n:\n                        continue\n                    step = max(1, (len(t) - n) // cap)\n                    for j in range(0, len(t) - n + 1, step):\n                        gs.append(\"G:\" + t[j : j + n])\n                        if len(gs) >= cap * max(1, len(doc)):\n                            break\n                docs_ngrams.append(gs)\n            self.ngram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_ngrams]\n        else:\n            docs_ngrams = [[] for _ in documents]\n            self.ngram_doc_tf_dicts = [Counter() for _ in documents]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged ngrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, gdoc in zip(documents, docs_prefix, docs_ngrams):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for g in gdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, gdoc) in enumerate(zip(documents, docs_prefix, docs_ngrams)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            gram_counts = Counter(gdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in gram_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for all channels (prefix + ngram keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        # Query-level facet prior scale (computed from idf_array; query-only).\n        idf_vals = np.array([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        idf_vals = np.maximum(idf_vals, 0.0)\n        idf_pow = np.power(idf_vals, Config.facet_power, dtype=np.float64)\n        mean_idf_pow = float(np.mean(idf_pow)) if idf_pow.size > 0 else 1.0\n        mean_idf_pow = max(mean_idf_pow, eps)\n        mix = max(0.0, min(1.0, Config.facet_mix))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            facet = (idf_val ** Config.facet_power) / mean_idf_pow\n            idf_used = (1.0 - mix) * idf_val + mix * facet\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            cov_wt = wq * idf_val * clarity\n            cov_den += cov_wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += cov_wt * present\n            sum_evidence += (wq * clarity * idf_used) * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n            if Config.coord_beta != 0.0:\n                scores *= 1.0 + Config.coord_beta * (1.0 - np.exp(-3.0 * coverage))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query: tokens + (optional) tagged prefixes + (optional) tagged ngrams.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if Config.ngram_weight != 0.0 and Config.ngram_n > 1:\n            n = int(Config.ngram_n)\n            cap = max(1, int(Config.ngram_max_per_token))\n            gcounts: Counter[str] = Counter()\n            for t in query:\n                if len(t) < n:\n                    continue\n                step = max(1, (len(t) - n) // cap)\n                for j in range(0, len(t) - n + 1, step):\n                    gcounts[\"G:\" + t[j : j + n]] += 1\n            for g, c in gcounts.items():\n                tid = self.corpus.get_term_id(g)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.ngram_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1320), Performs well on bright_pony_recall@100 (0.3309), Performs well on bright_pony_index_time_ms (7652.7138), Performs well on bright_pony_query_time_ms (2335.3511), Performs well on beir_nfcorpus_ndcg@10 (0.3348), Performs well on beir_nfcorpus_recall@100 (0.2612), Performs well on beir_nfcorpus_index_time_ms (14049.6021), Performs well on beir_nfcorpus_query_time_ms (611.2380), Performs well on beir_scifact_ndcg@10 (0.6829), Performs well on beir_scifact_recall@100 (0.9277), Performs well on beir_scifact_index_time_ms (26666.3820), Performs well on beir_scifact_query_time_ms (2399.1917), Performs well on beir_arguana_ndcg@10 (0.2885), Performs well on beir_arguana_recall@100 (0.9222), Performs well on beir_arguana_index_time_ms (33638.4428), Performs well on beir_arguana_query_time_ms (111292.3698), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0379), Performs well on bright_theoremqa_theorems_recall@100 (0.1996), Performs well on bright_theoremqa_theorems_index_time_ms (62414.3157), Performs well on bright_theoremqa_theorems_query_time_ms (3371.5720), Performs well on beir_scidocs_ndcg@10 (0.1510), Performs well on beir_scidocs_recall@100 (0.3515), Performs well on beir_scidocs_index_time_ms (110659.8768), Performs well on beir_scidocs_query_time_ms (18673.6997), Performs well on bright_economics_ndcg@10 (0.1241), Performs well on bright_economics_recall@100 (0.3779), Performs well on bright_economics_index_time_ms (60700.7228), Performs well on bright_economics_query_time_ms (11344.0301), Performs well on bright_biology_ndcg@10 (0.2488), Performs well on bright_biology_recall@100 (0.5724), Performs well on bright_biology_index_time_ms (77664.5399), Performs well on bright_biology_query_time_ms (9726.0357), Performs well on bright_earth_science_ndcg@10 (0.2772), Performs well on bright_earth_science_recall@100 (0.6534), Performs well on bright_earth_science_index_time_ms (162864.8460), Performs well on bright_earth_science_query_time_ms (18749.9975), Performs well on beir_fiqa_ndcg@10 (0.2344), Performs well on beir_fiqa_recall@100 (0.5259), Performs well on beir_fiqa_index_time_ms (179909.2082), Performs well on beir_fiqa_query_time_ms (18439.9295), Performs well on bright_stackoverflow_ndcg@10 (0.1984), Performs well on bright_stackoverflow_recall@100 (0.5313), Performs well on bright_stackoverflow_index_time_ms (505777.9874), Performs well on bright_stackoverflow_query_time_ms (36715.0461), Performs well on beir_trec-covid_ndcg@10 (0.7260), Performs well on beir_trec-covid_recall@100 (0.1236), Performs well on beir_trec-covid_index_time_ms (738245.6470), Performs well on beir_trec-covid_query_time_ms (6052.4496), Performs well on avg_ndcg@10 (0.2863), Performs well on avg_recall@100 (0.4815), Performs well on combined_score (0.4424), Performs well on total_index_time_ms (1980244.2846), Performs well on total_query_time_ms (239710.9108), Performs well on total_time_ms (2219955.1954), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4417)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep BM25 knobs for candidate scoring / normalization compatibility.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence is concave in tf; using log evidence is often more robust cross-domain than BM25 tf/(tf+k).\n    tf_log_base: float = 1.0\n\n    # Soft-AND over informative query mass (bounded; recall-safe).\n    coverage_gamma: float = 0.25\n\n    # Gentle query-side clarity gate: suppress extremely common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting (keeps emphasis without verbosity blowups).\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior (stable across corpora).\n    dl_alpha: float = 0.15\n\n    # Avoid over-compressing IDF; concavity is handled via log-evidence and bounded boosts.\n    idf_power: float = 1.0\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # NEW: approximate \"term burstiness\" normalization via doc-frequency (not corpus df).\n    # If a term repeats many times in a doc, it may be boilerplate; penalize repeated tf\n    # relative to how many unique terms the doc has.\n    burst_k: float = 2.0\n\n    # NEW: ultra-cheap ordered-bigram channel to capture phrases / adjacency without positions.\n    # Helps nDCG@10 on QA/scientific queries where key concepts appear as bigrams.\n    bigram_weight: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Empirically more stable cross-domain than odds/log1p(N/df), and works well with\n    coverage-style soft-AND because it behaves like \"bits of evidence\".\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique lexical constraints + sublinear repetition weights.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + mild verbosity prior\n    + bounded rare-key presence multiplier.\n\n    Evidence view:\n      sum_t w(t) * log1p(tf/base)  then compress as log1p(total_evidence).\n    This behaves like accumulating diminishing \"bits\" of lexical evidence and\n    tends to improve early precision while staying recall-safe.\n\n    Coverage:\n      multiply by 1 + gamma * (covered informative mass / total informative mass).\n\n    Rare-key presence:\n      if an ultra-rare query token appears at least once, apply a small bounded boost.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        if tidf <= 0.0:\n            continue\n\n        # bounded clarity in [0,1]\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        # Burstiness normalization: repeated mentions within a doc are discounted if the\n        # doc has low lexical diversity (boilerplate/listings).\n        # Approximate unique_len via Counter size (works for all channels).\n        uniq_len = float(max(1, len(doc_tf)))\n        tf_eff = tf / (1.0 + float(Config.burst_k) * (tf / (uniq_len + eps)))\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf_eff / (base + eps))\n\n        # indicator-like rare-key presence\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds optional prefix + bigram channels (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(\n        q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n    )\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Tertiary channel: ordered bigrams (phrase proxy).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        btoks = [f\"B:{query[i]}_{query[i+1]}\" for i in range(len(query) - 1)]\n        if btoks:\n            bq = QueryRepr.from_tokens(btoks)\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq,\n                corpus.bigram_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Tertiary \"bigram lexicon\" view (ordered adjacency proxy).\n        docs_bigrams = [[f\"B:{doc[i]}_{doc[i+1]}\" for i in range(len(doc) - 1)] for doc in documents]\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigrams]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigrams):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                if bg not in self._vocab:\n                    self._vocab[bg] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n        self._doc_unique_len = np.array([max(1, len(set(doc))) for doc in documents], dtype=np.float64)\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigrams)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for all channels (prefix/bigram keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Use precomputed unique-term counts for the primary-token view as a proxy for diversity.\n        uniq_len = self.corpus._doc_unique_len[candidate_docs].astype(np.float64, copy=False)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            # Burstiness normalization (vectorized)\n            tf_eff = tf_row / (1.0 + float(Config.burst_k) * (tf_row / (uniq_len + eps)))\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_eff / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        if self.corpus.avgdl > 0.0 and Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel)\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (tertiary lexical channel)\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(f\"B:{query[i]}_{query[i+1]}\" for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1290), Performs well on bright_pony_recall@100 (0.3196), Performs well on bright_pony_index_time_ms (8700.5941), Performs well on bright_pony_query_time_ms (1604.7472), Performs well on beir_nfcorpus_ndcg@10 (0.3372), Performs well on beir_nfcorpus_recall@100 (0.2551), Performs well on beir_nfcorpus_index_time_ms (22372.0019), Performs well on beir_nfcorpus_query_time_ms (327.6606), Performs well on beir_scifact_ndcg@10 (0.6926), Performs well on beir_scifact_recall@100 (0.9220), Performs well on beir_scifact_index_time_ms (28891.3341), Performs well on beir_scifact_query_time_ms (1305.9961), Performs well on beir_arguana_ndcg@10 (0.3077), Performs well on beir_arguana_recall@100 (0.9358), Performs well on beir_arguana_index_time_ms (35495.2623), Performs well on beir_arguana_query_time_ms (94331.3494), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0318), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (59085.4594), Performs well on bright_theoremqa_theorems_query_time_ms (2380.3728), Performs well on beir_scidocs_ndcg@10 (0.1562), Performs well on beir_scidocs_recall@100 (0.3606), Performs well on beir_scidocs_index_time_ms (119007.0439), Performs well on beir_scidocs_query_time_ms (9902.6080), Performs well on bright_economics_ndcg@10 (0.1423), Performs well on bright_economics_recall@100 (0.3869), Performs well on bright_economics_index_time_ms (76000.0366), Performs well on bright_economics_query_time_ms (7838.5754), Performs well on bright_biology_ndcg@10 (0.2553), Performs well on bright_biology_recall@100 (0.5453), Performs well on bright_biology_index_time_ms (81238.9485), Performs well on bright_biology_query_time_ms (6800.6634), Performs well on bright_earth_science_ndcg@10 (0.2945), Performs well on bright_earth_science_recall@100 (0.6664), Performs well on bright_earth_science_index_time_ms (166467.9731), Performs well on bright_earth_science_query_time_ms (11754.1740), Performs well on beir_fiqa_ndcg@10 (0.2437), Performs well on beir_fiqa_recall@100 (0.5317), Performs well on beir_fiqa_index_time_ms (183284.1013), Performs well on beir_fiqa_query_time_ms (13699.4116), Performs well on bright_stackoverflow_ndcg@10 (0.1988), Performs well on bright_stackoverflow_recall@100 (0.5173), Performs well on bright_stackoverflow_index_time_ms (540675.0104), Performs well on bright_stackoverflow_query_time_ms (30021.7301), Performs well on beir_trec-covid_ndcg@10 (0.7023), Performs well on beir_trec-covid_recall@100 (0.1197), Performs well on beir_trec-covid_index_time_ms (778077.4931), Performs well on beir_trec-covid_query_time_ms (4007.0410), Performs well on avg_ndcg@10 (0.2909), Performs well on avg_recall@100 (0.4794), Performs well on combined_score (0.4417), Performs well on total_index_time_ms (2099295.2587), Performs well on total_query_time_ms (183974.3297), Performs well on total_time_ms (2283269.5884), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4381)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + informative coverage + rare-key matches,\n    # plus (A) cheap prefix robustness and (B) query-only facet/coordination shaping.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    qtf_power: float = 0.5\n    dl_alpha: float = 0.15\n\n    # Query-only \"facet prior\": reallocates weight toward the most discriminative query terms.\n    facet_mix: float = 0.12\n    facet_power: float = 1.6\n\n    # Saturating coordination bonus based on informative mass covered (soft-AND).\n    coord_beta: float = 0.08\n\n    # Rare-key presence: bounded multiplier for ultra-rare terms (keep conservative).\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Prefix channel: robust lexical matching for morphology/camelcase fragments.\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Compatibility leftovers (Corpus references b/k1)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float, uniq: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n        self.uniq = uniq  # number of unique terms (proxy for repetition/burstiness)\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)), uniq=float(len(tf)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    doc_uniq: float | None = None,\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + informative coverage, with:\n      (1) query-only \"facet prior\" (reallocates weight toward high-IDF constraints)\n      (2) saturating coordination bonus based on informative mass covered\n      (3) bounded rare-key presence multiplier (indicator-like)\n\n    Rationale:\n      - evidence is additive and concave in TF: repeated mentions add diminishing information\n      - facet prior approximates that queries mix a few decisive facets + many weak hints\n      - coordination uses *informative* coverage, avoiding brittle hard-AND behavior\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # Query-only scale for facet prior.\n    idf_pows: list[float] = []\n    for term in query_repr.terms:\n        tidf = float(idf(float(corpus_df.get(term, 1.0)), N))\n        if tidf > 0.0:\n            idf_pows.append(tidf ** Config.facet_power)\n    mean_idf_pow = (sum(idf_pows) / len(idf_pows)) if idf_pows else 1.0\n    mean_idf_pow = max(mean_idf_pow, eps)\n    mix = max(0.0, min(1.0, Config.facet_mix))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        facet = (term_idf ** Config.facet_power) / mean_idf_pow\n        idf_used = (1.0 - mix) * term_idf + mix * facet\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        cov_wt = wq * term_idf * clarity\n        cov_den += cov_wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += cov_wt\n        sum_evidence += (wq * clarity * idf_used) * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and term_idf > Config.rare_idf_pivot:\n            rare_hits += (term_idf - Config.rare_idf_pivot) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n        if Config.coord_beta != 0.0:\n            score *= 1.0 + Config.coord_beta * (1.0 - math.exp(-3.0 * coverage))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_repr = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        doc_repr.term_frequencies,\n        float(doc_repr.length),\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n        None,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Store per-doc representations (gives uniq-term count; kept for backward compatibility).\n        self.doc_repr: list[DocumentRepr] = [DocumentRepr.from_tokens(d) for d in documents]\n\n        # Optional prefix view (purely lexical robustness).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") to keep channels disjoint.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [r.term_frequencies for r in self.doc_repr]\n\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n\n        for doc_idx, (r, pdoc) in enumerate(zip(self.doc_repr, docs_prefix)):\n            term_counts = r.term_frequencies\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # Precompute unique-term counts per doc for vectorized burst normalization.\n        self.doc_uniq_array = np.array([r.uniq for r in self.doc_repr], dtype=np.float64)\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        # Query-only facet prior scale.\n        idf_vals = np.array([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        idf_vals = np.maximum(idf_vals, 0.0)\n        idf_pow = np.power(idf_vals, Config.facet_power, dtype=np.float64)\n        mean_idf_pow = float(np.mean(idf_pow)) if idf_pow.size > 0 else 1.0\n        mean_idf_pow = max(mean_idf_pow, eps)\n        mix = max(0.0, min(1.0, Config.facet_mix))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            facet = (idf_val ** Config.facet_power) / mean_idf_pow\n            idf_used = (1.0 - mix) * idf_val + mix * facet\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            cov_wt = wq * idf_val * clarity\n            cov_den += cov_wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += cov_wt * present\n            sum_evidence += (wq * clarity * idf_used) * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n            if Config.coord_beta != 0.0:\n                scores *= 1.0 + Config.coord_beta * (1.0 - np.exp(-3.0 * coverage))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        # Add tagged prefixes as additional pseudo-terms (small weight).\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1127), Performs well on bright_pony_recall@100 (0.3003), Performs well on bright_pony_index_time_ms (7786.9326), Performs well on bright_pony_query_time_ms (1245.3978), Performs well on beir_nfcorpus_ndcg@10 (0.3330), Performs well on beir_nfcorpus_recall@100 (0.2555), Performs well on beir_nfcorpus_index_time_ms (12821.6745), Performs well on beir_nfcorpus_query_time_ms (291.8281), Performs well on beir_scifact_ndcg@10 (0.6808), Performs well on beir_scifact_recall@100 (0.9187), Performs well on beir_scifact_index_time_ms (19246.7742), Performs well on beir_scifact_query_time_ms (1095.3024), Performs well on beir_arguana_ndcg@10 (0.2954), Performs well on beir_arguana_recall@100 (0.9279), Performs well on beir_arguana_index_time_ms (28904.2979), Performs well on beir_arguana_query_time_ms (55437.6446), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0310), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (59916.5198), Performs well on bright_theoremqa_theorems_query_time_ms (1679.0689), Performs well on beir_scidocs_ndcg@10 (0.1511), Performs well on beir_scidocs_recall@100 (0.3558), Performs well on beir_scidocs_index_time_ms (104119.6346), Performs well on beir_scidocs_query_time_ms (7257.6871), Performs well on bright_economics_ndcg@10 (0.1371), Performs well on bright_economics_recall@100 (0.3723), Performs well on bright_economics_index_time_ms (62405.5141), Performs well on bright_economics_query_time_ms (4796.7845), Performs well on bright_biology_ndcg@10 (0.2592), Performs well on bright_biology_recall@100 (0.5629), Performs well on bright_biology_index_time_ms (68112.6566), Performs well on bright_biology_query_time_ms (4353.8275), Performs well on bright_earth_science_ndcg@10 (0.2720), Performs well on bright_earth_science_recall@100 (0.6611), Performs well on bright_earth_science_index_time_ms (146198.1413), Performs well on bright_earth_science_query_time_ms (7256.5305), Performs well on beir_fiqa_ndcg@10 (0.2347), Performs well on beir_fiqa_recall@100 (0.5286), Performs well on beir_fiqa_index_time_ms (160862.4540), Performs well on beir_fiqa_query_time_ms (9429.7614), Performs well on bright_stackoverflow_ndcg@10 (0.1973), Performs well on bright_stackoverflow_recall@100 (0.5224), Performs well on bright_stackoverflow_index_time_ms (479393.2771), Performs well on bright_stackoverflow_query_time_ms (15898.8657), Performs well on beir_trec-covid_ndcg@10 (0.7053), Performs well on beir_trec-covid_recall@100 (0.1200), Performs well on beir_trec-covid_index_time_ms (620428.5713), Performs well on beir_trec-covid_query_time_ms (2649.8167), Performs well on avg_ndcg@10 (0.2841), Performs well on avg_recall@100 (0.4765), Performs well on combined_score (0.4381), Performs well on total_index_time_ms (1770196.4477), Performs well on total_query_time_ms (111392.5154), Performs well on total_time_ms (1881588.9632), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + IDF-weighted coverage + bounded query-clarity gating.\n    epsilon: float = 1e-9\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Evidence: wt * log1p(tf/base), then log1p(total evidence).\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the *informative* query mass.\n    coverage_gamma: float = 0.25\n\n    # Query-side clarity gate: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting: count**p (keeps emphasis w/o verbosity blowups)\n    qtf_power: float = 0.5\n\n    # Mild length prior: downweight extremely long docs gently (helps precision w/o killing recall).\n    dl_alpha: float = 0.15\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage.\n\n    Evidence:\n      sum_t [ wq(t) * idf(t) * clarity(t) * log1p(tf/base) ]\n      then compress with log1p(total) to get diminishing returns and encourage coverage.\n\n    Coverage prior:\n      multiply by 1 + gamma * (covered_idf_mass / total_idf_mass)\n      (soft-AND over *informative* constraints).\n\n    This removes pairwise synergy/common-damp which tended to be brittle cross-dataset.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        # bounded clarity gate in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # Presence-gated rare-key evidence (bounded): helpful for identifiers/symbols.\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 0.0):\n            rare_hits += (term_idf - float(Config.rare_idf_pivot)) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    # mild verbosity prior\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view for robust lexical matching (secondary channel).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only (not used by current scorer): standard BM25-style length norm.\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 0.0):\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1077), Performs well on bright_pony_recall@100 (0.2917), Performs well on bright_pony_index_time_ms (7555.1467), Performs well on bright_pony_query_time_ms (1137.3704), Performs well on beir_nfcorpus_ndcg@10 (0.3336), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (17450.9895), Performs well on beir_nfcorpus_query_time_ms (277.8385), Performs well on beir_scifact_ndcg@10 (0.6840), Performs well on beir_scifact_recall@100 (0.9187), Performs well on beir_scifact_index_time_ms (24339.7861), Performs well on beir_scifact_query_time_ms (930.2466), Performs well on beir_arguana_ndcg@10 (0.3002), Performs well on beir_arguana_recall@100 (0.9336), Performs well on beir_arguana_index_time_ms (30392.3920), Performs well on beir_arguana_query_time_ms (52064.5465), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0284), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (55095.1949), Performs well on bright_theoremqa_theorems_query_time_ms (1639.9845), Performs well on beir_scidocs_ndcg@10 (0.1519), Performs well on beir_scidocs_recall@100 (0.3563), Performs well on beir_scidocs_index_time_ms (99729.6748), Performs well on beir_scidocs_query_time_ms (7263.2105), Performs well on bright_economics_ndcg@10 (0.1394), Performs well on bright_economics_recall@100 (0.3855), Performs well on bright_economics_index_time_ms (55509.1828), Performs well on bright_economics_query_time_ms (4439.6979), Performs well on bright_biology_ndcg@10 (0.2602), Performs well on bright_biology_recall@100 (0.5494), Performs well on bright_biology_index_time_ms (70518.6986), Performs well on bright_biology_query_time_ms (4190.5752), Performs well on bright_earth_science_ndcg@10 (0.2809), Performs well on bright_earth_science_recall@100 (0.6551), Performs well on bright_earth_science_index_time_ms (133077.5971), Performs well on bright_earth_science_query_time_ms (7543.9347), Performs well on beir_fiqa_ndcg@10 (0.2353), Performs well on beir_fiqa_recall@100 (0.5303), Performs well on beir_fiqa_index_time_ms (153522.3755), Performs well on beir_fiqa_query_time_ms (9892.9281), Performs well on bright_stackoverflow_ndcg@10 (0.1960), Performs well on bright_stackoverflow_recall@100 (0.5224), Performs well on bright_stackoverflow_index_time_ms (482095.8043), Performs well on bright_stackoverflow_query_time_ms (16319.4399), Performs well on beir_trec-covid_ndcg@10 (0.7080), Performs well on beir_trec-covid_recall@100 (0.1209), Performs well on beir_trec-covid_index_time_ms (626876.5627), Performs well on beir_trec-covid_query_time_ms (2594.5513), Performs well on avg_ndcg@10 (0.2855), Performs well on avg_recall@100 (0.4760), Performs well on combined_score (0.4379), Performs well on total_index_time_ms (1756163.4049), Performs well on total_query_time_ms (108294.3241), Performs well on total_time_ms (1864457.7290), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep BM25 knobs for candidate scoring / normalization compatibility.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence is concave in tf; using log evidence is often more robust cross-domain than BM25 tf/(tf+k).\n    tf_log_base: float = 1.0\n\n    # Soft-AND over informative query mass (bounded; recall-safe).\n    coverage_gamma: float = 0.25\n\n    # Gentle query-side clarity gate: suppress extremely common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting (keeps emphasis without verbosity blowups).\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior (stable across corpora).\n    dl_alpha: float = 0.15\n\n    # Avoid over-compressing IDF; concavity is handled via log-evidence and bounded boosts.\n    idf_power: float = 1.0\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Empirically more stable cross-domain than odds/log1p(N/df), and works well with\n    coverage-style soft-AND because it behaves like \"bits of evidence\".\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique lexical constraints + sublinear repetition weights.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + mild verbosity prior\n    + bounded rare-key presence multiplier.\n\n    Evidence view:\n      sum_t w(t) * log1p(tf/base)  then compress as log1p(total_evidence).\n    This behaves like accumulating diminishing \"bits\" of lexical evidence and\n    tends to improve early precision while staying recall-safe.\n\n    Coverage:\n      multiply by 1 + gamma * (covered informative mass / total informative mass).\n\n    Rare-key presence:\n      if an ultra-rare query token appears at least once, apply a small bounded boost.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        if tidf <= 0.0:\n            continue\n\n        # bounded clarity in [0,1]\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # indicator-like rare-key presence\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        if self.corpus.avgdl > 0.0 and Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel)\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D2 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    epsilon: float = 1e-9\n\n    # Concave TF evidence: log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the informative query mass.\n    coverage_gamma: float = 0.25\n\n    # Query-side clarity gate (bounded in [0,1]) to suppress very common terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting.\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior (gentle; keeps recall@100 healthy).\n    dl_alpha: float = 0.15\n\n    # Secondary lexical channel: fixed-length prefixes (morphology/camelcase robustness).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: bounded rare-key presence boost.\n    # Intuition: for identifier/symbol heavy queries, \"appears at all\" can be highly diagnostic.\n    # Kept bounded and small to avoid spiky cross-domain failures.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Keep for compatibility with any external expectations (not used by scorer).\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a *sublinear* repetition signal.\n\n        Motivation: repetition sometimes encodes emphasis (esp. noisy QA queries),\n        but linear qtf can over-weight verbosity. Use qtf^p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + mild verbosity prior\n    + bounded rare-key *presence* multiplier.\n\n    The rare-key multiplier is indicator-like: a single hit on an ultra-rare query token\n    (theorem name, gene symbol, error code) often matters more than repeated tf, but the\n    boost is bounded to stay stable cross-domain.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            # bounded per-term contribution in (0,1); aggregate then log1p to keep bounded overall\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    # mild verbosity prior (recall-safe)\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view (secondary lexical channel).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for exact-token score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility: keep a BM25-like norm array (not used by current scorer).\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # DF exposed for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query: tokens + (optional) tagged prefixes as extra pseudo-terms.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D3 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep base method but add two *cheap* lexical robustness signals seen to help\n    # across BEIR+BRIGHT mixes:\n    #  (1) prefix channel (morphology/camelcase tolerance)\n    #  (2) bounded rare-key presence boost (identifier/symbol queries)\n    epsilon: float = 1e-9\n\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    qtf_power: float = 0.5\n    dl_alpha: float = 0.15\n\n    # Secondary lexical channel: match fixed-length prefixes as extra pseudo-terms.\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Rare-key presence multiplier (indicator-like; bounded).\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one style smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique constraints + sublinear repetition (keeps emphasis without verbosity blowups).\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage,\n    plus a bounded *rare-key presence* multiplier.\n\n    Rare-key presence is indicator-like: a single hit on a very high-IDF query token\n    (identifiers, theorem names, gene symbols) should be disproportionately informative,\n    but the boost must be bounded for cross-domain stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 1e9):\n            # bounded in (0,1) per term; aggregated then log1p\n            rare_hits += (term_idf - Config.rare_idf_pivot) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a small prefix view (still purely lexical) and store per-doc prefix TFs\n        # for score() path. Prefix terms are tagged (\"P:\") so they don't collide.\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary: tokens + tagged prefixes so rank() can use postings for both.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # DF exposed for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 1e9):\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query: tokens + (optional) tagged prefixes as extra pseudo-terms.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D4 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Evidence should be \"bits-like\" and stable cross-domain; we will use concave TF (log1p) rather\n    # than BM25 saturation to reduce over-rewarding boilerplate repetition.\n    epsilon: float = 1e-9\n\n    # Term utility concavity: log1p(tf / base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND over *informative query mass covered* (bounded multiplicative prior).\n    coverage_gamma: float = 0.25\n\n    # Query term repetition is sublinear: count**p (prevents verbose queries from dominating).\n    qtf_power: float = 0.5\n\n    # Downweight extremely common query terms smoothly:\n    # clarity(t) = (idf/(idf+1))**p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Mild verbosity prior (gentle; helps precision without killing recall@100).\n    dl_alpha: float = 0.15\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    # Intuition: one hit on an ultra-rare query token (symbol/identifier) is very informative.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Keep for compatibility with Corpus.norm_array construction (not used by scorer).\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Compared to log1p(N/df), this is more stable for very common terms and avoids\n    over-spiking ultra-rare terms across heterogeneous corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique lexical constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage + mild verbosity prior.\n\n    Evidence:\n      sum_t [ wq(t) * idf(t) * clarity(t) * log1p(tf/base) ], then log1p(total).\n    Coverage prior (soft-AND over informative constraints):\n      score *= 1 + gamma * (covered_mass / total_mass)\n    Length prior:\n      score /= 1 + alpha*log1p((dl+1)/(avgdl+1))\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        # bounded clarity in [0,1] to suppress very common query tokens without hard stopwording\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # Presence-gated rare-key evidence (bounded): helpful for identifiers/symbols.\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only: current scorer uses its own mild verbosity prior.\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n        # Match QueryRepr: unique constraints + sublinear qtf weights.\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel)\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3483, Type: Exploratory)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core BM25 knobs\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization exponent (reduces over-penalizing long docs).\n    # This was consistently helpful in the best historical variants.\n    dl_p: float = 0.75\n\n    # Soft-AND / coverage pressure (multiplicative, bounded).\n    # Lower than current: the present 0.6 tends to hurt recall@100 on long/verbose corpora.\n    and_power: float = 0.25\n\n    # Add back a *bounded* coordination boost (helps nDCG@10 without hard AND-ing).\n    coord_alpha: float = 0.20\n\n    # Query-adaptive length prior:\n    # apply a pivoted-length penalty only when the query is \"specific\" (high avg idf).\n    focus_lambda: float = 0.20   # milder to protect recall\n    focus_idf_pivot: float = 2.2 # slightly easier to trigger for short/technical queries\n\n    # Mild compression of extreme IDF (kept, but used on new IDF)\n    idf_power: float = 0.9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Bounded self-information: log(1 + N/df).\n\n    Rationale: keeps the \"surprisal\" interpretation (rare -> informative) but avoids\n    very large spikes for ultra-rare terms that can dominate rankings in noisy queries.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique lexical constraints (set-like).\n        Repeated tokens are usually not meaningful and can destabilize coverage math.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Query-Adaptive Soft-AND BM25 (QASA-BM25), revised for recall safety.\n\n    Changes vs current:\n    - Sublinear length normalization dl^p (p<1): long docs are penalized less harshly.\n    - Weaken soft-AND (smaller and_power) and add a bounded coordination boost:\n        score *= (1 + coord_alpha * coverage)\n      This keeps the \"more query constraints is better\" intuition, but avoids\n      collapsing scores for partial matches (important for recall@100).\n    - Keep the query-adaptive focus prior, but milder.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear-length BM25 norm (matches vectorized path).\n    p = Config.dl_p\n    dl = float(doc_length) ** p\n    adl = float(avgdl) ** p if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    sum_idf = 0.0\n    uq = float(len(query_repr.terms))\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0.0))\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        sum_idf += tidf\n\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += tidf * tf_part\n\n    if score <= 0.0 or uq <= 0.0:\n        return 0.0\n\n    coverage = matched / (uq + eps)\n\n    # Soft-AND (kept mild) + bounded coordination (recall-safer than exponent alone).\n    if Config.and_power != 0.0:\n        score *= (coverage + eps) ** Config.and_power\n    if Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * coverage\n\n    # Query-adaptive focus prior (milder + uses same query specificity summary as before).\n    avg_idf = sum_idf / (uq + eps)\n    gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n    if gate > 0.0 and avgdl > 0.0:\n        pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (doc_length / (avgdl + eps)))\n        score *= pivot\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        sum_idf = 0.0\n        uq = float(max(1, len(query_term_ids)))\n\n        for term_id in query_term_ids:\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            sum_idf += idf_val\n            if idf_val <= 0.0:\n                continue\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += idf_val * tf_part\n\n        coverage = matched / (uq + eps)\n\n        if Config.and_power != 0.0:\n            scores *= np.power(coverage + eps, Config.and_power)\n        if Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * coverage\n\n        # query-adaptive focus prior (scalar gate shared across candidate docs)\n        avg_idf = sum_idf / (uq + eps)\n        gate = 1.0 - math.exp(-max(0.0, avg_idf - Config.focus_idf_pivot))\n        if gate > 0.0 and self.corpus.avgdl > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            pivot = 1.0 / (1.0 + Config.focus_lambda * gate * (dl / (self.corpus.avgdl + eps)))\n            scores *= pivot\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Use unique query terms (matches QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = None\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4348, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave-evidence + informative coverage baseline.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    qtf_power: float = 0.5\n    dl_alpha: float = 0.15\n\n    # Residual-IDF: keep, but avoid \"hard zeroing\" on long docs by using a smooth ReLU.\n    resid_rho: float = 0.35\n    resid_floor: float = 0.0\n    ridf_tau: float = 0.35   # softness for ridf = tau*log1p(exp((x-floor)/tau))\n\n    burst_beta: float = 0.12\n\n    # NEW: document-specificity prior (anti-background).\n    # Penalize docs whose token distribution is too close to the corpus background.\n    # This helps nDCG@10 on verbose/boilerplate-heavy corpora without a harsh length penalty.\n    spec_lambda: float = 0.08\n\n    # NEW: lightweight \"rare key\" presence channel (indicator-like, not TF-driven).\n    # Helps symbol/name-heavy BRIGHT tasks where 1 hit on an identifier matters.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.18\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Rationale: treats df/N as an empirical occurrence probability. This tends to be\n    more stable cross-domain than BM25's odds-style IDF (less brittle on BRIGHT).\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints but keep a sublinear repetition signal.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Residual-IDF evidence + informative coverage + two small priors:\n\n    (1) Smooth residual-IDF:\n        ridf = softplus((idf - rho*v) - floor)\n      instead of hard max(0,\u00b7), which can abruptly kill recall on long-doc corpora.\n\n    (2) Specificity prior (anti-background):\n        multiply by 1 + spec_lambda * (1 - H_d / log(V_d))\n      where H_d is the entropy of the document unigram distribution.\n      Boilerplatey/repetitive docs have low effective vocabulary -> lower entropy ratio -> higher penalty.\n\n    (3) Rare-key presence:\n        a bounded bonus for having any query terms with very high IDF present at least once.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    v = math.log1p((doc_length + 1.0) / (avgdl + 1.0))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    # Precompute doc specificity (entropy-normalized); cheap from Counter.\n    spec = 0.0\n    if Config.spec_lambda != 0.0 and doc_length > 1.0 and len(doc_tf) > 1:\n        inv_dl = 1.0 / (doc_length + eps)\n        H = 0.0\n        for c in doc_tf.values():\n            p = float(c) * inv_dl\n            H -= p * math.log(p + eps)\n        Hmax = math.log(float(len(doc_tf)) + eps)\n        spec = max(0.0, 1.0 - (H / (Hmax + eps)))  # 0..1 (higher => more peaked => less background-like)\n\n    tau = max(Config.ridf_tau, 1e-6)\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        # smooth residualization (softplus around floor)\n        x = (term_idf - Config.resid_rho * v) - Config.resid_floor\n        ridf = tau * math.log1p(math.exp(x / tau))\n        if ridf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * ridf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        tfu = math.log1p(tf / (base + eps))\n        burst = Config.burst_beta * tfu * math.log1p(N / (df + eps))\n        sum_evidence += wt * (tfu + burst)\n\n        if Config.rare_boost != 0.0 and term_idf > Config.rare_idf_pivot:\n            rare_hits += (term_idf - Config.rare_idf_pivot) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    qmass = cov_den if cov_den > 0.0 else 1.0\n    score = math.log1p(sum_evidence / (qmass + eps))\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    if Config.spec_lambda != 0.0 and spec > 0.0:\n        score /= (1.0 + Config.spec_lambda * spec)\n\n    dl_damp = 1.0 + Config.dl_alpha * v\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n        # cheap per-doc proxy used by vectorized specificity prior\n        self.doc_unique_counts = np.array([len(tf) for tf in self._doc_tf_dicts], dtype=np.float64)\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        v = np.log1p((self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        tau = max(Config.ridf_tau, 1e-6)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Smooth residual-IDF (softplus around floor)\n            x = (idf_val - Config.resid_rho * v) - Config.resid_floor\n            ridf = tau * np.log1p(np.exp(x / tau))\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            cov_den += wq * idf_val * clarity\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            tfu = np.log1p(tf_row / (base + eps))\n            burst = Config.burst_beta * tfu * math.log1p(self.corpus.N / (float(self.corpus._df[term_id]) + eps))\n\n            wt_doc = (wq * clarity) * ridf\n            cov_num += (wq * idf_val * clarity) * present\n            sum_evidence += wt_doc * (tfu + burst)\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0) / (max(cov_den, eps)))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        # Specificity prior: approximate from unique/length ratio (cheap proxy in vectorized path).\n        # True entropy would require per-doc counts; this proxy still penalizes repetitive/boilerplate docs.\n        if Config.spec_lambda != 0.0:\n            u = np.maximum(self.corpus.doc_unique_counts[candidate_docs], 1.0)\n            rep = self.corpus.doc_lengths[candidate_docs] / (u + 1.0)\n            spec = np.log1p(rep) / (1.0 + np.log1p(rep))  # bounded 0..1\n            scores /= (1.0 + Config.spec_lambda * spec)\n\n        dl_damp = 1.0 + Config.dl_alpha * v\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4417, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep BM25 knobs for candidate scoring / normalization compatibility.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence is concave in tf; using log evidence is often more robust cross-domain than BM25 tf/(tf+k).\n    tf_log_base: float = 1.0\n\n    # Soft-AND over informative query mass (bounded; recall-safe).\n    coverage_gamma: float = 0.25\n\n    # Gentle query-side clarity gate: suppress extremely common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting (keeps emphasis without verbosity blowups).\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior (stable across corpora).\n    dl_alpha: float = 0.15\n\n    # Avoid over-compressing IDF; concavity is handled via log-evidence and bounded boosts.\n    idf_power: float = 1.0\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # NEW: approximate \"term burstiness\" normalization via doc-frequency (not corpus df).\n    # If a term repeats many times in a doc, it may be boilerplate; penalize repeated tf\n    # relative to how many unique terms the doc has.\n    burst_k: float = 2.0\n\n    # NEW: ultra-cheap ordered-bigram channel to capture phrases / adjacency without positions.\n    # Helps nDCG@10 on QA/scientific queries where key concepts appear as bigrams.\n    bigram_weight: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Empirically more stable cross-domain than odds/log1p(N/df), and works well with\n    coverage-style soft-AND because it behaves like \"bits of evidence\".\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique lexical constraints + sublinear repetition weights.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + mild verbosity prior\n    + bounded rare-key presence multiplier.\n\n    Evidence view:\n      sum_t w(t) * log1p(tf/base)  then compress as log1p(total_evidence).\n    This behaves like accumulating diminishing \"bits\" of lexical evidence and\n    tends to improve early precision while staying recall-safe.\n\n    Coverage:\n      multiply by 1 + gamma * (covered informative mass / total informative mass).\n\n    Rare-key presence:\n      if an ultra-rare query token appears at least once, apply a small bounded boost.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        if tidf <= 0.0:\n            continue\n\n        # bounded clarity in [0,1]\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        # Burstiness normalization: repeated mentions within a doc are discounted if the\n        # doc has low lexical diversity (boilerplate/listings).\n        # Approximate unique_len via Counter size (works for all channels).\n        uniq_len = float(max(1, len(doc_tf)))\n        tf_eff = tf / (1.0 + float(Config.burst_k) * (tf / (uniq_len + eps)))\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf_eff / (base + eps))\n\n        # indicator-like rare-key presence\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds optional prefix + bigram channels (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(\n        q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n    )\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # Tertiary channel: ordered bigrams (phrase proxy).\n    if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n        btoks = [f\"B:{query[i]}_{query[i+1]}\" for i in range(len(query) - 1)]\n        if btoks:\n            bq = QueryRepr.from_tokens(btoks)\n            s += float(Config.bigram_weight) * retrieval_score(\n                bq,\n                corpus.bigram_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Tertiary \"bigram lexicon\" view (ordered adjacency proxy).\n        docs_bigrams = [[f\"B:{doc[i]}_{doc[i+1]}\" for i in range(len(doc) - 1)] for doc in documents]\n        self.bigram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_bigrams]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged bigrams so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, bdoc in zip(documents, docs_prefix, docs_bigrams):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for bg in bdoc:\n                if bg not in self._vocab:\n                    self._vocab[bg] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n        self._doc_unique_len = np.array([max(1, len(set(doc))) for doc in documents], dtype=np.float64)\n\n        for doc_idx, (doc, pdoc, bdoc) in enumerate(zip(documents, docs_prefix, docs_bigrams)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            bigr_counts = Counter(bdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in bigr_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for all channels (prefix/bigram keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Use precomputed unique-term counts for the primary-token view as a proxy for diversity.\n        uniq_len = self.corpus._doc_unique_len[candidate_docs].astype(np.float64, copy=False)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            # Burstiness normalization (vectorized)\n            tf_eff = tf_row / (1.0 + float(Config.burst_k) * (tf_row / (uniq_len + eps)))\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_eff / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        if self.corpus.avgdl > 0.0 and Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel)\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        # Bigram pseudo-terms (tertiary lexical channel)\n        if getattr(Config, \"bigram_weight\", 0.0) != 0.0 and len(query) >= 2:\n            bcounts = Counter(f\"B:{query[i]}_{query[i+1]}\" for i in range(len(query) - 1))\n            for bg, c in bcounts.items():\n                tid = self.corpus.get_term_id(bg)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.bigram_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4284, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep the method simple but add *one* principled extra signal: informative coverage.\n    # This tends to improve nDCG@10 while keeping recall@100 stable.\n    k1: float = 0.9  # compatibility (Corpus.norm_array)\n    b: float = 0.4   # compatibility (Corpus.norm_array)\n    epsilon: float = 1e-9\n\n    # Concave TF utility\n    tf_log_base: float = 1.0\n\n    # Soft-AND over query constraints, weighted by term informativeness (IDF-mass coverage).\n    coverage_gamma: float = 0.25\n\n    # Suppress very common query terms smoothly (keeps lexical, but reduces noise).\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition (verbose queries shouldn\u2019t dominate).\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior (gentle length penalty; keep recall-safe).\n    dl_alpha: float = 0.15\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc), with add-one smoothing.\n\n    This is typically more stable across heterogeneous corpora than BM25's odds IDF,\n    and avoids extreme spikes that can hurt cross-dataset robustness.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent the query as (mostly) unique constraints, but keep a sublinear\n        repetition weight as a weak emphasis signal.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + clarity gate + informative coverage (soft-AND).\n\n    Evidence:\n      sum_t [ wq(t) * idf(t) * clarity(t) * log1p(tf/base) ], then log1p(total).\n    Coverage prior:\n      score *= 1 + gamma * (covered_idf_mass / total_idf_mass)\n\n    Why this helps fitness:\n    - coverage improves early precision (nDCG@10) without hard AND.\n    - clarity suppresses very common query terms (noise) without stopword lists.\n    - concave TF keeps repeats from dominating.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        # bounded common-term suppression in [0,1]\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Match QueryRepr: unique terms with sublinear qtf weights.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence accumulation + informative coverage.\n    epsilon: float = 1e-9\n\n    # Evidence: wt * log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Mild verbosity prior (keep recall-friendly).\n    dl_alpha: float = 0.15\n\n    # Soft-AND on informative query mass covered.\n    coverage_gamma: float = 0.25\n\n    # Query-side common-term gating (bounded in [0,1]).\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting.\n    qtf_power: float = 0.5\n\n    # --- New: verbosity-aware discriminativity (residual-IDF) ---\n    # In long docs, matches are less surprising; require extra surprisal beyond verbosity.\n    resid_rho: float = 0.35\n    resid_floor: float = 0.0\n    ridf_tau: float = 0.35  # soft hinge temperature (softplus)\n\n    # --- New: burstiness reward (keyword-like concentration) ---\n    burst_beta: float = 0.12\n\n    # --- Compatibility: Corpus expects BM25's b for norm_array construction ---\n    b: float = 0.4\n\n    # Keep the old discrete hit bonus but default to 0; residual/coverage handle coordination more smoothly.\n    hit_bonus: float = 0.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Interprets df/N as an empirical occurrence probability but avoids infinities/extremes.\n    This was consistently robust across heterogeneous BEIR+BRIGHT mixes in prior attempts.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as a set of lexical constraints with sublinear qtf.\n        This usually helps early precision (nDCG@10) on verbose QA/scientific queries\n        while keeping recall robust.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        counts = Counter(tokens)\n        terms = list(counts.keys())  # unique constraints\n        weights = {t: float(c) ** Config.qtf_power for t, c in counts.items()}\n        return cls(terms=terms, term_weights=weights)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Smooth residual-IDF concave evidence + informative coverage + mild verbosity prior.\n\n    Core idea:\n      Use term surprisal (idf) but *residualize* it by document verbosity:\n        ridf(t,d) = softplus( idf(t) - rho*log1p(dl/avgdl) - floor )\n      so long documents don't get free matches \"by chance\".\n\n    Then add a small burstiness bonus:\n      burst ~ log1p(tf) * log1p(N/df)\n    favoring concentrated keyword mentions common in technical corpora.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # Verbosity signal shared by residualization and length prior\n    v = math.log1p((doc_length + 1.0) / (avgdl + 1.0))\n    tau = max(Config.ridf_tau, 1e-6)\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        # Query-side clarity gate (downweight very common terms smoothly)\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # Smooth residual-IDF (soft hinge rather than hard max)\n        x = (tidf - Config.resid_rho * v) - Config.resid_floor\n        ridf = tau * math.log1p(math.exp(x / tau))\n        if ridf <= 0.0:\n            continue\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # Coverage stays query-centric: measure mass of informative constraints covered\n        cov_wt = wq * tidf * clarity\n        cov_den += cov_wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += cov_wt\n\n        tfu = math.log1p(tf / (base + eps))\n        burst = Config.burst_beta * tfu * math.log1p(N / (df + eps))\n        sum_evidence += (wq * ridf * clarity) * (tfu + burst)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    # Normalize by query mass for stability across query lengths\n    qmass = cov_den if cov_den > 0.0 else 1.0\n    score = math.log1p(sum_evidence / (qmass + eps))\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    dl_damp = 1.0 + Config.dl_alpha * v\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        v = np.log1p((self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0))\n        tau = max(Config.ridf_tau, 1e-6)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            cov_wt = wq * idf_val * clarity\n            cov_den += cov_wt\n\n            # smooth residual-idf per doc\n            x = (idf_val - Config.resid_rho * v) - Config.resid_floor\n            ridf = tau * np.log1p(np.exp(x / tau))\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += cov_wt * present\n\n            tfu = np.log1p(tf_row / (base + eps))\n            burst = Config.burst_beta * tfu * math.log1p(self.corpus.N / (float(self.corpus._df[term_id]) + eps))\n\n            sum_evidence += (wq * clarity) * ridf * (tfu + burst)\n\n        qmass = max(cov_den, eps)\n        scores = np.log1p(np.maximum(sum_evidence, 0.0) / qmass)\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        dl_damp = 1.0 + Config.dl_alpha * v\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Match QueryRepr: unique terms + sublinear qtf weights.\n        counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, c in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # Concave evidence accumulation + informative coverage.\n    epsilon: float = 1e-9\n\n    # Evidence: wt * log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Mild verbosity prior (keep recall-friendly).\n    dl_alpha: float = 0.15\n\n    # Soft-AND on informative query mass covered.\n    coverage_gamma: float = 0.25\n\n    # Query-side common-term gating (bounded in [0,1]).\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting.\n    qtf_power: float = 0.5\n\n    # --- New: verbosity-aware discriminativity (residual-IDF) ---\n    # In long docs, matches are less surprising; require extra surprisal beyond verbosity.\n    resid_rho: float = 0.35\n    resid_floor: float = 0.0\n    ridf_tau: float = 0.35  # soft hinge temperature (softplus)\n\n    # --- New: burstiness reward (keyword-like concentration) ---\n    burst_beta: float = 0.12\n\n    # --- Compatibility: Corpus expects BM25's b for norm_array construction ---\n    b: float = 0.4\n\n    # Keep the old discrete hit bonus but default to 0; residual/coverage handle coordination more smoothly.\n    hit_bonus: float = 0.0\n=======\nclass Config:\n    # Concave evidence accumulation + informative coverage.\n    epsilon: float = 1e-9\n\n    # Evidence: wt * log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Mild verbosity prior (keep recall-friendly).\n    dl_alpha: float = 0.15\n\n    # Soft-AND on informative query mass covered.\n    coverage_gamma: float = 0.25\n\n    # Query-side common-term gating (bounded in [0,1]).\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting.\n    qtf_power: float = 0.5\n\n    # Verbosity-aware discriminativity (residual-IDF).\n    resid_rho: float = 0.35\n    resid_floor: float = 0.0\n    ridf_tau: float = 0.35  # soft hinge temperature (softplus)\n\n    # Burstiness reward (keyword-like concentration).\n    burst_beta: float = 0.12\n\n    # NEW: query-facet sharpening (query-only). Reallocates weight toward high-idf constraints\n    # to improve early precision without hard AND.\n    facet_mix: float = 0.12\n    facet_power: float = 1.6\n\n    # NEW: bounded rare-key presence boost (indicator-like). Helps BRIGHT-style identifier queries.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # --- Compatibility: Corpus expects BM25's b for norm_array construction ---\n    b: float = 0.4\n\n    # Keep the old discrete hit bonus but default to 0; residual/coverage handle coordination more smoothly.\n    hit_bonus: float = 0.0\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Smooth residual-IDF concave evidence + informative coverage + mild verbosity prior.\n\n    Core idea:\n      Use term surprisal (idf) but *residualize* it by document verbosity:\n        ridf(t,d) = softplus( idf(t) - rho*log1p(dl/avgdl) - floor )\n      so long documents don't get free matches \"by chance\".\n\n    Then add a small burstiness bonus:\n      burst ~ log1p(tf) * log1p(N/df)\n    favoring concentrated keyword mentions common in technical corpora.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # Verbosity signal shared by residualization and length prior\n    v = math.log1p((doc_length + 1.0) / (avgdl + 1.0))\n    tau = max(Config.ridf_tau, 1e-6)\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        # Query-side clarity gate (downweight very common terms smoothly)\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # Smooth residual-IDF (soft hinge rather than hard max)\n        x = (tidf - Config.resid_rho * v) - Config.resid_floor\n        ridf = tau * math.log1p(math.exp(x / tau))\n        if ridf <= 0.0:\n            continue\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # Coverage stays query-centric: measure mass of informative constraints covered\n        cov_wt = wq * tidf * clarity\n        cov_den += cov_wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += cov_wt\n\n        tfu = math.log1p(tf / (base + eps))\n        burst = Config.burst_beta * tfu * math.log1p(N / (df + eps))\n        sum_evidence += (wq * ridf * clarity) * (tfu + burst)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    # Normalize by query mass for stability across query lengths\n    qmass = cov_den if cov_den > 0.0 else 1.0\n    score = math.log1p(sum_evidence / (qmass + eps))\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    dl_damp = 1.0 + Config.dl_alpha * v\n    return score / (dl_damp + eps)\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Smooth residual-IDF concave evidence + informative coverage + mild verbosity prior,\n    with two query-only shaping signals:\n\n      (1) Facet prior: queries often contain a few decisive, high-IDF constraints and\n          many weak hints. Reweight IDF inside the query to sharpen early precision.\n      (2) Rare-key presence: a bounded multiplier when ultra-rare query tokens appear\n          at least once (identifiers/symbols), improving BRIGHT-style retrieval.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    v = math.log1p((doc_length + 1.0) / (avgdl + 1.0))\n    tau = max(Config.ridf_tau, 1e-6)\n\n    # Query-only facet scale.\n    idf_pows = []\n    for term in query_repr.terms:\n        tidf0 = float(idf(float(corpus_df.get(term, 1.0)), N))\n        if tidf0 > 0.0:\n            idf_pows.append(tidf0 ** Config.facet_power)\n    mean_idf_pow = (sum(idf_pows) / len(idf_pows)) if idf_pows else 1.0\n    mean_idf_pow = max(mean_idf_pow, eps)\n    mix = max(0.0, min(1.0, Config.facet_mix))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # residualize by verbosity\n        x = (tidf - Config.resid_rho * v) - Config.resid_floor\n        ridf = tau * math.log1p(math.exp(x / tau))\n        if ridf <= 0.0:\n            continue\n\n        # facet-reweighted idf (query-only) used for evidence, but keep coverage on raw IDF mass\n        facet = (tidf ** Config.facet_power) / mean_idf_pow\n        idf_used = (1.0 - mix) * tidf + mix * facet\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        cov_wt = wq * tidf * clarity\n        cov_den += cov_wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += cov_wt\n\n        tfu = math.log1p(tf / (base + eps))\n        burst = Config.burst_beta * tfu * math.log1p(N / (df + eps))\n        sum_evidence += (wq * clarity * ridf * idf_used / (tidf + eps)) * (tfu + burst)\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    qmass = cov_den if cov_den > 0.0 else 1.0\n    score = math.log1p(sum_evidence / (qmass + eps))\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    dl_damp = 1.0 + Config.dl_alpha * v\n    return score / (dl_damp + eps)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        v = np.log1p((self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0))\n        tau = max(Config.ridf_tau, 1e-6)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            cov_wt = wq * idf_val * clarity\n            cov_den += cov_wt\n\n            # smooth residual-idf per doc\n            x = (idf_val - Config.resid_rho * v) - Config.resid_floor\n            ridf = tau * np.log1p(np.exp(x / tau))\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += cov_wt * present\n\n            tfu = np.log1p(tf_row / (base + eps))\n            burst = Config.burst_beta * tfu * math.log1p(self.corpus.N / (float(self.corpus._df[term_id]) + eps))\n\n            sum_evidence += (wq * clarity) * ridf * (tfu + burst)\n\n        qmass = max(cov_den, eps)\n        scores = np.log1p(np.maximum(sum_evidence, 0.0) / qmass)\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        dl_damp = 1.0 + Config.dl_alpha * v\n        return scores / (dl_damp + eps)\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        v = np.log1p((self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0))\n        tau = max(Config.ridf_tau, 1e-6)\n\n        # Query-only facet scale (computed from idf_array; query-only).\n        idf_vals = np.array([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        idf_vals = np.maximum(idf_vals, 0.0)\n        idf_pow = np.power(idf_vals, Config.facet_power, dtype=np.float64)\n        mean_idf_pow = float(np.mean(idf_pow)) if idf_pow.size > 0 else 1.0\n        mean_idf_pow = max(mean_idf_pow, eps)\n        mix = max(0.0, min(1.0, Config.facet_mix))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            facet = (idf_val ** Config.facet_power) / mean_idf_pow\n            idf_used = (1.0 - mix) * idf_val + mix * facet\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            cov_wt = wq * idf_val * clarity\n            cov_den += cov_wt\n\n            x = (idf_val - Config.resid_rho * v) - Config.resid_floor\n            ridf = tau * np.log1p(np.exp(x / tau))\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += cov_wt * present\n\n            tfu = np.log1p(tf_row / (base + eps))\n            burst = Config.burst_beta * tfu * math.log1p(self.corpus.N / (float(self.corpus._df[term_id]) + eps))\n\n            sum_evidence += (wq * clarity) * ridf * (idf_used / (idf_val + eps)) * (tfu + burst)\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        qmass = max(cov_den, eps)\n        scores = np.log1p(np.maximum(sum_evidence, 0.0) / qmass)\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        dl_damp = 1.0 + Config.dl_alpha * v\n        return scores / (dl_damp + eps)\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}