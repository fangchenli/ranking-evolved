{"id": "88ed3f63-7009-4fc6-9e22-cee6d83531f0", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave surprisal evidence + informative-coverage boost + mild verbosity prior.\n    k1: float = 0.9   # kept for backwards compatibility (Corpus.norm_array)\n    b: float = 0.4    # kept for backwards compatibility (Corpus.norm_array)\n    epsilon: float = 1e-9\n\n    tf_log_base: float = 1.0\n    dl_alpha: float = 0.15\n    q_clarity_power: float = 0.6\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n\n    # \"Facet prior\": reallocates query mass toward discriminative constraints (query-only).\n    facet_mix: float = 0.12\n    facet_power: float = 1.6\n\n    # Soft-AND / coordination pressure based on informative mass covered.\n    coord_beta: float = 0.08\n\n    # --- Robust lexical matching (secondary channel) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: character n-gram channel for tokenization mismatch (URLs, code, hyphenation).\n    # We keep it tiny + cheap: only query ngrams, treated as pseudo-terms.\n    ngram_n: int = 4\n    ngram_max_per_token: int = 2\n    ngram_weight: float = 0.10\n\n    # --- Rare-key presence (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Surprisal IDF: log1p(N/df).\n\n    Interprets df/N as an occurrence probability; matching a term yields self-information.\n    This is typically smoother and more robust cross-domain than BM25-odds IDF.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent the query as unique lexical constraints but keep sublinear repetition.\n\n        Motivation: repeated tokens can encode emphasis, but linear qtf tends to\n        over-weight verbosity/noisy tokenization.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + clarity gating + coverage, plus:\n      (1) query-internal \"facet prior\" (reallocates weight toward high-IDF constraints)\n      (2) saturating coordination bonus based on informative mass covered\n      (3) bounded rare-key presence multiplier\n\n    Facet prior story:\n      A query is often a mixture of a few decisive facets and many background hints.\n      We approximate this by transforming per-term IDF inside the query:\n        idf_used = (1-m)*idf + m*(idf^p / mean(idf^p))\n      This sharpens ranking (nDCG@10) while keeping the evidence still additive/recall-friendly.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # Precompute query-level scale for facet prior (query-only; stable across documents).\n    idf_pows = []\n    for term in query_repr.terms:\n        tidf = float(idf(float(corpus_df.get(term, 1.0)), N))\n        if tidf > 0.0:\n            idf_pows.append(tidf ** Config.facet_power)\n    mean_idf_pow = (sum(idf_pows) / len(idf_pows)) if idf_pows else 1.0\n    mean_idf_pow = max(mean_idf_pow, eps)\n    mix = max(0.0, min(1.0, Config.facet_mix))\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        # bounded query clarity in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # facet-reweighted idf (query-dependent only)\n        facet = (term_idf ** Config.facet_power) / mean_idf_pow\n        idf_used = (1.0 - mix) * term_idf + mix * facet\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # coverage uses plain IDF mass (keeps recall stable across corpora)\n        cov_wt = wq * term_idf * clarity\n        cov_den += cov_wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += cov_wt\n        sum_evidence += (wq * clarity * idf_used) * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and term_idf > Config.rare_idf_pivot:\n            rare_hits += (term_idf - Config.rare_idf_pivot) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    # soft-AND via informative coverage\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n        if Config.coord_beta != 0.0:\n            # saturating coordination: emphasizes \"more constraints satisfied\" without hard AND\n            score *= 1.0 + Config.coord_beta * (1.0 - math.exp(-3.0 * coverage))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel (exact tokens).\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel (prefixes) for robust lexical matching.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n\n    # NEW: tiny character n-gram channel (query-only extraction) to survive tokenization gaps.\n    # This is still lexical and uses the same retrieval_score machinery via pseudo-terms.\n    if Config.ngram_weight != 0.0 and Config.ngram_n > 1:\n        n = int(Config.ngram_n)\n        cap = max(1, int(Config.ngram_max_per_token))\n        grams: list[str] = []\n        for t in query:\n            if len(t) < n:\n                continue\n            step = max(1, (len(t) - n) // cap)  # take a couple spaced grams, not all\n            for j in range(0, len(t) - n + 1, step):\n                grams.append(\"G:\" + t[j : j + n])\n                if len(grams) >= cap * max(1, len(query)):\n                    break\n        if grams:\n            gq = QueryRepr.from_tokens(grams)\n            s += Config.ngram_weight * retrieval_score(\n                gq,\n                corpus.ngram_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # NEW: tiny character n-gram view (disjoint channel) for tokenization mismatch.\n        n = int(Config.ngram_n) if getattr(Config, \"ngram_n\", 0) else 0\n        cap = max(1, int(Config.ngram_max_per_token)) if getattr(Config, \"ngram_max_per_token\", 0) else 1\n        if n > 1:\n            docs_ngrams: list[list[str]] = []\n            for doc in documents:\n                gs: list[str] = []\n                for t in doc:\n                    if len(t) < n:\n                        continue\n                    step = max(1, (len(t) - n) // cap)\n                    for j in range(0, len(t) - n + 1, step):\n                        gs.append(\"G:\" + t[j : j + n])\n                        if len(gs) >= cap * max(1, len(doc)):\n                            break\n                docs_ngrams.append(gs)\n            self.ngram_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_ngrams]\n        else:\n            docs_ngrams = [[] for _ in documents]\n            self.ngram_doc_tf_dicts = [Counter() for _ in documents]\n\n        # Joint vocabulary over tokens + tagged prefixes + tagged ngrams.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, gdoc in zip(documents, docs_prefix, docs_ngrams):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for g in gdoc:\n                if g not in self._vocab:\n                    self._vocab[g] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, gdoc) in enumerate(zip(documents, docs_prefix, docs_ngrams)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            gram_counts = Counter(gdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in gram_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for all channels (prefix + ngram keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        # Query-level facet prior scale (computed from idf_array; query-only).\n        idf_vals = np.array([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        idf_vals = np.maximum(idf_vals, 0.0)\n        idf_pow = np.power(idf_vals, Config.facet_power, dtype=np.float64)\n        mean_idf_pow = float(np.mean(idf_pow)) if idf_pow.size > 0 else 1.0\n        mean_idf_pow = max(mean_idf_pow, eps)\n        mix = max(0.0, min(1.0, Config.facet_mix))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            facet = (idf_val ** Config.facet_power) / mean_idf_pow\n            idf_used = (1.0 - mix) * idf_val + mix * facet\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            cov_wt = wq * idf_val * clarity\n            cov_den += cov_wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += cov_wt * present\n            sum_evidence += (wq * clarity * idf_used) * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n            if Config.coord_beta != 0.0:\n                scores *= 1.0 + Config.coord_beta * (1.0 - np.exp(-3.0 * coverage))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query: tokens + (optional) tagged prefixes + (optional) tagged ngrams.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if Config.ngram_weight != 0.0 and Config.ngram_n > 1:\n            n = int(Config.ngram_n)\n            cap = max(1, int(Config.ngram_max_per_token))\n            gcounts: Counter[str] = Counter()\n            for t in query:\n                if len(t) < n:\n                    continue\n                step = max(1, (len(t) - n) // cap)\n                for j in range(0, len(t) - n + 1, step):\n                    gcounts[\"G:\" + t[j : j + n]] += 1\n            for g, c in gcounts.items():\n                tid = self.corpus.get_term_id(g)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.ngram_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "6cd66546-e527-487e-b7f3-747009921b70", "generation": 5, "timestamp": 1770195244.420095, "iteration_found": 0, "metrics": {"bright_pony_ndcg@10": 0.13204918670714963, "bright_pony_recall@100": 0.33092288775600315, "bright_pony_index_time_ms": 7652.7137868106365, "bright_pony_query_time_ms": 2335.3510899469256, "beir_nfcorpus_ndcg@10": 0.3347718004398883, "beir_nfcorpus_recall@100": 0.26122311886681265, "beir_nfcorpus_index_time_ms": 14049.602115061134, "beir_nfcorpus_query_time_ms": 611.2379971891642, "beir_scifact_ndcg@10": 0.6829450578323311, "beir_scifact_recall@100": 0.9276666666666668, "beir_scifact_index_time_ms": 26666.382011026144, "beir_scifact_query_time_ms": 2399.1917008534074, "beir_arguana_ndcg@10": 0.28846290489127985, "beir_arguana_recall@100": 0.9221984296930764, "beir_arguana_index_time_ms": 33638.4427822195, "beir_arguana_query_time_ms": 111292.36978897825, "bright_theoremqa_theorems_ndcg@10": 0.037872000728343985, "bright_theoremqa_theorems_recall@100": 0.1995614035087719, "bright_theoremqa_theorems_index_time_ms": 62414.31570798159, "bright_theoremqa_theorems_query_time_ms": 3371.5719687752426, "beir_scidocs_ndcg@10": 0.15095477608433192, "beir_scidocs_recall@100": 0.35148333333333337, "beir_scidocs_index_time_ms": 110659.87683692947, "beir_scidocs_query_time_ms": 18673.69970306754, "bright_economics_ndcg@10": 0.12408112848997234, "bright_economics_recall@100": 0.37786089383999444, "bright_economics_index_time_ms": 60700.72283688933, "bright_economics_query_time_ms": 11344.030080828816, "bright_biology_ndcg@10": 0.24880930861427172, "bright_biology_recall@100": 0.5724109564426375, "bright_biology_index_time_ms": 77664.53988291323, "bright_biology_query_time_ms": 9726.035742089152, "bright_earth_science_ndcg@10": 0.2771898625364392, "bright_earth_science_recall@100": 0.6533956200585058, "bright_earth_science_index_time_ms": 162864.84597995877, "bright_earth_science_query_time_ms": 18749.99754782766, "beir_fiqa_ndcg@10": 0.23442758163834215, "beir_fiqa_recall@100": 0.5258634541041949, "beir_fiqa_index_time_ms": 179909.20822834596, "beir_fiqa_query_time_ms": 18439.929470885545, "bright_stackoverflow_ndcg@10": 0.1983704845607518, "bright_stackoverflow_recall@100": 0.5312730257239509, "bright_stackoverflow_index_time_ms": 505777.98742987216, "bright_stackoverflow_query_time_ms": 36715.04609007388, "beir_trec-covid_ndcg@10": 0.7259532957970433, "beir_trec-covid_recall@100": 0.12363662146135239, "beir_trec-covid_index_time_ms": 738245.6469559111, "beir_trec-covid_query_time_ms": 6052.449631039053, "avg_ndcg@10": 0.28632394902667874, "avg_recall@100": 0.4814580342879416, "combined_score": 0.442431217235689, "total_index_time_ms": 1980244.284553919, "total_query_time_ms": 239710.91081155464, "total_time_ms": 2219955.1953654736, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 29 lines with 32 lines\nChange 2: Replace 25 lines with 51 lines\nChange 3: Replace 70 lines with 103 lines\nChange 4: Replace 19 lines with 35 lines", "parent_metrics": {"bright_pony_ndcg@10": 0.11088415390276284, "bright_pony_recall@100": 0.30223104761439384, "bright_pony_index_time_ms": 8419.560065027326, "bright_pony_query_time_ms": 1221.1149837821722, "beir_nfcorpus_ndcg@10": 0.33329331649123234, "beir_nfcorpus_recall@100": 0.2553583764390518, "beir_nfcorpus_index_time_ms": 18785.498880781233, "beir_nfcorpus_query_time_ms": 283.83290814235806, "beir_scifact_ndcg@10": 0.6814573060242558, "beir_scifact_recall@100": 0.9142222222222222, "beir_scifact_index_time_ms": 21237.28014715016, "beir_scifact_query_time_ms": 1006.3493140041828, "beir_arguana_ndcg@10": 0.2928999586702495, "beir_arguana_recall@100": 0.9214846538187009, "beir_arguana_index_time_ms": 28807.364993263036, "beir_arguana_query_time_ms": 51286.43027925864, "bright_theoremqa_theorems_ndcg@10": 0.027434003533501748, "bright_theoremqa_theorems_recall@100": 0.19298245614035087, "bright_theoremqa_theorems_index_time_ms": 54012.080061249435, "bright_theoremqa_theorems_query_time_ms": 1625.4523708485067, "beir_scidocs_ndcg@10": 0.1507972214127471, "beir_scidocs_recall@100": 0.3556333333333333, "beir_scidocs_index_time_ms": 97082.08636380732, "beir_scidocs_query_time_ms": 7360.746135935187, "bright_biology_ndcg@10": 0.2584368555214894, "bright_biology_recall@100": 0.5620549693876215, "bright_biology_index_time_ms": 44181.93410104141, "bright_biology_query_time_ms": 4414.838165976107, "bright_economics_ndcg@10": 0.13657593661602294, "bright_economics_recall@100": 0.37065513624498553, "bright_economics_index_time_ms": 51910.83888197318, "bright_economics_query_time_ms": 4739.144870080054, "bright_earth_science_ndcg@10": 0.2729980163180642, "bright_earth_science_recall@100": 0.6605641340158129, "bright_earth_science_index_time_ms": 139781.06199903414, "bright_earth_science_query_time_ms": 7695.736947003752, "beir_fiqa_ndcg@10": 0.23182716615364307, "beir_fiqa_recall@100": 0.5269767698008438, "beir_fiqa_index_time_ms": 149896.08096703887, "beir_fiqa_query_time_ms": 9526.700061745942, "bright_stackoverflow_ndcg@10": 0.19864280229695302, "bright_stackoverflow_recall@100": 0.5223924519288091, "bright_stackoverflow_index_time_ms": 480612.9056722857, "bright_stackoverflow_query_time_ms": 15743.111405987293, "beir_trec-covid_ndcg@10": 0.7149694152107524, "beir_trec-covid_recall@100": 0.12096216699419603, "beir_trec-covid_index_time_ms": 619256.475104019, "beir_trec-covid_query_time_ms": 2698.3094890601933, "avg_ndcg@10": 0.28418467934597286, "avg_recall@100": 0.47545980982836006, "combined_score": 0.43720478373188265, "total_index_time_ms": 1713983.1672366709, "total_query_time_ms": 107601.76693182439, "total_time_ms": 1821584.9341684952, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}