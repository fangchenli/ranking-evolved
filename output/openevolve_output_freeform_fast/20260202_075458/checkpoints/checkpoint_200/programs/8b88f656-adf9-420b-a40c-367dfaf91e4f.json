{"id": "8b88f656-adf9-420b-a40c-367dfaf91e4f", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + informative coverage + rare-key matches,\n    # plus a tiny \"anti-verbosity\" normalization of repeated terms inside a doc.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    qtf_power: float = 0.5\n    dl_alpha: float = 0.15\n\n    rare_idf_pivot: float = 4.0\n    rare_boost: float = 0.25\n\n    # NEW: normalize each term's TF by how \"bursty\" the document is.\n    # Intuition: repeated tokens are less informative in generally repetitive/boilerplate docs.\n    # This often helps nDCG@10 on StackOverflow-like / long scientific documents without hard length penalties.\n    burst_alpha: float = 0.4  # 0 -> off; higher -> stronger per-doc burst normalization\n\n    # Compatibility leftovers (Corpus references b/k1)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float, uniq: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n        self.uniq = uniq  # number of unique terms (proxy for repetition/burstiness)\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)), uniq=float(len(tf)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    doc_uniq: float | None = None,\n) -> float:\n    \"\"\"\n    Two-channel concave evidence:\n\n    Channel A (robust): clarity-gated surprisal evidence with log1p TF utility + IDF-mass coverage.\n    Channel B (key-match): extra boost for *very rare* query terms that appear at least once.\n\n    Intuition for Channel B:\n    In theorem/proof/technical QA, a single exact match on a rare symbol/name often matters more\n    than repeated matches on medium-common vocabulary. We add a bounded hinge on IDF so it only\n    activates for truly rare terms, helping BRIGHT theoremqa/pony without destabilizing BEIR.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # Per-document burst normalization factor.\n    # If doc_uniq is small relative to doc_length, the doc repeats itself a lot (boilerplate).\n    u = float(doc_uniq) if doc_uniq is not None else float(len(doc_tf))\n    rep = doc_length / (u + 1.0)  # >= ~1\n    burst_norm = 1.0 + Config.burst_alpha * math.log1p(rep)\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        # Burst-normalized TF utility: repeated occurrences in a generally repetitive doc\n        # contribute less marginal evidence.\n        tf_eff = tf / (burst_norm + eps)\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf_eff / (base + eps))\n\n        # Rare-term key-match: count a hit if idf above pivot (hinge, not a hard filter).\n        if term_idf > Config.rare_idf_pivot:\n            rare_hits += (term_idf - Config.rare_idf_pivot) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    # Add the bounded rare-hit channel (kept small and length-penalized with the main score).\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_repr = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        doc_repr.term_frequencies,\n        float(doc_repr.length),\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n        doc_repr.uniq,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Store per-doc representations (gives uniq-term count for burst normalization).\n        self.doc_repr: list[DocumentRepr] = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [r.term_frequencies for r in self.doc_repr]\n\n        for doc_idx, r in enumerate(self.doc_repr):\n            term_counts = r.term_frequencies\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # Precompute unique-term counts per doc for vectorized burst normalization.\n        self.doc_uniq_array = np.array([r.uniq for r in self.doc_repr], dtype=np.float64)\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            # Vectorized burst normalization: use precomputed doc unique counts.\n            u = np.maximum(self.corpus.doc_uniq_array[candidate_docs], 1.0)\n            rep = self.corpus.doc_lengths[candidate_docs] / (u + 1.0)\n            burst_norm = 1.0 + Config.burst_alpha * np.log1p(rep)\n            tf_eff = tf_row / (burst_norm + eps)\n\n            sum_evidence += wt * np.log1p(tf_eff / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "d535dc97-f7d3-4614-82d9-0cfb97c6bfdb", "generation": 6, "timestamp": 1770107518.8969364, "iteration_found": 0, "metrics": {"bright_pony_ndcg@10": 0.11899952432169902, "bright_pony_recall@100": 0.31652403613731867, "bright_pony_index_time_ms": 6244.950022082776, "bright_pony_query_time_ms": 1086.000175215304, "beir_nfcorpus_ndcg@10": 0.32868235328247153, "beir_nfcorpus_recall@100": 0.24962451595236154, "beir_nfcorpus_index_time_ms": 16440.22521097213, "beir_nfcorpus_query_time_ms": 201.86113892123103, "beir_scifact_ndcg@10": 0.6736593781589943, "beir_scifact_recall@100": 0.914888888888889, "beir_scifact_index_time_ms": 18224.39617291093, "beir_scifact_query_time_ms": 693.6748726293445, "beir_arguana_ndcg@10": 0.29229549313979897, "beir_arguana_recall@100": 0.9343326195574589, "beir_arguana_index_time_ms": 26429.155289195478, "beir_arguana_query_time_ms": 46704.11087106913, "bright_theoremqa_theorems_ndcg@10": 0.03760883044959823, "bright_theoremqa_theorems_recall@100": 0.18421052631578946, "bright_theoremqa_theorems_index_time_ms": 53353.625408839434, "bright_theoremqa_theorems_query_time_ms": 1569.9657262302935, "beir_scidocs_ndcg@10": 0.14963885234472057, "beir_scidocs_recall@100": 0.35896666666666666, "beir_scidocs_index_time_ms": 94851.52767272666, "beir_scidocs_query_time_ms": 5669.4941548630595, "bright_economics_ndcg@10": 0.12378209539984117, "bright_economics_recall@100": 0.35051015422633736, "bright_economics_index_time_ms": 52960.7897689566, "bright_economics_query_time_ms": 4634.779321029782, "bright_biology_ndcg@10": 0.25278161226076257, "bright_biology_recall@100": 0.5505407469382941, "bright_biology_index_time_ms": 58247.527649160475, "bright_biology_query_time_ms": 4173.205548897386, "bright_earth_science_ndcg@10": 0.250648554186306, "bright_earth_science_recall@100": 0.6416172762758516, "bright_earth_science_index_time_ms": 125844.7121148929, "bright_earth_science_query_time_ms": 7308.712358120829, "beir_fiqa_ndcg@10": 0.23085073128966607, "beir_fiqa_recall@100": 0.535147302971377, "beir_fiqa_index_time_ms": 141490.19204685465, "beir_fiqa_query_time_ms": 9205.110860988498, "bright_stackoverflow_ndcg@10": 0.19022005766453118, "bright_stackoverflow_recall@100": 0.4890571209540079, "bright_stackoverflow_index_time_ms": 446111.40976380557, "bright_stackoverflow_query_time_ms": 14985.540786292404, "beir_trec-covid_ndcg@10": 0.6908505759706537, "beir_trec-covid_recall@100": 0.11857384361265931, "beir_trec-covid_index_time_ms": 585793.2875496335, "beir_trec-covid_query_time_ms": 2371.1788738146424, "avg_ndcg@10": 0.2783348382057536, "avg_recall@100": 0.47033280820808426, "combined_score": 0.4319332142076182, "total_index_time_ms": 1625991.7986700311, "total_query_time_ms": 98603.6346880719, "total_time_ms": 1724595.433358103, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 30 lines with 22 lines\nChange 2: Replace 9 lines with 10 lines\nChange 3: Replace 8 lines with 9 lines\nChange 4: Replace 7 lines with 13 lines\nChange 5: Replace 6 lines with 10 lines\nChange 6: Replace 10 lines with 17 lines\nChange 7: Replace 33 lines with 36 lines\nChange 8: Replace 5 lines with 11 lines\nChange 9: Replace 6 lines with 9 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.33000067639734615, "beir_nfcorpus_recall@100": 0.25564909208331527, "beir_nfcorpus_index_time_ms": 3495.996334007941, "beir_nfcorpus_query_time_ms": 99.05750001780689, "beir_scifact_ndcg@10": 0.6736425274786124, "beir_scifact_recall@100": 0.9215555555555557, "beir_scifact_index_time_ms": 4731.484083051328, "beir_scifact_query_time_ms": 356.3154999865219, "bright_pony_ndcg@10": 0.11538305716720353, "bright_pony_recall@100": 0.31907974991974053, "bright_pony_index_time_ms": 1346.4296250022016, "bright_pony_query_time_ms": 484.77770801400766, "beir_arguana_ndcg@10": 0.29386100465110815, "beir_arguana_recall@100": 0.9336188436830836, "beir_arguana_index_time_ms": 5717.277417017613, "beir_arguana_query_time_ms": 21007.511541014537, "bright_theoremqa_theorems_ndcg@10": 0.04070284482286954, "bright_theoremqa_theorems_recall@100": 0.1732456140350877, "bright_theoremqa_theorems_index_time_ms": 10410.264250007458, "bright_theoremqa_theorems_query_time_ms": 893.8102499814704, "beir_scidocs_ndcg@10": 0.15116124643440568, "beir_scidocs_recall@100": 0.35686666666666667, "beir_scidocs_index_time_ms": 19094.293707981706, "beir_scidocs_query_time_ms": 3500.4537089844234, "bright_economics_ndcg@10": 0.12621127458096612, "bright_economics_recall@100": 0.35049237265515776, "bright_economics_index_time_ms": 13593.233041989151, "bright_economics_query_time_ms": 2437.049458967522, "bright_biology_ndcg@10": 0.25144601623478285, "bright_biology_recall@100": 0.5442840047592219, "bright_biology_index_time_ms": 15004.151458037086, "bright_biology_query_time_ms": 2428.802667011041, "beir_fiqa_ndcg@10": 0.2327043840367459, "beir_fiqa_recall@100": 0.535770804983768, "beir_fiqa_index_time_ms": 33112.87670803722, "beir_fiqa_query_time_ms": 6427.97800002154, "bright_earth_science_ndcg@10": 0.244096550442501, "bright_earth_science_recall@100": 0.6398931383448172, "bright_earth_science_index_time_ms": 30516.311041021254, "bright_earth_science_query_time_ms": 3964.8637080099434, "bright_stackoverflow_ndcg@10": 0.1852114164716337, "bright_stackoverflow_recall@100": 0.48193944264988886, "bright_stackoverflow_index_time_ms": 97138.70254199719, "bright_stackoverflow_query_time_ms": 8666.331084037665, "beir_trec-covid_ndcg@10": 0.6927829227978157, "beir_trec-covid_recall@100": 0.11761236671019755, "beir_trec-covid_index_time_ms": 130104.9669169588, "beir_trec-covid_query_time_ms": 1608.848333999049, "avg_ndcg@10": 0.27810032679299923, "avg_recall@100": 0.46916730433720844, "combined_score": 0.43095390882836665, "total_index_time_ms": 364265.98712510895, "total_query_time_ms": 51875.79946004553, "total_time_ms": 416141.7865851545, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}