{"id": "e4cb4670-be56-4fda-ac9b-71bd5eab3250", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # Reward capturing the query's \"informativeness mass\" (IDF-weighted coverage).\n    idf_cov_gamma: float = 0.22\n\n    # Extra mild length prior beyond BM25 norm (controls verbosity wins).\n    dl_alpha: float = 0.08\n\n    # NEW: per-term evidence concavity: log1p(tf/k) instead of tf/(tf+...).\n    # Rationale: in verbose domains (StackOverflow, long scientific docs), repeated mentions\n    # should add diminishing evidence but not be overly coupled to length norm.\n    tf_log_k: float = 1.0\n\n    # NEW: make coverage prefer satisfying the *rarest* constraints via a soft-max.\n    # This helps \"key term\" queries (BRIGHT theorem/econ) where one rare token is decisive.\n    cov_softmax_temp: float = 1.2\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    \"Key-constraint\" evidence model (lexical, non-neural):\n\n    - Per-term evidence uses log1p(tf/k): interpretable as diminishing returns of repeated mentions.\n      This is less sensitive to doc length than BM25 tf/(tf+k1*norm), which can over-favor\n      long documents that repeat a term many times.\n    - Coverage is computed twice:\n        (a) IDF-mass fraction (like before) for robustness/recall.\n        (b) softmax-over-weights (\"did we match the rarest constraint?\") for early precision.\n      Both are bounded multipliers (safe for recall@100).\n    - Keep coordination and mild length prior.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Softmax coverage stats (temperature controls how \"max-like\" it is).\n    sm_temp = max(Config.cov_softmax_temp, eps)\n    sm_den = 0.0\n    sm_num = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * clarity * term_idf\n\n        cov_den += wt\n\n        # softmax over query weights to emphasize the most informative constraint\n        e = math.exp(min(50.0, wt / sm_temp))\n        sm_den += e\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sm_num += e\n\n        # evidence: concave in tf, lightly normalized by document length norm\n        score += wt * math.log1p(tf / (Config.tf_log_k + k1 * norm + eps))\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    # Old coverage: smooth, recall-friendly.\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        idf_cov = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    # New coverage: \"did we hit the decisive rare term?\"\n    if sm_den > 0.0:\n        soft_cov = sm_num / (sm_den + eps)  # in [0,1]\n        score *= 1.0 + 0.15 * soft_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        sm_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        sm_den = 0.0\n        sm_temp = max(Config.cov_softmax_temp, eps)\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            e = float(np.exp(min(50.0, wt / sm_temp)))\n            sm_den += e\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sm_num += e * present\n\n            # concave tf evidence, lightly length-normalized\n            scores += wt * np.log1p(tf_row / (Config.tf_log_k + k1 * norms + eps))\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if sm_den > 0.0:\n            soft_cov = sm_num / (sm_den + eps)\n            scores *= 1.0 + 0.15 * soft_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "d79572a4-1896-4082-8293-0368218dd24b", "generation": 4, "timestamp": 1770102816.852155, "iteration_found": 72, "metrics": {"bright_pony_ndcg@10": 0.08127654969605277, "bright_pony_recall@100": 0.2746403792142289, "bright_pony_index_time_ms": 7044.104954227805, "bright_pony_query_time_ms": 865.9393461421132, "beir_nfcorpus_ndcg@10": 0.32841851477646966, "beir_nfcorpus_recall@100": 0.24943414725936605, "beir_nfcorpus_index_time_ms": 10738.485320005566, "beir_nfcorpus_query_time_ms": 199.07483272254467, "beir_scifact_ndcg@10": 0.666849866334975, "beir_scifact_recall@100": 0.9092222222222222, "beir_scifact_index_time_ms": 15643.186080735177, "beir_scifact_query_time_ms": 657.4080889113247, "beir_arguana_ndcg@10": 0.2364353815952809, "beir_arguana_recall@100": 0.8957887223411849, "beir_arguana_index_time_ms": 28991.525472607464, "beir_arguana_query_time_ms": 37119.19187102467, "bright_theoremqa_theorems_ndcg@10": 0.03391651834262386, "bright_theoremqa_theorems_recall@100": 0.1293859649122807, "bright_theoremqa_theorems_index_time_ms": 55086.004673969, "bright_theoremqa_theorems_query_time_ms": 1328.8654428906739, "beir_scidocs_ndcg@10": 0.14167865164379834, "beir_scidocs_recall@100": 0.3513166666666667, "beir_scidocs_index_time_ms": 89859.48020778596, "beir_scidocs_query_time_ms": 4831.717652734369, "bright_economics_ndcg@10": 0.15061462143848778, "bright_economics_recall@100": 0.3930581525101558, "bright_economics_index_time_ms": 50539.11695163697, "bright_economics_query_time_ms": 3706.4822372049093, "bright_biology_ndcg@10": 0.2872618399398397, "bright_biology_recall@100": 0.5655083844787472, "bright_biology_index_time_ms": 60496.30523240194, "bright_biology_query_time_ms": 3268.8092472963035, "beir_fiqa_ndcg@10": 0.19173782138604237, "beir_fiqa_recall@100": 0.4869097466319689, "beir_fiqa_index_time_ms": 92804.34397887439, "beir_fiqa_query_time_ms": 7960.768540855497, "bright_earth_science_ndcg@10": 0.34213226397394075, "bright_earth_science_recall@100": 0.6756450540536294, "bright_earth_science_index_time_ms": 122720.41492024437, "bright_earth_science_query_time_ms": 5252.479043789208, "bright_stackoverflow_ndcg@10": 0.20629185972712777, "bright_stackoverflow_recall@100": 0.5075388300256207, "bright_stackoverflow_index_time_ms": 390448.32992972806, "bright_stackoverflow_query_time_ms": 11876.93647807464, "beir_trec-covid_ndcg@10": 0.6932001765063751, "beir_trec-covid_recall@100": 0.11619314800095908, "beir_trec-covid_index_time_ms": 498790.55662918836, "beir_trec-covid_query_time_ms": 1997.4941601976752, "avg_ndcg@10": 0.2799845054467512, "avg_recall@100": 0.4628867848597525, "combined_score": 0.42630632897715226, "total_index_time_ms": 1423161.854351405, "total_query_time_ms": 79065.16694184393, "total_time_ms": 1502227.021293249, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 24 lines with 32 lines\nChange 2: Replace 78 lines with 95 lines\nChange 3: Replace 52 lines with 65 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3172857871382087, "beir_nfcorpus_recall@100": 0.2557217356264306, "beir_nfcorpus_index_time_ms": 3516.05804200517, "beir_nfcorpus_query_time_ms": 96.3961660163477, "beir_scifact_ndcg@10": 0.6820707022580955, "beir_scifact_recall@100": 0.9313333333333332, "beir_scifact_index_time_ms": 4747.291915991809, "beir_scifact_query_time_ms": 347.4122919724323, "bright_pony_ndcg@10": 0.12729707687404868, "bright_pony_recall@100": 0.3228037337849061, "bright_pony_index_time_ms": 1362.3353330185637, "bright_pony_query_time_ms": 440.1478330255486, "beir_arguana_ndcg@10": 0.27937040754017817, "beir_arguana_recall@100": 0.9093504639543183, "beir_arguana_index_time_ms": 5698.903332988266, "beir_arguana_query_time_ms": 18766.322625044268, "bright_theoremqa_theorems_ndcg@10": 0.0298532584476064, "bright_theoremqa_theorems_recall@100": 0.15789473684210525, "bright_theoremqa_theorems_index_time_ms": 10599.510916974396, "bright_theoremqa_theorems_query_time_ms": 798.2328329817392, "beir_scidocs_ndcg@10": 0.1468898067723501, "beir_scidocs_recall@100": 0.34471666666666667, "beir_scidocs_index_time_ms": 19194.39641700592, "beir_scidocs_query_time_ms": 3270.872958994005, "bright_economics_ndcg@10": 0.13342313101096143, "bright_economics_recall@100": 0.35352249617759235, "bright_economics_index_time_ms": 13856.981292017736, "bright_economics_query_time_ms": 2225.6662500440143, "bright_biology_ndcg@10": 0.2494866209185038, "bright_biology_recall@100": 0.53322966989293, "bright_biology_index_time_ms": 14731.527541996911, "bright_biology_query_time_ms": 2143.2887080009095, "beir_fiqa_ndcg@10": 0.2284365336401491, "beir_fiqa_recall@100": 0.5318324636611673, "beir_fiqa_index_time_ms": 32486.81995802326, "beir_fiqa_query_time_ms": 5758.850709011313, "bright_earth_science_ndcg@10": 0.3020754317304195, "bright_earth_science_recall@100": 0.6455451013243663, "bright_earth_science_index_time_ms": 30528.6479589995, "bright_earth_science_query_time_ms": 3539.072666026186, "bright_stackoverflow_ndcg@10": 0.17263911109994032, "bright_stackoverflow_recall@100": 0.44831009137489153, "bright_stackoverflow_index_time_ms": 98096.7196659767, "bright_stackoverflow_query_time_ms": 7925.734792021103, "beir_trec-covid_ndcg@10": 0.6407249258969625, "beir_trec-covid_recall@100": 0.10592706406512097, "beir_trec-covid_index_time_ms": 131244.8082920164, "beir_trec-covid_query_time_ms": 1413.061833998654, "avg_ndcg@10": 0.2757960661106187, "avg_recall@100": 0.4616822963919857, "combined_score": 0.42450505033571234, "total_index_time_ms": 366064.0006670146, "total_query_time_ms": 46725.05966713652, "total_time_ms": 412789.06033415115, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4245\n- Feature coordinates: \n- Focus areas: - Fitness unchanged at 0.4245\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization (reduces over-penalizing long docs).\n      dl_p: float = 0.75\n  \n      # Soft coordination / coverage pressure (bounded multiplicative boost).\n      coord_alpha: float = 0.2\n  \n      # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n      # clarity = (idf/(idf+1))^p\n      q_clarity_power: float = 0.6\n  \n      # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n      qtf_power: float = 0.5\nwith:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization (reduces over-penalizing long docs).\n      dl_p: float = 0.75\n  \n      # Soft coordination / coverage pressure (bounded multiplicative boost).\n      coord_alpha: float = 0.2\n  \n      # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n      # clarity = (idf/(idf+1))^p\n      q_clarity_power: float = 0.6\n  \n      # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n      qtf_power: float = 0.5\n  \n      # NEW: reward covering a larger fraction of the query's informative mass.\n      # This is a bounded multiplier, so it tends to lift nDCG@10 without destabilizing recall@100.\n      idf_cov_gamma: float = 0.22\n  \n      # NEW: mild extra length prior beyond BM25 norm (reduces \"long doc wins by chance\").\n      dl_alpha: float = 0.08\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 with sublinear length norm + soft coordination.\n  \n      - Evidence per term: idf(t) * tf/(tf + k1*norm)\n      - Clarity gate: (idf/(idf+1))^p in [0,1] suppresses very common query terms.\n      - Coordination: bounded boost for matching more distinct query constraints.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n  \n      # Sublinear length normalization (dl^p), must match vectorized path.\n      p_len = Config.dl_p\n      dl = float(doc_length) ** p_len\n      adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      matched = 0\n      uq = 0\n  \n  ... (24 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 + two bounded priors:\n  \n      (1) Coordination: reward satisfying more distinct query constraints.\n      (2) IDF-mass coverage: reward capturing a larger fraction of the query's *informative* weight.\n          Intuition: queries often mix generic + specific tokens; matching the rare constraints\n          should matter disproportionately for early precision.\n      (3) Mild length prior: reduces verbosity wins beyond BM25's norm.\n  \n      All additions are bounded/mild to preserve recall@100 stability.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n  \n      # Sublinear length normalization (dl^p), must match vectorized path.\n      p_len = Config.dl_p\n      dl = float(doc_length) ** p_len\n      adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n  ... (46 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          uq = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              present = (tf_row > 0.0).astype(np.float64)\n              matched += present\n  ... (8 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n  \n          uq = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = (w * clarity) * idf_val\n  ... (22 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3173, beir_nfcorpus_recall@100: 0.2557, beir_nfcorpus_index_time_ms: 3512.5851, beir_nfcorpus_query_time_ms: 96.7448, beir_scifact_ndcg@10: 0.6821, beir_scifact_recall@100: 0.9313, beir_scifact_index_time_ms: 4737.2734, beir_scifact_query_time_ms: 347.3703, bright_pony_ndcg@10: 0.1273, bright_pony_recall@100: 0.3228, bright_pony_index_time_ms: 1359.9523, bright_pony_query_time_ms: 439.1256, beir_arguana_ndcg@10: 0.2794, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 5703.7116, beir_arguana_query_time_ms: 18703.8916, bright_theoremqa_theorems_ndcg@10: 0.0299, bright_theoremqa_theorems_recall@100: 0.1579, bright_theoremqa_theorems_index_time_ms: 10384.3091, bright_theoremqa_theorems_query_time_ms: 796.3169, beir_scidocs_ndcg@10: 0.1469, beir_scidocs_recall@100: 0.3447, beir_scidocs_index_time_ms: 19077.8816, beir_scidocs_query_time_ms: 3181.1090, bright_economics_ndcg@10: 0.1334, bright_economics_recall@100: 0.3535, bright_economics_index_time_ms: 13426.6931, bright_economics_query_time_ms: 2224.8237, bright_biology_ndcg@10: 0.2495, bright_biology_recall@100: 0.5332, bright_biology_index_time_ms: 14391.5791, bright_biology_query_time_ms: 2141.4373, beir_fiqa_ndcg@10: 0.2284, beir_fiqa_recall@100: 0.5318, beir_fiqa_index_time_ms: 33080.5522, beir_fiqa_query_time_ms: 5526.6159, bright_earth_science_ndcg@10: 0.3021, bright_earth_science_recall@100: 0.6455, bright_earth_science_index_time_ms: 31101.5420, bright_earth_science_query_time_ms: 3627.8712, bright_stackoverflow_ndcg@10: 0.1726, bright_stackoverflow_recall@100: 0.4483, bright_stackoverflow_index_time_ms: 97417.2485, bright_stackoverflow_query_time_ms: 7821.1751, beir_trec-covid_ndcg@10: 0.6407, beir_trec-covid_recall@100: 0.1059, beir_trec-covid_index_time_ms: 131585.9465, beir_trec-covid_query_time_ms: 1400.3436, avg_ndcg@10: 0.2758, avg_recall@100: 0.4617, combined_score: 0.4245, total_index_time_ms: 365779.2745, total_query_time_ms: 46306.8250, total_time_ms: 412086.0995, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization (reduces over-penalizing long docs).\n      dl_p: float = 0.75\n  \n      # Soft coordination / coverage pressure (bounded multiplicative boost).\n      coord_alpha: float = 0.2\n  \n      # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n      # clarity = (idf/(idf+1))^p\n      q_clarity_power: float = 0.6\n  \n      # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n      qtf_power: float = 0.5\nwith:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization (reduces over-penalizing long docs).\n      dl_p: float = 0.75\n  \n      # Soft coordination / coverage pressure (bounded multiplicative boost).\n      coord_alpha: float = 0.2\n  \n      # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n      # clarity = (idf/(idf+1))^p\n      q_clarity_power: float = 0.6\n  \n      # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n      qtf_power: float = 0.5\n  \n      # NEW: reward covering the query's informative \"IDF mass\" (bounded).\n      # Intuition: matching rare constraints should matter more than matching common ones.\n      idf_cov_gamma: float = 0.22\n  \n      # NEW: mild extra length prior (separate from BM25 norm) to reduce verbosity wins.\n      dl_alpha: float = 0.08\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 with sublinear length norm + soft coordination.\n  \n      - Evidence per term: idf(t) * tf/(tf + k1*norm)\n      - Clarity gate: (idf/(idf+1))^p in [0,1] suppresses very common query terms.\n      - Coordination: bounded boost for matching more distinct query constraints.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n  \n      # Sublinear length normalization (dl^p), must match vectorized path.\n      p_len = Config.dl_p\n      dl = float(doc_length) ** p_len\n      adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      matched = 0\n      uq = 0\n  \n  ... (24 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 with sublinear length norm + two bounded priors:\n  \n      (1) Coordination: rewards satisfying more distinct query constraints.\n      (2) IDF-mass coverage: rewards covering a larger fraction of the query's *informative* weight.\n          This helps mixed queries (generic + specific tokens) and tends to lift nDCG@10.\n      (3) Mild length prior: reduces long-doc \"wins by chance\" without destabilizing recall.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n  \n      # Sublinear length normalization (dl^p), must match vectorized path.\n      p_len = Config.dl_p\n      dl = float(doc_length) ** p_len\n      adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      matched = 0.0\n      uq = 0.0\n  ... (44 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          uq = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              present = (tf_row > 0.0).astype(np.float64)\n              matched += present\n  ... (8 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n  \n          uq = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = (w * clarity) * idf_val\n  ... (22 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3173, beir_nfcorpus_recall@100: 0.2557, beir_nfcorpus_index_time_ms: 3531.5238, beir_nfcorpus_query_time_ms: 96.0431, beir_scifact_ndcg@10: 0.6821, beir_scifact_recall@100: 0.9313, beir_scifact_index_time_ms: 4827.7183, beir_scifact_query_time_ms: 350.3395, bright_pony_ndcg@10: 0.1273, bright_pony_recall@100: 0.3228, bright_pony_index_time_ms: 1404.1253, bright_pony_query_time_ms: 443.0239, beir_arguana_ndcg@10: 0.2794, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 5815.6166, beir_arguana_query_time_ms: 19216.7044, bright_theoremqa_theorems_ndcg@10: 0.0299, bright_theoremqa_theorems_recall@100: 0.1579, bright_theoremqa_theorems_index_time_ms: 10826.6740, bright_theoremqa_theorems_query_time_ms: 773.5783, beir_scidocs_ndcg@10: 0.1469, beir_scidocs_recall@100: 0.3447, beir_scidocs_index_time_ms: 19917.5373, beir_scidocs_query_time_ms: 3340.9190, bright_economics_ndcg@10: 0.1334, bright_economics_recall@100: 0.3535, bright_economics_index_time_ms: 13860.2573, bright_economics_query_time_ms: 2215.1571, bright_biology_ndcg@10: 0.2495, bright_biology_recall@100: 0.5332, bright_biology_index_time_ms: 14727.1112, bright_biology_query_time_ms: 2109.6670, beir_fiqa_ndcg@10: 0.2284, beir_fiqa_recall@100: 0.5318, beir_fiqa_index_time_ms: 33864.1550, beir_fiqa_query_time_ms: 6005.6215, bright_earth_science_ndcg@10: 0.3021, bright_earth_science_recall@100: 0.6455, bright_earth_science_index_time_ms: 32238.0540, bright_earth_science_query_time_ms: 3817.2926, bright_stackoverflow_ndcg@10: 0.1726, bright_stackoverflow_recall@100: 0.4483, bright_stackoverflow_index_time_ms: 99248.9410, bright_stackoverflow_query_time_ms: 7738.9360, beir_trec-covid_ndcg@10: 0.6407, beir_trec-covid_recall@100: 0.1059, beir_trec-covid_index_time_ms: 132040.7075, beir_trec-covid_query_time_ms: 1364.9959, avg_ndcg@10: 0.2758, avg_recall@100: 0.4617, combined_score: 0.4245, total_index_time_ms: 372302.4213, total_query_time_ms: 47472.2782, total_time_ms: 419774.6995, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization (reduces over-penalizing long docs).\n      dl_p: float = 0.75\n  \n      # Soft coordination / coverage pressure (bounded multiplicative boost).\n      coord_alpha: float = 0.2\n  \n      # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n      # clarity = (idf/(idf+1))^p\n      q_clarity_power: float = 0.6\n  \n      # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n      qtf_power: float = 0.5\nwith:\n  class Config:\n      k1: float = 0.9\n      b: float = 0.4\n      epsilon: float = 1e-9\n  \n      # Sublinear length normalization (reduces over-penalizing long docs).\n      dl_p: float = 0.75\n  \n      # Soft coordination / coverage pressure (bounded multiplicative boost).\n      coord_alpha: float = 0.2\n  \n      # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n      # clarity = (idf/(idf+1))^p\n      q_clarity_power: float = 0.6\n  \n      # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n      qtf_power: float = 0.5\n  \n      # NEW: reward capturing the query's \"informativeness mass\" (IDF-weighted coverage).\n      # Bounded multiplier => tends to help nDCG without destabilizing recall@100.\n      idf_cov_gamma: float = 0.22\n  \n      # NEW: extra mild length prior beyond BM25 norm (controls verbosity wins).\n      dl_alpha: float = 0.08\nChange 2: Replace:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 with sublinear length norm + soft coordination.\n  \n      - Evidence per term: idf(t) * tf/(tf + k1*norm)\n      - Clarity gate: (idf/(idf+1))^p in [0,1] suppresses very common query terms.\n      - Coordination: bounded boost for matching more distinct query constraints.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n  \n      # Sublinear length normalization (dl^p), must match vectorized path.\n      p_len = Config.dl_p\n      dl = float(doc_length) ** p_len\n      adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n      matched = 0\n      uq = 0\n  \n  ... (24 more lines)\nwith:\n  def retrieval_score(\n      query_repr: QueryRepr,\n      doc_tf: Counter[str],\n      doc_length: float,\n      N: int,\n      avgdl: float,\n      corpus_df: Counter[str],\n  ) -> float:\n      \"\"\"\n      Clarity-gated BM25 + two bounded priors:\n  \n      (1) Coordination (count-based): prefer docs matching more distinct query constraints.\n      (2) IDF-mass coverage: reward capturing a larger fraction of the query's informative mass.\n          This helps when queries mix generic + specific terms (common in BEIR/BRIGHT).\n      (3) Mild length prior: reduces \"long doc wins by chance\" beyond BM25's norm.\n  \n      All additions are bounded/mild to preserve recall@100 stability.\n      \"\"\"\n      if not query_repr.terms:\n          return 0.0\n  \n      k1, b, eps = Config.k1, Config.b, Config.epsilon\n  \n      # Sublinear length normalization (dl^p), must match vectorized path.\n      p_len = Config.dl_p\n      dl = float(doc_length) ** p_len\n      adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n      norm = 1.0 - b + b * (dl / (adl + eps))\n  \n      score = 0.0\n  ... (48 more lines)\nChange 3: Replace:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          uq = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n  \n              present = (tf_row > 0.0).astype(np.float64)\n              matched += present\n  ... (8 more lines)\nwith:\n  def _score_candidates_vectorized(\n          self,\n          query_term_ids: list[int],\n          candidate_docs: NDArray[np.int64],\n          query_term_weights: NDArray[np.float64] | None = None,\n      ) -> NDArray[np.float64]:\n          \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n          if len(candidate_docs) == 0:\n              return np.array([], dtype=np.float64)\n  \n          k1, eps = Config.k1, Config.epsilon\n          norms = self.corpus.norm_array[candidate_docs]\n  \n          scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n          cov_den = 0.0\n  \n          uq = float(len(query_term_ids))\n          for i, term_id in enumerate(query_term_ids):\n              idf_val = float(self.corpus.idf_array[term_id])\n              if idf_val <= 0.0:\n                  continue\n  \n              rarity = idf_val / (idf_val + 1.0)\n              clarity = rarity ** Config.q_clarity_power\n  \n              w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n              wt = (w * clarity) * idf_val\n  ... (22 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3173, beir_nfcorpus_recall@100: 0.2557, beir_nfcorpus_index_time_ms: 3516.0580, beir_nfcorpus_query_time_ms: 96.3962, beir_scifact_ndcg@10: 0.6821, beir_scifact_recall@100: 0.9313, beir_scifact_index_time_ms: 4747.2919, beir_scifact_query_time_ms: 347.4123, bright_pony_ndcg@10: 0.1273, bright_pony_recall@100: 0.3228, bright_pony_index_time_ms: 1362.3353, bright_pony_query_time_ms: 440.1478, beir_arguana_ndcg@10: 0.2794, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 5698.9033, beir_arguana_query_time_ms: 18766.3226, bright_theoremqa_theorems_ndcg@10: 0.0299, bright_theoremqa_theorems_recall@100: 0.1579, bright_theoremqa_theorems_index_time_ms: 10599.5109, bright_theoremqa_theorems_query_time_ms: 798.2328, beir_scidocs_ndcg@10: 0.1469, beir_scidocs_recall@100: 0.3447, beir_scidocs_index_time_ms: 19194.3964, beir_scidocs_query_time_ms: 3270.8730, bright_economics_ndcg@10: 0.1334, bright_economics_recall@100: 0.3535, bright_economics_index_time_ms: 13856.9813, bright_economics_query_time_ms: 2225.6663, bright_biology_ndcg@10: 0.2495, bright_biology_recall@100: 0.5332, bright_biology_index_time_ms: 14731.5275, bright_biology_query_time_ms: 2143.2887, beir_fiqa_ndcg@10: 0.2284, beir_fiqa_recall@100: 0.5318, beir_fiqa_index_time_ms: 32486.8200, beir_fiqa_query_time_ms: 5758.8507, bright_earth_science_ndcg@10: 0.3021, bright_earth_science_recall@100: 0.6455, bright_earth_science_index_time_ms: 30528.6480, bright_earth_science_query_time_ms: 3539.0727, bright_stackoverflow_ndcg@10: 0.1726, bright_stackoverflow_recall@100: 0.4483, bright_stackoverflow_index_time_ms: 98096.7197, bright_stackoverflow_query_time_ms: 7925.7348, beir_trec-covid_ndcg@10: 0.6407, beir_trec-covid_recall@100: 0.1059, beir_trec-covid_index_time_ms: 131244.8083, beir_trec-covid_query_time_ms: 1413.0618, avg_ndcg@10: 0.2758, avg_recall@100: 0.4617, combined_score: 0.4245, total_index_time_ms: 366064.0007, total_query_time_ms: 46725.0597, total_time_ms: 412789.0603, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4270)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave-evidence scoring + explicit query-term coverage pressure.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence: idf * log1p(tf/base) then log1p(sum_evidence)\n    tf_log_base: float = 1.0\n\n    # Length prior: prefer focused docs, but keep it mild.\n    dl_alpha: float = 0.15\n\n    # Soft \"AND pressure\" to reward documents that cover more query constraints.\n    coverage_gamma: float = 0.25\n\n    # Query-side common-term gating (bounded in [0,1]).\n    q_clarity_power: float = 0.6\n\n    # New: normalize query-term repetition; verbose queries otherwise overcount easy terms.\n    # weight = count**qtf_power (0.5 ~ sqrt) and (optionally) treat query as unique constraints.\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Self-information IDF: log1p(N/df).\n\n    Rationale: interprets df/N as an occurrence probability; matching a term yields\n    surprisal. This tends to be smoother than BM25-odds IDF across heterogeneous corpora,\n    avoiding very negative/flat weights for frequent terms.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as a set of lexical constraints with sublinear qtf.\n        This usually helps early precision (nDCG@10) on verbose QA/scientific queries\n        while keeping recall robust.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        counts = Counter(tokens)\n        terms = list(counts.keys())  # unique constraints\n        weights = {t: float(c) ** Config.qtf_power for t, c in counts.items()}\n        return cls(terms=terms, term_weights=weights)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave evidence + coverage pressure + query-clarity gating.\n\n    Improvements vs current:\n    - Use unique query constraints (handled in QueryRepr) to stabilize denom/matched.\n    - Keep concavity but make the coverage multiplier depend on *IDF mass covered*,\n      not just a raw count. This rewards satisfying informative constraints.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        # query-side bounded clarity in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = query_repr.term_weights.get(term, 1.0)\n        wt = w * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)  # ~[0,1]\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Match QueryRepr: unique terms + sublinear qtf weights.\n        counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, c in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3297), Performs well on beir_nfcorpus_recall@100 (0.2562), Performs well on beir_nfcorpus_index_time_ms (3713.8193), Performs well on beir_nfcorpus_query_time_ms (101.7529), Performs well on beir_scifact_ndcg@10 (0.6814), Performs well on beir_scifact_recall@100 (0.9142), Performs well on beir_scifact_index_time_ms (5040.8995), Performs well on beir_scifact_query_time_ms (402.5420), Performs well on bright_pony_ndcg@10 (0.1003), Performs well on bright_pony_recall@100 (0.2847), Performs well on bright_pony_index_time_ms (1499.6320), Performs well on bright_pony_query_time_ms (489.7649), Performs well on beir_arguana_ndcg@10 (0.2899), Performs well on beir_arguana_recall@100 (0.9193), Performs well on beir_arguana_index_time_ms (6063.9265), Performs well on beir_arguana_query_time_ms (22360.2629), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0289), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11132.1687), Performs well on bright_theoremqa_theorems_query_time_ms (937.0960), Performs well on beir_scidocs_ndcg@10 (0.1520), Performs well on beir_scidocs_recall@100 (0.3542), Performs well on beir_scidocs_index_time_ms (20489.3998), Performs well on beir_scidocs_query_time_ms (3753.5755), Performs well on bright_economics_ndcg@10 (0.1493), Performs well on bright_economics_recall@100 (0.3667), Performs well on bright_economics_index_time_ms (15249.5355), Performs well on bright_economics_query_time_ms (2662.4329), Performs well on bright_biology_ndcg@10 (0.2412), Performs well on bright_biology_recall@100 (0.5169), Performs well on bright_biology_index_time_ms (17107.5830), Performs well on bright_biology_query_time_ms (2513.0199), Performs well on beir_fiqa_ndcg@10 (0.2285), Performs well on beir_fiqa_recall@100 (0.5185), Performs well on beir_fiqa_index_time_ms (37245.2325), Performs well on beir_fiqa_query_time_ms (6213.1188), Performs well on bright_earth_science_ndcg@10 (0.3009), Performs well on bright_earth_science_recall@100 (0.6427), Performs well on bright_earth_science_index_time_ms (34468.6744), Performs well on bright_earth_science_query_time_ms (4001.5292), Performs well on bright_stackoverflow_ndcg@10 (0.1936), Performs well on bright_stackoverflow_recall@100 (0.5094), Performs well on bright_stackoverflow_index_time_ms (103291.2602), Performs well on bright_stackoverflow_query_time_ms (8714.9653), Performs well on beir_trec-covid_ndcg@10 (0.7113), Performs well on beir_trec-covid_recall@100 (0.1163), Performs well on beir_trec-covid_index_time_ms (139683.6847), Performs well on beir_trec-covid_query_time_ms (1477.9168), Performs well on avg_ndcg@10 (0.2839), Performs well on avg_recall@100 (0.4627), Performs well on combined_score (0.4270), Performs well on total_index_time_ms (394985.8162), Performs well on total_query_time_ms (53627.9770), Performs well on total_time_ms (448613.7932), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4245)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # NEW: reward capturing the query's \"informativeness mass\" (IDF-weighted coverage).\n    # Bounded multiplier => tends to help nDCG without destabilizing recall@100.\n    idf_cov_gamma: float = 0.22\n\n    # NEW: extra mild length prior beyond BM25 norm (controls verbosity wins).\n    dl_alpha: float = 0.08\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 + two bounded priors:\n\n    (1) Coordination (count-based): prefer docs matching more distinct query constraints.\n    (2) IDF-mass coverage: reward capturing a larger fraction of the query's informative mass.\n        This helps when queries mix generic + specific terms (common in BEIR/BRIGHT).\n    (3) Mild length prior: reduces \"long doc wins by chance\" beyond BM25's norm.\n\n    All additions are bounded/mild to preserve recall@100 stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    # IDF-mass coverage (query-side normalizer and doc-side numerator).\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        # In [0,1], but weighted by informativeness rather than raw term count.\n        idf_cov = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3173), Performs well on beir_nfcorpus_recall@100 (0.2557), Performs well on beir_nfcorpus_index_time_ms (3516.0580), Performs well on beir_nfcorpus_query_time_ms (96.3962), Performs well on beir_scifact_ndcg@10 (0.6821), Performs well on beir_scifact_recall@100 (0.9313), Performs well on beir_scifact_index_time_ms (4747.2919), Performs well on beir_scifact_query_time_ms (347.4123), Performs well on bright_pony_ndcg@10 (0.1273), Performs well on bright_pony_recall@100 (0.3228), Performs well on bright_pony_index_time_ms (1362.3353), Performs well on bright_pony_query_time_ms (440.1478), Performs well on beir_arguana_ndcg@10 (0.2794), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (5698.9033), Performs well on beir_arguana_query_time_ms (18766.3226), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0299), Performs well on bright_theoremqa_theorems_recall@100 (0.1579), Performs well on bright_theoremqa_theorems_index_time_ms (10599.5109), Performs well on bright_theoremqa_theorems_query_time_ms (798.2328), Performs well on beir_scidocs_ndcg@10 (0.1469), Performs well on beir_scidocs_recall@100 (0.3447), Performs well on beir_scidocs_index_time_ms (19194.3964), Performs well on beir_scidocs_query_time_ms (3270.8730), Performs well on bright_economics_ndcg@10 (0.1334), Performs well on bright_economics_recall@100 (0.3535), Performs well on bright_economics_index_time_ms (13856.9813), Performs well on bright_economics_query_time_ms (2225.6663), Performs well on bright_biology_ndcg@10 (0.2495), Performs well on bright_biology_recall@100 (0.5332), Performs well on bright_biology_index_time_ms (14731.5275), Performs well on bright_biology_query_time_ms (2143.2887), Performs well on beir_fiqa_ndcg@10 (0.2284), Performs well on beir_fiqa_recall@100 (0.5318), Performs well on beir_fiqa_index_time_ms (32486.8200), Performs well on beir_fiqa_query_time_ms (5758.8507), Performs well on bright_earth_science_ndcg@10 (0.3021), Performs well on bright_earth_science_recall@100 (0.6455), Performs well on bright_earth_science_index_time_ms (30528.6480), Performs well on bright_earth_science_query_time_ms (3539.0727), Performs well on bright_stackoverflow_ndcg@10 (0.1726), Performs well on bright_stackoverflow_recall@100 (0.4483), Performs well on bright_stackoverflow_index_time_ms (98096.7197), Performs well on bright_stackoverflow_query_time_ms (7925.7348), Performs well on beir_trec-covid_ndcg@10 (0.6407), Performs well on beir_trec-covid_recall@100 (0.1059), Performs well on beir_trec-covid_index_time_ms (131244.8083), Performs well on beir_trec-covid_query_time_ms (1413.0618), Performs well on avg_ndcg@10 (0.2758), Performs well on avg_recall@100 (0.4617), Performs well on combined_score (0.4245), Performs well on total_index_time_ms (366064.0007), Performs well on total_query_time_ms (46725.0597), Performs well on total_time_ms (412789.0603), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4245)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # NEW: reward covering the query's informative \"IDF mass\" (bounded).\n    # Intuition: matching rare constraints should matter more than matching common ones.\n    idf_cov_gamma: float = 0.22\n\n    # NEW: mild extra length prior (separate from BM25 norm) to reduce verbosity wins.\n    dl_alpha: float = 0.08\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 with sublinear length norm + two bounded priors:\n\n    (1) Coordination: rewards satisfying more distinct query constraints.\n    (2) IDF-mass coverage: rewards covering a larger fraction of the query's *informative* weight.\n        This helps mixed queries (generic + specific tokens) and tends to lift nDCG@10.\n    (3) Mild length prior: reduces long-doc \"wins by chance\" without destabilizing recall.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = (wq * clarity) * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        idf_cov = cov_num / (cov_den + eps)  # ~[0,1]\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3173), Performs well on beir_nfcorpus_recall@100 (0.2557), Performs well on beir_nfcorpus_index_time_ms (3531.5238), Performs well on beir_nfcorpus_query_time_ms (96.0431), Performs well on beir_scifact_ndcg@10 (0.6821), Performs well on beir_scifact_recall@100 (0.9313), Performs well on beir_scifact_index_time_ms (4827.7183), Performs well on beir_scifact_query_time_ms (350.3395), Performs well on bright_pony_ndcg@10 (0.1273), Performs well on bright_pony_recall@100 (0.3228), Performs well on bright_pony_index_time_ms (1404.1253), Performs well on bright_pony_query_time_ms (443.0239), Performs well on beir_arguana_ndcg@10 (0.2794), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (5815.6166), Performs well on beir_arguana_query_time_ms (19216.7044), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0299), Performs well on bright_theoremqa_theorems_recall@100 (0.1579), Performs well on bright_theoremqa_theorems_index_time_ms (10826.6740), Performs well on bright_theoremqa_theorems_query_time_ms (773.5783), Performs well on beir_scidocs_ndcg@10 (0.1469), Performs well on beir_scidocs_recall@100 (0.3447), Performs well on beir_scidocs_index_time_ms (19917.5373), Performs well on beir_scidocs_query_time_ms (3340.9190), Performs well on bright_economics_ndcg@10 (0.1334), Performs well on bright_economics_recall@100 (0.3535), Performs well on bright_economics_index_time_ms (13860.2573), Performs well on bright_economics_query_time_ms (2215.1571), Performs well on bright_biology_ndcg@10 (0.2495), Performs well on bright_biology_recall@100 (0.5332), Performs well on bright_biology_index_time_ms (14727.1112), Performs well on bright_biology_query_time_ms (2109.6670), Performs well on beir_fiqa_ndcg@10 (0.2284), Performs well on beir_fiqa_recall@100 (0.5318), Performs well on beir_fiqa_index_time_ms (33864.1550), Performs well on beir_fiqa_query_time_ms (6005.6215), Performs well on bright_earth_science_ndcg@10 (0.3021), Performs well on bright_earth_science_recall@100 (0.6455), Performs well on bright_earth_science_index_time_ms (32238.0540), Performs well on bright_earth_science_query_time_ms (3817.2926), Performs well on bright_stackoverflow_ndcg@10 (0.1726), Performs well on bright_stackoverflow_recall@100 (0.4483), Performs well on bright_stackoverflow_index_time_ms (99248.9410), Performs well on bright_stackoverflow_query_time_ms (7738.9360), Performs well on beir_trec-covid_ndcg@10 (0.6407), Performs well on beir_trec-covid_recall@100 (0.1059), Performs well on beir_trec-covid_index_time_ms (132040.7075), Performs well on beir_trec-covid_query_time_ms (1364.9959), Performs well on avg_ndcg@10 (0.2758), Performs well on avg_recall@100 (0.4617), Performs well on combined_score (0.4245), Performs well on total_index_time_ms (372302.4213), Performs well on total_query_time_ms (47472.2782), Performs well on total_time_ms (419774.6995), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4245)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # NEW: reward covering a larger fraction of the query's informative mass.\n    # This is a bounded multiplier, so it tends to lift nDCG@10 without destabilizing recall@100.\n    idf_cov_gamma: float = 0.22\n\n    # NEW: mild extra length prior beyond BM25 norm (reduces \"long doc wins by chance\").\n    dl_alpha: float = 0.08\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 + two bounded priors:\n\n    (1) Coordination: reward satisfying more distinct query constraints.\n    (2) IDF-mass coverage: reward capturing a larger fraction of the query's *informative* weight.\n        Intuition: queries often mix generic + specific tokens; matching the rare constraints\n        should matter disproportionately for early precision.\n    (3) Mild length prior: reduces verbosity wins beyond BM25's norm.\n\n    All additions are bounded/mild to preserve recall@100 stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    # IDF-mass coverage (query-side normalizer and doc-side numerator).\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = (wq * clarity) * term_idf\n        cov_den += wt\n        cov_num += wt\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        idf_cov = cov_num / (cov_den + eps)  # ~[0,1]\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3173), Performs well on beir_nfcorpus_recall@100 (0.2557), Performs well on beir_nfcorpus_index_time_ms (3512.5851), Performs well on beir_nfcorpus_query_time_ms (96.7448), Performs well on beir_scifact_ndcg@10 (0.6821), Performs well on beir_scifact_recall@100 (0.9313), Performs well on beir_scifact_index_time_ms (4737.2734), Performs well on beir_scifact_query_time_ms (347.3703), Performs well on bright_pony_ndcg@10 (0.1273), Performs well on bright_pony_recall@100 (0.3228), Performs well on bright_pony_index_time_ms (1359.9523), Performs well on bright_pony_query_time_ms (439.1256), Performs well on beir_arguana_ndcg@10 (0.2794), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (5703.7116), Performs well on beir_arguana_query_time_ms (18703.8916), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0299), Performs well on bright_theoremqa_theorems_recall@100 (0.1579), Performs well on bright_theoremqa_theorems_index_time_ms (10384.3091), Performs well on bright_theoremqa_theorems_query_time_ms (796.3169), Performs well on beir_scidocs_ndcg@10 (0.1469), Performs well on beir_scidocs_recall@100 (0.3447), Performs well on beir_scidocs_index_time_ms (19077.8816), Performs well on beir_scidocs_query_time_ms (3181.1090), Performs well on bright_economics_ndcg@10 (0.1334), Performs well on bright_economics_recall@100 (0.3535), Performs well on bright_economics_index_time_ms (13426.6931), Performs well on bright_economics_query_time_ms (2224.8237), Performs well on bright_biology_ndcg@10 (0.2495), Performs well on bright_biology_recall@100 (0.5332), Performs well on bright_biology_index_time_ms (14391.5791), Performs well on bright_biology_query_time_ms (2141.4373), Performs well on beir_fiqa_ndcg@10 (0.2284), Performs well on beir_fiqa_recall@100 (0.5318), Performs well on beir_fiqa_index_time_ms (33080.5522), Performs well on beir_fiqa_query_time_ms (5526.6159), Performs well on bright_earth_science_ndcg@10 (0.3021), Performs well on bright_earth_science_recall@100 (0.6455), Performs well on bright_earth_science_index_time_ms (31101.5420), Performs well on bright_earth_science_query_time_ms (3627.8712), Performs well on bright_stackoverflow_ndcg@10 (0.1726), Performs well on bright_stackoverflow_recall@100 (0.4483), Performs well on bright_stackoverflow_index_time_ms (97417.2485), Performs well on bright_stackoverflow_query_time_ms (7821.1751), Performs well on beir_trec-covid_ndcg@10 (0.6407), Performs well on beir_trec-covid_recall@100 (0.1059), Performs well on beir_trec-covid_index_time_ms (131585.9465), Performs well on beir_trec-covid_query_time_ms (1400.3436), Performs well on avg_ndcg@10 (0.2758), Performs well on avg_recall@100 (0.4617), Performs well on combined_score (0.4245), Performs well on total_index_time_ms (365779.2745), Performs well on total_query_time_ms (46306.8250), Performs well on total_time_ms (412086.0995), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4241)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Prefer \"focused\" evidence but with diminishing penalty for long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.15\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    # clarity = (idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting: qtf^p (keeps repetition signal without verbosity blowups).\n    qtf_power: float = 0.5\n\n    # NEW: \"IDF-mass coverage\" multiplier.\n    # Intuition: satisfying rare/specific constraints should matter more than satisfying common ones.\n    # This tends to help nDCG@10 on mixed queries (generic + specific tokens) without hurting recall@100.\n    idf_cov_gamma: float = 0.25\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a *sublinear* repetition signal.\n\n        Motivation: repetition sometimes encodes emphasis (esp. noisy QA queries),\n        but linear qtf can over-weight verbosity. Use qtf^p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 with sublinear length norm + (optional) IDF-mass coverage.\n\n    Added idea (novel vs plain BM25 knobs):\n    - Treat the query as distributing a fixed \"informativeness mass\" across terms (wt = idf*clarity*wq).\n      A document is better if it captures more of that mass, not just more raw terms.\n      This is a bounded multiplier in [1, 1+gamma] to keep recall@100 stable.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (must match vectorized path).\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = (wq * clarity) * term_idf\n\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        cov_num += wt\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        cov = cov_num / (cov_den + eps)  # ~[0,1]\n        score *= 1.0 + Config.idf_cov_gamma * cov\n\n    return float(score)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4243)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # Reward capturing the query's \"informativeness mass\" (IDF-weighted coverage).\n    idf_cov_gamma: float = 0.22\n\n    # Extra mild length prior beyond BM25 norm (controls verbosity wins).\n    dl_alpha: float = 0.08\n\n    # NEW: \"informativeness-mass\" concavity. After summing per-term evidence, apply log1p()\n    # to reduce domination by one very high-tf term (helps early precision on verbose docs).\n    evidence_log_alpha: float = 0.35\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 + bounded priors + *concave informativeness aggregation*.\n\n    Key change: after summing weighted BM25 evidence, apply a soft concavity:\n        score := (1-a)*score + a*log1p(score)\n    This keeps recall stable while improving nDCG@10 by preventing a single repeated\n    term from dominating (common failure mode on long/verbose docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    # IDF-mass coverage (query-side normalizer and doc-side numerator).\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # NEW: concave aggregation of total evidence (bounded by convex combo).\n    a = float(getattr(Config, \"evidence_log_alpha\", 0.0))\n    if a > 0.0:\n        score = (1.0 - a) * score + a * math.log1p(score)\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        # In [0,1], but weighted by informativeness rather than raw term count.\n        idf_cov = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        # Must mirror retrieval_score(): concave aggregation of total evidence.\n        a = float(getattr(Config, \"evidence_log_alpha\", 0.0))\n        if a > 0.0:\n            scores = (1.0 - a) * scores + a * np.log1p(np.maximum(scores, 0.0))\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4231)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 with sublinear length norm + soft coordination.\n\n    - Evidence per term: idf(t) * tf/(tf + k1*norm)\n    - Clarity gate: (idf/(idf+1))^p in [0,1] suppresses very common query terms.\n    - Coordination: bounded boost for matching more distinct query constraints.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n    uq = 0\n\n    for term in query_repr.terms:\n        uq += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += (wq * clarity) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (float(uq) + eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4231)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Prefer \"focused\" evidence but with diminishing penalty for long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    # clarity = (idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting: qtf^p (keeps repetition signal without verbosity blowups).\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a *sublinear* repetition signal.\n\n        Motivation: repetition sometimes encodes emphasis (esp. noisy QA queries),\n        but linear qtf can over-weight verbosity. Use qtf^p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 with sublinear length norm + soft coordination.\n\n    Rationale:\n    - Term evidence is still BM25-like (saturated TF with length norm).\n    - Query-side clarity(t)=(idf/(idf+1))^p suppresses common query terms smoothly.\n    - Coordination prefers docs satisfying more distinct query constraints (helps nDCG@10).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (must match vectorized path).\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n    uq = 0\n\n    for term in query_repr.terms:\n        uq += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += (wq * clarity) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (float(uq) + eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4231, Type: Migrant)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 with sublinear length norm + soft coordination.\n\n    - Evidence per term: idf(t) * tf/(tf + k1*norm)\n    - Clarity gate: (idf/(idf+1))^p in [0,1] suppresses very common query terms.\n    - Coordination: bounded boost for matching more distinct query constraints.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n    uq = 0\n\n    for term in query_repr.terms:\n        uq += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += (wq * clarity) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (float(uq) + eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4270, Type: Migrant)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave-evidence scoring + explicit query-term coverage pressure.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence: idf * log1p(tf/base) then log1p(sum_evidence)\n    tf_log_base: float = 1.0\n\n    # Length prior: prefer focused docs, but keep it mild.\n    dl_alpha: float = 0.15\n\n    # Soft \"AND pressure\" to reward documents that cover more query constraints.\n    coverage_gamma: float = 0.25\n\n    # Query-side common-term gating (bounded in [0,1]).\n    q_clarity_power: float = 0.6\n\n    # New: normalize query-term repetition; verbose queries otherwise overcount easy terms.\n    # weight = count**qtf_power (0.5 ~ sqrt) and (optionally) treat query as unique constraints.\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Self-information IDF: log1p(N/df).\n\n    Rationale: interprets df/N as an occurrence probability; matching a term yields\n    surprisal. This tends to be smoother than BM25-odds IDF across heterogeneous corpora,\n    avoiding very negative/flat weights for frequent terms.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log1p(N / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as a set of lexical constraints with sublinear qtf.\n        This usually helps early precision (nDCG@10) on verbose QA/scientific queries\n        while keeping recall robust.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        counts = Counter(tokens)\n        terms = list(counts.keys())  # unique constraints\n        weights = {t: float(c) ** Config.qtf_power for t, c in counts.items()}\n        return cls(terms=terms, term_weights=weights)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave evidence + coverage pressure + query-clarity gating.\n\n    Improvements vs current:\n    - Use unique query constraints (handled in QueryRepr) to stabilize denom/matched.\n    - Keep concavity but make the coverage multiplier depend on *IDF mass covered*,\n      not just a raw count. This rewards satisfying informative constraints.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        # query-side bounded clarity in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        w = query_repr.term_weights.get(term, 1.0)\n        wt = w * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)  # ~[0,1]\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = w * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Match QueryRepr: unique terms + sublinear qtf weights.\n        counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, c in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4243, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # Reward capturing the query's \"informativeness mass\" (IDF-weighted coverage).\n    idf_cov_gamma: float = 0.22\n\n    # Extra mild length prior beyond BM25 norm (controls verbosity wins).\n    dl_alpha: float = 0.08\n\n    # NEW: \"informativeness-mass\" concavity. After summing per-term evidence, apply log1p()\n    # to reduce domination by one very high-tf term (helps early precision on verbose docs).\n    evidence_log_alpha: float = 0.35\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 + bounded priors + *concave informativeness aggregation*.\n\n    Key change: after summing weighted BM25 evidence, apply a soft concavity:\n        score := (1-a)*score + a*log1p(score)\n    This keeps recall stable while improving nDCG@10 by preventing a single repeated\n    term from dominating (common failure mode on long/verbose docs).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    # IDF-mass coverage (query-side normalizer and doc-side numerator).\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # NEW: concave aggregation of total evidence (bounded by convex combo).\n    a = float(getattr(Config, \"evidence_log_alpha\", 0.0))\n    if a > 0.0:\n        score = (1.0 - a) * score + a * math.log1p(score)\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        # In [0,1], but weighted by informativeness rather than raw term count.\n        idf_cov = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        # Must mirror retrieval_score(): concave aggregation of total evidence.\n        a = float(getattr(Config, \"evidence_log_alpha\", 0.0))\n        if a > 0.0:\n            scores = (1.0 - a) * scores + a * np.log1p(np.maximum(scores, 0.0))\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4231, Type: Migrant)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Prefer \"focused\" evidence but with diminishing penalty for long docs.\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gating: downweight very common query terms smoothly.\n    # clarity = (idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting: qtf^p (keeps repetition signal without verbosity blowups).\n    qtf_power: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a *sublinear* repetition signal.\n\n        Motivation: repetition sometimes encodes emphasis (esp. noisy QA queries),\n        but linear qtf can over-weight verbosity. Use qtf^p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 with sublinear length norm + soft coordination.\n\n    Rationale:\n    - Term evidence is still BM25-like (saturated TF with length norm).\n    - Query-side clarity(t)=(idf/(idf+1))^p suppresses common query terms smoothly.\n    - Coordination prefers docs satisfying more distinct query constraints (helps nDCG@10).\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (must match vectorized path).\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0\n    uq = 0\n\n    for term in query_repr.terms:\n        uq += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n        matched += 1\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += (wq * clarity) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (float(uq) + eps))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * clarity) * idf_val * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # NEW: reward capturing the query's \"informativeness mass\" (IDF-weighted coverage).\n    # Bounded multiplier => tends to help nDCG without destabilizing recall@100.\n    idf_cov_gamma: float = 0.22\n\n    # NEW: extra mild length prior beyond BM25 norm (controls verbosity wins).\n    dl_alpha: float = 0.08\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF: -log p(t in doc).\n    Using df/N as an empirical probability makes IDF behave more robustly across\n    corpora with very spiky vs. very flat term distributions.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)  # add-one smoothing\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique constraints but keep a sublinear repetition signal.\n        Repetition can indicate emphasis; use qtf^p (p\u22480.5) to avoid verbosity blowups.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 + two bounded priors:\n\n    (1) Coordination (count-based): prefer docs matching more distinct query constraints.\n    (2) IDF-mass coverage: reward capturing a larger fraction of the query's informative mass.\n        This helps when queries mix generic + specific terms (common in BEIR/BRIGHT).\n    (3) Mild length prior: reduces \"long doc wins by chance\" beyond BM25's norm.\n\n    All additions are bounded/mild to preserve recall@100 stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    # IDF-mass coverage (query-side normalizer and doc-side numerator).\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        # In [0,1], but weighted by informativeness rather than raw term count.\n        idf_cov = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p).\n        b = Config.b\n        p_len = Config.dl_p\n        dl = np.power(self.doc_lengths, p_len, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p_len\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # NEW: reward capturing the query's \"informativeness mass\" (IDF-weighted coverage).\n    # Bounded multiplier => tends to help nDCG without destabilizing recall@100.\n    idf_cov_gamma: float = 0.22\n\n    # NEW: extra mild length prior beyond BM25 norm (controls verbosity wins).\n    dl_alpha: float = 0.08\n=======\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Sublinear length normalization (reduces over-penalizing long docs).\n    dl_p: float = 0.75\n\n    # Soft coordination / coverage pressure (bounded multiplicative boost).\n    coord_alpha: float = 0.2\n\n    # Query-side clarity gate: bounded [0,1] suppression for common query terms.\n    # clarity = (idf/(idf+1))^p\n    q_clarity_power: float = 0.6\n\n    # Sublinear query TF weighting to preserve emphasis without verbosity blowups.\n    qtf_power: float = 0.5\n\n    # Reward capturing the query's \"informativeness mass\" (IDF-weighted coverage).\n    idf_cov_gamma: float = 0.22\n\n    # Extra mild length prior beyond BM25 norm (controls verbosity wins).\n    dl_alpha: float = 0.08\n\n    # NEW: per-term evidence concavity: log1p(tf/k) instead of tf/(tf+...).\n    # Rationale: in verbose domains (StackOverflow, long scientific docs), repeated mentions\n    # should add diminishing evidence but not be overly coupled to length norm.\n    tf_log_k: float = 1.0\n\n    # NEW: make coverage prefer satisfying the *rarest* constraints via a soft-max.\n    # This helps \"key term\" queries (BRIGHT theorem/econ) where one rare token is decisive.\n    cov_softmax_temp: float = 1.2\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Clarity-gated BM25 + two bounded priors:\n\n    (1) Coordination (count-based): prefer docs matching more distinct query constraints.\n    (2) IDF-mass coverage: reward capturing a larger fraction of the query's informative mass.\n        This helps when queries mix generic + specific terms (common in BEIR/BRIGHT).\n    (3) Mild length prior: reduces \"long doc wins by chance\" beyond BM25's norm.\n\n    All additions are bounded/mild to preserve recall@100 stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    # IDF-mass coverage (query-side normalizer and doc-side numerator).\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * clarity * term_idf\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wt * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        # In [0,1], but weighted by informativeness rather than raw term count.\n        idf_cov = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    \"Key-constraint\" evidence model (lexical, non-neural):\n\n    - Per-term evidence uses log1p(tf/k): interpretable as diminishing returns of repeated mentions.\n      This is less sensitive to doc length than BM25 tf/(tf+k1*norm), which can over-favor\n      long documents that repeat a term many times.\n    - Coverage is computed twice:\n        (a) IDF-mass fraction (like before) for robustness/recall.\n        (b) softmax-over-weights (\"did we match the rarest constraint?\") for early precision.\n      Both are bounded multipliers (safe for recall@100).\n    - Keep coordination and mild length prior.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Sublinear length normalization (dl^p), must match vectorized path.\n    p_len = Config.dl_p\n    dl = float(doc_length) ** p_len\n    adl = float(avgdl) ** p_len if avgdl > 0 else 1.0\n    norm = 1.0 - b + b * (dl / (adl + eps))\n\n    score = 0.0\n    matched = 0.0\n    uq = 0.0\n\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Softmax coverage stats (temperature controls how \"max-like\" it is).\n    sm_temp = max(Config.cov_softmax_temp, eps)\n    sm_den = 0.0\n    sm_num = 0.0\n\n    for term in query_repr.terms:\n        uq += 1.0\n\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * clarity * term_idf\n\n        cov_den += wt\n\n        # softmax over query weights to emphasize the most informative constraint\n        e = math.exp(min(50.0, wt / sm_temp))\n        sm_den += e\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        cov_num += wt\n        sm_num += e\n\n        # evidence: concave in tf, lightly normalized by document length norm\n        score += wt * math.log1p(tf / (Config.tf_log_k + k1 * norm + eps))\n\n    if score <= 0.0:\n        return 0.0\n\n    if uq > 0.0 and Config.coord_alpha != 0.0:\n        score *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n    # Old coverage: smooth, recall-friendly.\n    if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n        idf_cov = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n    # New coverage: \"did we hit the decisive rare term?\"\n    if sm_den > 0.0:\n        soft_cov = sm_num / (sm_den + eps)  # in [0,1]\n        score *= 1.0 + 0.15 * soft_cov\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return float(score)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n            cov_num += wt * present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wt * tf_part\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        sm_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        sm_den = 0.0\n        sm_temp = max(Config.cov_softmax_temp, eps)\n\n        uq = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = (w * clarity) * idf_val\n            cov_den += wt\n\n            e = float(np.exp(min(50.0, wt / sm_temp)))\n            sm_den += e\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            cov_num += wt * present\n            sm_num += e * present\n\n            # concave tf evidence, lightly length-normalized\n            scores += wt * np.log1p(tf_row / (Config.tf_log_k + k1 * norms + eps))\n\n        if uq > 0.0 and Config.coord_alpha != 0.0:\n            scores *= 1.0 + Config.coord_alpha * (matched / (uq + eps))\n\n        if cov_den > 0.0 and Config.idf_cov_gamma != 0.0:\n            idf_cov = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.idf_cov_gamma * idf_cov\n\n        if sm_den > 0.0:\n            soft_cov = sm_num / (sm_den + eps)\n            scores *= 1.0 + 0.15 * soft_cov\n\n        if Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}