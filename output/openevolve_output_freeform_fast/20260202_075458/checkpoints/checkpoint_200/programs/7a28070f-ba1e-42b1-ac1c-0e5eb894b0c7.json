{"id": "7a28070f-ba1e-42b1-ac1c-0e5eb894b0c7", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    epsilon: float = 1e-9\n\n    # Evidence concavity for TF: log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND on informative mass covered.\n    coverage_gamma: float = 0.25\n\n    # Downweight very common query terms smoothly: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Mild verbosity prior: divide by 1 + alpha*log1p((dl+1)/(avgdl+1))\n    dl_alpha: float = 0.15\n\n    # NEW: bounded \"key-term presence\" multiplier for ultra-rare query terms.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # NEW: cheap secondary lexical channel using fixed-length prefixes (morphology/camelcase tolerance).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: suffix channel (helps morphology like plural/verb forms when prefix collides).\n    suffix_len: int = 5\n    suffix_weight: float = 0.08\n\n    # NEW: query-distribution sharpening for very hard/technical queries.\n    # Interpretable as a Renyi-style \"focus\": emphasize the tail of rare terms.\n    q_focus_power: float = 1.15\n\n    # NEW: co-presence synergy (bounded) to reward matching multiple informative terms together.\n    synergy_lambda: float = 0.12\n    synergy_cap: float = 1.0\n\n    # Compatibility leftovers (Corpus builds norm_array; scorer no longer uses BM25 k1/b)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds/log(N/df) on heterogeneous corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Query weights = (qtf^0.5) * (focus^power), where focus is a query-internal\n        rarity distribution over terms: p(t) \u221d idf(t) (approximated later in scorer).\n\n        We keep qtf sublinear here; the focus power is applied in retrieval_score\n        where IDF is available.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** 0.5 for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    DSD (Distribution-Sharpened Dependence) lexical scoring.\n\n    1) Evidence: concave TF evidence in \"surprisal bits\" (idf) with a bounded\n       clarity gate for very common terms.\n    2) Query focus sharpening: within a query, emphasize the tail of rare terms\n       by using a Renyi-style power transform over the IDF mass.\n    3) Soft coverage: reward covering informative mass (recall-safe).\n    4) Synergy: a bounded co-presence bonus for matching multiple informative terms.\n       This approximates dependence/proximity without positions: relevant docs often\n       contain several query constraints together.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # --- build query-internal focus distribution p(t) \u221d idf(t)*qtf(t) ---\n    q_mass: list[tuple[str, float, float]] = []\n    total_mass = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        m = tidf * wq\n        q_mass.append((term, tidf, wq))\n        total_mass += m\n\n    if total_mass <= 0.0:\n        return 0.0\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n    matched_idf: list[float] = []\n\n    focus_p = float(getattr(Config, \"q_focus_power\", 1.0))\n    for term, tidf, wq in q_mass:\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # focus weight in [0,1], sharper when q_focus_power>1\n        p = (tidf * wq) / (total_mass + eps)\n        focus = p ** focus_p\n\n        wt = wq * tidf * clarity * focus\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n        matched_idf.append(tidf)\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - float(Config.rare_idf_pivot)) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    # bounded dependence/synergy among matched terms\n    lam = float(getattr(Config, \"synergy_lambda\", 0.0))\n    if lam > 0.0 and len(matched_idf) >= 2:\n        # normalized mean harmonic-IDF across pairs; then bounded.\n        s = 0.0\n        pairs = 0.0\n        for i in range(len(matched_idf)):\n            ai = matched_idf[i]\n            for j in range(i + 1, len(matched_idf)):\n                aj = matched_idf[j]\n                s += (ai * aj) / (ai + aj + eps)\n                pairs += 1.0\n        s = s / (pairs + eps)\n        s = min(s, float(getattr(Config, \"synergy_cap\", 1.0)))\n        score *= 1.0 + lam * s\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds optional prefix/suffix channels (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq,\n                corpus.prefix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    if getattr(Config, \"suffix_weight\", 0.0) != 0.0 and getattr(Config, \"suffix_len\", 0) > 0:\n        sfx = max(1, int(Config.suffix_len))\n        stoks = [t[-sfx:] for t in query if len(t) >= sfx]\n        if stoks:\n            sq = QueryRepr.from_tokens([\"S:\" + t for t in stoks])\n            s += float(Config.suffix_weight) * retrieval_score(\n                sq,\n                corpus.suffix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Optional prefix/suffix views for robustness (secondary lexical channels).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        sfx = max(1, int(getattr(Config, \"suffix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        docs_suffix = [[t[-sfx:] for t in doc if len(t) >= sfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n        self.suffix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_suffix]\n\n        # Joint vocabulary over tokens + tagged prefixes/suffixes so rank() can use postings.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, sdoc in zip(documents, docs_prefix, docs_suffix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for s in sdoc:\n                key = \"S:\" + s\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc, sdoc) in enumerate(zip(documents, docs_prefix, docs_suffix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            suf_counts = Counter(\"S:\" + s for s in sdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in suf_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p). (compat only)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # DF exposed for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        # Need total query mass for focus weights: p(t) \u221d idf*qtf\n        idf_q = []\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                idf_q.append(0.0)\n                continue\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            idf_q.append(idf_val * wq)\n        total_mass = float(np.sum(idf_q))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        focus_p = float(getattr(Config, \"q_focus_power\", 1.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            p = (idf_val * wq) / (total_mass + eps)\n            focus = p ** focus_p\n\n            wt = wq * idf_val * clarity * focus\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            matched_idf_sum += present * idf_val\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        lam = float(getattr(Config, \"synergy_lambda\", 0.0))\n        if lam > 0.0:\n            # Approximate pairwise synergy by a bounded function of (matched_count, matched_idf_sum).\n            # For fixed matched_count, higher matched_idf_sum => more informative conjunction.\n            denom = np.maximum(matched, 1.0)\n            avg_idf = matched_idf_sum / (denom + eps)\n            s = np.minimum(avg_idf / (avg_idf + 3.0), float(getattr(Config, \"synergy_cap\", 1.0)))\n            scores *= 1.0 + lam * s * (matched >= 2.0)\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** 0.5)\n\n        # Add prefix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** 0.5))\n\n        # Add suffix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"suffix_weight\", 0.0) != 0.0 and getattr(Config, \"suffix_len\", 0) > 0:\n            sfx = max(1, int(Config.suffix_len))\n            scounts = Counter(t[-sfx:] for t in query if len(t) >= sfx)\n            for s, c in scounts.items():\n                tid = self.corpus.get_term_id(\"S:\" + s)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.suffix_weight) * (float(c) ** 0.5))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "97d0d3ad-71ee-4622-bc4c-fd0a683ec32c", "generation": 6, "timestamp": 1770198148.7600071, "iteration_found": 189, "metrics": {"bright_pony_ndcg@10": 0.048382570988594366, "bright_pony_recall@100": 0.236551837871305, "bright_pony_index_time_ms": 8555.500745773315, "bright_pony_query_time_ms": 1486.934487707913, "beir_nfcorpus_ndcg@10": 0.32362340610608314, "beir_nfcorpus_recall@100": 0.2563763478000774, "beir_nfcorpus_index_time_ms": 12634.601179044694, "beir_nfcorpus_query_time_ms": 370.68197317421436, "beir_scifact_ndcg@10": 0.647458625107821, "beir_scifact_recall@100": 0.9266666666666666, "beir_scifact_index_time_ms": 24322.157921735197, "beir_scifact_query_time_ms": 1352.6043631136417, "beir_arguana_ndcg@10": 0.2682458171969832, "beir_arguana_recall@100": 0.9350463954318344, "beir_arguana_index_time_ms": 31656.628659926355, "beir_arguana_query_time_ms": 72665.114056319, "bright_theoremqa_theorems_ndcg@10": 0.02333780507225129, "bright_theoremqa_theorems_recall@100": 0.12719298245614033, "bright_theoremqa_theorems_index_time_ms": 61921.157710719854, "bright_theoremqa_theorems_query_time_ms": 2214.2572989687324, "beir_scidocs_ndcg@10": 0.1340916292539385, "beir_scidocs_recall@100": 0.33923333333333333, "beir_scidocs_index_time_ms": 105048.33871219307, "beir_scidocs_query_time_ms": 10484.105770941824, "bright_economics_ndcg@10": 0.1285261020070394, "bright_economics_recall@100": 0.3884295628013504, "bright_economics_index_time_ms": 67647.44512131438, "bright_economics_query_time_ms": 6539.121313020587, "bright_biology_ndcg@10": 0.26141250483607964, "bright_biology_recall@100": 0.5731632416864916, "bright_biology_index_time_ms": 73087.89737895131, "bright_biology_query_time_ms": 5753.946594893932, "bright_earth_science_ndcg@10": 0.258753019135466, "bright_earth_science_recall@100": 0.6737170610696019, "bright_earth_science_index_time_ms": 142708.75338511541, "bright_earth_science_query_time_ms": 10637.531018350273, "beir_fiqa_ndcg@10": 0.1987207165386581, "beir_fiqa_recall@100": 0.50582518476037, "beir_fiqa_index_time_ms": 156057.72835714743, "beir_fiqa_query_time_ms": 12609.656572341919, "bright_stackoverflow_ndcg@10": 0.17919845512835794, "bright_stackoverflow_recall@100": 0.4866380537278774, "bright_stackoverflow_index_time_ms": 489053.3506530337, "bright_stackoverflow_query_time_ms": 23452.961722854525, "beir_trec-covid_ndcg@10": 0.5652648417080123, "beir_trec-covid_recall@100": 0.10336354841180889, "beir_trec-covid_index_time_ms": 654383.5294446908, "beir_trec-covid_query_time_ms": 3592.633679974824, "avg_ndcg@10": 0.25308462442327373, "avg_recall@100": 0.46268368466807147, "combined_score": 0.42076387261911197, "total_index_time_ms": 1827077.0892696455, "total_query_time_ms": 151159.54885166138, "total_time_ms": 1978236.638121307, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 29 lines with 39 lines\nChange 2: Replace 14 lines with 20 lines\nChange 3: Replace 63 lines with 101 lines\nChange 4: Replace 27 lines with 41 lines\nChange 5: Replace 27 lines with 34 lines\nChange 6: Replace 20 lines with 29 lines\nChange 7: Replace 46 lines with 56 lines\nChange 8: Replace 50 lines with 82 lines", "parent_metrics": {"bright_pony_ndcg@10": 0.10773673587782054, "bright_pony_recall@100": 0.2916632677512969, "bright_pony_index_time_ms": 7254.613634198904, "bright_pony_query_time_ms": 1114.7042838856578, "beir_scifact_ndcg@10": 0.6839828370855742, "beir_scifact_recall@100": 0.9186666666666667, "beir_scifact_index_time_ms": 16083.95127626136, "beir_scifact_query_time_ms": 1044.1877767443657, "beir_nfcorpus_ndcg@10": 0.33359584446534, "beir_nfcorpus_recall@100": 0.25525124288782763, "beir_nfcorpus_index_time_ms": 18647.507401183248, "beir_nfcorpus_query_time_ms": 282.55260130390525, "beir_arguana_ndcg@10": 0.3001861728275296, "beir_arguana_recall@100": 0.9336188436830836, "beir_arguana_index_time_ms": 29422.13106993586, "beir_arguana_query_time_ms": 52884.361307602376, "bright_theoremqa_theorems_ndcg@10": 0.02836005899802385, "bright_theoremqa_theorems_recall@100": 0.19298245614035087, "bright_theoremqa_theorems_index_time_ms": 56225.96116969362, "bright_theoremqa_theorems_query_time_ms": 1669.3961517885327, "beir_scidocs_ndcg@10": 0.15192411034368308, "beir_scidocs_recall@100": 0.3563333333333334, "beir_scidocs_index_time_ms": 99329.90624196827, "beir_scidocs_query_time_ms": 7453.50049296394, "bright_economics_ndcg@10": 0.13936928483191932, "bright_economics_recall@100": 0.3854787115573041, "bright_economics_index_time_ms": 60955.844203941524, "bright_economics_query_time_ms": 4414.490792900324, "bright_biology_ndcg@10": 0.2601951961750609, "bright_biology_recall@100": 0.5494080608541518, "bright_biology_index_time_ms": 70358.68610395119, "bright_biology_query_time_ms": 4122.265390120447, "bright_earth_science_ndcg@10": 0.28086020593581806, "bright_earth_science_recall@100": 0.6551337526156038, "bright_earth_science_index_time_ms": 136285.4552171193, "bright_earth_science_query_time_ms": 7150.6423726677895, "beir_fiqa_ndcg@10": 0.23531521253522653, "beir_fiqa_recall@100": 0.530267207929245, "beir_fiqa_index_time_ms": 152628.52987786755, "beir_fiqa_query_time_ms": 9177.38767620176, "bright_stackoverflow_ndcg@10": 0.19601844795117987, "bright_stackoverflow_recall@100": 0.5223592467673988, "bright_stackoverflow_index_time_ms": 485616.1803607829, "bright_stackoverflow_query_time_ms": 15339.77712597698, "beir_trec-covid_ndcg@10": 0.7080270535503809, "beir_trec-covid_recall@100": 0.12087060373030202, "beir_trec-covid_index_time_ms": 610531.0241049156, "beir_trec-covid_query_time_ms": 2647.6763216778636, "avg_ndcg@10": 0.28546426338146313, "avg_recall@100": 0.47600278282638037, "combined_score": 0.437895078937397, "total_index_time_ms": 1743339.7906618193, "total_query_time_ms": 107300.94229383394, "total_time_ms": 1850640.7329556532, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4379\n- Feature coordinates: \n- Focus areas: - Fitness unchanged at 0.4379\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace 22 lines with 31 lines\nChange 2: Replace 63 lines with 76 lines\nChange 3: Replace 10 lines with 25 lines\nChange 4: Replace 48 lines with 70 lines\nChange 5: Replace 48 lines with 57 lines\nChange 6: Replace 11 lines with 24 lines\n- Metrics: bright_pony_ndcg@10: 0.1077, bright_pony_recall@100: 0.2917, bright_pony_index_time_ms: 5743.7935, bright_pony_query_time_ms: 1177.8861, beir_nfcorpus_ndcg@10: 0.3336, beir_nfcorpus_recall@100: 0.2553, beir_nfcorpus_index_time_ms: 18238.8063, beir_nfcorpus_query_time_ms: 273.0773, beir_scifact_ndcg@10: 0.6840, beir_scifact_recall@100: 0.9187, beir_scifact_index_time_ms: 23127.5693, beir_scifact_query_time_ms: 929.8062, beir_arguana_ndcg@10: 0.3002, beir_arguana_recall@100: 0.9336, beir_arguana_index_time_ms: 22715.9847, beir_arguana_query_time_ms: 52649.2999, bright_theoremqa_theorems_ndcg@10: 0.0284, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 53907.1202, bright_theoremqa_theorems_query_time_ms: 1649.4710, beir_scidocs_ndcg@10: 0.1519, beir_scidocs_recall@100: 0.3563, beir_scidocs_index_time_ms: 102218.6445, beir_scidocs_query_time_ms: 7775.2492, bright_economics_ndcg@10: 0.1394, bright_economics_recall@100: 0.3855, bright_economics_index_time_ms: 59084.4968, bright_economics_query_time_ms: 4623.6243, bright_biology_ndcg@10: 0.2602, bright_biology_recall@100: 0.5494, bright_biology_index_time_ms: 66528.8805, bright_biology_query_time_ms: 4307.9212, beir_fiqa_ndcg@10: 0.2353, beir_fiqa_recall@100: 0.5303, beir_fiqa_index_time_ms: 133642.7703, beir_fiqa_query_time_ms: 9591.4837, bright_earth_science_ndcg@10: 0.2809, bright_earth_science_recall@100: 0.6551, bright_earth_science_index_time_ms: 141400.2445, bright_earth_science_query_time_ms: 7493.0984, bright_stackoverflow_ndcg@10: 0.1960, bright_stackoverflow_recall@100: 0.5224, bright_stackoverflow_index_time_ms: 447371.5814, bright_stackoverflow_query_time_ms: 15299.5478, beir_trec-covid_ndcg@10: 0.7080, beir_trec-covid_recall@100: 0.1209, beir_trec-covid_index_time_ms: 627042.9537, beir_trec-covid_query_time_ms: 2663.7730, avg_ndcg@10: 0.2855, avg_recall@100: 0.4760, combined_score: 0.4379, total_index_time_ms: 1701022.8456, total_query_time_ms: 108434.2382, total_time_ms: 1809457.0838, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace 24 lines with 33 lines\nChange 2: Replace 24 lines with 29 lines\nChange 3: Replace 6 lines with 9 lines\nChange 4: Replace 10 lines with 25 lines\nChange 5: Replace 17 lines with 27 lines\nChange 6: Replace 10 lines with 20 lines\nChange 7: Replace 12 lines with 23 lines\nChange 8: Replace 28 lines with 35 lines\n- Metrics: bright_pony_ndcg@10: 0.1077, bright_pony_recall@100: 0.2917, bright_pony_index_time_ms: 7108.1746, bright_pony_query_time_ms: 1102.5165, beir_nfcorpus_ndcg@10: 0.3336, beir_nfcorpus_recall@100: 0.2553, beir_nfcorpus_index_time_ms: 18339.3556, beir_nfcorpus_query_time_ms: 270.4392, beir_scifact_ndcg@10: 0.6840, beir_scifact_recall@100: 0.9187, beir_scifact_index_time_ms: 24760.0964, beir_scifact_query_time_ms: 926.0353, beir_arguana_ndcg@10: 0.3002, beir_arguana_recall@100: 0.9336, beir_arguana_index_time_ms: 24743.2577, beir_arguana_query_time_ms: 50175.8848, bright_theoremqa_theorems_ndcg@10: 0.0284, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 57851.6889, bright_theoremqa_theorems_query_time_ms: 1622.4178, beir_scidocs_ndcg@10: 0.1519, beir_scidocs_recall@100: 0.3563, beir_scidocs_index_time_ms: 102970.3873, beir_scidocs_query_time_ms: 7676.9358, bright_economics_ndcg@10: 0.1394, bright_economics_recall@100: 0.3855, bright_economics_index_time_ms: 36856.4345, bright_economics_query_time_ms: 4671.1143, bright_biology_ndcg@10: 0.2602, bright_biology_recall@100: 0.5494, bright_biology_index_time_ms: 49115.5319, bright_biology_query_time_ms: 4710.2418, bright_earth_science_ndcg@10: 0.2809, bright_earth_science_recall@100: 0.6551, bright_earth_science_index_time_ms: 114998.8974, bright_earth_science_query_time_ms: 7237.1024, beir_fiqa_ndcg@10: 0.2353, beir_fiqa_recall@100: 0.5303, beir_fiqa_index_time_ms: 128040.1438, beir_fiqa_query_time_ms: 9313.4119, bright_stackoverflow_ndcg@10: 0.1960, bright_stackoverflow_recall@100: 0.5224, bright_stackoverflow_index_time_ms: 459001.7747, bright_stackoverflow_query_time_ms: 14933.3317, beir_trec-covid_ndcg@10: 0.7080, beir_trec-covid_recall@100: 0.1209, beir_trec-covid_index_time_ms: 510714.7687, beir_trec-covid_query_time_ms: 2569.3354, avg_ndcg@10: 0.2855, avg_recall@100: 0.4760, combined_score: 0.4379, total_index_time_ms: 1534500.5116, total_query_time_ms: 105208.7669, total_time_ms: 1639709.2785, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 23 lines with 24 lines\nChange 2: Replace 48 lines with 71 lines\nChange 3: Replace 58 lines with 66 lines\nChange 4: Replace 10 lines with 25 lines\nChange 5: Replace 30 lines with 46 lines\nChange 6: Replace 43 lines with 50 lines\n- Metrics: bright_pony_ndcg@10: 0.1077, bright_pony_recall@100: 0.2917, bright_pony_index_time_ms: 7580.6382, bright_pony_query_time_ms: 1168.9065, beir_scifact_ndcg@10: 0.6840, beir_scifact_recall@100: 0.9187, beir_scifact_index_time_ms: 15867.4661, beir_scifact_query_time_ms: 987.1224, beir_nfcorpus_ndcg@10: 0.3336, beir_nfcorpus_recall@100: 0.2553, beir_nfcorpus_index_time_ms: 17828.3069, beir_nfcorpus_query_time_ms: 265.4496, beir_arguana_ndcg@10: 0.3002, beir_arguana_recall@100: 0.9336, beir_arguana_index_time_ms: 30993.8117, beir_arguana_query_time_ms: 50963.0279, bright_theoremqa_theorems_ndcg@10: 0.0284, bright_theoremqa_theorems_recall@100: 0.1930, bright_theoremqa_theorems_index_time_ms: 46735.0437, bright_theoremqa_theorems_query_time_ms: 1974.0632, beir_scidocs_ndcg@10: 0.1519, beir_scidocs_recall@100: 0.3563, beir_scidocs_index_time_ms: 88045.0514, beir_scidocs_query_time_ms: 7888.1449, bright_biology_ndcg@10: 0.2602, bright_biology_recall@100: 0.5494, bright_biology_index_time_ms: 54529.2363, bright_biology_query_time_ms: 4175.3844, bright_economics_ndcg@10: 0.1394, bright_economics_recall@100: 0.3855, bright_economics_index_time_ms: 55450.8391, bright_economics_query_time_ms: 4777.5835, bright_earth_science_ndcg@10: 0.2809, bright_earth_science_recall@100: 0.6551, bright_earth_science_index_time_ms: 119828.3776, bright_earth_science_query_time_ms: 7048.1918, beir_fiqa_ndcg@10: 0.2353, beir_fiqa_recall@100: 0.5303, beir_fiqa_index_time_ms: 146174.3209, beir_fiqa_query_time_ms: 9838.0823, bright_stackoverflow_ndcg@10: 0.1960, bright_stackoverflow_recall@100: 0.5224, bright_stackoverflow_index_time_ms: 479170.7615, bright_stackoverflow_query_time_ms: 15830.7316, beir_trec-covid_ndcg@10: 0.7080, beir_trec-covid_recall@100: 0.1209, beir_trec-covid_index_time_ms: 618749.9685, beir_trec-covid_query_time_ms: 2712.4938, avg_ndcg@10: 0.2855, avg_recall@100: 0.4760, combined_score: 0.4379, total_index_time_ms: 1680953.8220, total_query_time_ms: 107629.1821, total_time_ms: 1788583.0040, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + IDF-weighted coverage + bounded query-clarity gating.\n    epsilon: float = 1e-9\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Evidence: wt * log1p(tf/base), then log1p(total evidence).\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the *informative* query mass.\n    coverage_gamma: float = 0.25\n\n    # Query-side clarity gate: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting: count**p (keeps emphasis w/o verbosity blowups)\n    qtf_power: float = 0.5\n\n    # Mild length prior: downweight extremely long docs gently (helps precision w/o killing recall).\n    dl_alpha: float = 0.15\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage.\n\n    Evidence:\n      sum_t [ wq(t) * idf(t) * clarity(t) * log1p(tf/base) ]\n      then compress with log1p(total) to get diminishing returns and encourage coverage.\n\n    Coverage prior:\n      multiply by 1 + gamma * (covered_idf_mass / total_idf_mass)\n      (soft-AND over *informative* constraints).\n\n    This removes pairwise synergy/common-damp which tended to be brittle cross-dataset.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        # bounded clarity gate in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # Presence-gated rare-key evidence (bounded): helpful for identifiers/symbols.\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 0.0):\n            rare_hits += (term_idf - float(Config.rare_idf_pivot)) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    # mild verbosity prior\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view for robust lexical matching (secondary channel).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only (not used by current scorer): standard BM25-style length norm.\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 0.0):\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1077), Performs well on bright_pony_recall@100 (0.2917), Performs well on bright_pony_index_time_ms (7555.1467), Performs well on bright_pony_query_time_ms (1137.3704), Performs well on beir_nfcorpus_ndcg@10 (0.3336), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (17450.9895), Performs well on beir_nfcorpus_query_time_ms (277.8385), Performs well on beir_scifact_ndcg@10 (0.6840), Performs well on beir_scifact_recall@100 (0.9187), Performs well on beir_scifact_index_time_ms (24339.7861), Performs well on beir_scifact_query_time_ms (930.2466), Performs well on beir_arguana_ndcg@10 (0.3002), Performs well on beir_arguana_recall@100 (0.9336), Performs well on beir_arguana_index_time_ms (30392.3920), Performs well on beir_arguana_query_time_ms (52064.5465), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0284), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (55095.1949), Performs well on bright_theoremqa_theorems_query_time_ms (1639.9845), Performs well on beir_scidocs_ndcg@10 (0.1519), Performs well on beir_scidocs_recall@100 (0.3563), Performs well on beir_scidocs_index_time_ms (99729.6748), Performs well on beir_scidocs_query_time_ms (7263.2105), Performs well on bright_economics_ndcg@10 (0.1394), Performs well on bright_economics_recall@100 (0.3855), Performs well on bright_economics_index_time_ms (55509.1828), Performs well on bright_economics_query_time_ms (4439.6979), Performs well on bright_biology_ndcg@10 (0.2602), Performs well on bright_biology_recall@100 (0.5494), Performs well on bright_biology_index_time_ms (70518.6986), Performs well on bright_biology_query_time_ms (4190.5752), Performs well on bright_earth_science_ndcg@10 (0.2809), Performs well on bright_earth_science_recall@100 (0.6551), Performs well on bright_earth_science_index_time_ms (133077.5971), Performs well on bright_earth_science_query_time_ms (7543.9347), Performs well on beir_fiqa_ndcg@10 (0.2353), Performs well on beir_fiqa_recall@100 (0.5303), Performs well on beir_fiqa_index_time_ms (153522.3755), Performs well on beir_fiqa_query_time_ms (9892.9281), Performs well on bright_stackoverflow_ndcg@10 (0.1960), Performs well on bright_stackoverflow_recall@100 (0.5224), Performs well on bright_stackoverflow_index_time_ms (482095.8043), Performs well on bright_stackoverflow_query_time_ms (16319.4399), Performs well on beir_trec-covid_ndcg@10 (0.7080), Performs well on beir_trec-covid_recall@100 (0.1209), Performs well on beir_trec-covid_index_time_ms (626876.5627), Performs well on beir_trec-covid_query_time_ms (2594.5513), Performs well on avg_ndcg@10 (0.2855), Performs well on avg_recall@100 (0.4760), Performs well on combined_score (0.4379), Performs well on total_index_time_ms (1756163.4049), Performs well on total_query_time_ms (108294.3241), Performs well on total_time_ms (1864457.7290), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep base method but add two *cheap* lexical robustness signals seen to help\n    # across BEIR+BRIGHT mixes:\n    #  (1) prefix channel (morphology/camelcase tolerance)\n    #  (2) bounded rare-key presence boost (identifier/symbol queries)\n    epsilon: float = 1e-9\n\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    qtf_power: float = 0.5\n    dl_alpha: float = 0.15\n\n    # Secondary lexical channel: match fixed-length prefixes as extra pseudo-terms.\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Rare-key presence multiplier (indicator-like; bounded).\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one style smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique constraints + sublinear repetition (keeps emphasis without verbosity blowups).\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage,\n    plus a bounded *rare-key presence* multiplier.\n\n    Rare-key presence is indicator-like: a single hit on a very high-IDF query token\n    (identifiers, theorem names, gene symbols) should be disproportionately informative,\n    but the boost must be bounded for cross-domain stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 1e9):\n            # bounded in (0,1) per term; aggregated then log1p\n            rare_hits += (term_idf - Config.rare_idf_pivot) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a small prefix view (still purely lexical) and store per-doc prefix TFs\n        # for score() path. Prefix terms are tagged (\"P:\") so they don't collide.\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary: tokens + tagged prefixes so rank() can use postings for both.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # DF exposed for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 1e9):\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Combined query: tokens + (optional) tagged prefixes as extra pseudo-terms.\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1077), Performs well on bright_pony_recall@100 (0.2917), Performs well on bright_pony_index_time_ms (7580.6382), Performs well on bright_pony_query_time_ms (1168.9065), Performs well on beir_scifact_ndcg@10 (0.6840), Performs well on beir_scifact_recall@100 (0.9187), Performs well on beir_scifact_index_time_ms (15867.4661), Performs well on beir_scifact_query_time_ms (987.1224), Performs well on beir_nfcorpus_ndcg@10 (0.3336), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (17828.3069), Performs well on beir_nfcorpus_query_time_ms (265.4496), Performs well on beir_arguana_ndcg@10 (0.3002), Performs well on beir_arguana_recall@100 (0.9336), Performs well on beir_arguana_index_time_ms (30993.8117), Performs well on beir_arguana_query_time_ms (50963.0279), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0284), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (46735.0437), Performs well on bright_theoremqa_theorems_query_time_ms (1974.0632), Performs well on beir_scidocs_ndcg@10 (0.1519), Performs well on beir_scidocs_recall@100 (0.3563), Performs well on beir_scidocs_index_time_ms (88045.0514), Performs well on beir_scidocs_query_time_ms (7888.1449), Performs well on bright_biology_ndcg@10 (0.2602), Performs well on bright_biology_recall@100 (0.5494), Performs well on bright_biology_index_time_ms (54529.2363), Performs well on bright_biology_query_time_ms (4175.3844), Performs well on bright_economics_ndcg@10 (0.1394), Performs well on bright_economics_recall@100 (0.3855), Performs well on bright_economics_index_time_ms (55450.8391), Performs well on bright_economics_query_time_ms (4777.5835), Performs well on bright_earth_science_ndcg@10 (0.2809), Performs well on bright_earth_science_recall@100 (0.6551), Performs well on bright_earth_science_index_time_ms (119828.3776), Performs well on bright_earth_science_query_time_ms (7048.1918), Performs well on beir_fiqa_ndcg@10 (0.2353), Performs well on beir_fiqa_recall@100 (0.5303), Performs well on beir_fiqa_index_time_ms (146174.3209), Performs well on beir_fiqa_query_time_ms (9838.0823), Performs well on bright_stackoverflow_ndcg@10 (0.1960), Performs well on bright_stackoverflow_recall@100 (0.5224), Performs well on bright_stackoverflow_index_time_ms (479170.7615), Performs well on bright_stackoverflow_query_time_ms (15830.7316), Performs well on beir_trec-covid_ndcg@10 (0.7080), Performs well on beir_trec-covid_recall@100 (0.1209), Performs well on beir_trec-covid_index_time_ms (618749.9685), Performs well on beir_trec-covid_query_time_ms (2712.4938), Performs well on avg_ndcg@10 (0.2855), Performs well on avg_recall@100 (0.4760), Performs well on combined_score (0.4379), Performs well on total_index_time_ms (1680953.8220), Performs well on total_query_time_ms (107629.1821), Performs well on total_time_ms (1788583.0040), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Evidence should be \"bits-like\" and stable cross-domain; we will use concave TF (log1p) rather\n    # than BM25 saturation to reduce over-rewarding boilerplate repetition.\n    epsilon: float = 1e-9\n\n    # Term utility concavity: log1p(tf / base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND over *informative query mass covered* (bounded multiplicative prior).\n    coverage_gamma: float = 0.25\n\n    # Query term repetition is sublinear: count**p (prevents verbose queries from dominating).\n    qtf_power: float = 0.5\n\n    # Downweight extremely common query terms smoothly:\n    # clarity(t) = (idf/(idf+1))**p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Mild verbosity prior (gentle; helps precision without killing recall@100).\n    dl_alpha: float = 0.15\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    # Intuition: one hit on an ultra-rare query token (symbol/identifier) is very informative.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Keep for compatibility with Corpus.norm_array construction (not used by scorer).\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Compared to log1p(N/df), this is more stable for very common terms and avoids\n    over-spiking ultra-rare terms across heterogeneous corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique lexical constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage + mild verbosity prior.\n\n    Evidence:\n      sum_t [ wq(t) * idf(t) * clarity(t) * log1p(tf/base) ], then log1p(total).\n    Coverage prior (soft-AND over informative constraints):\n      score *= 1 + gamma * (covered_mass / total_mass)\n    Length prior:\n      score /= 1 + alpha*log1p((dl+1)/(avgdl+1))\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        # bounded clarity in [0,1] to suppress very common query tokens without hard stopwording\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # Presence-gated rare-key evidence (bounded): helpful for identifiers/symbols.\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only: current scorer uses its own mild verbosity prior.\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n        # Match QueryRepr: unique constraints + sublinear qtf weights.\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel)\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1077), Performs well on bright_pony_recall@100 (0.2917), Performs well on bright_pony_index_time_ms (7108.1746), Performs well on bright_pony_query_time_ms (1102.5165), Performs well on beir_nfcorpus_ndcg@10 (0.3336), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (18339.3556), Performs well on beir_nfcorpus_query_time_ms (270.4392), Performs well on beir_scifact_ndcg@10 (0.6840), Performs well on beir_scifact_recall@100 (0.9187), Performs well on beir_scifact_index_time_ms (24760.0964), Performs well on beir_scifact_query_time_ms (926.0353), Performs well on beir_arguana_ndcg@10 (0.3002), Performs well on beir_arguana_recall@100 (0.9336), Performs well on beir_arguana_index_time_ms (24743.2577), Performs well on beir_arguana_query_time_ms (50175.8848), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0284), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (57851.6889), Performs well on bright_theoremqa_theorems_query_time_ms (1622.4178), Performs well on beir_scidocs_ndcg@10 (0.1519), Performs well on beir_scidocs_recall@100 (0.3563), Performs well on beir_scidocs_index_time_ms (102970.3873), Performs well on beir_scidocs_query_time_ms (7676.9358), Performs well on bright_economics_ndcg@10 (0.1394), Performs well on bright_economics_recall@100 (0.3855), Performs well on bright_economics_index_time_ms (36856.4345), Performs well on bright_economics_query_time_ms (4671.1143), Performs well on bright_biology_ndcg@10 (0.2602), Performs well on bright_biology_recall@100 (0.5494), Performs well on bright_biology_index_time_ms (49115.5319), Performs well on bright_biology_query_time_ms (4710.2418), Performs well on bright_earth_science_ndcg@10 (0.2809), Performs well on bright_earth_science_recall@100 (0.6551), Performs well on bright_earth_science_index_time_ms (114998.8974), Performs well on bright_earth_science_query_time_ms (7237.1024), Performs well on beir_fiqa_ndcg@10 (0.2353), Performs well on beir_fiqa_recall@100 (0.5303), Performs well on beir_fiqa_index_time_ms (128040.1438), Performs well on beir_fiqa_query_time_ms (9313.4119), Performs well on bright_stackoverflow_ndcg@10 (0.1960), Performs well on bright_stackoverflow_recall@100 (0.5224), Performs well on bright_stackoverflow_index_time_ms (459001.7747), Performs well on bright_stackoverflow_query_time_ms (14933.3317), Performs well on beir_trec-covid_ndcg@10 (0.7080), Performs well on beir_trec-covid_recall@100 (0.1209), Performs well on beir_trec-covid_index_time_ms (510714.7687), Performs well on beir_trec-covid_query_time_ms (2569.3354), Performs well on avg_ndcg@10 (0.2855), Performs well on avg_recall@100 (0.4760), Performs well on combined_score (0.4379), Performs well on total_index_time_ms (1534500.5116), Performs well on total_query_time_ms (105208.7669), Performs well on total_time_ms (1639709.2785), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep BM25 knobs for candidate scoring / normalization compatibility.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence is concave in tf; using log evidence is often more robust cross-domain than BM25 tf/(tf+k).\n    tf_log_base: float = 1.0\n\n    # Soft-AND over informative query mass (bounded; recall-safe).\n    coverage_gamma: float = 0.25\n\n    # Gentle query-side clarity gate: suppress extremely common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting (keeps emphasis without verbosity blowups).\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior (stable across corpora).\n    dl_alpha: float = 0.15\n\n    # Avoid over-compressing IDF; concavity is handled via log-evidence and bounded boosts.\n    idf_power: float = 1.0\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Empirically more stable cross-domain than odds/log1p(N/df), and works well with\n    coverage-style soft-AND because it behaves like \"bits of evidence\".\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique lexical constraints + sublinear repetition weights.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + mild verbosity prior\n    + bounded rare-key presence multiplier.\n\n    Evidence view:\n      sum_t w(t) * log1p(tf/base)  then compress as log1p(total_evidence).\n    This behaves like accumulating diminishing \"bits\" of lexical evidence and\n    tends to improve early precision while staying recall-safe.\n\n    Coverage:\n      multiply by 1 + gamma * (covered informative mass / total informative mass).\n\n    Rare-key presence:\n      if an ultra-rare query token appears at least once, apply a small bounded boost.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        if tidf <= 0.0:\n            continue\n\n        # bounded clarity in [0,1]\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # indicator-like rare-key presence\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        if self.corpus.avgdl > 0.0 and Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel)\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1077), Performs well on bright_pony_recall@100 (0.2917), Performs well on bright_pony_index_time_ms (5743.7935), Performs well on bright_pony_query_time_ms (1177.8861), Performs well on beir_nfcorpus_ndcg@10 (0.3336), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (18238.8063), Performs well on beir_nfcorpus_query_time_ms (273.0773), Performs well on beir_scifact_ndcg@10 (0.6840), Performs well on beir_scifact_recall@100 (0.9187), Performs well on beir_scifact_index_time_ms (23127.5693), Performs well on beir_scifact_query_time_ms (929.8062), Performs well on beir_arguana_ndcg@10 (0.3002), Performs well on beir_arguana_recall@100 (0.9336), Performs well on beir_arguana_index_time_ms (22715.9847), Performs well on beir_arguana_query_time_ms (52649.2999), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0284), Performs well on bright_theoremqa_theorems_recall@100 (0.1930), Performs well on bright_theoremqa_theorems_index_time_ms (53907.1202), Performs well on bright_theoremqa_theorems_query_time_ms (1649.4710), Performs well on beir_scidocs_ndcg@10 (0.1519), Performs well on beir_scidocs_recall@100 (0.3563), Performs well on beir_scidocs_index_time_ms (102218.6445), Performs well on beir_scidocs_query_time_ms (7775.2492), Performs well on bright_economics_ndcg@10 (0.1394), Performs well on bright_economics_recall@100 (0.3855), Performs well on bright_economics_index_time_ms (59084.4968), Performs well on bright_economics_query_time_ms (4623.6243), Performs well on bright_biology_ndcg@10 (0.2602), Performs well on bright_biology_recall@100 (0.5494), Performs well on bright_biology_index_time_ms (66528.8805), Performs well on bright_biology_query_time_ms (4307.9212), Performs well on beir_fiqa_ndcg@10 (0.2353), Performs well on beir_fiqa_recall@100 (0.5303), Performs well on beir_fiqa_index_time_ms (133642.7703), Performs well on beir_fiqa_query_time_ms (9591.4837), Performs well on bright_earth_science_ndcg@10 (0.2809), Performs well on bright_earth_science_recall@100 (0.6551), Performs well on bright_earth_science_index_time_ms (141400.2445), Performs well on bright_earth_science_query_time_ms (7493.0984), Performs well on bright_stackoverflow_ndcg@10 (0.1960), Performs well on bright_stackoverflow_recall@100 (0.5224), Performs well on bright_stackoverflow_index_time_ms (447371.5814), Performs well on bright_stackoverflow_query_time_ms (15299.5478), Performs well on beir_trec-covid_ndcg@10 (0.7080), Performs well on beir_trec-covid_recall@100 (0.1209), Performs well on beir_trec-covid_index_time_ms (627042.9537), Performs well on beir_trec-covid_query_time_ms (2663.7730), Performs well on avg_ndcg@10 (0.2855), Performs well on avg_recall@100 (0.4760), Performs well on combined_score (0.4379), Performs well on total_index_time_ms (1701022.8456), Performs well on total_query_time_ms (108434.2382), Performs well on total_time_ms (1809457.0838), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core: concave evidence + informative coverage + mild verbosity prior.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Query repetition: keep emphasis but avoid verbose blow-ups (used in QueryRepr + rank()).\n    qtf_power: float = 0.5\n\n    # Cheap robustness channel: prefix pseudo-terms (helps StackOverflow/TheoremQA/BRIGHT).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Rare-key presence boost (bounded): a single hit on ultra-rare identifiers is decisive.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Compatibility leftovers (Corpus builds norm_array; scorer no longer uses BM25 k1/b)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds/log(N/df) on heterogeneous corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique constraints + sublinear repetition: keeps emphasis without verbosity blowups.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage\n    + bounded rare-key presence multiplier.\n\n    Rare-key presence is indicator-like: for identifier-heavy queries, tf magnitude\n    matters less than *appearing at all*; boost is bounded for stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and term_idf > Config.rare_idf_pivot:\n            rare_hits += (term_idf - Config.rare_idf_pivot) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view for robust lexical matching (secondary channel).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p). (compat only)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel).\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D2 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + informative coverage, plus two tiny recall-friendly robustness signals:\n    #  (1) prefix pseudo-terms (morphology/camelcase tolerance)\n    #  (2) bounded rare-key presence boost (identifiers/symbols)\n    epsilon: float = 1e-9\n\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    q_clarity_power: float = 0.6\n    qtf_power: float = 0.5\n    dl_alpha: float = 0.15\n\n    # Secondary lexical channel: fixed-length prefixes (tagged as \"P:xxxxx\" to avoid collisions).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Rare-key presence multiplier (indicator-like, bounded).\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Compatibility knobs (Corpus builds norm_array); not used by this scorer.\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n    and_power: float = 0.0\n    coord_alpha: float = 0.0\n    focus_lambda: float = 0.0\n    focus_idf_pivot: float = 0.0\n    idf_power: float = 1.0\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Interprets df/N as an empirical occurrence probability and turns a match into\n    \"surprisal\" evidence. This is typically more stable cross-domain than BM25-odds\n    or log1p(N/df) on heterogeneous corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique constraints + sublinear repetition: keeps emphasis without verbosity blowups.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + mild verbosity prior,\n    plus a bounded rare-key *presence* multiplier.\n\n    The presence multiplier helps identifier-like queries (StackOverflow, TheoremQA, BRIGHT):\n    one match on a very high-IDF token is disproportionately informative, but we keep it bounded.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += Config.prefix_weight * retrieval_score(\n                pq,\n                corpus.prefix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so rank() can use postings.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # DF exposed for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel).\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D3 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Evolved core: \"bits-like\" concave evidence + informative coverage + mild verbosity prior.\n    # Keep k1/b only for backwards compatibility with Corpus.norm_array; scorer doesn't rely on it.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence concavity: log1p(tf / base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the informative query mass (bounded; recall-safe)\n    coverage_gamma: float = 0.25\n\n    # Gentle query-side clarity gate: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior\n    dl_alpha: float = 0.15\n\n    # Secondary lexical channel: short prefixes (robust to morphology/identifiers)\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Bounded rare-key presence boost:\n    # For identifier/symbol queries, \"present at least once\" can be more informative than tf.\n    # Keep bounded to avoid destabilizing general BEIR corpora.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Empirically more stable cross-domain than odds/log1p(N/df), and works well with\n    coverage-style soft-AND because it behaves like \"bits of evidence\".\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique lexical constraints + sublinear repetition weights.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + informative-mass coverage + mild verbosity prior,\n    plus a bounded rare-key *presence* multiplier.\n\n    Presence boost intuition: on identifier-heavy queries (StackOverflow/TheoremQA/BRIGHT),\n    a single match on an ultra-rare token should sharply increase relevance, but we keep the\n    boost bounded and smooth (log1p) for cross-domain stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)  # in (0,1)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and tidf > getattr(Config, \"rare_idf_pivot\", 1e9):\n            # bounded (0,1) per term; aggregate then log1p at the end\n            rare_hits += (tidf - float(Config.rare_idf_pivot)) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary prefix view (purely lexical robustness).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for the score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only (not used by the main scorer anymore).\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # DF exposed for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 1e9):\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        if self.corpus.avgdl > 0.0 and Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Unique constraints + sublinear qtf weights (matches QueryRepr).\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        # Add tagged prefixes as additional pseudo-terms (gentle weight).\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D4 (Score: 0.4379)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    epsilon: float = 1e-9\n\n    # Evidence concavity for TF: log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND on informative mass covered.\n    coverage_gamma: float = 0.25\n\n    # Downweight very common query terms smoothly: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Mild verbosity prior: divide by 1 + alpha*log1p((dl+1)/(avgdl+1))\n    dl_alpha: float = 0.15\n\n    # NEW: bounded \"key-term presence\" multiplier for ultra-rare query terms.\n    # Intuition: in identifier-heavy domains (StackOverflow, TheoremQA, some BRIGHT),\n    # a single hit on an ultra-rare token is disproportionately diagnostic; keep bounded.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # NEW: cheap secondary lexical channel using fixed-length prefixes (morphology/camelcase tolerance).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Compatibility leftovers (Corpus builds norm_array; scorer no longer uses BM25 k1/b)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds/log(N/df) on heterogeneous corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique constraints + sublinear repetition: keeps emphasis without verbosity blowups.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** 0.5 for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage\n    + bounded rare-key presence multiplier.\n\n    Rare-key presence is indicator-like: for ultra-rare query tokens, *appearing at all*\n    is a strong lexical signal (identifiers/symbols). The boost is bounded for stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 1e9):\n            rare_hits += (term_idf - float(Config.rare_idf_pivot)) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq,\n                corpus.prefix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Optional prefix view for robustness (secondary lexical channel).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so rank() can use postings.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p). (compat only)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # DF exposed for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 1e9):\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** 0.5)\n\n        # Add prefix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** 0.5))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4379, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep BM25 knobs for candidate scoring / normalization compatibility.\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Evidence is concave in tf; using log evidence is often more robust cross-domain than BM25 tf/(tf+k).\n    tf_log_base: float = 1.0\n\n    # Soft-AND over informative query mass (bounded; recall-safe).\n    coverage_gamma: float = 0.25\n\n    # Gentle query-side clarity gate: suppress extremely common query terms smoothly.\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting (keeps emphasis without verbosity blowups).\n    qtf_power: float = 0.5\n\n    # Mild verbosity prior (stable across corpora).\n    dl_alpha: float = 0.15\n\n    # Avoid over-compressing IDF; concavity is handled via log-evidence and bounded boosts.\n    idf_power: float = 1.0\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n\n    Empirically more stable cross-domain than odds/log1p(N/df), and works well with\n    coverage-style soft-AND because it behaves like \"bits of evidence\".\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique lexical constraints + sublinear repetition weights.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + IDF-mass coverage + mild verbosity prior\n    + bounded rare-key presence multiplier.\n\n    Evidence view:\n      sum_t w(t) * log1p(tf/base)  then compress as log1p(total_evidence).\n    This behaves like accumulating diminishing \"bits\" of lexical evidence and\n    tends to improve early precision while staying recall-safe.\n\n    Coverage:\n      multiply by 1 + gamma * (covered informative mass / total informative mass).\n\n    Rare-key presence:\n      if an ultra-rare query token appears at least once, apply a small bounded boost.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        tidf = max(0.0, tidf) ** Config.idf_power\n        if tidf <= 0.0:\n            continue\n\n        # bounded clarity in [0,1]\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * tidf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # indicator-like rare-key presence\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    if avgdl > 0.0 and Config.dl_alpha != 0.0:\n        length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n        score /= 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Secondary \"prefix lexicon\" view (cheap robustness to morphology/identifiers).\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            idf_val = max(0.0, idf_val) ** Config.idf_power\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wt = float(q_w[i]) * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        if self.corpus.avgdl > 0.0 and Config.dl_alpha != 0.0:\n            length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n            scores /= 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel)\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4379, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Concave evidence + IDF-weighted coverage + bounded query-clarity gating.\n    epsilon: float = 1e-9\n\n    # --- Secondary robust lexical channel: prefixes (morphology/camelcase tolerance) ---\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # --- Rare-key presence boost (bounded multiplier) ---\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Evidence: wt * log1p(tf/base), then log1p(total evidence).\n    tf_log_base: float = 1.0\n\n    # Soft-AND: reward covering more of the *informative* query mass.\n    coverage_gamma: float = 0.25\n\n    # Query-side clarity gate: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Sublinear query repetition weighting: count**p (keeps emphasis w/o verbosity blowups)\n    qtf_power: float = 0.5\n\n    # Mild length prior: downweight extremely long docs gently (helps precision w/o killing recall).\n    dl_alpha: float = 0.15\n\n    # Compatibility leftovers (Corpus references b/k1; keep but don't use in scoring)\n    k1: float = 0.9\n    b: float = 0.4\n\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Unique query constraints + sublinear repetition weights.\n\n        Rationale: repetition sometimes encodes emphasis, but linear qtf is brittle\n        on verbose queries. Use count**p with p\u22480.5.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage.\n\n    Evidence:\n      sum_t [ wq(t) * idf(t) * clarity(t) * log1p(tf/base) ]\n      then compress with log1p(total) to get diminishing returns and encourage coverage.\n\n    Coverage prior:\n      multiply by 1 + gamma * (covered_idf_mass / total_idf_mass)\n      (soft-AND over *informative* constraints).\n\n    This removes pairwise synergy/common-damp which tended to be brittle cross-dataset.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        # bounded clarity gate in [0,1]\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        # Presence-gated rare-key evidence (bounded): helpful for identifiers/symbols.\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 0.0):\n            rare_hits += (term_idf - float(Config.rare_idf_pivot)) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= 1.0 + Config.coverage_gamma * coverage\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    # mild verbosity prior\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds an optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary channel: exact tokens.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary channel: prefixes for morphology/camelcase robustness.\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Prefix view for robust lexical matching (secondary channel).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so channels don't collide.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Compatibility only (not used by current scorer): standard BM25-style length norm.\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += wt * present\n\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 0.0):\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= 1.0 + Config.coverage_gamma * coverage\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, cnt in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(cnt) ** Config.qtf_power)\n\n        # Prefix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4076, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length norm\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Soft AND-pressure (kept from current best)\n    coverage_gamma: float = 0.35\n\n    # Damp overly common query terms\n    common_term_beta: float = 0.6\n\n    # NEW: reward *discriminative* co-occurrence among matched query terms.\n    # Intuition: in many corpora (BRIGHT, StackOverflow, SciDocs), relevant docs\n    # tend to contain multiple query terms *together*; PMI-like synergy captures\n    # this without needing positions or learning.\n    synergy_lambda: float = 0.15   # strength of pairwise synergy\n    synergy_cap: float = 1.0       # prevents long queries from exploding\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed \"surprisal\" IDF:\n    - Interprets df/N as an empirical occurrence probability p(t in doc).\n    - Uses -log(p) with add-one style smoothing to avoid infinities.\n    This tends to behave better than classic BM25 IDF on very spiky corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Deduplicate to represent a query as a *set* of lexical constraints.\n        Repeated query tokens often come from tokenization artifacts; set-style\n        matching tends to improve early precision across BEIR/BRIGHT mixes.\n        \"\"\"\n        seen = set()\n        uniq = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                uniq.append(t)\n        return cls(terms=uniq, term_weights={t: 1.0 for t in uniq})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    CRSM + Pairwise Synergy (PMI-style)\n\n    Keep the strong parts of CRSM:\n      - surprisal-ish IDF\n      - BM25-style TF saturation\n      - soft coverage boost\n      - query common-term dampening\n\n    Add a *synergy* bonus for documents that match multiple query terms:\n    - For matched terms i,j, add ~ idf(i)*idf(j) / (idf(i)+idf(j)).\n      This is a smooth \"both rare => strong conjunction\" signal.\n    - Acts like an inexpensive approximation to discriminative phrase/proximity:\n      without positions, co-presence is the simplest lexical interaction.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    beta = Config.common_term_beta\n    gamma = Config.coverage_gamma\n    lam = Config.synergy_lambda\n\n    if not query_repr.terms:\n        return 0.0\n\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    total = 0\n\n    matched_idf: list[float] = []\n    for term in query_repr.terms:\n        total += 1\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        df = float(corpus_df.get(term, 1.0))\n        # Query-side commonness dampening: (1 - p)^beta where p\u2248df/N.\n        p = (df + 1.0) / (N + 2.0)\n        common_damp = (1.0 - p) ** beta\n\n        matched += 1\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n\n        score += (w * common_damp) * term_idf * tf_part\n        matched_idf.append(term_idf)\n\n    if score <= 0.0:\n        return 0.0\n\n    # Soft coverage: 1 + gamma * (matched/total)\n    if total > 0 and matched > 0:\n        score *= (1.0 + gamma * (matched / float(total)))\n\n    # Pairwise synergy among matched terms (cap to avoid long-query blowups).\n    if lam > 0.0 and len(matched_idf) >= 2:\n        # harmonic-ish mean emphasizes \"both must be informative\"\n        s = 0.0\n        for i in range(len(matched_idf)):\n            ai = matched_idf[i]\n            for j in range(i + 1, len(matched_idf)):\n                aj = matched_idf[j]\n                s += (ai * aj) / (ai + aj + eps)\n        # normalize by number of pairs to be query-length robust\n        pairs = len(matched_idf) * (len(matched_idf) - 1) / 2.0\n        s = s / (pairs + eps)\n        s = min(s, Config.synergy_cap)\n        score *= (1.0 + lam * s)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        # Precompute df-probability p(t) used for query-side commonness dampening.\n        self.p_df_array = (self._df + 1.0) / (self.N + 2.0)\n\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        beta = Config.common_term_beta\n        gamma = Config.coverage_gamma\n        lam = Config.synergy_lambda\n\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For synergy we need to know, per query term, which candidates contain it.\n        present_masks: list[NDArray[np.float64]] = []\n        idf_vals: list[float] = []\n\n        total = float(len(query_term_ids))\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            w = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            p = float(self.corpus.p_df_array[term_id])\n            common_damp = (1.0 - p) ** beta\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += (w * common_damp) * idf_val * tf_part\n\n            present_masks.append(present)\n            idf_vals.append(idf_val)\n\n        if total > 0.0:\n            scores *= (1.0 + gamma * (matched / total))\n\n        # Synergy: for each pair of query terms, add a bonus if both are present.\n        if lam > 0.0 and len(present_masks) >= 2:\n            s = np.zeros(len(candidate_docs), dtype=np.float64)\n            denom = 0.0\n            for i in range(len(present_masks)):\n                ai = idf_vals[i]\n                for j in range(i + 1, len(present_masks)):\n                    aj = idf_vals[j]\n                    pair_w = (ai * aj) / (ai + aj + eps)\n                    s += pair_w * (present_masks[i] * present_masks[j])\n                    denom += pair_w\n            if denom > 0.0:\n                s = s / (denom + eps)\n                s = np.minimum(s, Config.synergy_cap)\n                scores *= (1.0 + lam * s)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        # Represent query as unique constraints (match QueryRepr behavior).\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term in term_counts.keys():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(1.0)\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4355, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Keep core: concave evidence + informative coverage.\n    epsilon: float = 1e-9\n    tf_log_base: float = 1.0\n    coverage_gamma: float = 0.25\n    qtf_power: float = 0.5\n    q_clarity_power: float = 0.6\n    dl_alpha: float = 0.15\n\n    # Two-channel matching: prefixes add gentle robustness to tokenization/morphology mismatch.\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: residual-IDF (verbosity-aware discriminativity), using a smooth hinge (softplus).\n    # Intuition: in long/verbose docs, matches are less \"surprising\"; demand extra IDF mass.\n    resid_rho: float = 0.30          # strength of verbosity discount\n    resid_floor: float = 0.0         # hinge floor\n    ridf_tau: float = 0.35           # softplus temperature (smoothness)\n\n    # NEW: rare-key presence multiplier (indicator-like).\n    # Intuition: in technical corpora, 1 hit on an ultra-rare identifier is decisive.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # Compatibility (Corpus still builds norm_array); not used by the new core scorer.\n    k1: float = 0.9\n    b: float = 0.4\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds and avoids extreme spikes.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Treat query as unique constraints; keep a sublinear repetition signal as weight.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** Config.qtf_power for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave evidence + IDF-mass coverage, augmented with two bounded priors:\n\n    (1) Residual-IDF (verbosity-aware) with smooth hinge:\n        ridf = softplus((idf - rho*v) - floor),  v = log1p((dl+1)/(avgdl+1))\n      Long documents create more incidental matches; require extra surprisal.\n\n    (2) Rare-key presence multiplier:\n      A small bounded boost when ultra-rare query terms (high IDF) appear at least once.\n      Helps identifier/symbol-heavy corpora without changing average behavior much.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # verbosity signal used both for residualization and mild length prior\n    v = math.log1p((doc_length + 1.0) / (avgdl + 1.0))\n    tau = max(Config.ridf_tau, 1e-6)\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n\n        # residual-IDF soft hinge (smoothly >=0)\n        x = (tidf - Config.resid_rho * v) - Config.resid_floor\n        ridf = tau * math.log1p(math.exp(x / tau))\n        if ridf <= 0.0:\n            continue\n\n        rarity = tidf / (tidf + 1.0)  # (0,1)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # Coverage is query-centric: use non-residual informative mass.\n        cov_wt = wq * tidf * clarity\n        cov_den += cov_wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += cov_wt\n\n        # Evidence uses residual discriminativity.\n        sum_evidence += (wq * clarity) * ridf * math.log1p(tf / (base + eps))\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - Config.rare_idf_pivot) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + Config.rare_boost * math.log1p(rare_hits)\n\n    dl_damp = 1.0 + Config.dl_alpha * v\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    # Primary token-channel score.\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    # Secondary prefix-channel score (helps when exact token match is brittle).\n    if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            pdoc_tf = corpus.prefix_doc_tf_dicts[doc_idx]\n            s += Config.prefix_weight * retrieval_score(\n                pq, pdoc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Build a tiny \"prefix lexicon\" view in addition to raw tokens.\n        # This is still lexical (no semantics) but increases robustness to:\n        # - morphology: \"oxidize\"/\"oxidation\"\n        # - symbols/camelcase fragments common in BRIGHT/StackOverflow/TheoremQA\n        pfx = max(1, int(Config.prefix_len))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over (token) U (prefixes), keeping them disjoint by tagging prefixes.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Keep original doc TF dicts for score() path.\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n\n        # Expose df for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        # v = log1p((dl+1)/(avgdl+1))\n        v = np.log1p((self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0))\n        tau = max(Config.ridf_tau, 1e-6)\n\n        if query_term_weights is None:\n            q_w = np.ones(len(query_term_ids), dtype=np.float64)\n        else:\n            q_w = query_term_weights.astype(np.float64, copy=False)\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n            wq = float(q_w[i])\n\n            cov_wt = wq * idf_val * clarity\n            cov_den += cov_wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n            cov_num += cov_wt * present\n\n            # residual-IDF soft hinge per doc\n            x = (idf_val - Config.resid_rho * v) - Config.resid_floor\n            ridf = tau * np.log1p(np.exp(x / tau))\n\n            sum_evidence += (wq * clarity) * ridf * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - Config.rare_idf_pivot) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + Config.rare_boost * np.log1p(np.maximum(rare_hits, 0.0))\n\n        dl_damp = 1.0 + Config.dl_alpha * v\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Build combined query over tokens + (optional) prefixes as additional pseudo-terms.\n        counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, count in counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_power)\n\n        if Config.prefix_weight != 0.0 and Config.prefix_len > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(Config.prefix_weight * (float(c) ** Config.qtf_power))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Now the scorer naturally handles both channels since prefixes are just extra terms.\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    epsilon: float = 1e-9\n\n    # Evidence concavity for TF: log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND on informative mass covered.\n    coverage_gamma: float = 0.25\n\n    # Downweight very common query terms smoothly: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Mild verbosity prior: divide by 1 + alpha*log1p((dl+1)/(avgdl+1))\n    dl_alpha: float = 0.15\n\n    # NEW: bounded \"key-term presence\" multiplier for ultra-rare query terms.\n    # Intuition: in identifier-heavy domains (StackOverflow, TheoremQA, some BRIGHT),\n    # a single hit on an ultra-rare token is disproportionately diagnostic; keep bounded.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # NEW: cheap secondary lexical channel using fixed-length prefixes (morphology/camelcase tolerance).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Compatibility leftovers (Corpus builds norm_array; scorer no longer uses BM25 k1/b)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed surprisal IDF: -log p(t in doc) with add-one smoothing.\n    More stable cross-domain than BM25-odds/log(N/df) on heterogeneous corpora.\n    \"\"\"\n    df = np.asarray(df, dtype=np.float64)\n    p = (df + 1.0) / (N + 2.0)\n    return -np.log(p)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique constraints + sublinear repetition: keeps emphasis without verbosity blowups.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** 0.5 for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage\n    + bounded rare-key presence multiplier.\n\n    Rare-key presence is indicator-like: for ultra-rare query tokens, *appearing at all*\n    is a strong lexical signal (identifiers/symbols). The boost is bounded for stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 1e9):\n            rare_hits += (term_idf - float(Config.rare_idf_pivot)) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq,\n                corpus.prefix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Optional prefix view for robustness (secondary lexical channel).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so rank() can use postings.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n\n        # Must match retrieval_score(): sublinear length normalization (dl^p). (compat only)\n        b = Config.b\n        p = Config.dl_p\n        dl = np.power(self.doc_lengths, p, dtype=np.float64)\n        adl = float(max(self.avgdl, 1.0)) ** p\n        self.norm_array = 1.0 - b + b * (dl / max(adl, 1.0))\n\n        # DF exposed for both channels (prefix keys included).\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 1e9):\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** 0.5)\n\n        # Add prefix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** 0.5))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    epsilon: float = 1e-9\n\n    # Evidence concavity for TF: log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND on informative mass covered.\n    coverage_gamma: float = 0.25\n\n    # Downweight very common query terms smoothly: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Mild verbosity prior: divide by 1 + alpha*log1p((dl+1)/(avgdl+1))\n    dl_alpha: float = 0.15\n\n    # NEW: bounded \"key-term presence\" multiplier for ultra-rare query terms.\n    # Intuition: in identifier-heavy domains (StackOverflow, TheoremQA, some BRIGHT),\n    # a single hit on an ultra-rare token is disproportionately diagnostic; keep bounded.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # NEW: cheap secondary lexical channel using fixed-length prefixes (morphology/camelcase tolerance).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # Compatibility leftovers (Corpus builds norm_array; scorer no longer uses BM25 k1/b)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n=======\nclass Config:\n    epsilon: float = 1e-9\n\n    # Evidence concavity for TF: log1p(tf/base)\n    tf_log_base: float = 1.0\n\n    # Soft-AND on informative mass covered.\n    coverage_gamma: float = 0.25\n\n    # Downweight very common query terms smoothly: clarity=(idf/(idf+1))^p in [0,1]\n    q_clarity_power: float = 0.6\n\n    # Mild verbosity prior: divide by 1 + alpha*log1p((dl+1)/(avgdl+1))\n    dl_alpha: float = 0.15\n\n    # NEW: bounded \"key-term presence\" multiplier for ultra-rare query terms.\n    rare_idf_pivot: float = 4.5\n    rare_boost: float = 0.12\n\n    # NEW: cheap secondary lexical channel using fixed-length prefixes (morphology/camelcase tolerance).\n    prefix_len: int = 5\n    prefix_weight: float = 0.18\n\n    # NEW: suffix channel (helps morphology like plural/verb forms when prefix collides).\n    suffix_len: int = 5\n    suffix_weight: float = 0.08\n\n    # NEW: query-distribution sharpening for very hard/technical queries.\n    # Interpretable as a Renyi-style \"focus\": emphasize the tail of rare terms.\n    q_focus_power: float = 1.15\n\n    # NEW: co-presence synergy (bounded) to reward matching multiple informative terms together.\n    synergy_lambda: float = 0.12\n    synergy_cap: float = 1.0\n\n    # Compatibility leftovers (Corpus builds norm_array; scorer no longer uses BM25 k1/b)\n    k1: float = 0.9\n    b: float = 0.4\n    dl_p: float = 0.75\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        # Unique constraints + sublinear repetition: keeps emphasis without verbosity blowups.\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** 0.5 for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n=======\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Query weights = (qtf^0.5) * (focus^power), where focus is a query-internal\n        rarity distribution over terms: p(t) \u221d idf(t) (approximated later in scorer).\n\n        We keep qtf sublinear here; the focus power is applied in retrieval_score\n        where IDF is available.\n        \"\"\"\n        if not tokens:\n            return cls(terms=[], term_weights={})\n        c = Counter(tokens)\n        terms = list(c.keys())\n        w = {t: float(cnt) ** 0.5 for t, cnt in c.items()}\n        return cls(terms=terms, term_weights=w)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Concave surprisal evidence + bounded query-clarity gating + IDF-mass coverage\n    + bounded rare-key presence multiplier.\n\n    Rare-key presence is indicator-like: for ultra-rare query tokens, *appearing at all*\n    is a strong lexical signal (identifiers/symbols). The boost is bounded for stability.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        term_idf = float(idf(df, N))\n        if term_idf <= 0.0:\n            continue\n\n        rarity = term_idf / (term_idf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        wt = wq * term_idf * clarity\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0 and term_idf > getattr(Config, \"rare_idf_pivot\", 1e9):\n            rare_hits += (term_idf - float(Config.rare_idf_pivot)) / (term_idf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    if getattr(Config, \"rare_boost\", 0.0) != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    DSD (Distribution-Sharpened Dependence) lexical scoring.\n\n    1) Evidence: concave TF evidence in \"surprisal bits\" (idf) with a bounded\n       clarity gate for very common terms.\n    2) Query focus sharpening: within a query, emphasize the tail of rare terms\n       by using a Renyi-style power transform over the IDF mass.\n    3) Soft coverage: reward covering informative mass (recall-safe).\n    4) Synergy: a bounded co-presence bonus for matching multiple informative terms.\n       This approximates dependence/proximity without positions: relevant docs often\n       contain several query constraints together.\n    \"\"\"\n    if not query_repr.terms:\n        return 0.0\n\n    eps = Config.epsilon\n    base = Config.tf_log_base\n\n    # --- build query-internal focus distribution p(t) \u221d idf(t)*qtf(t) ---\n    q_mass: list[tuple[str, float, float]] = []\n    total_mass = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1.0))\n        tidf = float(idf(df, N))\n        if tidf <= 0.0:\n            continue\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        m = tidf * wq\n        q_mass.append((term, tidf, wq))\n        total_mass += m\n\n    if total_mass <= 0.0:\n        return 0.0\n\n    sum_evidence = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    rare_hits = 0.0\n    matched_idf: list[float] = []\n\n    focus_p = float(getattr(Config, \"q_focus_power\", 1.0))\n    for term, tidf, wq in q_mass:\n        rarity = tidf / (tidf + 1.0)\n        clarity = rarity ** Config.q_clarity_power\n\n        # focus weight in [0,1], sharper when q_focus_power>1\n        p = (tidf * wq) / (total_mass + eps)\n        focus = p ** focus_p\n\n        wt = wq * tidf * clarity * focus\n        cov_den += wt\n\n        tf = float(doc_tf.get(term, 0.0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += wt\n        sum_evidence += wt * math.log1p(tf / (base + eps))\n        matched_idf.append(tidf)\n\n        if Config.rare_boost != 0.0 and tidf > Config.rare_idf_pivot:\n            rare_hits += (tidf - float(Config.rare_idf_pivot)) / (tidf + eps)\n\n    if sum_evidence <= 0.0:\n        return 0.0\n\n    score = math.log1p(sum_evidence)\n\n    if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n        score *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n    # bounded dependence/synergy among matched terms\n    lam = float(getattr(Config, \"synergy_lambda\", 0.0))\n    if lam > 0.0 and len(matched_idf) >= 2:\n        # normalized mean harmonic-IDF across pairs; then bounded.\n        s = 0.0\n        pairs = 0.0\n        for i in range(len(matched_idf)):\n            ai = matched_idf[i]\n            for j in range(i + 1, len(matched_idf)):\n                aj = matched_idf[j]\n                s += (ai * aj) / (ai + aj + eps)\n                pairs += 1.0\n        s = s / (pairs + eps)\n        s = min(s, float(getattr(Config, \"synergy_cap\", 1.0)))\n        score *= 1.0 + lam * s\n\n    if Config.rare_boost != 0.0 and rare_hits > 0.0:\n        score *= 1.0 + float(Config.rare_boost) * math.log1p(rare_hits)\n\n    length_ratio = (doc_length + 1.0) / (avgdl + 1.0)\n    dl_damp = 1.0 + Config.dl_alpha * math.log1p(length_ratio)\n    return score / (dl_damp + eps)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds optional prefix channel (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq,\n                corpus.prefix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n=======\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). Adds optional prefix/suffix channels (purely lexical).\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n\n    s = retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n    if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n        pfx = max(1, int(Config.prefix_len))\n        ptoks = [t[:pfx] for t in query if len(t) >= pfx]\n        if ptoks:\n            pq = QueryRepr.from_tokens([\"P:\" + t for t in ptoks])\n            s += float(Config.prefix_weight) * retrieval_score(\n                pq,\n                corpus.prefix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n\n    if getattr(Config, \"suffix_weight\", 0.0) != 0.0 and getattr(Config, \"suffix_len\", 0) > 0:\n        sfx = max(1, int(Config.suffix_len))\n        stoks = [t[-sfx:] for t in query if len(t) >= sfx]\n        if stoks:\n            sq = QueryRepr.from_tokens([\"S:\" + t for t in stoks])\n            s += float(Config.suffix_weight) * retrieval_score(\n                sq,\n                corpus.suffix_doc_tf_dicts[doc_idx],\n                doc_length,\n                corpus.N,\n                corpus.avgdl,\n                corpus.document_frequency,\n            )\n    return s\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Optional prefix view for robustness (secondary lexical channel).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n\n        # Joint vocabulary over tokens + tagged prefixes (\"P:xxxxx\") so rank() can use postings.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc in zip(documents, docs_prefix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n=======\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Optional prefix/suffix views for robustness (secondary lexical channels).\n        pfx = max(1, int(getattr(Config, \"prefix_len\", 0) or 1))\n        sfx = max(1, int(getattr(Config, \"suffix_len\", 0) or 1))\n        docs_prefix = [[t[:pfx] for t in doc if len(t) >= pfx] for doc in documents]\n        docs_suffix = [[t[-sfx:] for t in doc if len(t) >= sfx] for doc in documents]\n        self.prefix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_prefix]\n        self.suffix_doc_tf_dicts: list[Counter[str]] = [Counter(d) for d in docs_suffix]\n\n        # Joint vocabulary over tokens + tagged prefixes/suffixes so rank() can use postings.\n        self._vocab: dict[str, int] = {}\n        for doc, pdoc, sdoc in zip(documents, docs_prefix, docs_suffix):\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n            for p in pdoc:\n                key = \"P:\" + p\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n            for s in sdoc:\n                key = \"S:\" + s\n                if key not in self._vocab:\n                    self._vocab[key] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        for doc_idx, (doc, pdoc) in enumerate(zip(documents, docs_prefix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n=======\n        for doc_idx, (doc, pdoc, sdoc) in enumerate(zip(documents, docs_prefix, docs_suffix)):\n            term_counts = Counter(doc)\n            pref_counts = Counter(\"P:\" + p for p in pdoc)\n            suf_counts = Counter(\"S:\" + s for s in sdoc)\n            seen = set()\n\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in pref_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n            for term, count in suf_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** 0.5)\n\n        # Add prefix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** 0.5))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n=======\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n\n        for term, c in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(c) ** 0.5)\n\n        # Add prefix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"prefix_weight\", 0.0) != 0.0 and getattr(Config, \"prefix_len\", 0) > 0:\n            pfx = max(1, int(Config.prefix_len))\n            pcounts = Counter(t[:pfx] for t in query if len(t) >= pfx)\n            for p, c in pcounts.items():\n                tid = self.corpus.get_term_id(\"P:\" + p)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.prefix_weight) * (float(c) ** 0.5))\n\n        # Add suffix pseudo-terms (secondary lexical channel).\n        if getattr(Config, \"suffix_weight\", 0.0) != 0.0 and getattr(Config, \"suffix_len\", 0) > 0:\n            sfx = max(1, int(Config.suffix_len))\n            scounts = Counter(t[-sfx:] for t in query if len(t) >= sfx)\n            for s, c in scounts.items():\n                tid = self.corpus.get_term_id(\"S:\" + s)\n                if tid is not None:\n                    query_term_ids.append(tid)\n                    query_term_weights.append(float(Config.suffix_weight) * (float(c) ** 0.5))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            wt = wq * idf_val * clarity\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if getattr(Config, \"rare_boost\", 0.0) != 0.0 and idf_val > getattr(Config, \"rare_idf_pivot\", 1e9):\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        if getattr(Config, \"rare_boost\", 0.0) != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        eps = Config.epsilon\n        base = Config.tf_log_base\n\n        # Need total query mass for focus weights: p(t) \u221d idf*qtf\n        idf_q = []\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                idf_q.append(0.0)\n                continue\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n            idf_q.append(idf_val * wq)\n        total_mass = float(np.sum(idf_q))\n\n        sum_evidence = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n        rare_hits = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        focus_p = float(getattr(Config, \"q_focus_power\", 1.0))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            rarity = idf_val / (idf_val + 1.0)\n            clarity = rarity ** Config.q_clarity_power\n\n            wq = float(query_term_weights[i]) if query_term_weights is not None else 1.0\n\n            p = (idf_val * wq) / (total_mass + eps)\n            focus = p ** focus_p\n\n            wt = wq * idf_val * clarity * focus\n            cov_den += wt\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0.0).astype(np.float64)\n\n            matched += present\n            matched_idf_sum += present * idf_val\n\n            cov_num += wt * present\n            sum_evidence += wt * np.log1p(tf_row / (base + eps))\n\n            if Config.rare_boost != 0.0 and idf_val > Config.rare_idf_pivot:\n                rare_hits += present * ((idf_val - float(Config.rare_idf_pivot)) / (idf_val + eps))\n\n        scores = np.log1p(np.maximum(sum_evidence, 0.0))\n\n        if cov_den > 0.0 and Config.coverage_gamma != 0.0:\n            scores *= 1.0 + Config.coverage_gamma * (cov_num / (cov_den + eps))\n\n        lam = float(getattr(Config, \"synergy_lambda\", 0.0))\n        if lam > 0.0:\n            # Approximate pairwise synergy by a bounded function of (matched_count, matched_idf_sum).\n            # For fixed matched_count, higher matched_idf_sum => more informative conjunction.\n            denom = np.maximum(matched, 1.0)\n            avg_idf = matched_idf_sum / (denom + eps)\n            s = np.minimum(avg_idf / (avg_idf + 3.0), float(getattr(Config, \"synergy_cap\", 1.0)))\n            scores *= 1.0 + lam * s * (matched >= 2.0)\n\n        if Config.rare_boost != 0.0:\n            scores *= 1.0 + float(Config.rare_boost) * np.log1p(np.maximum(rare_hits, 0.0))\n\n        length_ratio = (self.corpus.doc_lengths[candidate_docs] + 1.0) / (self.corpus.avgdl + 1.0)\n        dl_damp = 1.0 + Config.dl_alpha * np.log1p(length_ratio)\n        return scores / (dl_damp + eps)\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}