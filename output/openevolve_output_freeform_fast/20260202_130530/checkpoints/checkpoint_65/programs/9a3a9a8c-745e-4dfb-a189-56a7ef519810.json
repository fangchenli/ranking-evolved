{"id": "9a3a9a8c-745e-4dfb-a189-56a7ef519810", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\"\n    cov_idf_power: float = 1.0\n\n    # \"Informativeness pivot\" normalization:\n    # normalize TF saturation by an IDF-weighted document mass so common-term verbosity\n    # is discounted without over-penalizing long but information-dense docs.\n    info_power: float = 0.6\n    info_mix: float = 0.22\n\n    # Cheap phrase/proximity reward. Only uses adjacent query term pairs.\n    prox_window: int = 8\n    prox_alpha: float = 0.08  # slightly lower to reduce overfitting/recall loss\n\n    # NEW: query-dependent gating for proximity.\n    # Idea: proximity is most meaningful when the query is \"specific\" (high IDF mass).\n    # For broad/keyword queries it can overfit (hurting recall); gate it by query specificity.\n    prox_gate_alpha: float = 0.7   # how strongly specificity can boost proximity (0 => no gating)\n    prox_gate_floor: float = 0.35  # minimum multiplier to keep some benefit when relevant\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float, positions: dict[str, list[int]]):\n        self.term_frequencies = term_frequencies\n        self.length = length\n        self.positions = positions  # for lightweight proximity/phrase evidence\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Store positions for proximity reward (kept minimal: only per-term position lists).\"\"\"\n        tf = Counter(tokens)\n        pos: dict[str, list[int]] = {}\n        for i, t in enumerate(tokens):\n            pos.setdefault(t, []).append(i)\n        return cls(term_frequencies=tf, length=float(len(tokens)), positions=pos)\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    doc_pos: dict[str, list[int]] | None = None,\n) -> float:\n    \"\"\"\n    IDF-weighted evidence + IDF-mass coordination + informativeness-pivot normalization\n    + (optional) bounded proximity reward.\n\n    Key change vs current:\n    - Adds an \"informativeness pivot\" to reduce the advantage of documents that are\n      long due to common terms; this typically improves early precision/nDCG while\n      keeping recall stable (soft blend with classic length norm).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Focused length (token length blended with unique-term length)\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # IDF-weighted document \"information mass\" (computed on the fly; no extra storage)\n    info_mass = 0.0\n    ip = Config.idf_power * Config.info_power\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        if base_t <= 0.0:\n            continue\n        info_mass += float(tfv) * (base_t ** ip)\n\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Query \"specificity\" mass: average powered-IDF over query terms.\n    # Used to gate proximity so it helps phrase-like/specific queries more than broad ones.\n    q_idf_sum = 0.0\n    q_idf_n = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        q_idf_sum += term_idf\n        q_idf_n += 1.0\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # Proximity reward (kept bounded; mainly helps \"phrasey\" queries).\n    # NEW: gate by query specificity so broad queries don't over-benefit from accidental proximity.\n    if doc_pos is not None and len(query_repr.terms) > 1 and Config.prox_alpha > 0.0:\n        q_spec = (q_idf_sum / (q_idf_n + eps)) if q_idf_n > 0.0 else 0.0\n        # squashed to (0,1): higher when query terms are rarer\n        q_gate = q_spec / (q_spec + 1.0)\n        prox_scale = Config.prox_gate_floor + Config.prox_gate_alpha * q_gate\n\n        hits = 0.0\n        pairs = 0.0\n        w = int(max(1, Config.prox_window))\n        for a, bterm in zip(query_repr.terms, query_repr.terms[1:]):\n            pa = doc_pos.get(a)\n            pb = doc_pos.get(bterm)\n            pairs += 1.0\n            if not pa or not pb:\n                continue\n            i = j = 0\n            best = 1_000_000\n            while i < len(pa) and j < len(pb):\n                da = pa[i] - pb[j]\n                ad = da if da >= 0 else -da\n                if ad < best:\n                    best = ad\n                    if best == 0:\n                        break\n                if da < 0:\n                    i += 1\n                else:\n                    j += 1\n            if best <= w:\n                hits += 1.0\n        if pairs > 0.0 and hits > 0.0:\n            score *= 1.0 + (Config.prox_alpha * prox_scale) * (hits / pairs)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency, d.positions)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build richer doc repr once (tf + positions) for proximity scoring in score().\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized core score for rank(); mirrors retrieval_score (except proximity).\n\n        NOTE: Proximity is intentionally omitted here for speed; retrieval_score gates it\n        by query specificity, and we accept that rank() is the main path used by the evaluator.\n        The gating change mainly improves the *robustness* of proximity when score() is used.\n        Uses a cheap proxy for info_norm: matched IDF-mass (presence-weighted).\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            mass = float(idf_val ** Config.cov_idf_power)\n            cov_den += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "b35fffef-dcc5-4854-9758-830e26592dd1", "generation": 5, "timestamp": 1770081495.2684019, "iteration_found": 0, "metrics": {"beir_nfcorpus_ndcg@10": 0.31954865812413646, "beir_nfcorpus_recall@100": 0.254448732630092, "beir_nfcorpus_index_time_ms": 4013.2014580012765, "beir_nfcorpus_query_time_ms": 103.93487502005883, "bright_pony_ndcg@10": 0.10669994312468109, "bright_pony_recall@100": 0.3011379635719698, "bright_pony_index_time_ms": 1545.9449579939246, "bright_pony_query_time_ms": 568.3704580005724, "beir_scifact_ndcg@10": 0.6762824901569473, "beir_scifact_recall@100": 0.9313333333333332, "beir_scifact_index_time_ms": 5555.117915995652, "beir_scifact_query_time_ms": 370.29249998158775, "beir_arguana_ndcg@10": 0.26767978616466087, "beir_arguana_recall@100": 0.8986438258386866, "beir_arguana_index_time_ms": 6538.426458981121, "beir_arguana_query_time_ms": 23098.024833016098, "bright_theoremqa_theorems_ndcg@10": 0.03168168744952312, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 11672.600332996808, "bright_theoremqa_theorems_query_time_ms": 918.0317079881206, "beir_scidocs_ndcg@10": 0.14440506446623258, "beir_scidocs_recall@100": 0.3421166666666667, "beir_scidocs_index_time_ms": 20864.365707995603, "beir_scidocs_query_time_ms": 3598.110416001873, "bright_biology_ndcg@10": 0.270636554837653, "bright_biology_recall@100": 0.54919514993608, "bright_biology_index_time_ms": 19319.337166991318, "bright_biology_query_time_ms": 3256.9143750006333, "bright_economics_ndcg@10": 0.1441328441786231, "bright_economics_recall@100": 0.3835880249997322, "bright_economics_index_time_ms": 21697.739958006423, "bright_economics_query_time_ms": 3177.883166994434, "beir_fiqa_ndcg@10": 0.21935471052680494, "beir_fiqa_recall@100": 0.5183661203337129, "beir_fiqa_index_time_ms": 48762.33970798785, "beir_fiqa_query_time_ms": 7145.826457999647, "bright_earth_science_ndcg@10": 0.3558899382645518, "bright_earth_science_recall@100": 0.6576743992295263, "bright_earth_science_index_time_ms": 47877.03799997689, "bright_earth_science_query_time_ms": 4707.883208000567, "bright_stackoverflow_ndcg@10": 0.18952817483316842, "bright_stackoverflow_recall@100": 0.4589736277160509, "bright_stackoverflow_index_time_ms": 119597.76741699898, "bright_stackoverflow_query_time_ms": 10819.658833992435, "beir_trec-covid_ndcg@10": 0.6470416307677291, "beir_trec-covid_recall@100": 0.1074189403794063, "beir_trec-covid_index_time_ms": 170540.0274579879, "beir_trec-covid_query_time_ms": 1508.1032500020228, "avg_ndcg@10": 0.2810734569078927, "avg_recall@100": 0.4630337963804234, "combined_score": 0.4266417284859173, "total_index_time_ms": 477983.90653991373, "total_query_time_ms": 59273.03408199805, "total_time_ms": 537256.9406219118, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 34 lines with 40 lines\nChange 2: Replace 9 lines with 9 lines\nChange 3: Replace 21 lines with 29 lines\nChange 4: Replace 30 lines with 36 lines\nChange 5: Replace 10 lines with 14 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.31954865812413646, "beir_nfcorpus_recall@100": 0.254448732630092, "beir_nfcorpus_index_time_ms": 3978.142709005624, "beir_nfcorpus_query_time_ms": 105.86287500336766, "bright_pony_ndcg@10": 0.10669994312468109, "bright_pony_recall@100": 0.3011379635719698, "bright_pony_index_time_ms": 1719.547209009761, "bright_pony_query_time_ms": 558.2679999934044, "beir_scifact_ndcg@10": 0.6762824901569473, "beir_scifact_recall@100": 0.9313333333333332, "beir_scifact_index_time_ms": 5426.147457998013, "beir_scifact_query_time_ms": 374.7493750124704, "beir_arguana_ndcg@10": 0.26767978616466087, "beir_arguana_recall@100": 0.8986438258386866, "beir_arguana_index_time_ms": 6551.885416993173, "beir_arguana_query_time_ms": 23176.224291994004, "bright_theoremqa_theorems_ndcg@10": 0.03168168744952312, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 11685.750292002922, "bright_theoremqa_theorems_query_time_ms": 914.4157500122674, "beir_scidocs_ndcg@10": 0.14440506446623258, "beir_scidocs_recall@100": 0.3421166666666667, "beir_scidocs_index_time_ms": 21026.76129201427, "beir_scidocs_query_time_ms": 3649.893542024074, "bright_economics_ndcg@10": 0.1441328441786231, "bright_economics_recall@100": 0.3835880249997322, "bright_economics_index_time_ms": 22032.761375012342, "bright_economics_query_time_ms": 3032.037290977314, "bright_biology_ndcg@10": 0.270636554837653, "bright_biology_recall@100": 0.54919514993608, "bright_biology_index_time_ms": 24624.43337502191, "bright_biology_query_time_ms": 2618.8066670147236, "bright_earth_science_ndcg@10": 0.3558899382645518, "bright_earth_science_recall@100": 0.6576743992295263, "bright_earth_science_index_time_ms": 47162.499792000744, "bright_earth_science_query_time_ms": 4542.861041001743, "beir_fiqa_ndcg@10": 0.21935471052680494, "beir_fiqa_recall@100": 0.5183661203337129, "beir_fiqa_index_time_ms": 51010.17125000362, "beir_fiqa_query_time_ms": 6877.975875017, "bright_stackoverflow_ndcg@10": 0.18952817483316842, "bright_stackoverflow_recall@100": 0.4589736277160509, "bright_stackoverflow_index_time_ms": 126815.88483299129, "bright_stackoverflow_query_time_ms": 10534.2762079963, "beir_trec-covid_ndcg@10": 0.6470416307677291, "beir_trec-covid_recall@100": 0.1074189403794063, "beir_trec-covid_index_time_ms": 171133.47929200972, "beir_trec-covid_query_time_ms": 1602.789834025316, "avg_ndcg@10": 0.2810734569078927, "avg_recall@100": 0.46303379638042347, "combined_score": 0.4266417284859173, "total_index_time_ms": 493167.4642940634, "total_query_time_ms": 57988.160750071984, "total_time_ms": 551155.6250441354, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 2, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}