{"id": "17ebe3a6-627b-40e2-8d34-afa6cd1d961c", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # --- New: bounded \"informative-mass coverage\" reward (recall-safe) ---\n    # Intuition: queries have an \"information budget\" sum(IDF). A good doc covers a large\n    # fraction of that budget (especially on verbose/noisy queries), improving nDCG@10.\n    # Implement as a multiplier >= 1, so recall@100 is usually not harmed.\n    info_cov_alpha: float = 0.10   # small strength\n    info_cov_gamma: float = 0.70   # concave: first informative matches matter most\n    info_cov_tf_gamma: float = 0.50  # soften evidence via sqrt(tf_sat)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive saturated evidence + (A) peakiness-gated coordination/pair synergy\n    + (B) bounded informative-mass coverage.\n\n    (B) Treat shaped IDF as a proxy for term surprisal. A query has total surprisal mass\n    sum_t IDF(t); reward documents that cover a larger fraction of that mass with\n    non-trivial evidence (tf saturation). Keep it as a multiplier >= 1 to stay recall-safe.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate + total info mass (uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_info = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        q_info += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    match_info = 0.0  # evidence-weighted matched info mass\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        # Soft evidence prevents one-off mentions of rare terms from dominating \"coverage\".\n        match_info += term_idf * (tf_part ** Config.info_cov_tf_gamma)\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # Bounded informative-mass coverage reward (multiplier >= 1).\n    if Config.info_cov_alpha > 0.0 and q_info > 0.0 and match_info > 0.0:\n        cov = match_info / (q_info + eps)  # ~[0,1]\n        cov = min(1.0, max(0.0, cov))\n        score *= 1.0 + Config.info_cov_alpha * (cov ** Config.info_cov_gamma)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Info-mass coverage bookkeeping (doc-independent total q_info, doc-dependent match_info).\n        q_info = 0.0\n        match_info = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_info += idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            # Evidence-weighted matched info mass (softened by sqrt(tf_sat)).\n            match_info += idf_val * np.power(tf_part, Config.info_cov_tf_gamma) * present\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Mirror retrieval_score(): bounded informative-mass coverage reward.\n        if Config.info_cov_alpha > 0.0 and q_info > 0.0:\n            cov = match_info / (q_info + eps)\n            cov = np.minimum(1.0, np.maximum(0.0, cov))\n            scores *= 1.0 + Config.info_cov_alpha * np.power(cov, Config.info_cov_gamma)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "da9ace2a-6788-4869-b880-b692cd34743c", "generation": 5, "timestamp": 1770129919.137559, "iteration_found": 170, "metrics": {"beir_nfcorpus_ndcg@10": 0.31992936453805976, "beir_nfcorpus_recall@100": 0.255562298460272, "beir_nfcorpus_index_time_ms": 3807.2072920040227, "beir_nfcorpus_query_time_ms": 114.11379199125804, "bright_pony_ndcg@10": 0.1040510184859845, "bright_pony_recall@100": 0.29253287445425397, "bright_pony_index_time_ms": 1482.0048340188805, "bright_pony_query_time_ms": 658.9586249901913, "beir_scifact_ndcg@10": 0.6733257515960215, "beir_scifact_recall@100": 0.932, "beir_scifact_index_time_ms": 5210.716708010295, "beir_scifact_query_time_ms": 405.70137498434633, "beir_arguana_ndcg@10": 0.27895100084038194, "beir_arguana_recall@100": 0.9093504639543183, "beir_arguana_index_time_ms": 6243.985582987079, "beir_arguana_query_time_ms": 32083.21062498726, "bright_theoremqa_theorems_ndcg@10": 0.040423220229772364, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 10984.318958013318, "bright_theoremqa_theorems_query_time_ms": 1275.7234580058139, "beir_scidocs_ndcg@10": 0.14322868149950635, "beir_scidocs_recall@100": 0.3433833333333333, "beir_scidocs_index_time_ms": 19957.278875022894, "beir_scidocs_query_time_ms": 4038.3701660030056, "bright_economics_ndcg@10": 0.14089591451969175, "bright_economics_recall@100": 0.390907487552556, "bright_economics_index_time_ms": 21755.65054200706, "bright_economics_query_time_ms": 4156.8197080050595, "bright_biology_ndcg@10": 0.24595262994949196, "bright_biology_recall@100": 0.545365592223027, "bright_biology_index_time_ms": 23727.40012500435, "bright_biology_query_time_ms": 3883.922333974624, "beir_fiqa_ndcg@10": 0.22624230705103332, "beir_fiqa_recall@100": 0.5261501444371814, "beir_fiqa_index_time_ms": 44129.586208000546, "beir_fiqa_query_time_ms": 7396.847625001101, "bright_earth_science_ndcg@10": 0.3492883481024951, "bright_earth_science_recall@100": 0.6647502376631234, "bright_earth_science_index_time_ms": 45675.92891701497, "bright_earth_science_query_time_ms": 7016.08612498967, "bright_stackoverflow_ndcg@10": 0.17314775534805105, "bright_stackoverflow_recall@100": 0.49296548663986156, "bright_stackoverflow_index_time_ms": 110573.41983402148, "bright_stackoverflow_query_time_ms": 11821.552250010427, "beir_trec-covid_ndcg@10": 0.6150065816695269, "beir_trec-covid_recall@100": 0.10473463934532519, "beir_trec-covid_index_time_ms": 152326.44541701302, "beir_trec-covid_query_time_ms": 1616.415250027785, "avg_ndcg@10": 0.2758702144858347, "avg_recall@100": 0.4676009441660897, "combined_score": 0.4292547982300387, "total_index_time_ms": 445873.9432931179, "total_query_time_ms": 74467.72133297054, "total_time_ms": 520341.66462608846, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 36 lines with 41 lines\nChange 2: Replace 107 lines with 112 lines\nChange 3: Replace 76 lines with 88 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3200468649013623, "beir_nfcorpus_recall@100": 0.25527972929225395, "beir_nfcorpus_index_time_ms": 3730.9816670021974, "beir_nfcorpus_query_time_ms": 104.74454198265448, "bright_pony_ndcg@10": 0.10206184664180705, "bright_pony_recall@100": 0.2906975569939365, "bright_pony_index_time_ms": 1504.448666004464, "bright_pony_query_time_ms": 586.403541994514, "beir_scifact_ndcg@10": 0.6749924182626882, "beir_scifact_recall@100": 0.932, "beir_scifact_index_time_ms": 5022.029166022548, "beir_scifact_query_time_ms": 374.3780410150066, "beir_arguana_ndcg@10": 0.27897096168745067, "beir_arguana_recall@100": 0.9114917915774446, "beir_arguana_index_time_ms": 6018.225208012154, "beir_arguana_query_time_ms": 29252.28425001842, "bright_theoremqa_theorems_ndcg@10": 0.040423220229772364, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 10863.940458017169, "bright_theoremqa_theorems_query_time_ms": 1203.9313330024015, "beir_scidocs_ndcg@10": 0.14317869851971954, "beir_scidocs_recall@100": 0.34298333333333336, "beir_scidocs_index_time_ms": 19813.92529202276, "beir_scidocs_query_time_ms": 3647.5089589948766, "bright_economics_ndcg@10": 0.14096139848729317, "bright_economics_recall@100": 0.390907487552556, "bright_economics_index_time_ms": 17039.452458993765, "bright_economics_query_time_ms": 3852.5785419915337, "bright_biology_ndcg@10": 0.24327326048284922, "bright_biology_recall@100": 0.545365592223027, "bright_biology_index_time_ms": 20535.623042000225, "bright_biology_query_time_ms": 3441.96137500694, "beir_fiqa_ndcg@10": 0.22599243658671295, "beir_fiqa_recall@100": 0.5253013790050827, "beir_fiqa_index_time_ms": 43185.21604099078, "beir_fiqa_query_time_ms": 7016.158082988113, "bright_earth_science_ndcg@10": 0.3513546941933217, "bright_earth_science_recall@100": 0.6647502376631234, "bright_earth_science_index_time_ms": 45724.76408298826, "bright_earth_science_query_time_ms": 6465.890708990628, "bright_stackoverflow_ndcg@10": 0.17266920793762008, "bright_stackoverflow_recall@100": 0.49296548663986156, "bright_stackoverflow_index_time_ms": 111039.5411659847, "bright_stackoverflow_query_time_ms": 10289.653624990024, "beir_trec-covid_ndcg@10": 0.6185599747268605, "beir_trec-covid_recall@100": 0.10472873117470223, "beir_trec-covid_index_time_ms": 150803.10637500952, "beir_trec-covid_query_time_ms": 1551.8009999941569, "avg_ndcg@10": 0.2760404152214548, "avg_recall@100": 0.4674983414487622, "combined_score": 0.42920675620330073, "total_index_time_ms": 435281.25362304854, "total_query_time_ms": 67787.29400096927, "total_time_ms": 503068.5476240178, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4292\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4292 \u2192 0.4292. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace 36 lines with 41 lines\nChange 2: Replace 13 lines with 16 lines\nChange 3: Replace 21 lines with 27 lines\nChange 4: Replace 21 lines with 28 lines\nChange 5: Replace 24 lines with 32 lines\nChange 6: Replace 20 lines with 27 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3200, beir_nfcorpus_recall@100: 0.2556, beir_nfcorpus_index_time_ms: 3906.2043, beir_nfcorpus_query_time_ms: 114.9515, bright_pony_ndcg@10: 0.1041, bright_pony_recall@100: 0.2925, bright_pony_index_time_ms: 1484.0520, bright_pony_query_time_ms: 651.6207, beir_scifact_ndcg@10: 0.6733, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5106.9241, beir_scifact_query_time_ms: 414.3820, beir_arguana_ndcg@10: 0.2789, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 6308.9597, beir_arguana_query_time_ms: 31888.3960, bright_theoremqa_theorems_ndcg@10: 0.0404, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 11167.7459, bright_theoremqa_theorems_query_time_ms: 1270.9665, beir_scidocs_ndcg@10: 0.1432, beir_scidocs_recall@100: 0.3432, beir_scidocs_index_time_ms: 20079.0888, beir_scidocs_query_time_ms: 4004.4494, bright_economics_ndcg@10: 0.1409, bright_economics_recall@100: 0.3909, bright_economics_index_time_ms: 20155.0085, bright_economics_query_time_ms: 4252.2882, bright_biology_ndcg@10: 0.2443, bright_biology_recall@100: 0.5454, bright_biology_index_time_ms: 20408.3955, bright_biology_query_time_ms: 4012.9714, beir_fiqa_ndcg@10: 0.2263, beir_fiqa_recall@100: 0.5262, beir_fiqa_index_time_ms: 46183.3432, beir_fiqa_query_time_ms: 7666.8556, bright_earth_science_ndcg@10: 0.3504, bright_earth_science_recall@100: 0.6643, bright_earth_science_index_time_ms: 46481.8213, bright_earth_science_query_time_ms: 7241.9472, bright_stackoverflow_ndcg@10: 0.1725, bright_stackoverflow_recall@100: 0.4930, bright_stackoverflow_index_time_ms: 114496.8051, bright_stackoverflow_query_time_ms: 12027.9551, beir_trec-covid_ndcg@10: 0.6164, beir_trec-covid_recall@100: 0.1047, beir_trec-covid_index_time_ms: 156640.2838, beir_trec-covid_query_time_ms: 1677.1991, avg_ndcg@10: 0.2759, avg_recall@100: 0.4675, combined_score: 0.4292, total_index_time_ms: 452418.6321, total_query_time_ms: 75223.9827, total_time_ms: 527642.6148, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace 18 lines with 20 lines\nChange 2: Replace 10 lines with 9 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3200, beir_nfcorpus_recall@100: 0.2557, beir_nfcorpus_index_time_ms: 3869.2824, beir_nfcorpus_query_time_ms: 109.5810, bright_pony_ndcg@10: 0.1022, bright_pony_recall@100: 0.2918, bright_pony_index_time_ms: 1452.5282, bright_pony_query_time_ms: 623.3708, beir_scifact_ndcg@10: 0.6749, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5103.6113, beir_scifact_query_time_ms: 418.9222, beir_arguana_ndcg@10: 0.2794, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 6129.9829, beir_arguana_query_time_ms: 30810.8848, bright_theoremqa_theorems_ndcg@10: 0.0404, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 11002.1853, bright_theoremqa_theorems_query_time_ms: 1232.2993, beir_scidocs_ndcg@10: 0.1433, beir_scidocs_recall@100: 0.3430, beir_scidocs_index_time_ms: 19832.2060, beir_scidocs_query_time_ms: 3911.7007, bright_biology_ndcg@10: 0.2439, bright_biology_recall@100: 0.5454, bright_biology_index_time_ms: 21227.3977, bright_biology_query_time_ms: 4555.5830, bright_economics_ndcg@10: 0.1410, bright_economics_recall@100: 0.3909, bright_economics_index_time_ms: 22992.3340, bright_economics_query_time_ms: 4710.3947, beir_fiqa_ndcg@10: 0.2262, beir_fiqa_recall@100: 0.5253, beir_fiqa_index_time_ms: 48810.5611, beir_fiqa_query_time_ms: 7618.9365, bright_earth_science_ndcg@10: 0.3509, bright_earth_science_recall@100: 0.6648, bright_earth_science_index_time_ms: 48583.2160, bright_earth_science_query_time_ms: 7283.6999, bright_stackoverflow_ndcg@10: 0.1731, bright_stackoverflow_recall@100: 0.4930, bright_stackoverflow_index_time_ms: 114759.1229, bright_stackoverflow_query_time_ms: 12821.6832, beir_trec-covid_ndcg@10: 0.6202, beir_trec-covid_recall@100: 0.1051, beir_trec-covid_index_time_ms: 160328.1247, beir_trec-covid_query_time_ms: 1658.9538, avg_ndcg@10: 0.2763, avg_recall@100: 0.4675, combined_score: 0.4292, total_index_time_ms: 464090.5525, total_query_time_ms: 75756.0099, total_time_ms: 539846.5625, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 35 lines with 40 lines\nChange 2: Replace 102 lines with 117 lines\nChange 3: Replace 77 lines with 92 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3199, beir_nfcorpus_recall@100: 0.2556, beir_nfcorpus_index_time_ms: 3943.6580, beir_nfcorpus_query_time_ms: 112.7444, bright_pony_ndcg@10: 0.1041, bright_pony_recall@100: 0.2925, bright_pony_index_time_ms: 1445.4964, bright_pony_query_time_ms: 653.8696, beir_scifact_ndcg@10: 0.6733, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5271.7055, beir_scifact_query_time_ms: 404.2423, beir_arguana_ndcg@10: 0.2790, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 6062.6266, beir_arguana_query_time_ms: 32133.3866, bright_theoremqa_theorems_ndcg@10: 0.0404, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10821.1346, bright_theoremqa_theorems_query_time_ms: 1290.8897, beir_scidocs_ndcg@10: 0.1432, beir_scidocs_recall@100: 0.3434, beir_scidocs_index_time_ms: 19971.3345, beir_scidocs_query_time_ms: 3973.4888, bright_economics_ndcg@10: 0.1409, bright_economics_recall@100: 0.3909, bright_economics_index_time_ms: 17370.9126, bright_economics_query_time_ms: 4169.3113, bright_biology_ndcg@10: 0.2460, bright_biology_recall@100: 0.5454, bright_biology_index_time_ms: 24702.9104, bright_biology_query_time_ms: 4138.0919, beir_fiqa_ndcg@10: 0.2262, beir_fiqa_recall@100: 0.5262, beir_fiqa_index_time_ms: 47079.5272, beir_fiqa_query_time_ms: 7783.2865, bright_earth_science_ndcg@10: 0.3493, bright_earth_science_recall@100: 0.6648, bright_earth_science_index_time_ms: 47970.0616, bright_earth_science_query_time_ms: 7514.5581, bright_stackoverflow_ndcg@10: 0.1731, bright_stackoverflow_recall@100: 0.4930, bright_stackoverflow_index_time_ms: 113742.8793, bright_stackoverflow_query_time_ms: 11864.0736, beir_trec-covid_ndcg@10: 0.6150, beir_trec-covid_recall@100: 0.1047, beir_trec-covid_index_time_ms: 155312.9804, beir_trec-covid_query_time_ms: 1630.9252, avg_ndcg@10: 0.2759, avg_recall@100: 0.4676, combined_score: 0.4293, total_index_time_ms: 453695.2272, total_query_time_ms: 75668.8679, total_time_ms: 529364.0951, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4293)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query \"peakiness\" gate for AND-like boosts (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: Information-mass coverage (soft-AND over *surprisal mass*).\n    # Intuition: for verbose queries, matching the high-information subset matters most for nDCG.\n    # We reward documents that cover a larger fraction of the query's IDF mass, but only as a\n    # bounded multiplier >= 1 to be recall-safe.\n    info_cov_alpha: float = 0.10   # small, to avoid overfitting\n    info_cov_gamma: float = 0.70   # concave: first informative matches help most\n    info_cov_tf_gamma: float = 0.50  # soften evidence inside coverage via sqrt(tf_sat)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Base evidence (saturated TF + focus-normalized length) with bounded \"AND-ish\" rewards.\n\n    Added signal: information-mass coverage (soft-AND over surprisal mass).\n    View shaped IDF as a proxy for surprisal; a query carries an \"information budget\"\n    sum_t IDF(t). Reward docs that cover a larger fraction of this budget, but keep it\n    bounded (multiplier >= 1) so recall isn't harmed.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query bookkeeping for peakiness gate + info-mass coverage.\n    q_sum = 0.0\n    q_max = 0.0\n    q_info = 0.0  # total (penalized) IDF mass\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        v = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        q_sum += v\n        q_info += v\n        if v > q_max:\n            q_max = v\n    spec = (q_max / (q_sum + eps)) if q_sum > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    match_info = 0.0  # evidence-weighted matched info mass\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        # Soft evidence inside coverage so one-off mentions don't dominate:\n        match_info += term_idf * (tf_part ** Config.info_cov_tf_gamma)\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs by residualizing vs max idf.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # Information-mass coverage reward (bounded, recall-safe).\n    if Config.info_cov_alpha > 0.0 and q_info > 0.0 and match_info > 0.0:\n        cov = match_info / (q_info + eps)  # ~[0,1]\n        cov = min(1.0, max(0.0, cov))\n        score *= 1.0 + Config.info_cov_alpha * (cov ** Config.info_cov_gamma)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query peakiness gate inputs + info-mass coverage bookkeeping.\n        q_sum = 0.0\n        q_max = 0.0\n        q_info = 0.0\n        match_info = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_sum += idf_val\n            q_info += idf_val\n            if idf_val > q_max:\n                q_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            match_info += idf_val * np.power(tf_part, Config.info_cov_tf_gamma) * present\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        spec = (q_max / (q_sum + eps)) if q_sum > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Mirror retrieval_score(): info-mass coverage reward (bounded, recall-safe).\n        if Config.info_cov_alpha > 0.0 and q_info > 0.0:\n            cov = match_info / (q_info + eps)\n            cov = np.minimum(1.0, np.maximum(0.0, cov))\n            scores *= 1.0 + Config.info_cov_alpha * np.power(cov, Config.info_cov_gamma)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3199), Performs well on beir_nfcorpus_recall@100 (0.2556), Performs well on beir_nfcorpus_index_time_ms (3853.3557), Performs well on beir_nfcorpus_query_time_ms (110.7611), Performs well on bright_pony_ndcg@10 (0.1041), Performs well on bright_pony_recall@100 (0.2925), Performs well on bright_pony_index_time_ms (1634.9604), Performs well on bright_pony_query_time_ms (644.9532), Performs well on beir_scifact_ndcg@10 (0.6733), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5325.2254), Performs well on beir_scifact_query_time_ms (410.5913), Performs well on beir_arguana_ndcg@10 (0.2790), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (6127.5980), Performs well on beir_arguana_query_time_ms (32107.7562), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10862.8145), Performs well on bright_theoremqa_theorems_query_time_ms (1295.2932), Performs well on beir_scidocs_ndcg@10 (0.1432), Performs well on beir_scidocs_recall@100 (0.3434), Performs well on beir_scidocs_index_time_ms (19909.3754), Performs well on beir_scidocs_query_time_ms (3989.3045), Performs well on bright_biology_ndcg@10 (0.2460), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (20964.5065), Performs well on bright_biology_query_time_ms (3964.3422), Performs well on bright_economics_ndcg@10 (0.1409), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (21695.9922), Performs well on bright_economics_query_time_ms (4160.2739), Performs well on beir_fiqa_ndcg@10 (0.2262), Performs well on beir_fiqa_recall@100 (0.5262), Performs well on beir_fiqa_index_time_ms (46272.8470), Performs well on beir_fiqa_query_time_ms (7825.5151), Performs well on bright_earth_science_ndcg@10 (0.3493), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (46792.6636), Performs well on bright_earth_science_query_time_ms (7462.1292), Performs well on bright_stackoverflow_ndcg@10 (0.1731), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (112082.8855), Performs well on bright_stackoverflow_query_time_ms (12131.5285), Performs well on beir_trec-covid_ndcg@10 (0.6150), Performs well on beir_trec-covid_recall@100 (0.1047), Performs well on beir_trec-covid_index_time_ms (155256.6162), Performs well on beir_trec-covid_query_time_ms (1635.8385), Performs well on avg_ndcg@10 (0.2759), Performs well on avg_recall@100 (0.4676), Performs well on combined_score (0.4293), Performs well on total_index_time_ms (450778.8403), Performs well on total_query_time_ms (75738.2871), Performs well on total_time_ms (526517.1275), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4293)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept modest; mainly helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms tend to be \"glue\"; softly downweight instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query-specificity gate for AND-like rewards (coordination + pair synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: \"information mass coverage\" multiplier (soft-AND over surprisal mass).\n    # Intuition: nDCG@10 often improves when top results cover a larger fraction of the\n    # query's *informative* terms. Keep bounded (multiplier >= 1) to be recall-safe.\n    info_cov_alpha: float = 0.10   # small, precision-oriented\n    info_cov_gamma: float = 0.70   # concave: early informative matches help most\n    info_cov_tf_gamma: float = 0.50  # evidence softening inside coverage (sqrt(tf_sat))\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + bounded coordination reward + light co-occurrence synergy.\n\n    Refinement: gate the \"AND-like\" rewards (coordination + pair synergy) by a\n    doc-independent query-specificity proxy.\n\n    NEW: information-mass coverage (soft-AND over *surprisal mass*).\n    Treat shaped IDF as a proxy for surprisal; reward docs that cover more of the\n    query's total surprisal mass with non-trivial evidence (tf saturation), but as a\n    bounded multiplier >= 1 to avoid harming recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query-specificity gate (computed with the same common-term penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_info = 0.0  # total penalized IDF mass (surprisal budget)\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        q_info += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0  # higher => peakier\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    match_info = 0.0  # evidence-weighted matched surprisal mass\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n\n        # Soft evidence: prevents one-off mentions of rare terms from dominating \"coverage\".\n        match_info += term_idf * (tf_part ** Config.info_cov_tf_gamma)\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # NEW: information-mass coverage multiplier (bounded, recall-safe).\n    if Config.info_cov_alpha > 0.0 and q_info > 0.0 and match_info > 0.0:\n        cov = match_info / (q_info + eps)  # ~[0,1]\n        cov = min(1.0, max(0.0, cov))\n        score *= 1.0 + Config.info_cov_alpha * (cov ** Config.info_cov_gamma)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Keep matched-term components for synergy (doc-major arrays per term).\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # For info-mass coverage multiplier.\n        q_info = 0.0\n        match_info = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_info += idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            # Evidence-weighted matched surprisal mass (softened by sqrt(tf_sat)).\n            match_info += idf_val * np.power(tf_part, Config.info_cov_tf_gamma) * present\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query-specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Co-occurrence synergy: emphasize distinctive pairs (residualized vs max-idf),\n        # but gate it by query specificity to avoid over-rewarding broad modifiers.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Mirror retrieval_score(): info-mass coverage multiplier (bounded).\n        if Config.info_cov_alpha > 0.0 and q_info > 0.0:\n            cov = match_info / (q_info + eps)\n            cov = np.minimum(1.0, np.maximum(0.0, cov))\n            scores *= 1.0 + Config.info_cov_alpha * np.power(cov, Config.info_cov_gamma)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3199), Performs well on beir_nfcorpus_recall@100 (0.2556), Performs well on beir_nfcorpus_index_time_ms (3943.6580), Performs well on beir_nfcorpus_query_time_ms (112.7444), Performs well on bright_pony_ndcg@10 (0.1041), Performs well on bright_pony_recall@100 (0.2925), Performs well on bright_pony_index_time_ms (1445.4964), Performs well on bright_pony_query_time_ms (653.8696), Performs well on beir_scifact_ndcg@10 (0.6733), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5271.7055), Performs well on beir_scifact_query_time_ms (404.2423), Performs well on beir_arguana_ndcg@10 (0.2790), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (6062.6266), Performs well on beir_arguana_query_time_ms (32133.3866), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10821.1346), Performs well on bright_theoremqa_theorems_query_time_ms (1290.8897), Performs well on beir_scidocs_ndcg@10 (0.1432), Performs well on beir_scidocs_recall@100 (0.3434), Performs well on beir_scidocs_index_time_ms (19971.3345), Performs well on beir_scidocs_query_time_ms (3973.4888), Performs well on bright_economics_ndcg@10 (0.1409), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (17370.9126), Performs well on bright_economics_query_time_ms (4169.3113), Performs well on bright_biology_ndcg@10 (0.2460), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (24702.9104), Performs well on bright_biology_query_time_ms (4138.0919), Performs well on beir_fiqa_ndcg@10 (0.2262), Performs well on beir_fiqa_recall@100 (0.5262), Performs well on beir_fiqa_index_time_ms (47079.5272), Performs well on beir_fiqa_query_time_ms (7783.2865), Performs well on bright_earth_science_ndcg@10 (0.3493), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (47970.0616), Performs well on bright_earth_science_query_time_ms (7514.5581), Performs well on bright_stackoverflow_ndcg@10 (0.1731), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (113742.8793), Performs well on bright_stackoverflow_query_time_ms (11864.0736), Performs well on beir_trec-covid_ndcg@10 (0.6150), Performs well on beir_trec-covid_recall@100 (0.1047), Performs well on beir_trec-covid_index_time_ms (155312.9804), Performs well on beir_trec-covid_query_time_ms (1630.9252), Performs well on avg_ndcg@10 (0.2759), Performs well on avg_recall@100 (0.4676), Performs well on combined_score (0.4293), Performs well on total_index_time_ms (453695.2272), Performs well on total_query_time_ms (75668.8679), Performs well on total_time_ms (529364.0951), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: super-additive evidence when multiple distinctive\n    # query terms are present in the same doc (a proxy for topical coherence).\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query-specificity gate for \"AND-like\" effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: idf-mass coverage reward (soft-AND over *information*, not term count).\n    # Helps nDCG@10 on verbose queries where matching the \"important\" terms matters most.\n    cov_alpha: float = 0.18\n    cov_beta: float = 0.85\n    cov_idf_power: float = 1.0\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with a \"focus prior\" + bounded coordination reward,\n    plus two precision-oriented refinements:\n\n    (1) common-term soft penalty: ultra-high-df terms behave like glue and create noisy ties.\n    (2) distinctive co-occurrence synergy: weakly reward multi-term matches when the matched\n        terms are distinctive (a proxy for topical coherence without positions).\n\n    NEW: query-specificity gating for the AND-like effects (coord + synergy).\n    Intuition: queries with one very distinctive term + several broad modifiers are common;\n    naive coordination/synergy then over-rewards generic docs that match many modifiers.\n    Gate these effects down for \"peaky\" queries using spec = max_idf/sum_idf.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Precompute query specificity gate (doc-independent).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        # Two complementary \"AND-like\" notions:\n        # (A) term-count coverage (good for short factoid queries)\n        # (B) Evidence-weighted IDF-mass coverage:\n        #     cover \"important\" terms *with strength* (tf saturation), not just presence.\n        cov_count = matched / (qn + eps)\n\n        cov_idf = 0.0\n        if q_idf_sum > 0.0 and m_idf:\n            # Evidence-weighted matched info mass: sum(idf(term) * tf_part(term, doc)).\n            # This is a soft-AND over *information actually supported by the document*,\n            # reducing over-reward for one-off mentions of rare terms.\n            matched_mass = 0.0\n            for v, tp in zip(m_idf, m_tfpart):\n                matched_mass += v * tp\n            cov_idf = (matched_mass / (q_idf_sum + eps)) ** Config.cov_idf_power\n\n        coord = (1.0 + (Config.coord_alpha * spec_gate) * cov_count) ** Config.coord_beta\n        covr = (1.0 + (Config.cov_alpha * spec_gate) * cov_idf) ** Config.cov_beta\n        score *= coord * covr\n\n    # Co-occurrence synergy: emphasize distinctive pairs by residualizing vs max idf.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # For the query-specificity gate we need query-level idf stats (after common penalty).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            cov_count = matched / (qn + eps)\n\n            # Evidence-weighted IDF-mass coverage in vectorized form.\n            # matched_mass(doc) = sum(idf(term) * tf_part(term, doc))\n            cov_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n            if q_idf_sum > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                tfp = np.vstack(tfparts) if len(tfparts) > 0 else None\n                if tfp is not None:\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    cov_idf = np.power(matched_mass / (q_idf_sum + eps), Config.cov_idf_power)\n\n            coord = np.power(1.0 + (Config.coord_alpha * spec_gate) * cov_count, Config.coord_beta)\n            covr = np.power(1.0 + (Config.cov_alpha * spec_gate) * cov_idf, Config.cov_beta)\n            scores *= coord * covr\n\n        # Mirror retrieval_score(): distinctive-pair synergy (gated by specificity).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3200), Performs well on beir_nfcorpus_recall@100 (0.2557), Performs well on beir_nfcorpus_index_time_ms (3869.2824), Performs well on beir_nfcorpus_query_time_ms (109.5810), Performs well on bright_pony_ndcg@10 (0.1022), Performs well on bright_pony_recall@100 (0.2918), Performs well on bright_pony_index_time_ms (1452.5282), Performs well on bright_pony_query_time_ms (623.3708), Performs well on beir_scifact_ndcg@10 (0.6749), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5103.6113), Performs well on beir_scifact_query_time_ms (418.9222), Performs well on beir_arguana_ndcg@10 (0.2794), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (6129.9829), Performs well on beir_arguana_query_time_ms (30810.8848), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11002.1853), Performs well on bright_theoremqa_theorems_query_time_ms (1232.2993), Performs well on beir_scidocs_ndcg@10 (0.1433), Performs well on beir_scidocs_recall@100 (0.3430), Performs well on beir_scidocs_index_time_ms (19832.2060), Performs well on beir_scidocs_query_time_ms (3911.7007), Performs well on bright_biology_ndcg@10 (0.2439), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (21227.3977), Performs well on bright_biology_query_time_ms (4555.5830), Performs well on bright_economics_ndcg@10 (0.1410), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (22992.3340), Performs well on bright_economics_query_time_ms (4710.3947), Performs well on beir_fiqa_ndcg@10 (0.2262), Performs well on beir_fiqa_recall@100 (0.5253), Performs well on beir_fiqa_index_time_ms (48810.5611), Performs well on beir_fiqa_query_time_ms (7618.9365), Performs well on bright_earth_science_ndcg@10 (0.3509), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (48583.2160), Performs well on bright_earth_science_query_time_ms (7283.6999), Performs well on bright_stackoverflow_ndcg@10 (0.1731), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (114759.1229), Performs well on bright_stackoverflow_query_time_ms (12821.6832), Performs well on beir_trec-covid_ndcg@10 (0.6202), Performs well on beir_trec-covid_recall@100 (0.1051), Performs well on beir_trec-covid_index_time_ms (160328.1247), Performs well on beir_trec-covid_query_time_ms (1658.9538), Performs well on avg_ndcg@10 (0.2763), Performs well on avg_recall@100 (0.4675), Performs well on combined_score (0.4292), Performs well on total_index_time_ms (464090.5525), Performs well on total_query_time_ms (75756.0099), Performs well on total_time_ms (539846.5625), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: information-mass coverage reward (soft-AND over surprisal mass).\n    # Treat shaped IDF as term surprisal. Reward docs covering more of the query's\n    # total surprisal mass, but as a bounded multiplier >= 1 (recall-safe).\n    info_cov_alpha: float = 0.12   # strength (keep small)\n    info_cov_beta: float = 0.70    # concave: early informative matches matter most\n    info_cov_tf_gamma: float = 0.50  # soften evidence via sqrt(tf_sat)\n    info_cov_gate_by_spec: bool = True  # disable for peaky queries\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base signals as before, but with a *query-specificity gate* on the\n    \"AND-like\" effects (coordination + pair synergy).\n\n    Why this can help:\n    - Many BEIR/BRIGHT queries contain one sharp entity/term + several broad modifiers.\n      Coordination/pair boosts can then over-promote generic docs that match many broad terms.\n    - Use a simple, doc-independent specificity proxy:\n        spec = max_idf / sum_idf  (peaky queries -> larger spec)\n      We down-gate coordination/synergy for peaky queries, keeping recall from the\n      main additive evidence but improving nDCG@10 by reducing noisy \"multi-match\" wins.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    # Also accumulate query \"information mass\" for the coverage multiplier.\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_info = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        q_info += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    match_info = 0.0  # evidence-weighted matched information mass\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n\n        # Soft evidence inside coverage so one-off rare mentions don't dominate.\n        match_info += term_idf * (tf_part ** Config.info_cov_tf_gamma)\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, but gate by query specificity too.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # NEW: information-mass coverage reward (bounded multiplier, recall-safe).\n    if Config.info_cov_alpha > 0.0 and q_info > 0.0 and match_info > 0.0:\n        cov = match_info / (q_info + eps)  # ~[0,1]\n        cov = min(1.0, max(0.0, cov))\n        gate = spec_gate if Config.info_cov_gate_by_spec else 1.0\n        score *= 1.0 + (Config.info_cov_alpha * gate) * (cov ** Config.info_cov_beta)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Info-mass coverage bookkeeping (doc-independent q_info, doc-dependent match_info).\n        q_info = 0.0\n        match_info = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_info += idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            match_info += idf_val * np.power(tf_part, Config.info_cov_tf_gamma) * present\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Mirror retrieval_score(): info-mass coverage reward.\n        if Config.info_cov_alpha > 0.0 and q_info > 0.0:\n            cov = match_info / (q_info + eps)\n            cov = np.minimum(1.0, np.maximum(0.0, cov))\n            gate = float(spec_gate) if Config.info_cov_gate_by_spec else 1.0\n            scores *= 1.0 + (Config.info_cov_alpha * gate) * np.power(cov, Config.info_cov_beta)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3200), Performs well on beir_nfcorpus_recall@100 (0.2556), Performs well on beir_nfcorpus_index_time_ms (3906.2043), Performs well on beir_nfcorpus_query_time_ms (114.9515), Performs well on bright_pony_ndcg@10 (0.1041), Performs well on bright_pony_recall@100 (0.2925), Performs well on bright_pony_index_time_ms (1484.0520), Performs well on bright_pony_query_time_ms (651.6207), Performs well on beir_scifact_ndcg@10 (0.6733), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5106.9241), Performs well on beir_scifact_query_time_ms (414.3820), Performs well on beir_arguana_ndcg@10 (0.2789), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (6308.9597), Performs well on beir_arguana_query_time_ms (31888.3960), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11167.7459), Performs well on bright_theoremqa_theorems_query_time_ms (1270.9665), Performs well on beir_scidocs_ndcg@10 (0.1432), Performs well on beir_scidocs_recall@100 (0.3432), Performs well on beir_scidocs_index_time_ms (20079.0888), Performs well on beir_scidocs_query_time_ms (4004.4494), Performs well on bright_economics_ndcg@10 (0.1409), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (20155.0085), Performs well on bright_economics_query_time_ms (4252.2882), Performs well on bright_biology_ndcg@10 (0.2443), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (20408.3955), Performs well on bright_biology_query_time_ms (4012.9714), Performs well on beir_fiqa_ndcg@10 (0.2263), Performs well on beir_fiqa_recall@100 (0.5262), Performs well on beir_fiqa_index_time_ms (46183.3432), Performs well on beir_fiqa_query_time_ms (7666.8556), Performs well on bright_earth_science_ndcg@10 (0.3504), Performs well on bright_earth_science_recall@100 (0.6643), Performs well on bright_earth_science_index_time_ms (46481.8213), Performs well on bright_earth_science_query_time_ms (7241.9472), Performs well on bright_stackoverflow_ndcg@10 (0.1725), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (114496.8051), Performs well on bright_stackoverflow_query_time_ms (12027.9551), Performs well on beir_trec-covid_ndcg@10 (0.6164), Performs well on beir_trec-covid_recall@100 (0.1047), Performs well on beir_trec-covid_index_time_ms (156640.2838), Performs well on beir_trec-covid_query_time_ms (1677.1991), Performs well on avg_ndcg@10 (0.2759), Performs well on avg_recall@100 (0.4675), Performs well on combined_score (0.4292), Performs well on total_index_time_ms (452418.6321), Performs well on total_query_time_ms (75223.9827), Performs well on total_time_ms (527642.6148), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # NEW: query specificity gating for \"AND-like\" effects.\n    # Intuition: when a query is dominated by one highly distinctive term + many vague terms,\n    # coordination/pairs can over-reward noisy matches. Gate those effects by specificity.\n    # spec = max_idf / sum_idf in [~1/|q|, 1]. Higher => more \"peaky\" query.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base signals as before, but with a *query-specificity gate* on the\n    \"AND-like\" effects (coordination + pair synergy).\n\n    Why this can help:\n    - Many BEIR/BRIGHT queries contain one sharp entity/term + several broad modifiers.\n      Coordination/pair boosts can then over-promote generic docs that match many broad terms.\n    - Use a simple, doc-independent specificity proxy:\n        spec = max_idf / sum_idf  (peaky queries -> larger spec)\n      We down-gate coordination/synergy for peaky queries, keeping recall from the\n      main additive evidence but improving nDCG@10 by reducing noisy \"multi-match\" wins.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, but gate by query specificity too.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n    focus_mix: float = 0.65\n    idf_power: float = 1.12\n\n    # Soft-AND reward; keep as-is but gate by query \"peakiness\"\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    use_log_qtf: bool = True\n\n    # Common-term soft penalty\n    common_df_cut: float = 0.12\n    common_penalty: float = 0.35\n\n    # Pair synergy; also gated by query peakiness\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # NEW: peakiness gate for \"AND-like\" effects (coord + pair).\n    # spec = max_idf / sum_idf; peaky queries (one dominant term) => smaller gate.\n    spec_floor: float = 0.55\n    spec_power: float = 1.2\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base model, with a *query-peakiness gate* on coordination + pair synergy.\n\n    Why: many queries contain one very distinctive anchor term plus several broad modifiers.\n    Raw coordination/pair boosts can over-promote generic docs matching many broad terms.\n    Gate those \"AND-like\" effects by spec = max_idf/sum_idf (peaky => smaller gate).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query peakiness (doc-independent) using same shaped IDF + common penalty.\n    q_sum = 0.0\n    q_max = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        v = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        q_sum += v\n        if v > q_max:\n            q_max = v\n    spec = (q_max / (q_sum + eps)) if q_sum > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Peakiness gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D3 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12\n    common_penalty: float = 0.35\n\n    # Gate \"AND-like\" rewards (coord + pair) by query peakiness:\n    # spec = max_idf / sum_idf. If one term dominates (peaky query), coordination/pairs\n    # tend to over-promote generic docs matching many broad modifiers.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base model, but gate \"AND-like\" boosts (coordination + pair synergy)\n    by a doc-independent query peakiness proxy:\n\n        spec = max_idf / sum_idf\n\n    Intuition: many queries have one sharp anchor + broad modifiers; rewarding\n    raw coverage/pairs can then over-rank generic docs. Gating protects nDCG@10\n    while keeping recall from the additive evidence.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query peakiness gate computed using the same shaped IDF (+ common-term penalty).\n    q_sum = 0.0\n    q_max = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        v = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        q_sum += v\n        if v > q_max:\n            q_max = v\n    spec = (q_max / (q_sum + eps)) if q_sum > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query peakiness gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.asarray(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    # (A bounded \"AND-ish\" effect that usually helps nDCG@10 without killing recall.)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards.\n\n    Design:\n    - Base evidence: BM25-like saturating TF evidence (diminishing returns).\n    - Length prior: pivoted normalization on a blend of token length and unique-term length\n      (prefers focused docs without crushing long informative ones).\n    - Collection prior: smoothly downweight ultra-common (high-df) terms.\n    - Coordination + distinctive pair synergy: multiplicative rewards gated by query specificity\n      (avoid over-rewarding generic multi-matches when a query is dominated by one sharp term).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms.\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, gated by query specificity.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4287, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Informative-mass coverage reward (bounded multiplier >= 1).\n    info_cov_alpha: float = 0.12\n    info_cov_gamma: float = 0.75\n    info_cov_tf_gamma: float = 0.50\n\n    # \"Elitist\" coverage reward.\n    elite_frac: float = 0.45\n    elite_alpha: float = 0.16\n    elite_gamma: float = 0.75\n\n    # NEW: proximity/phrase hint via adjacent query bigrams, computed on token-IDs.\n    # Intuition: exact adjacency is a strong lexical cue, especially for entity/technical queries.\n    # Keep bounded and small to avoid harming recall.\n    bigram_alpha: float = 0.10\n    bigram_gamma: float = 0.80  # concave: one hit already helps a lot\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms (deduped) + sublinear query-TF, while preserving\n        the query's original term order (important for adjacency/bigram signals).\n        \"\"\"\n        c = Counter(tokens)\n\n        # Preserve first-occurrence order (Counter(keys) order is not the original token order).\n        seen: set[str] = set()\n        terms: list[str] = []\n        for t in tokens:\n            if t not in seen:\n                seen.add(t)\n                terms.append(t)\n\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    corpus: \"Corpus\",\n    doc_idx: int,\n) -> float:\n    \"\"\"\n    Base BM25-like evidence + (A) peakiness-gated coordination/pair synergy\n    + (B) bounded informative-mass coverage reward + (C) elite anchor coverage.\n\n    (B) Interpretation: shaped IDF approximates term surprisal; sum(IDF) is query\n    information mass. Reward docs covering a larger fraction of that mass.\n    This is recall-safe (multiplier >= 1) and tends to improve nDCG@10 on noisy queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity/anchor bookkeeping (doc-independent) + info mass.\n    q_idfs: list[float] = []\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_info = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        q_info += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Matched info mass (softened by TF saturation).\n    match_info = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        match_info += term_idf * (tf_part ** Config.info_cov_tf_gamma)\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Proximity/phrase hint: reward adjacent query-term bigrams that appear in the doc.\n        # Uses token-ID bigram hashes precomputed in the corpus; bounded multiplier >= 1.\n        if Config.bigram_alpha > 0.0 and hasattr(corpus, \"_doc_bigrams\"):\n            qb = 0.0\n            hit = 0.0\n            bg = corpus._doc_bigrams[doc_idx]\n            for a, b in zip(query_repr.terms, query_repr.terms[1:]):\n                ta = corpus.get_term_id(a)\n                tb = corpus.get_term_id(b)\n                if ta is None or tb is None:\n                    continue\n                qb += 1.0\n                if (int(ta) ^ (int(tb) * 16777619)) in bg:\n                    hit += 1.0\n            if qb > 0.0 and hit > 0.0:\n                frac = hit / (qb + eps)\n                score *= 1.0 + Config.bigram_alpha * (frac ** Config.bigram_gamma)\n\n        # Elite/anchor reward: reward matching top-IDF query terms.\n        if Config.elite_alpha > 0.0 and q_idf_sum > 0.0:\n            m = int(max(1, math.ceil(Config.elite_frac * qn)))\n            qidf_arr = np.asarray(q_idfs, dtype=np.float64)\n            elite_idx = np.argsort(-qidf_arr)[:m]\n            elite_total = float(np.sum(qidf_arr[elite_idx]))\n            if elite_total > 0.0:\n                elite_hit = 0.0\n                for j in elite_idx.tolist():\n                    if doc_tf.get(query_repr.terms[j], 0) > 0:\n                        elite_hit += q_idfs[j]\n                elite_cov = elite_hit / (elite_total + eps)\n                score *= 1.0 + Config.elite_alpha * (elite_cov ** Config.elite_gamma)\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # Informative-mass coverage reward (bounded, recall-safe).\n    if Config.info_cov_alpha > 0.0 and q_info > 0.0 and match_info > 0.0:\n        cov = match_info / (q_info + eps)  # ~[0,1]\n        cov = min(1.0, max(0.0, cov))\n        score *= 1.0 + Config.info_cov_alpha * (cov ** Config.info_cov_gamma)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency, corpus, doc_idx)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        # Precompute doc bigram hashes on term-IDs for a cheap phrase/proximity hint.\n        # hash(a,b) = a ^ (b * 16777619) (simple mix; stable within the run).\n        self._doc_bigrams: list[set[int]] = []\n        for doc in documents:\n            s: set[int] = set()\n            for a, b in zip(doc, doc[1:]):\n                ta = self._vocab.get(a)\n                tb = self._vocab.get(b)\n                if ta is None or tb is None:\n                    continue\n                s.add(int(ta) ^ (int(tb) * 16777619))\n            self._doc_bigrams.append(s)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        # Informative-mass coverage bookkeeping (vectorized).\n        q_info = 0.0\n        match_info = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_info += idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            match_info += idf_val * np.power(tf_part, Config.info_cov_tf_gamma) * present\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # Mirror retrieval_score(): proximity/phrase hint via adjacent query bigrams.\n            if Config.bigram_alpha > 0.0 and hasattr(self.corpus, \"_doc_bigrams\") and len(query_term_ids) >= 2:\n                qb = float(len(query_term_ids) - 1)\n                if qb > 0.0:\n                    qpair_hash = [int(query_term_ids[i]) ^ (int(query_term_ids[i + 1]) * 16777619) for i in range(len(query_term_ids) - 1)]\n                    add = np.zeros(len(candidate_docs), dtype=np.float64)\n                    bg = self.corpus._doc_bigrams\n                    for di, d in enumerate(candidate_docs.tolist()):\n                        s = bg[int(d)]\n                        h = 0.0\n                        for ph in qpair_hash:\n                            if ph in s:\n                                h += 1.0\n                        if h > 0.0:\n                            add[di] = (h / qb) ** Config.bigram_gamma\n                    scores *= 1.0 + Config.bigram_alpha * add\n\n            # Mirror retrieval_score(): anchor/elite reward (top-IDF query terms).\n            if Config.elite_alpha > 0.0 and q_idf_sum > 0.0 and len(idfs) > 0:\n                m_elite = int(max(1, math.ceil(Config.elite_frac * qn)))\n                qidf_arr = np.asarray(idfs, dtype=np.float64)\n                elite_idx = np.argsort(-qidf_arr)[:m_elite]\n                elite_total = float(np.sum(qidf_arr[elite_idx]))\n                if elite_total > 0.0:\n                    elite_hit = np.zeros(len(candidate_docs), dtype=np.float64)\n                    elite_set = set(elite_idx.tolist())\n                    for ii in range(len(idfs)):\n                        if ii in elite_set:\n                            elite_hit += presents[ii] * qidf_arr[ii]\n                    elite_cov = elite_hit / (elite_total + eps)\n                    scores *= 1.0 + Config.elite_alpha * np.power(elite_cov, Config.elite_gamma)\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Mirror retrieval_score(): informative-mass coverage reward.\n        if Config.info_cov_alpha > 0.0 and q_info > 0.0:\n            cov = match_info / (q_info + eps)\n            cov = np.minimum(1.0, np.maximum(0.0, cov))\n            scores *= 1.0 + Config.info_cov_alpha * np.power(cov, Config.info_cov_gamma)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms in original order + optional log-qtf\n        term_counts = Counter(query)\n        seen: set[str] = set()\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n        for term in query:\n            if term in seen:\n                continue\n            seen.add(term)\n            tid = self.corpus.get_term_id(term)\n            if tid is None:\n                continue\n            query_term_ids.append(tid)\n            count = float(term_counts[term])\n            if Config.use_log_qtf:\n                query_term_weights.append(1.0 + math.log(count))\n            else:\n                query_term_weights.append(count)\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4290, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n    focus_mix: float = 0.65\n    idf_power: float = 1.12\n\n    # Soft-AND reward; keep as-is but gate by query \"peakiness\"\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    use_log_qtf: bool = True\n\n    # Common-term soft penalty\n    common_df_cut: float = 0.12\n    common_penalty: float = 0.35\n\n    # Pair synergy; also gated by query peakiness\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Peakiness gate for \"AND-like\" effects (coord + pair).\n    # spec = max_idf / sum_idf; peaky queries (one dominant term) => smaller gate.\n    spec_floor: float = 0.55\n    spec_power: float = 1.2\n\n    # NEW: \"anchor consistency\" reward.\n    # If a query has a single dominant anchor term (very peaky), prefer documents\n    # that do NOT drown that anchor in lots of weak modifier-only matches.\n    # Implemented as a bounded multiplier based on anchor_contrast = anchor_contrib / total_score.\n    anchor_alpha: float = 0.20\n    anchor_power: float = 0.70\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Add a *peaky-query anchor-consistency* reward on top of the current scheme.\n\n    Motivation (targets nDCG@10 without hurting recall@100 much):\n    - For peaky queries (one dominant IDF anchor + broad modifiers), coordination/pair boosts\n      can still over-rank generic docs that match many modifiers.\n    - We explicitly reward documents where the dominant anchor accounts for a healthy fraction\n      of the total evidence (anchor_contrib / total_score). This promotes \"about the anchor\"\n      documents, not merely ones that happen to contain many broad terms.\n    - The reward is bounded (>=1) and gated by peakiness, so it does not distort balanced queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query peakiness (doc-independent) using same shaped IDF + common penalty.\n    q_sum = 0.0\n    q_max = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        v = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        q_sum += v\n        if v > q_max:\n            q_max = v\n    spec = (q_max / (q_sum + eps)) if q_sum > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Track dominant-anchor contribution (dominant by query-side IDF, not doc-side).\n    anchor_term = None\n    anchor_qidf = -1.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        v = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        if v > anchor_qidf:\n            anchor_qidf = v\n            anchor_term = t\n\n    anchor_contrib = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        c = wq * term_idf * tf_part\n        score += c\n        if anchor_term is not None and term == anchor_term:\n            anchor_contrib += c\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # NEW: anchor-consistency reward (only meaningful for peaky queries).\n    # For very peaky queries, spec is large => spec_gate small; use (1-spec_gate) as activation.\n    if Config.anchor_alpha > 0.0 and anchor_contrib > 0.0:\n        peaky = max(0.0, 1.0 - spec_gate)\n        frac = anchor_contrib / (score + eps)\n        score *= 1.0 + Config.anchor_alpha * peaky * (frac ** Config.anchor_power)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Peakiness gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Mirror retrieval_score(): anchor-consistency reward.\n        if Config.anchor_alpha > 0.0 and len(idfs) > 0:\n            peaky = max(0.0, 1.0 - spec_gate)\n            # Anchor term = max IDF query term as seen in vectorized loop.\n            anchor_i = int(np.argmax(np.asarray(idfs, dtype=np.float64)))\n            anchor_contrib = ws[anchor_i] * idfs[anchor_i] * tfparts[anchor_i]\n            frac = anchor_contrib / (scores + eps)\n            scores *= 1.0 + Config.anchor_alpha * peaky * np.power(frac, Config.anchor_power)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4292, Type: Migrant)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    # (A bounded \"AND-ish\" effect that usually helps nDCG@10 without killing recall.)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards.\n\n    Design:\n    - Base evidence: BM25-like saturating TF evidence (diminishing returns).\n    - Length prior: pivoted normalization on a blend of token length and unique-term length\n      (prefers focused docs without crushing long informative ones).\n    - Collection prior: smoothly downweight ultra-common (high-df) terms.\n    - Coordination + distinctive pair synergy: multiplicative rewards gated by query specificity\n      (avoid over-rewarding generic multi-matches when a query is dominated by one sharp term).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms.\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, gated by query specificity.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4292, Type: Migrant)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept modest; mainly helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms tend to be \"glue\"; softly downweight instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # NEW: query-specificity gate for AND-like rewards (coordination + pair synergy).\n    # Intuition: \"peaky\" queries (one rare anchor + many broad modifiers) should not\n    # get strong multi-term rewards, else generic docs that match many modifiers win.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + bounded coordination reward + light co-occurrence synergy.\n\n    Refinement: gate the \"AND-like\" rewards (coordination + pair synergy) by a\n    doc-independent query-specificity proxy.\n    - Balanced multi-term queries: rewarding coverage helps nDCG@10.\n    - Peaky queries (one rare anchor + broad modifiers): too much coverage reward\n      promotes generic docs; gating protects nDCG@10 while preserving recall via\n      the base additive evidence.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query-specificity gate (computed with the same common-term penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0  # higher => peakier\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Keep matched-term components for synergy (doc-major arrays per term).\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query-specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Co-occurrence synergy: emphasize distinctive pairs (residualized vs max-idf),\n        # but gate it by query specificity to avoid over-rewarding broad modifiers.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # NEW: query specificity gating for \"AND-like\" effects.\n    # Intuition: when a query is dominated by one highly distinctive term + many vague terms,\n    # coordination/pairs can over-reward noisy matches. Gate those effects by specificity.\n    # spec = max_idf / sum_idf in [~1/|q|, 1]. Higher => more \"peaky\" query.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base signals as before, but with a *query-specificity gate* on the\n    \"AND-like\" effects (coordination + pair synergy).\n\n    Why this can help:\n    - Many BEIR/BRIGHT queries contain one sharp entity/term + several broad modifiers.\n      Coordination/pair boosts can then over-promote generic docs that match many broad terms.\n    - Use a simple, doc-independent specificity proxy:\n        spec = max_idf / sum_idf  (peaky queries -> larger spec)\n      We down-gate coordination/synergy for peaky queries, keeping recall from the\n      main additive evidence but improving nDCG@10 by reducing noisy \"multi-match\" wins.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, but gate by query specificity too.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # NEW: query specificity gating for \"AND-like\" effects.\n    # Intuition: when a query is dominated by one highly distinctive term + many vague terms,\n    # coordination/pairs can over-reward noisy matches. Gate those effects by specificity.\n    # spec = max_idf / sum_idf in [~1/|q|, 1]. Higher => more \"peaky\" query.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n=======\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # --- New: bounded \"informative-mass coverage\" reward (recall-safe) ---\n    # Intuition: queries have an \"information budget\" sum(IDF). A good doc covers a large\n    # fraction of that budget (especially on verbose/noisy queries), improving nDCG@10.\n    # Implement as a multiplier >= 1, so recall@100 is usually not harmed.\n    info_cov_alpha: float = 0.10   # small strength\n    info_cov_gamma: float = 0.70   # concave: first informative matches matter most\n    info_cov_tf_gamma: float = 0.50  # soften evidence via sqrt(tf_sat)\n\n    epsilon: float = 1e-9\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base signals as before, but with a *query-specificity gate* on the\n    \"AND-like\" effects (coordination + pair synergy).\n\n    Why this can help:\n    - Many BEIR/BRIGHT queries contain one sharp entity/term + several broad modifiers.\n      Coordination/pair boosts can then over-promote generic docs that match many broad terms.\n    - Use a simple, doc-independent specificity proxy:\n        spec = max_idf / sum_idf  (peaky queries -> larger spec)\n      We down-gate coordination/synergy for peaky queries, keeping recall from the\n      main additive evidence but improving nDCG@10 by reducing noisy \"multi-match\" wins.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, but gate by query specificity too.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive saturated evidence + (A) peakiness-gated coordination/pair synergy\n    + (B) bounded informative-mass coverage.\n\n    (B) Treat shaped IDF as a proxy for term surprisal. A query has total surprisal mass\n    sum_t IDF(t); reward documents that cover a larger fraction of that mass with\n    non-trivial evidence (tf saturation). Keep it as a multiplier >= 1 to stay recall-safe.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate + total info mass (uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_info = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        q_info += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    match_info = 0.0  # evidence-weighted matched info mass\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        # Soft evidence prevents one-off mentions of rare terms from dominating \"coverage\".\n        match_info += term_idf * (tf_part ** Config.info_cov_tf_gamma)\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # Bounded informative-mass coverage reward (multiplier >= 1).\n    if Config.info_cov_alpha > 0.0 and q_info > 0.0 and match_info > 0.0:\n        cov = match_info / (q_info + eps)  # ~[0,1]\n        cov = min(1.0, max(0.0, cov))\n        score *= 1.0 + Config.info_cov_alpha * (cov ** Config.info_cov_gamma)\n\n    return score\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Info-mass coverage bookkeeping (doc-independent total q_info, doc-dependent match_info).\n        q_info = 0.0\n        match_info = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_info += idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            # Evidence-weighted matched info mass (softened by sqrt(tf_sat)).\n            match_info += idf_val * np.power(tf_part, Config.info_cov_tf_gamma) * present\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Mirror retrieval_score(): bounded informative-mass coverage reward.\n        if Config.info_cov_alpha > 0.0 and q_info > 0.0:\n            cov = match_info / (q_info + eps)\n            cov = np.minimum(1.0, np.maximum(0.0, cov))\n            scores *= 1.0 + Config.info_cov_alpha * np.power(cov, Config.info_cov_gamma)\n\n        return scores\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}