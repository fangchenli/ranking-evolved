{"id": "8b462f16-1e93-4f1c-b452-0b2fa7257342", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # --- NEW: discriminativity shaping ---\n    # Convert raw IDF into a *lift* over the collection's average IDF:\n    #   idf_lift = idf / mean_idf\n    # This makes \"important\" mean \"more discriminative than average\" and reduces\n    # dataset-to-dataset drift where absolute idf scale differs.\n    idf_lift_power: float = 0.45  # 0 disables lift; small power keeps it gentle\n\n    # --- NEW: query DF dropout (only for long/noisy queries) ---\n    # For long queries, extremely common tokens behave like glue and increase false positives.\n    # We drop terms with df/N above threshold, but only when query length >= q_drop_min_len.\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    # NEW: for long queries, drop ultra-common tokens (df/N above threshold).\n    # NEW: apply an IDF \"lift\" normalization vs mean IDF to stabilize importance across corpora.\n    q_idfs: list[float] = []\n    q_terms: list[str] = []\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Approx mean idf: compute from corpus_df on the fly if corpus doesn't provide it.\n    # (Corpus path will provide mean_idf; this fallback keeps function standalone.)\n    mean_idf = 1.0\n    if hasattr(corpus_df, \"_mean_idf_hint\"):\n        mean_idf = float(getattr(corpus_df, \"_mean_idf_hint\"))\n    # If no hint, keep mean_idf=1.0 (lift becomes near-no-op).\n\n    long_query = len(query_repr.terms) >= Config.q_drop_min_len\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        if long_query and Config.q_drop_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.q_drop_df_ratio:\n                continue\n\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        if Config.idf_lift_power > 0.0:\n            lift = q_idf / (mean_idf + eps)\n            q_idf *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n        q_terms.append(term)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # NEW: collection mean IDF (after base BM25 idf, before power is already applied above).\n        # Used for \"idf lift\" normalization: terms matter insofar as they are more\n        # discriminative than the average term in this corpus.\n        self.mean_idf = float(np.mean(np.maximum(self.idf_array, 0.0))) if self.vocab_size > 0 else 1.0\n        if self.mean_idf <= 0.0:\n            self.mean_idf = 1.0\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        mean_idf = float(getattr(self.corpus, \"mean_idf\", 1.0))\n        if mean_idf <= 0.0:\n            mean_idf = 1.0\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        long_query = len(query_term_ids) >= Config.q_drop_min_len\n\n        for i, term_id in enumerate(query_term_ids):\n            df = float(self.corpus._df[term_id])\n\n            if long_query and Config.q_drop_df_ratio > 0.0 and self.corpus.N > 0:\n                if (df / float(self.corpus.N)) >= Config.q_drop_df_ratio:\n                    continue\n\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            if Config.idf_lift_power > 0.0:\n                lift = idf_val / (mean_idf + eps)\n                idf_val *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "b37ae06a-63f6-4c31-9721-309746e6fbc7", "generation": 9, "timestamp": 1770130701.206589, "iteration_found": 0, "metrics": {"beir_nfcorpus_ndcg@10": 0.31742044221832977, "beir_nfcorpus_recall@100": 0.25425230387056674, "beir_nfcorpus_index_time_ms": 3869.408041005954, "beir_nfcorpus_query_time_ms": 157.99241702188738, "bright_pony_ndcg@10": 0.08831939228304377, "bright_pony_recall@100": 0.29450063814291294, "bright_pony_index_time_ms": 1693.3144580107182, "bright_pony_query_time_ms": 565.2502499870025, "beir_scifact_ndcg@10": 0.6786990193358997, "beir_scifact_recall@100": 0.9333333333333333, "beir_scifact_index_time_ms": 5183.679291018052, "beir_scifact_query_time_ms": 347.84679202130064, "beir_arguana_ndcg@10": 0.29710978851277986, "beir_arguana_recall@100": 0.9350463954318344, "beir_arguana_index_time_ms": 6363.7125410023145, "beir_arguana_query_time_ms": 21581.710458995076, "bright_theoremqa_theorems_ndcg@10": 0.03214339983403284, "bright_theoremqa_theorems_recall@100": 0.16228070175438594, "bright_theoremqa_theorems_index_time_ms": 10975.161999987904, "bright_theoremqa_theorems_query_time_ms": 903.9419160108082, "beir_scidocs_ndcg@10": 0.14249321471711743, "beir_scidocs_recall@100": 0.3438166666666667, "beir_scidocs_index_time_ms": 19887.363459012704, "beir_scidocs_query_time_ms": 3393.2400410121772, "bright_economics_ndcg@10": 0.1378720742700584, "bright_economics_recall@100": 0.40138648215866596, "bright_economics_index_time_ms": 18808.22858400643, "bright_economics_query_time_ms": 2988.683374976972, "bright_biology_ndcg@10": 0.24119464773958527, "bright_biology_recall@100": 0.5191519999900174, "bright_biology_index_time_ms": 21750.637291988824, "bright_biology_query_time_ms": 2690.704208012903, "beir_fiqa_ndcg@10": 0.2315634826397288, "beir_fiqa_recall@100": 0.5332856529615789, "beir_fiqa_index_time_ms": 42870.38333300734, "beir_fiqa_query_time_ms": 6278.185375005705, "bright_earth_science_ndcg@10": 0.3553115616158419, "bright_earth_science_recall@100": 0.6647046773804941, "bright_earth_science_index_time_ms": 46361.95320900879, "bright_earth_science_query_time_ms": 4651.926833001198, "bright_stackoverflow_ndcg@10": 0.17946754795103553, "bright_stackoverflow_recall@100": 0.48791407281651794, "bright_stackoverflow_index_time_ms": 110902.29741702206, "bright_stackoverflow_query_time_ms": 9752.567582996562, "beir_trec-covid_ndcg@10": 0.57890195074689, "beir_trec-covid_recall@100": 0.10030087106937359, "beir_trec-covid_index_time_ms": 154719.760707987, "beir_trec-covid_query_time_ms": 1408.5600419784896, "avg_ndcg@10": 0.2733747101553619, "avg_recall@100": 0.4691644829646957, "combined_score": 0.43000652840282894, "total_index_time_ms": 443385.9003330581, "total_query_time_ms": 54720.60929102008, "total_time_ms": 498106.5096240782, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 16 lines\nChange 2: Replace 3 lines with 10 lines\nChange 3: Replace 8 lines with 8 lines\nChange 4: Replace 14 lines with 39 lines\nChange 5: 'for term, q_idf in zip(query_repr.terms, q_idfs):' to 'for term, q_idf in zip(q_terms, q_idfs):'\nChange 6: 'qn = float(len(query_repr.terms))' to 'qn = float(len(q_terms))'\nChange 7: Replace 2 lines with 5 lines\nChange 8: Replace 9 lines with 20 lines", "parent_metrics": {"bright_pony_ndcg@10": 0.10189566544743357, "bright_pony_recall@100": 0.2910164345449569, "bright_pony_index_time_ms": 1436.8697910103947, "bright_pony_query_time_ms": 598.0350419995375, "beir_nfcorpus_ndcg@10": 0.3209927569076926, "beir_nfcorpus_recall@100": 0.25557055765971176, "beir_nfcorpus_index_time_ms": 3861.29170798813, "beir_nfcorpus_query_time_ms": 108.48579101730138, "beir_scifact_ndcg@10": 0.6757178552383171, "beir_scifact_recall@100": 0.932, "beir_scifact_index_time_ms": 5037.169042014284, "beir_scifact_query_time_ms": 394.73016699776053, "beir_arguana_ndcg@10": 0.27947478680084137, "beir_arguana_recall@100": 0.9114917915774446, "beir_arguana_index_time_ms": 6429.801791004138, "beir_arguana_query_time_ms": 29602.29395798524, "bright_theoremqa_theorems_ndcg@10": 0.040423220229772364, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 11061.825417011278, "bright_theoremqa_theorems_query_time_ms": 1337.4527080159169, "beir_scidocs_ndcg@10": 0.14338763587371925, "beir_scidocs_recall@100": 0.34358333333333335, "beir_scidocs_index_time_ms": 20424.154333013576, "beir_scidocs_query_time_ms": 3743.7196249957196, "bright_economics_ndcg@10": 0.14101591304484054, "bright_economics_recall@100": 0.390907487552556, "bright_economics_index_time_ms": 22252.141999982996, "bright_economics_query_time_ms": 3953.8070410198998, "bright_biology_ndcg@10": 0.24462672356871215, "bright_biology_recall@100": 0.5429384077570076, "bright_biology_index_time_ms": 23209.672333003255, "bright_biology_query_time_ms": 3676.819791988237, "bright_earth_science_ndcg@10": 0.3510645668918352, "bright_earth_science_recall@100": 0.6647502376631234, "bright_earth_science_index_time_ms": 43457.04224999645, "bright_earth_science_query_time_ms": 7364.10487498506, "beir_fiqa_ndcg@10": 0.22698557961966917, "beir_fiqa_recall@100": 0.5249265994636365, "beir_fiqa_index_time_ms": 48066.90545799211, "beir_fiqa_query_time_ms": 7517.083499988075, "bright_stackoverflow_ndcg@10": 0.1724747784934034, "bright_stackoverflow_recall@100": 0.49296548663986156, "bright_stackoverflow_index_time_ms": 113555.1873749937, "bright_stackoverflow_query_time_ms": 11592.487833026098, "beir_trec-covid_ndcg@10": 0.6214939754374251, "beir_trec-covid_recall@100": 0.1059340492869673, "beir_trec-covid_index_time_ms": 158377.57408301695, "beir_trec-covid_query_time_ms": 1507.0705830003135, "avg_ndcg@10": 0.2766294547961385, "avg_recall@100": 0.467466096450702, "combined_score": 0.4292987681197893, "total_index_time_ms": 457169.63558102725, "total_query_time_ms": 71396.09091501916, "total_time_ms": 528565.7264960464, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 2, "migrant": true}, "prompts": null, "artifacts_json": null, "artifact_dir": null, "embedding": null}