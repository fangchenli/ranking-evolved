{"id": "b5855cb5-c6d2-4905-8121-c51822be3aec", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n\n    # Coordination as a bounded reward (>=1), not a penalty (<1).\n    # Helps nDCG@10 while protecting recall@100.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Use IDF-mass coverage (informative terms matter more).\n    use_idf_coord: bool = True\n\n    # Concave verbosity normalization: reduces over-penalization of long docs.\n    dl_power: float = 0.65\n\n    use_log_qtf: bool = True\n    query_length_norm: bool = True\n\n    # Rarity shaping: cap extreme IDF and mildly sharpen.\n    idf_cap: float = 8.0\n    idf_power: float = 1.1\n\n    # NEW: \"anchor\" (elite) term reward.\n    # Intuition: for many queries, a few highest-IDF terms are intent anchors.\n    # Rewarding their coverage improves early precision (nDCG@10) but is recall-safe\n    # because it's a bounded multiplier (never < 1).\n    elite_frac: float = 0.45     # fraction of query terms considered anchors\n    elite_alpha: float = 0.25    # reward strength\n    elite_gamma: float = 0.7     # curvature (<1 is forgiving)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Term importance from document frequency.\n\n    Use self-information log((N+1)/df), then (optionally) cap downstream.\n    We keep it simple here and apply the cap where we already have access to Config.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log((N + 1.0) / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique terms with weights.\n\n        Two stabilizers:\n        - sublinear qtf (1+log(qtf)) to avoid repetition dominating;\n        - optional query-length normalization so long queries don't just sum larger.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n\n        if Config.query_length_norm and len(tw) > 0:\n            # L2 normalize weights (keeps relative emphasis but fixes scale across queries).\n            norm = math.sqrt(sum(v * v for v in tw.values()))\n            if norm > 0:\n                inv = 1.0 / norm\n                tw = {t: v * inv for t, v in tw.items()}\n\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    BM25-like evidence + (1) IDF-mass coordination reward + (2) anchor-term reward.\n\n    (1) is the existing recall-safe soft-AND.\n    (2) rewards matching the highest-IDF query terms (intent anchors). This is a\n        bounded multiplier (>=1), so it tends to lift correct docs into top ranks\n        (nDCG@10) without suppressing partial matches (recall@100).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Concave verbosity normalization (robust on very long docs).\n    if avgdl > 0:\n        dl_ratio = (doc_length + eps) / (avgdl + eps)\n        norm = (1.0 - b) + b * (dl_ratio ** Config.dl_power)\n    else:\n        norm = 1.0\n\n    score = 0.0\n    qterms = query_repr.terms\n    qn = len(qterms)\n    if qn == 0:\n        return 0.0\n\n    # Precompute shaped (capped, powered) IDF for query terms (query is short).\n    qidf = []\n    for term in qterms:\n        df = float(corpus_df.get(term, 1))\n        v = float(idf(df, N))\n        if Config.idf_cap > 0:\n            v = min(v, Config.idf_cap)\n        qidf.append(float(max(v, 0.0) ** Config.idf_power))\n\n    idf_total = 0.0\n    idf_matched = 0.0\n    matched = 0.0\n\n    for term, tidf in zip(qterms, qidf):\n        idf_total += tidf\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        idf_matched += tidf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * tidf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    # (1) Coordination reward (optionally IDF-mass based)\n    if Config.use_idf_coord and idf_total > 0.0:\n        coverage = idf_matched / (idf_total + eps)\n    else:\n        coverage = matched / (qn + eps)\n    score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # (2) Anchor/elite reward: top-IDF query terms should be present for best ranks.\n    if Config.elite_alpha > 0.0:\n        m = int(max(1, math.ceil(Config.elite_frac * float(qn))))\n        elite_idx = np.argsort(-np.asarray(qidf, dtype=np.float64))[:m]\n        elite_total = float(np.sum(np.asarray(qidf, dtype=np.float64)[elite_idx]))\n        if elite_total > 0.0:\n            elite_hit = 0.0\n            for j in elite_idx.tolist():\n                if doc_tf.get(qterms[j], 0) > 0:\n                    elite_hit += qidf[j]\n            elite_cov = elite_hit / (elite_total + eps)\n            score *= 1.0 + Config.elite_alpha * (elite_cov ** Config.elite_gamma)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Precompute IDF, cap extremes, then sharpen (idf^power).\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        if Config.idf_cap > 0:\n            self.idf_array = np.minimum(self.idf_array, Config.idf_cap)\n        self.idf_array = np.power(np.maximum(self.idf_array, 0.0), Config.idf_power)\n\n        # Keep norm consistent with retrieval_score: concave verbosity normalization.\n        eps = Config.epsilon\n        dl_ratio = (self.doc_lengths + eps) / (max(self.avgdl, eps) + eps)\n        self.norm_array = (1.0 - Config.b) + Config.b * (dl_ratio ** Config.dl_power)\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For coordination: either count matched terms or sum matched idf mass.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_total = 0.0\n\n        # Elite/anchor bookkeeping (top-m query terms by shaped IDF).\n        qidf = np.asarray([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        qn = float(len(query_term_ids))\n        m = int(max(1.0, math.ceil(Config.elite_frac * qn)))\n        elite_idx = np.argsort(-qidf)[:m]\n        elite_total = float(np.sum(qidf[elite_idx])) + eps\n        elite_hit = np.zeros(len(candidate_docs), dtype=np.float64)\n        elite_set = set(elite_idx.tolist())\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            idf_matched += present * idf_val\n            if i in elite_set:\n                elite_hit += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if qn > 1.0 and scores.size > 0:\n            if Config.use_idf_coord and idf_total > 0.0:\n                coverage = idf_matched / (idf_total + eps)\n            else:\n                coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n            if Config.elite_alpha > 0.0 and elite_total > 0.0:\n                elite_cov = elite_hit / elite_total\n                scores *= 1.0 + Config.elite_alpha * (elite_cov ** Config.elite_gamma)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "01a1602b-18f3-4dd3-983b-c9799e251d23", "generation": 5, "timestamp": 1770079928.14889, "iteration_found": 49, "metrics": {"beir_nfcorpus_ndcg@10": 0.3170970758738513, "beir_nfcorpus_recall@100": 0.25491881196364724, "beir_nfcorpus_index_time_ms": 3885.2517080085818, "beir_nfcorpus_query_time_ms": 114.66533300699666, "bright_pony_ndcg@10": 0.1317927082251115, "bright_pony_recall@100": 0.33404047644123036, "bright_pony_index_time_ms": 1725.514874997316, "bright_pony_query_time_ms": 551.4651250268798, "beir_scifact_ndcg@10": 0.676576925876428, "beir_scifact_recall@100": 0.932, "beir_scifact_index_time_ms": 5306.667041004403, "beir_scifact_query_time_ms": 381.8368749925867, "beir_arguana_ndcg@10": 0.2575062181829261, "beir_arguana_recall@100": 0.8922198429693077, "beir_arguana_index_time_ms": 6479.555540980073, "beir_arguana_query_time_ms": 21319.444124994334, "bright_theoremqa_theorems_ndcg@10": 0.03482523354421036, "bright_theoremqa_theorems_recall@100": 0.14912280701754385, "bright_theoremqa_theorems_index_time_ms": 10891.916916996706, "bright_theoremqa_theorems_query_time_ms": 861.0984580009244, "beir_scidocs_ndcg@10": 0.14230183746008102, "beir_scidocs_recall@100": 0.3402833333333334, "beir_scidocs_index_time_ms": 19703.743749996647, "beir_scidocs_query_time_ms": 3534.5471249893308, "bright_economics_ndcg@10": 0.11338969155142571, "bright_economics_recall@100": 0.34785694189594807, "bright_economics_index_time_ms": 19548.282250005286, "bright_economics_query_time_ms": 2809.0739580220543, "bright_biology_ndcg@10": 0.24731364758263244, "bright_biology_recall@100": 0.5758090146950648, "bright_biology_index_time_ms": 21669.95904201758, "bright_biology_query_time_ms": 2548.857957997825, "beir_fiqa_ndcg@10": 0.20836191000665114, "beir_fiqa_recall@100": 0.5070706813762369, "beir_fiqa_index_time_ms": 49989.69570899499, "beir_fiqa_query_time_ms": 7274.884000013117, "bright_earth_science_ndcg@10": 0.23264094505925784, "bright_earth_science_recall@100": 0.6503585079469834, "bright_earth_science_index_time_ms": 49506.62495798315, "bright_earth_science_query_time_ms": 4554.416667000623, "bright_stackoverflow_ndcg@10": 0.1774992675285032, "bright_stackoverflow_recall@100": 0.45917086637482796, "bright_stackoverflow_index_time_ms": 119153.50729200873, "bright_stackoverflow_query_time_ms": 8813.325000024633, "beir_trec-covid_ndcg@10": 0.5880094902584251, "beir_trec-covid_recall@100": 0.10348440780723864, "beir_trec-covid_index_time_ms": 158938.32675000886, "beir_trec-covid_query_time_ms": 1503.8354579883162, "avg_ndcg@10": 0.26060957926245865, "avg_recall@100": 0.4621946409851135, "combined_score": 0.42187762864058254, "total_index_time_ms": 466799.0458330023, "total_query_time_ms": 54267.45008205762, "total_time_ms": 521066.49591505993, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 23 lines with 31 lines\nChange 2: Replace 62 lines with 81 lines\nChange 3: Replace 46 lines with 59 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.31967561572169073, "beir_nfcorpus_recall@100": 0.2546632153766958, "beir_nfcorpus_index_time_ms": 3749.056708009448, "beir_nfcorpus_query_time_ms": 108.23670800891705, "bright_pony_ndcg@10": 0.12732262147793474, "bright_pony_recall@100": 0.32544900186526327, "bright_pony_index_time_ms": 1455.6426659983117, "bright_pony_query_time_ms": 498.70383300003596, "beir_scifact_ndcg@10": 0.6749841777936594, "beir_scifact_recall@100": 0.9253333333333335, "beir_scifact_index_time_ms": 5069.054250023328, "beir_scifact_query_time_ms": 349.26245801034383, "beir_arguana_ndcg@10": 0.2568800999430232, "beir_arguana_recall@100": 0.892933618843683, "beir_arguana_index_time_ms": 6120.126708003227, "beir_arguana_query_time_ms": 20757.474042009562, "bright_theoremqa_theorems_ndcg@10": 0.03229130804440823, "bright_theoremqa_theorems_recall@100": 0.13596491228070173, "bright_theoremqa_theorems_index_time_ms": 10889.927250012988, "bright_theoremqa_theorems_query_time_ms": 843.8537920010276, "beir_scidocs_ndcg@10": 0.1410875293340981, "beir_scidocs_recall@100": 0.34138333333333337, "beir_scidocs_index_time_ms": 19790.54229200119, "beir_scidocs_query_time_ms": 3405.5311670235824, "bright_economics_ndcg@10": 0.10967975989598878, "bright_economics_recall@100": 0.3493644447831137, "bright_economics_index_time_ms": 20609.16012499365, "bright_economics_query_time_ms": 2621.2180419825017, "bright_biology_ndcg@10": 0.24817285188920138, "bright_biology_recall@100": 0.5749999532063916, "bright_biology_index_time_ms": 25070.572374999756, "bright_biology_query_time_ms": 2498.9112920011394, "beir_fiqa_ndcg@10": 0.2097441972225911, "beir_fiqa_recall@100": 0.5057350699711811, "beir_fiqa_index_time_ms": 41821.65966700995, "beir_fiqa_query_time_ms": 6688.625625014538, "bright_earth_science_ndcg@10": 0.2419109703603225, "bright_earth_science_recall@100": 0.6399157343463151, "bright_earth_science_index_time_ms": 46512.353625003016, "bright_earth_science_query_time_ms": 3875.9382079879288, "bright_stackoverflow_ndcg@10": 0.18219632937527983, "bright_stackoverflow_recall@100": 0.44535078814797013, "bright_stackoverflow_index_time_ms": 107372.6838750008, "bright_stackoverflow_query_time_ms": 8925.357458007056, "beir_trec-covid_ndcg@10": 0.6204580436502397, "beir_trec-covid_recall@100": 0.10663020876511586, "beir_trec-covid_index_time_ms": 149435.53033302305, "beir_trec-covid_query_time_ms": 1446.665291005047, "avg_ndcg@10": 0.2637002920590365, "avg_recall@100": 0.4581436345210916, "combined_score": 0.41925496602868056, "total_index_time_ms": 437896.3098740787, "total_query_time_ms": 52019.77791605168, "total_time_ms": 489916.0877901304, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 0}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4193\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4206 \u2192 0.4193. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace 23 lines with 28 lines\nChange 2: Replace 52 lines with 67 lines\nChange 3: Replace 21 lines with 35 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3145, beir_nfcorpus_recall@100: 0.2547, beir_nfcorpus_index_time_ms: 3811.6219, beir_nfcorpus_query_time_ms: 103.2944, bright_pony_ndcg@10: 0.1325, bright_pony_recall@100: 0.3316, bright_pony_index_time_ms: 1498.8447, bright_pony_query_time_ms: 496.8741, beir_scifact_ndcg@10: 0.6744, beir_scifact_recall@100: 0.9280, beir_scifact_index_time_ms: 5146.1089, beir_scifact_query_time_ms: 366.5880, beir_arguana_ndcg@10: 0.2548, beir_arguana_recall@100: 0.8765, beir_arguana_index_time_ms: 6022.2064, beir_arguana_query_time_ms: 20778.1610, bright_theoremqa_theorems_ndcg@10: 0.0377, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10860.9250, bright_theoremqa_theorems_query_time_ms: 855.8612, beir_scidocs_ndcg@10: 0.1418, beir_scidocs_recall@100: 0.3360, beir_scidocs_index_time_ms: 19945.6897, beir_scidocs_query_time_ms: 3636.9756, bright_economics_ndcg@10: 0.1288, bright_economics_recall@100: 0.3540, bright_economics_index_time_ms: 18333.9290, bright_economics_query_time_ms: 2596.7260, bright_biology_ndcg@10: 0.2652, bright_biology_recall@100: 0.5554, bright_biology_index_time_ms: 24484.7121, bright_biology_query_time_ms: 2621.8920, beir_fiqa_ndcg@10: 0.2124, beir_fiqa_recall@100: 0.5094, beir_fiqa_index_time_ms: 38019.3780, beir_fiqa_query_time_ms: 6764.4666, bright_earth_science_ndcg@10: 0.3001, bright_earth_science_recall@100: 0.6446, bright_earth_science_index_time_ms: 43931.9335, bright_earth_science_query_time_ms: 3919.2822, bright_stackoverflow_ndcg@10: 0.1832, bright_stackoverflow_recall@100: 0.4512, bright_stackoverflow_index_time_ms: 107451.1759, bright_stackoverflow_query_time_ms: 8855.3725, beir_trec-covid_ndcg@10: 0.6028, beir_trec-covid_recall@100: 0.1024, beir_trec-covid_index_time_ms: 165988.7337, beir_trec-covid_query_time_ms: 3406.5375, avg_ndcg@10: 0.2707, avg_recall@100: 0.4581, combined_score: 0.4206, total_index_time_ms: 445495.2589, total_query_time_ms: 54402.0312, total_time_ms: 499897.2901, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace 16 lines with 23 lines\nChange 2: Replace 9 lines with 9 lines\nChange 3: Replace 16 lines with 9 lines\nChange 4: Replace 79 lines with 52 lines\nChange 5: Replace 11 lines with 10 lines\nChange 6: Replace 4 lines with 2 lines\nChange 7: Replace 15 lines with 21 lines\nChange 8: Replace 17 lines with 17 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3198, beir_nfcorpus_recall@100: 0.2553, beir_nfcorpus_index_time_ms: 3679.8082, beir_nfcorpus_query_time_ms: 98.7347, bright_pony_ndcg@10: 0.0983, bright_pony_recall@100: 0.2912, bright_pony_index_time_ms: 1521.4957, bright_pony_query_time_ms: 497.6051, beir_scifact_ndcg@10: 0.6774, beir_scifact_recall@100: 0.9287, beir_scifact_index_time_ms: 4940.7074, beir_scifact_query_time_ms: 346.2315, beir_arguana_ndcg@10: 0.2729, beir_arguana_recall@100: 0.9086, beir_arguana_index_time_ms: 5991.9659, beir_arguana_query_time_ms: 20271.2993, bright_theoremqa_theorems_ndcg@10: 0.0332, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10617.5802, bright_theoremqa_theorems_query_time_ms: 834.2722, beir_scidocs_ndcg@10: 0.1459, beir_scidocs_recall@100: 0.3439, beir_scidocs_index_time_ms: 19459.6977, beir_scidocs_query_time_ms: 3413.5155, bright_economics_ndcg@10: 0.1627, bright_economics_recall@100: 0.3746, bright_economics_index_time_ms: 20363.4525, bright_economics_query_time_ms: 2526.9955, bright_biology_ndcg@10: 0.2590, bright_biology_recall@100: 0.5248, bright_biology_index_time_ms: 23925.6135, bright_biology_query_time_ms: 2347.8187, bright_earth_science_ndcg@10: 0.3489, bright_earth_science_recall@100: 0.6567, bright_earth_science_index_time_ms: 43422.2851, bright_earth_science_query_time_ms: 4105.9740, beir_fiqa_ndcg@10: 0.2225, beir_fiqa_recall@100: 0.5253, beir_fiqa_index_time_ms: 45846.3913, beir_fiqa_query_time_ms: 6542.4576, bright_stackoverflow_ndcg@10: 0.1885, bright_stackoverflow_recall@100: 0.4581, bright_stackoverflow_index_time_ms: 110449.2830, bright_stackoverflow_query_time_ms: 8393.3282, beir_trec-covid_ndcg@10: 0.6649, beir_trec-covid_recall@100: 0.1083, beir_trec-covid_index_time_ms: 148943.6612, beir_trec-covid_query_time_ms: 1452.7970, avg_ndcg@10: 0.2828, avg_recall@100: 0.4607, combined_score: 0.4252, total_index_time_ms: 439161.9417, total_query_time_ms: 50831.0292, total_time_ms: 489992.9710, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 15 lines with 21 lines\nChange 2: Replace 9 lines with 21 lines\nChange 3: Replace 38 lines with 48 lines\nChange 4: Replace 30 lines with 40 lines\nChange 5: Replace 21 lines with 32 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3200, beir_nfcorpus_recall@100: 0.2551, beir_nfcorpus_index_time_ms: 3744.4406, beir_nfcorpus_query_time_ms: 99.6717, bright_pony_ndcg@10: 0.0964, bright_pony_recall@100: 0.2917, bright_pony_index_time_ms: 1610.5340, bright_pony_query_time_ms: 483.9625, beir_scifact_ndcg@10: 0.6774, beir_scifact_recall@100: 0.9287, beir_scifact_index_time_ms: 5116.5406, beir_scifact_query_time_ms: 356.6414, beir_arguana_ndcg@10: 0.2733, beir_arguana_recall@100: 0.9086, beir_arguana_index_time_ms: 6162.8295, beir_arguana_query_time_ms: 19989.2767, bright_theoremqa_theorems_ndcg@10: 0.0332, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 11121.3160, bright_theoremqa_theorems_query_time_ms: 806.5492, beir_scidocs_ndcg@10: 0.1458, beir_scidocs_recall@100: 0.3443, beir_scidocs_index_time_ms: 20228.0705, beir_scidocs_query_time_ms: 3382.6788, bright_economics_ndcg@10: 0.1618, bright_economics_recall@100: 0.3889, bright_economics_index_time_ms: 21006.2314, bright_economics_query_time_ms: 2567.0599, bright_biology_ndcg@10: 0.2595, bright_biology_recall@100: 0.5246, bright_biology_index_time_ms: 21485.6100, bright_biology_query_time_ms: 2499.0561, bright_earth_science_ndcg@10: 0.3470, bright_earth_science_recall@100: 0.6555, bright_earth_science_index_time_ms: 45158.8237, bright_earth_science_query_time_ms: 4149.8207, beir_fiqa_ndcg@10: 0.2227, beir_fiqa_recall@100: 0.5231, beir_fiqa_index_time_ms: 47603.3641, beir_fiqa_query_time_ms: 6654.6757, bright_stackoverflow_ndcg@10: 0.1906, bright_stackoverflow_recall@100: 0.4609, bright_stackoverflow_index_time_ms: 115156.0035, bright_stackoverflow_query_time_ms: 8036.0785, beir_trec-covid_ndcg@10: 0.6636, beir_trec-covid_recall@100: 0.1082, beir_trec-covid_index_time_ms: 152908.6029, beir_trec-covid_query_time_ms: 1430.1705, avg_ndcg@10: 0.2826, avg_recall@100: 0.4619, combined_score: 0.4261, total_index_time_ms: 451302.3669, total_query_time_ms: 50455.6418, total_time_ms: 501758.0087, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4261)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (as in BM25, but we will change the \"norm\" definition)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # How much to trust \"lexical focus\" (unique terms) vs raw length.\n    # 0 -> pure length, 1 -> pure unique length.\n    focus_mix: float = 0.65\n\n    # Sharpen rarity/discriminativity (idf^power). power>1 penalizes common terms more.\n    idf_power: float = 1.12\n\n    # NEW: coordination should be a *reward* (>=1), not a penalty (<1).\n    # Helps early precision (nDCG@10) while keeping recall@100 safe.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # NEW: query repetition is often noise; sublinear qtf improves robustness.\n    use_log_qtf: bool = True\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use distinct query terms + sublinear query-TF weights.\n\n        Rationale:\n        - Coordination/facet coverage is only meaningful over distinct terms.\n        - Repetition in queries often comes from tokenization artifacts / verbosity;\n          (1+log qtf) is a stable, training-free dampener.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + bounded coordination reward.\n\n    Intuition:\n    - Keep BM25's saturating evidence per matched term (diminishing returns of repetition).\n    - Use an effective length mixing verbosity (tokens) and lexical breadth (unique terms),\n      favoring focused docs.\n    - Multiply by a *rewarding* coordination factor (>=1) so covering more distinct query\n      aspects nudges docs up (nDCG@10) without crushing partial matches (recall@100).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Approximate unique-length from the tf dict (cheap).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(\n                self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist()\n            )\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3200), Performs well on beir_nfcorpus_recall@100 (0.2551), Performs well on beir_nfcorpus_index_time_ms (3777.1753), Performs well on beir_nfcorpus_query_time_ms (108.4591), Performs well on bright_pony_ndcg@10 (0.0964), Performs well on bright_pony_recall@100 (0.2917), Performs well on bright_pony_index_time_ms (1749.3347), Performs well on bright_pony_query_time_ms (481.0766), Performs well on beir_scifact_ndcg@10 (0.6774), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (5207.6165), Performs well on beir_scifact_query_time_ms (349.7688), Performs well on beir_arguana_ndcg@10 (0.2733), Performs well on beir_arguana_recall@100 (0.9086), Performs well on beir_arguana_index_time_ms (6191.9899), Performs well on beir_arguana_query_time_ms (20004.8251), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0332), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10971.9725), Performs well on bright_theoremqa_theorems_query_time_ms (808.5562), Performs well on beir_scidocs_ndcg@10 (0.1458), Performs well on beir_scidocs_recall@100 (0.3443), Performs well on beir_scidocs_index_time_ms (20031.0070), Performs well on beir_scidocs_query_time_ms (3397.5760), Performs well on bright_biology_ndcg@10 (0.2595), Performs well on bright_biology_recall@100 (0.5246), Performs well on bright_biology_index_time_ms (18234.7670), Performs well on bright_biology_query_time_ms (2502.4633), Performs well on bright_economics_ndcg@10 (0.1618), Performs well on bright_economics_recall@100 (0.3889), Performs well on bright_economics_index_time_ms (21906.7446), Performs well on bright_economics_query_time_ms (2421.3189), Performs well on beir_fiqa_ndcg@10 (0.2227), Performs well on beir_fiqa_recall@100 (0.5231), Performs well on beir_fiqa_index_time_ms (46876.9944), Performs well on beir_fiqa_query_time_ms (6437.3602), Performs well on bright_earth_science_ndcg@10 (0.3470), Performs well on bright_earth_science_recall@100 (0.6555), Performs well on bright_earth_science_index_time_ms (45815.7916), Performs well on bright_earth_science_query_time_ms (4003.3096), Performs well on bright_stackoverflow_ndcg@10 (0.1906), Performs well on bright_stackoverflow_recall@100 (0.4609), Performs well on bright_stackoverflow_index_time_ms (111712.5567), Performs well on bright_stackoverflow_query_time_ms (8072.1932), Performs well on beir_trec-covid_ndcg@10 (0.6636), Performs well on beir_trec-covid_recall@100 (0.1082), Performs well on beir_trec-covid_index_time_ms (156384.0995), Performs well on beir_trec-covid_query_time_ms (1475.5649), Performs well on avg_ndcg@10 (0.2826), Performs well on avg_recall@100 (0.4619), Performs well on combined_score (0.4261), Performs well on total_index_time_ms (448860.0498), Performs well on total_query_time_ms (50062.4721), Performs well on total_time_ms (498922.5219), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4261)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + bounded coordination reward.\n\n    Key fix vs current: add a *rewarding* coordination factor (>=1) so documents\n    covering more distinct query aspects are nudged upward (better nDCG@10)\n    without crushing partial matches (protect recall@100).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3200), Performs well on beir_nfcorpus_recall@100 (0.2551), Performs well on beir_nfcorpus_index_time_ms (3744.4406), Performs well on beir_nfcorpus_query_time_ms (99.6717), Performs well on bright_pony_ndcg@10 (0.0964), Performs well on bright_pony_recall@100 (0.2917), Performs well on bright_pony_index_time_ms (1610.5340), Performs well on bright_pony_query_time_ms (483.9625), Performs well on beir_scifact_ndcg@10 (0.6774), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (5116.5406), Performs well on beir_scifact_query_time_ms (356.6414), Performs well on beir_arguana_ndcg@10 (0.2733), Performs well on beir_arguana_recall@100 (0.9086), Performs well on beir_arguana_index_time_ms (6162.8295), Performs well on beir_arguana_query_time_ms (19989.2767), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0332), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11121.3160), Performs well on bright_theoremqa_theorems_query_time_ms (806.5492), Performs well on beir_scidocs_ndcg@10 (0.1458), Performs well on beir_scidocs_recall@100 (0.3443), Performs well on beir_scidocs_index_time_ms (20228.0705), Performs well on beir_scidocs_query_time_ms (3382.6788), Performs well on bright_economics_ndcg@10 (0.1618), Performs well on bright_economics_recall@100 (0.3889), Performs well on bright_economics_index_time_ms (21006.2314), Performs well on bright_economics_query_time_ms (2567.0599), Performs well on bright_biology_ndcg@10 (0.2595), Performs well on bright_biology_recall@100 (0.5246), Performs well on bright_biology_index_time_ms (21485.6100), Performs well on bright_biology_query_time_ms (2499.0561), Performs well on bright_earth_science_ndcg@10 (0.3470), Performs well on bright_earth_science_recall@100 (0.6555), Performs well on bright_earth_science_index_time_ms (45158.8237), Performs well on bright_earth_science_query_time_ms (4149.8207), Performs well on beir_fiqa_ndcg@10 (0.2227), Performs well on beir_fiqa_recall@100 (0.5231), Performs well on beir_fiqa_index_time_ms (47603.3641), Performs well on beir_fiqa_query_time_ms (6654.6757), Performs well on bright_stackoverflow_ndcg@10 (0.1906), Performs well on bright_stackoverflow_recall@100 (0.4609), Performs well on bright_stackoverflow_index_time_ms (115156.0035), Performs well on bright_stackoverflow_query_time_ms (8036.0785), Performs well on beir_trec-covid_ndcg@10 (0.6636), Performs well on beir_trec-covid_recall@100 (0.1082), Performs well on beir_trec-covid_index_time_ms (152908.6029), Performs well on beir_trec-covid_query_time_ms (1430.1705), Performs well on avg_ndcg@10 (0.2826), Performs well on avg_recall@100 (0.4619), Performs well on combined_score (0.4261), Performs well on total_index_time_ms (451302.3669), Performs well on total_query_time_ms (50455.6418), Performs well on total_time_ms (501758.0087), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4252)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n\n    # Use a gentle *reward* (not a penalty) for covering more query aspects.\n    # This avoids recall@100 collapses from multiplicative coverage^alpha factors < 1.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening\n    use_log_qtf: bool = True\n\n    # IDF shaping: keep Lucene-like odds IDF, optionally sharpen (power>1) or soften (<1).\n    idf_power: float = 1.10\n\n    # Verbosity/focus normalization: mix token length with unique-term length.\n    focus_mix: float = 0.60\n\n    # Proximity: keep available for score() path, but default off for rank() speed/robustness.\n    prox_window: int = 16\n    prox_beta: float = 0.0\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed odds IDF (Lucene-style): log(1 + (N-df+0.5)/(df+0.5)).\n\n    Rationale: compared to pure self-information, this is less \"spiky\" for ultra-rare\n    tokens (IDs/typos) while still rewarding discriminativity.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep minimal per-doc state (TF + length).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique terms with weights.\n\n        Weighting: sublinear qtf (1+log(qtf)) is robust to repeated tokens and\n        tends to improve nDCG when queries contain redundancy/noise.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized BM25 evidence + *bounded* coordination reward.\n\n    Key changes vs current:\n    - Remove multiplicative coverage^alpha (<1) which can downrank partial matches (hurts recall@100).\n    - Normalize by an \"effective length\" mixing raw length and unique-term length (focus prior).\n    - Use idf^power to slightly sharpen discriminativity without adding brittle proximity logic.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Approximate lexical breadth from the tf dict (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n\n    avg_eff = max(1.0, (1.0 - mix) * max(avgdl, 1.0) + mix * max(avgdl ** 0.5, 1.0))\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps))\n\n    score = 0.0\n    matched = 0.0\n    qterms = query_repr.terms\n\n    for term in qterms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base_idf = float(idf(df, N))\n        term_idf = float(np.power(max(base_idf, 0.0), Config.idf_power))\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(qterms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n\n        # Minimal per-doc state: TF dicts only (faster, less memory).\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                self._inverted_index[tid].append(doc_idx)\n                self._df[tid] += 1\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Focus-normalized length: mix token length with unique-term length (precomputed via df proxy)\n        b = Config.b\n        mix = Config.focus_mix\n        # unique-term count per doc can be approximated as nnz in tf dicts (cheap here)\n        doc_uniq = np.array([len(tf) for tf in self._doc_tf_dicts], dtype=np.float64)\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(float(np.mean(doc_uniq)) if self.N > 0 else 1.0, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring for rank(); matches retrieval_score except proximity,\n        which is added in a lightweight per-candidate pass (only for top candidates).\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3198), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (3679.8082), Performs well on beir_nfcorpus_query_time_ms (98.7347), Performs well on bright_pony_ndcg@10 (0.0983), Performs well on bright_pony_recall@100 (0.2912), Performs well on bright_pony_index_time_ms (1521.4957), Performs well on bright_pony_query_time_ms (497.6051), Performs well on beir_scifact_ndcg@10 (0.6774), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (4940.7074), Performs well on beir_scifact_query_time_ms (346.2315), Performs well on beir_arguana_ndcg@10 (0.2729), Performs well on beir_arguana_recall@100 (0.9086), Performs well on beir_arguana_index_time_ms (5991.9659), Performs well on beir_arguana_query_time_ms (20271.2993), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0332), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10617.5802), Performs well on bright_theoremqa_theorems_query_time_ms (834.2722), Performs well on beir_scidocs_ndcg@10 (0.1459), Performs well on beir_scidocs_recall@100 (0.3439), Performs well on beir_scidocs_index_time_ms (19459.6977), Performs well on beir_scidocs_query_time_ms (3413.5155), Performs well on bright_economics_ndcg@10 (0.1627), Performs well on bright_economics_recall@100 (0.3746), Performs well on bright_economics_index_time_ms (20363.4525), Performs well on bright_economics_query_time_ms (2526.9955), Performs well on bright_biology_ndcg@10 (0.2590), Performs well on bright_biology_recall@100 (0.5248), Performs well on bright_biology_index_time_ms (23925.6135), Performs well on bright_biology_query_time_ms (2347.8187), Performs well on bright_earth_science_ndcg@10 (0.3489), Performs well on bright_earth_science_recall@100 (0.6567), Performs well on bright_earth_science_index_time_ms (43422.2851), Performs well on bright_earth_science_query_time_ms (4105.9740), Performs well on beir_fiqa_ndcg@10 (0.2225), Performs well on beir_fiqa_recall@100 (0.5253), Performs well on beir_fiqa_index_time_ms (45846.3913), Performs well on beir_fiqa_query_time_ms (6542.4576), Performs well on bright_stackoverflow_ndcg@10 (0.1885), Performs well on bright_stackoverflow_recall@100 (0.4581), Performs well on bright_stackoverflow_index_time_ms (110449.2830), Performs well on bright_stackoverflow_query_time_ms (8393.3282), Performs well on beir_trec-covid_ndcg@10 (0.6649), Performs well on beir_trec-covid_recall@100 (0.1083), Performs well on beir_trec-covid_index_time_ms (148943.6612), Performs well on beir_trec-covid_query_time_ms (1452.7970), Performs well on avg_ndcg@10 (0.2828), Performs well on avg_recall@100 (0.4607), Performs well on combined_score (0.4252), Performs well on total_index_time_ms (439161.9417), Performs well on total_query_time_ms (50831.0292), Performs well on total_time_ms (489992.9710), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4206)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (as in BM25, but we will change the \"norm\" definition)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # How much to trust \"lexical focus\" (unique terms) vs raw length.\n    # 0 -> pure length, 1 -> pure unique length.\n    focus_mix: float = 0.65\n\n    # Sharpen rarity/discriminativity (idf^power). power>1 penalizes common terms more.\n    idf_power: float = 1.12\n\n    # Coordination should be a *reward* (>=1), not a penalty (<1).\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # NEW: IDF-mass coverage gate.\n    # Intuition: matching only the \"easy/common\" query terms should not outrank\n    # documents that match the rare/informative parts. Use a *floor* to protect recall@100.\n    idf_gate_alpha: float = 0.7   # curvature; <1 gentler than power>1\n    idf_gate_floor: float = 0.25  # minimum multiplier, prevents crushing partial matches\n\n    # Query repetition is often noise; sublinear qtf improves robustness.\n    use_log_qtf: bool = True\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use distinct query terms + sublinear query-TF weights.\n\n        Rationale:\n        - Coordination/facet coverage is only meaningful over distinct terms.\n        - Repetition in queries often comes from tokenization artifacts / verbosity;\n          (1+log qtf) is a stable, training-free dampener.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + (1) coordination reward + (2) IDF-mass gate.\n\n    New piece (2) is an \"informativeness coverage\" multiplier:\n        gate = floor + (1-floor) * (idf_matched/idf_total)^alpha\n\n    Motivation:\n    - In many benchmarks, queries contain both common \"glue\" words and a few rare key terms.\n      BM25 evidence can let a document matching many common terms outrank a document that\n      matches fewer terms but includes the rare, diagnostic ones.\n    - We *do not* hard-filter; the floor keeps partial matches alive (protect recall@100),\n      while the gate improves top-rank precision (nDCG@10).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    idf_total = 0.0\n    idf_matched = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        idf_total += term_idf\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        idf_matched += term_idf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        if idf_total > 0.0 and Config.idf_gate_alpha > 0.0:\n            idf_cov = idf_matched / (idf_total + eps)  # in [0,1]\n            gate = Config.idf_gate_floor + (1.0 - Config.idf_gate_floor) * (\n                idf_cov ** Config.idf_gate_alpha\n            )\n            score *= gate\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # IDF-mass coverage bookkeeping (vectorized per doc)\n        idf_total = 0.0\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            idf_matched += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n            if idf_total > 0.0 and Config.idf_gate_alpha > 0.0:\n                idf_cov = idf_matched / (idf_total + eps)\n                gate = Config.idf_gate_floor + (1.0 - Config.idf_gate_floor) * (\n                    idf_cov ** Config.idf_gate_alpha\n                )\n                scores *= gate\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(\n                self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist()\n            )\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3145), Performs well on beir_nfcorpus_recall@100 (0.2547), Performs well on beir_nfcorpus_index_time_ms (3811.6219), Performs well on beir_nfcorpus_query_time_ms (103.2944), Performs well on bright_pony_ndcg@10 (0.1325), Performs well on bright_pony_recall@100 (0.3316), Performs well on bright_pony_index_time_ms (1498.8447), Performs well on bright_pony_query_time_ms (496.8741), Performs well on beir_scifact_ndcg@10 (0.6744), Performs well on beir_scifact_recall@100 (0.9280), Performs well on beir_scifact_index_time_ms (5146.1089), Performs well on beir_scifact_query_time_ms (366.5880), Performs well on beir_arguana_ndcg@10 (0.2548), Performs well on beir_arguana_recall@100 (0.8765), Performs well on beir_arguana_index_time_ms (6022.2064), Performs well on beir_arguana_query_time_ms (20778.1610), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0377), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10860.9250), Performs well on bright_theoremqa_theorems_query_time_ms (855.8612), Performs well on beir_scidocs_ndcg@10 (0.1418), Performs well on beir_scidocs_recall@100 (0.3360), Performs well on beir_scidocs_index_time_ms (19945.6897), Performs well on beir_scidocs_query_time_ms (3636.9756), Performs well on bright_economics_ndcg@10 (0.1288), Performs well on bright_economics_recall@100 (0.3540), Performs well on bright_economics_index_time_ms (18333.9290), Performs well on bright_economics_query_time_ms (2596.7260), Performs well on bright_biology_ndcg@10 (0.2652), Performs well on bright_biology_recall@100 (0.5554), Performs well on bright_biology_index_time_ms (24484.7121), Performs well on bright_biology_query_time_ms (2621.8920), Performs well on beir_fiqa_ndcg@10 (0.2124), Performs well on beir_fiqa_recall@100 (0.5094), Performs well on beir_fiqa_index_time_ms (38019.3780), Performs well on beir_fiqa_query_time_ms (6764.4666), Performs well on bright_earth_science_ndcg@10 (0.3001), Performs well on bright_earth_science_recall@100 (0.6446), Performs well on bright_earth_science_index_time_ms (43931.9335), Performs well on bright_earth_science_query_time_ms (3919.2822), Performs well on bright_stackoverflow_ndcg@10 (0.1832), Performs well on bright_stackoverflow_recall@100 (0.4512), Performs well on bright_stackoverflow_index_time_ms (107451.1759), Performs well on bright_stackoverflow_query_time_ms (8855.3725), Performs well on beir_trec-covid_ndcg@10 (0.6028), Performs well on beir_trec-covid_recall@100 (0.1024), Performs well on beir_trec-covid_index_time_ms (165988.7337), Performs well on beir_trec-covid_query_time_ms (3406.5375), Performs well on avg_ndcg@10 (0.2707), Performs well on avg_recall@100 (0.4581), Performs well on combined_score (0.4206), Performs well on total_index_time_ms (445495.2589), Performs well on total_query_time_ms (54402.0312), Performs well on total_time_ms (499897.2901), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4193)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n\n    # Coordination as a bounded reward (>=1), not a penalty (<1).\n    # Helps nDCG@10 while protecting recall@100.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Use IDF-mass coverage (informative terms matter more).\n    use_idf_coord: bool = True\n\n    # Concave verbosity normalization: reduces over-penalization of long docs.\n    dl_power: float = 0.65\n\n    use_log_qtf: bool = True\n    query_length_norm: bool = True\n\n    # Rarity shaping: cap extreme IDF and mildly sharpen.\n    idf_cap: float = 8.0\n    idf_power: float = 1.1\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Term importance from document frequency.\n\n    Use self-information log((N+1)/df), then (optionally) cap downstream.\n    We keep it simple here and apply the cap where we already have access to Config.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log((N + 1.0) / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique terms with weights.\n\n        Two stabilizers:\n        - sublinear qtf (1+log(qtf)) to avoid repetition dominating;\n        - optional query-length normalization so long queries don't just sum larger.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n\n        if Config.query_length_norm and len(tw) > 0:\n            # L2 normalize weights (keeps relative emphasis but fixes scale across queries).\n            norm = math.sqrt(sum(v * v for v in tw.values()))\n            if norm > 0:\n                inv = 1.0 / norm\n                tw = {t: v * inv for t, v in tw.items()}\n\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    BM25-like evidence + IDF-mass coordination as a bounded reward (>=1).\n\n    Key fix: never multiply evidence by a factor < 1 due to missing terms.\n    Instead, reward documents that cover more (informative) query mass:\n        score *= (1 + alpha * coverage)^beta,   coverage in [0,1].\n    This keeps recall@100 healthier while improving nDCG@10.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Concave verbosity normalization (robust on very long docs).\n    if avgdl > 0:\n        dl_ratio = (doc_length + eps) / (avgdl + eps)\n        norm = (1.0 - b) + b * (dl_ratio ** Config.dl_power)\n    else:\n        norm = 1.0\n\n    score = 0.0\n    qn = len(query_repr.terms)\n    if qn == 0:\n        return 0.0\n\n    idf_total = 0.0\n    idf_matched = 0.0\n    matched = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        tidf = float(idf(df, N))\n        if Config.idf_cap > 0:\n            tidf = min(tidf, Config.idf_cap)\n        tidf = float(max(tidf, 0.0) ** Config.idf_power)\n\n        idf_total += tidf\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        idf_matched += tidf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * tidf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    if Config.use_idf_coord and idf_total > 0.0:\n        coverage = idf_matched / (idf_total + eps)\n    else:\n        coverage = matched / (qn + eps)\n\n    return score * (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Precompute IDF, cap extremes, then sharpen (idf^power).\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        if Config.idf_cap > 0:\n            self.idf_array = np.minimum(self.idf_array, Config.idf_cap)\n        self.idf_array = np.power(np.maximum(self.idf_array, 0.0), Config.idf_power)\n\n        # Keep norm consistent with retrieval_score: concave verbosity normalization.\n        eps = Config.epsilon\n        dl_ratio = (self.doc_lengths + eps) / (max(self.avgdl, eps) + eps)\n        self.norm_array = (1.0 - Config.b) + Config.b * (dl_ratio ** Config.dl_power)\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For coordination: either count matched terms or sum matched idf mass.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idf_total = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            idf_matched += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0 and scores.size > 0:\n            if Config.use_idf_coord and idf_total > 0.0:\n                coverage = idf_matched / (idf_total + eps)\n            else:\n                coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4186)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (as in BM25, but we will change the \"norm\" definition)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # How much to trust \"lexical focus\" (unique terms) vs raw length.\n    # 0 -> pure length, 1 -> pure unique length.\n    focus_mix: float = 0.65\n\n    # Sharpen rarity/discriminativity (idf^power). power>1 penalizes common terms more.\n    idf_power: float = 1.15\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"EVOLVE: query expansion, term weighting, dedup, etc.\"\"\"\n        return cls(terms=tokens, term_weights={t: 1.0 for t in tokens})\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Score one document for one query. This is the lexical retrieval method.\n    EVOLVE: design a formulation with deep, fundamental, intuitive justification.\n    Default: Lucene BM25 (IDF \u00d7 saturated TF, length-normalized).\n    \"\"\"\n    # NOTE: score() is mostly used for single-document scoring.\n    # rank() uses the vectorized path; we mirror the same math here.\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    score = 0.0\n\n    # Approximate unique-length from the tf dict (cheap; avoids needing doc_uniq here).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    # avg \"unique\" length is not available here; fall back to avgdl as a stable proxy.\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().flatten()\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4205)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n\n    # Coordination as a bounded reward (>=1), not a penalty (<1).\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms)\n    focus_mix: float = 0.65\n\n    # Query TF dampening + self-normalization (robust on long/noisy queries)\n    use_log_qtf: bool = True\n    query_length_norm: bool = True\n\n    # Cap extreme IDF so typos/unique IDs don't dominate.\n    idf_cap: float = 8.0\n\n    # NEW: \"anchor-term\" (elite) coverage reward.\n    # Intuition: top-IDF query terms act as intent anchors; reward matching them.\n    elite_frac: float = 0.5     # fraction of query terms considered \"anchors\"\n    elite_alpha: float = 0.35   # reward strength\n    elite_gamma: float = 0.8    # curvature (<1 makes it forgiving)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Term importance from document frequency.\n\n    Use self-information log((N+1)/df), then (optionally) cap downstream.\n    We keep it simple here and apply the cap where we already have access to Config.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log((N + 1.0) / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique terms with weights.\n\n        Two stabilizers:\n        - sublinear qtf (1+log(qtf)) to avoid repetition dominating;\n        - optional query-length normalization so long queries don't just sum larger.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n\n        if Config.query_length_norm and len(tw) > 0:\n            # L2 normalize weights (keeps relative emphasis but fixes scale across queries).\n            norm = math.sqrt(sum(v * v for v in tw.values()))\n            if norm > 0:\n                inv = 1.0 / norm\n                tw = {t: v * inv for t, v in tw.items()}\n\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation + bounded coordination reward (>=1) with two coverages:\n\n    1) IDF-mass coverage (soft AND): how much informative query mass is matched.\n    2) NEW elite/anchor coverage reward: do we match the highest-IDF \"intent anchors\"?\n\n    Both are bounded rewards (never < 1) to keep recall@100 healthy.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    norm = 1.0 - b + b * (eff_len / (max(avgdl, 1.0) + eps))\n\n    qterms = query_repr.terms\n    qn = len(qterms)\n    if qn == 0:\n        return 0.0\n\n    # Precompute capped nonnegative IDF for query terms (query is short; OK to do here).\n    qidf: list[float] = []\n    for term in qterms:\n        df = float(corpus_df.get(term, 1))\n        v = float(idf(df, N))\n        if Config.idf_cap > 0:\n            v = min(v, Config.idf_cap)\n        qidf.append(max(v, 0.0))\n\n    score = 0.0\n    idf_total = 0.0\n    idf_matched = 0.0\n    matched = 0.0\n\n    for term, term_idf in zip(qterms, qidf):\n        idf_total += term_idf\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        idf_matched += term_idf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    # IDF-mass coverage coordination (always on; robust across datasets).\n    coverage = (idf_matched / (idf_total + eps)) if idf_total > 0.0 else (matched / (qn + eps))\n    score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # Elite/anchor reward: top-IDF terms should be present for best ranks.\n    m = int(max(1.0, math.ceil(Config.elite_frac * float(qn))))\n    elite_idx = np.argsort(-np.asarray(qidf, dtype=np.float64))[:m]\n    elite_total = float(np.sum(np.asarray(qidf, dtype=np.float64)[elite_idx]))\n    if elite_total > 0.0 and Config.elite_alpha > 0.0:\n        elite_hit = 0.0\n        for j in elite_idx.tolist():\n            if doc_tf.get(qterms[j], 0) > 0:\n                elite_hit += qidf[j]\n        elite_cov = elite_hit / (elite_total + eps)\n        score *= 1.0 + Config.elite_alpha * (elite_cov ** Config.elite_gamma)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Precompute IDF and apply a robustness cap (mirrors retrieval_score).\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        if Config.idf_cap > 0:\n            self.idf_array = np.minimum(self.idf_array, Config.idf_cap)\n        self.idf_array = np.maximum(self.idf_array, 0.0)\n\n        # Focus-normalized pivoted length normalization (mirrors retrieval_score).\n        self.doc_uniq = np.array([len(tf) for tf in self._doc_tf_dicts], dtype=np.float64)\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (eff_len / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_total = 0.0\n\n        # Elite bookkeeping: accumulate matched idf mass only for top-m idf query terms.\n        qidf = np.asarray([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        qn = float(len(query_term_ids))\n        m = int(max(1.0, math.ceil(Config.elite_frac * qn)))\n        elite_idx = np.argsort(-qidf)[:m]\n        elite_total = float(np.sum(qidf[elite_idx])) + eps\n        elite_hit = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        elite_set = set(elite_idx.tolist())\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            idf_matched += present * idf_val\n            if i in elite_set:\n                elite_hit += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if qn > 1.0 and scores.size > 0:\n            coverage = (idf_matched / (idf_total + eps)) if idf_total > 0.0 else (matched / (qn + eps))\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n            if Config.elite_alpha > 0.0 and elite_total > 0.0:\n                elite_cov = elite_hit / elite_total\n                scores *= 1.0 + Config.elite_alpha * (elite_cov ** Config.elite_gamma)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4197)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n\n    # Coordination as a bounded reward (>=1), not a penalty (<1).\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms)\n    # to avoid over-penalizing long but repetitive docs.\n    focus_mix: float = 0.65\n\n    # Query TF dampening\n    use_log_qtf: bool = True\n\n    # Make term weights self-normalizing so long/noisy queries don't dominate.\n    query_length_norm: bool = True\n\n    # IDF-aware \"soft AND\": reward coverage of informative terms more.\n    use_idf_coord: bool = True\n\n    # Saturate very large idf so single ultra-rare tokens don't overrule everything.\n    idf_cap: float = 8.0\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Term importance from document frequency.\n\n    Use self-information log((N+1)/df), then (optionally) cap downstream.\n    We keep it simple here and apply the cap where we already have access to Config.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log((N + 1.0) / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique terms with weights.\n\n        Two stabilizers:\n        - sublinear qtf (1+log(qtf)) to avoid repetition dominating;\n        - optional query-length normalization so long queries don't just sum larger.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n\n        if Config.query_length_norm and len(tw) > 0:\n            # L2 normalize weights (keeps relative emphasis but fixes scale across queries).\n            norm = math.sqrt(sum(v * v for v in tw.values()))\n            if norm > 0:\n                inv = 1.0 / norm\n                tw = {t: v * inv for t, v in tw.items()}\n\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation + bounded coordination reward (>=1) with IDF-mass coverage.\n\n    Why this helps:\n    - Multiplying by coverage^alpha (<1) can severely downrank partial matches, hurting recall@100.\n    - Using a reward (1 + alpha*coverage)^beta improves top-rank precision (nDCG@10)\n      while keeping partial matches alive.\n    - Focus-normalized length (mix raw length with unique-term count) reduces verbosity bias.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Focus-normalized effective length.\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    # Use avgdl as a stable global reference (unique avg not available in this function).\n    norm = 1.0 - b + b * (eff_len / (max(avgdl, 1.0) + eps))\n\n    score = 0.0\n    qn = len(query_repr.terms)\n\n    idf_total = 0.0\n    idf_matched = 0.0\n    matched = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        if Config.idf_cap > 0:\n            term_idf = min(term_idf, Config.idf_cap)\n        term_idf = max(term_idf, 0.0)\n\n        idf_total += term_idf\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        idf_matched += term_idf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    if Config.use_idf_coord and idf_total > 0.0:\n        coverage = idf_matched / (idf_total + eps)\n    else:\n        coverage = matched / (qn + eps)\n\n    return score * (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Precompute IDF and apply a robustness cap (mirrors retrieval_score).\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        if Config.idf_cap > 0:\n            self.idf_array = np.minimum(self.idf_array, Config.idf_cap)\n        self.idf_array = np.maximum(self.idf_array, 0.0)\n\n        # Focus-normalized pivoted length normalization (mirrors retrieval_score).\n        self.doc_uniq = np.array([len(tf) for tf in self._doc_tf_dicts], dtype=np.float64)\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (eff_len / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For coordination: either count matched terms or sum matched idf mass.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idf_total = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            idf_matched += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0 and scores.size > 0:\n            if Config.use_idf_coord and idf_total > 0.0:\n                coverage = idf_matched / (idf_total + eps)\n            else:\n                coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4206, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (as in BM25, but we will change the \"norm\" definition)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # How much to trust \"lexical focus\" (unique terms) vs raw length.\n    # 0 -> pure length, 1 -> pure unique length.\n    focus_mix: float = 0.65\n\n    # Sharpen rarity/discriminativity (idf^power). power>1 penalizes common terms more.\n    idf_power: float = 1.12\n\n    # Coordination should be a *reward* (>=1), not a penalty (<1).\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # NEW: IDF-mass coverage gate.\n    # Intuition: matching only the \"easy/common\" query terms should not outrank\n    # documents that match the rare/informative parts. Use a *floor* to protect recall@100.\n    idf_gate_alpha: float = 0.7   # curvature; <1 gentler than power>1\n    idf_gate_floor: float = 0.25  # minimum multiplier, prevents crushing partial matches\n\n    # Query repetition is often noise; sublinear qtf improves robustness.\n    use_log_qtf: bool = True\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use distinct query terms + sublinear query-TF weights.\n\n        Rationale:\n        - Coordination/facet coverage is only meaningful over distinct terms.\n        - Repetition in queries often comes from tokenization artifacts / verbosity;\n          (1+log qtf) is a stable, training-free dampener.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + (1) coordination reward + (2) IDF-mass gate.\n\n    New piece (2) is an \"informativeness coverage\" multiplier:\n        gate = floor + (1-floor) * (idf_matched/idf_total)^alpha\n\n    Motivation:\n    - In many benchmarks, queries contain both common \"glue\" words and a few rare key terms.\n      BM25 evidence can let a document matching many common terms outrank a document that\n      matches fewer terms but includes the rare, diagnostic ones.\n    - We *do not* hard-filter; the floor keeps partial matches alive (protect recall@100),\n      while the gate improves top-rank precision (nDCG@10).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    idf_total = 0.0\n    idf_matched = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        idf_total += term_idf\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        idf_matched += term_idf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        if idf_total > 0.0 and Config.idf_gate_alpha > 0.0:\n            idf_cov = idf_matched / (idf_total + eps)  # in [0,1]\n            gate = Config.idf_gate_floor + (1.0 - Config.idf_gate_floor) * (\n                idf_cov ** Config.idf_gate_alpha\n            )\n            score *= gate\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # IDF-mass coverage bookkeeping (vectorized per doc)\n        idf_total = 0.0\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            idf_matched += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n            if idf_total > 0.0 and Config.idf_gate_alpha > 0.0:\n                idf_cov = idf_matched / (idf_total + eps)\n                gate = Config.idf_gate_floor + (1.0 - Config.idf_gate_floor) * (\n                    idf_cov ** Config.idf_gate_alpha\n                )\n                scores *= gate\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(\n                self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist()\n            )\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4101, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # Core saturation / length normalization\n    k1: float = 0.9\n    b: float = 0.4\n    epsilon: float = 1e-9\n\n    # Coordination (kept)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Concave verbosity penalty\n    dl_power: float = 0.6\n\n    # Query-side: downweight very repetitive terms in the query itself\n    qtf_gamma: float = 0.6\n\n    # NEW: cap extreme IDF so typos/IDs don't dominate (robust on noisy corpora)\n    idf_cap: float = 8.0\n\n    # NEW: \"elitist AND\": softly prefers matching the most informative query terms.\n    # We compute coverage over top-m IDF terms only.\n    elite_frac: float = 0.6\n    elite_gamma: float = 0.5\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use distinct query terms (coordination becomes meaningful) and apply a\n        mild sublinear query-TF weight: qtf^gamma. This reduces the impact of\n        repeated tokens from tokenization artifacts (common in some corpora).\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        tw = {t: float(v) ** Config.qtf_gamma for t, v in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Add an \"elitist AND\" on top of BM25-like accumulation.\n\n    Intuition: for multi-term queries, the *most informative* terms (highest IDF)\n    act like anchors. Rewarding coverage of these anchors improves early precision\n    (nDCG@10) while keeping recall stable (soft factor, not hard filtering).\n\n    Also cap extreme IDF to reduce brittleness to unique tokens/typos.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    dl_ratio = (doc_length + eps) / (avgdl + eps) if avgdl > 0 else 1.0\n    norm = (1.0 - b) + b * (dl_ratio ** Config.dl_power)\n\n    # Precompute capped idf per query term (cheap; queries are short).\n    qterms = query_repr.terms\n    qidf = {}\n    for t in qterms:\n        v = float(idf(float(corpus_df.get(t, 1)), N))\n        if Config.idf_cap > 0:\n            v = min(v, Config.idf_cap)\n        qidf[t] = max(v, 0.0)\n\n    score = 0.0\n    matched = 0.0\n    for t in qterms:\n        tf = float(doc_tf.get(t, 0))\n        if tf <= 0:\n            continue\n        matched += 1.0\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(t, 1.0) * qidf[t] * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # Base coordination (as before)\n    qn = max(1.0, float(len(qterms)))\n    frac = matched / qn\n    coord = (1.0 + Config.coord_alpha * frac) ** Config.coord_beta\n\n    # Elitist coverage: only consider top-m IDF query terms.\n    m = int(math.ceil(max(1.0, Config.elite_frac * qn)))\n    elite = sorted(qterms, key=lambda t: qidf[t], reverse=True)[:m]\n    elite_total = sum(qidf[t] for t in elite)\n    if elite_total > 0:\n        elite_hit = sum(qidf[t] for t in elite if doc_tf.get(t, 0) > 0)\n        elite_cov = elite_hit / (elite_total + eps)\n        coord *= (elite_cov + eps) ** Config.elite_gamma\n\n    return score * coord\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        if Config.idf_cap > 0:\n            self.idf_array = np.minimum(self.idf_array, Config.idf_cap)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n\n        dl = self.corpus.doc_lengths[candidate_docs]\n        avgdl = max(self.corpus.avgdl, eps)\n        dl_ratio = (dl + eps) / (avgdl + eps)\n        norms = (1.0 - Config.b) + Config.b * (dl_ratio ** Config.dl_power)\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Elitist-AND bookkeeping (top-m IDF query terms)\n        qidf = np.array([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        qn = float(len(query_term_ids))\n        m = int(math.ceil(max(1.0, Config.elite_frac * qn)))\n        elite_idx = np.argsort(-qidf)[:m]\n        elite_total = float(np.sum(qidf[elite_idx])) + eps\n        elite_hit = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            if i in set(elite_idx.tolist()):\n                elite_hit += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        frac = matched / max(1.0, qn)\n        coord = (1.0 + Config.coord_alpha * frac) ** Config.coord_beta\n        elite_cov = elite_hit / elite_total\n        coord *= (elite_cov + eps) ** Config.elite_gamma\n\n        scores *= coord\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + sublinear query TF\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append(float(count) ** Config.qtf_gamma)\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtw = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtw)\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4156, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n\n    # Soft coordination reward (bounded, recall-safe).\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening\n    use_log_qtf: bool = True\n\n    # NEW: query-adaptive \"informative mass\" coverage gate.\n    # Instead of suppressing common terms individually, prefer documents that\n    # cover the rare/diagnostic part of the query (but never zero-out partial matches).\n    idf_mass_power: float = 0.85   # <1 compresses extreme idf, robust to typos/IDs\n    idf_gate_alpha: float = 0.8    # strength of the gate (0 disables)\n    idf_gate_floor: float = 0.20   # minimum multiplier to protect recall@100\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Smoothed odds-style IDF (Lucene-like, always >=0 after clamp).\n\n    Rationale: log((N-df)/df) is a better proxy for term discriminativity than\n    pure self-information when df is mid/high; it avoids overweighting ultra-rare\n    singletons that can be typos/IDs (important for noisy corpora like StackOverflow).\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique terms with weights.\n\n        Weighting: sublinear qtf (1+log(qtf)) is robust to repeated tokens and\n        tends to improve nDCG when queries contain redundancy/noise.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation + two recall-safe \"soft AND\" tie-breakers:\n\n    1) Distinct-term coverage reward (bounded): encourages covering more query aspects.\n    2) IDF-mass coverage gate: prefers documents that match the *informative* part\n       of the query, without suppressing any term directly.\n\n    This avoids the failure mode where per-term \"common suppression\" downweights\n    genuinely important domain terms (hurting recall), while still combating\n    boilerplate-only matches (improves nDCG).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n    qn = len(query_repr.terms)\n\n    idf_total = 0.0\n    idf_matched = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        # \"Informative mass\": compress extremes for robustness (typos/IDs).\n        tidf = float(idf(df, N))\n        if tidf < 0.0:\n            tidf = 0.0\n        mass = tidf ** Config.idf_mass_power\n        idf_total += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        idf_matched += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * tidf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    # (1) bounded distinct-term coverage reward\n    coverage = matched / (qn + eps)\n    coord = (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # (2) IDF-mass gate with a floor (recall-safe)\n    if Config.idf_gate_alpha > 0.0 and idf_total > 0.0:\n        idf_cov = idf_matched / (idf_total + eps)  # in [0,1]\n        gate = Config.idf_gate_floor + (1.0 - Config.idf_gate_floor) * (idf_cov ** Config.idf_gate_alpha)\n        coord *= gate\n\n    return score * coord\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_total = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val < 0.0:\n                idf_val = 0.0\n            if idf_val <= 0.0:\n                continue\n\n            # informative mass for gating\n            mass = idf_val ** Config.idf_mass_power\n            idf_total += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            idf_matched += present * mass\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n            if Config.idf_gate_alpha > 0.0 and idf_total > 0.0:\n                idf_cov = idf_matched / (idf_total + eps)\n                gate = Config.idf_gate_floor + (1.0 - Config.idf_gate_floor) * (idf_cov ** Config.idf_gate_alpha)\n                scores *= gate\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4013, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.4\n    # Coordination/coverage reward: promotes docs that match more distinct query terms.\n    coord_alpha: float = 0.7\n    # Query TF dampening: qtf_weight = 1 + log(qtf) (if enabled).\n    use_log_qtf: bool = True\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Term importance from document frequency.\n\n    Use a self-information flavored form: log((N+1)/df).\n    Compared to Lucene's smoothed odds ratio, this tends to put a bit more\n    separation between very rare and mid-frequency terms, often helping ranking\n    in technical corpora while staying stable (no negatives).\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log((N + 1.0) / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique terms with weights.\n\n        Weighting: sublinear qtf (1+log(qtf)) is robust to repeated tokens and\n        tends to improve nDCG when queries contain redundancy/noise.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Lexical score with a coordination/coverage factor.\n\n    Base is BM25-like: sum_t w_q(t) * idf(t) * sat_tf(t,d,|d|).\n    Then apply a smooth coordination reward favoring documents that cover more\n    distinct query terms (improves early precision for multi-term queries).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n    norm = 1.0 - b + b * (doc_length / (avgdl + eps)) if avgdl > 0 else 1.0\n\n    score = 0.0\n    matched = 0\n    qn = len(query_repr.terms)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0:\n            continue\n        matched += 1\n        df = float(corpus_df.get(term, 1))\n        term_idf = float(idf(df, N))\n        tf_part = tf / (tf + k1 * norm + eps)\n        w = query_repr.term_weights.get(term, 1.0)\n        score += w * term_idf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    coverage = matched / (qn + eps)\n    coord = (coverage + eps) ** Config.coord_alpha\n    return score * coord\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        b = Config.b\n        self.norm_array = 1.0 - b + b * (self.doc_lengths / max(self.avgdl, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = self.corpus.idf_array[term_id]\n            if idf_val <= 0:\n                continue\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (coverage + eps) ** Config.coord_alpha\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n\n    # Coordination as a bounded reward (>=1), not a penalty (<1).\n    # Helps nDCG@10 while protecting recall@100.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Use IDF-mass coverage (informative terms matter more).\n    use_idf_coord: bool = True\n\n    # Concave verbosity normalization: reduces over-penalization of long docs.\n    dl_power: float = 0.65\n\n    use_log_qtf: bool = True\n    query_length_norm: bool = True\n\n    # Rarity shaping: cap extreme IDF and mildly sharpen.\n    idf_cap: float = 8.0\n    idf_power: float = 1.1\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"\n    Term importance from document frequency.\n\n    Use self-information log((N+1)/df), then (optionally) cap downstream.\n    We keep it simple here and apply the cap where we already have access to Config.\n    \"\"\"\n    df = np.maximum(df, 1.0)\n    return np.log((N + 1.0) / df)\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Represent query as unique terms with weights.\n\n        Two stabilizers:\n        - sublinear qtf (1+log(qtf)) to avoid repetition dominating;\n        - optional query-length normalization so long queries don't just sum larger.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n\n        if Config.query_length_norm and len(tw) > 0:\n            # L2 normalize weights (keeps relative emphasis but fixes scale across queries).\n            norm = math.sqrt(sum(v * v for v in tw.values()))\n            if norm > 0:\n                inv = 1.0 / norm\n                tw = {t: v * inv for t, v in tw.items()}\n\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    BM25-like evidence + IDF-mass coordination as a bounded reward (>=1).\n\n    Key fix: never multiply evidence by a factor < 1 due to missing terms.\n    Instead, reward documents that cover more (informative) query mass:\n        score *= (1 + alpha * coverage)^beta,   coverage in [0,1].\n    This keeps recall@100 healthier while improving nDCG@10.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Concave verbosity normalization (robust on very long docs).\n    if avgdl > 0:\n        dl_ratio = (doc_length + eps) / (avgdl + eps)\n        norm = (1.0 - b) + b * (dl_ratio ** Config.dl_power)\n    else:\n        norm = 1.0\n\n    score = 0.0\n    qn = len(query_repr.terms)\n    if qn == 0:\n        return 0.0\n\n    idf_total = 0.0\n    idf_matched = 0.0\n    matched = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        tidf = float(idf(df, N))\n        if Config.idf_cap > 0:\n            tidf = min(tidf, Config.idf_cap)\n        tidf = float(max(tidf, 0.0) ** Config.idf_power)\n\n        idf_total += tidf\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        idf_matched += tidf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * tidf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    if Config.use_idf_coord and idf_total > 0.0:\n        coverage = idf_matched / (idf_total + eps)\n    else:\n        coverage = matched / (qn + eps)\n\n    return score * (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Precompute IDF, cap extremes, then sharpen (idf^power).\n        self.idf_array = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        if Config.idf_cap > 0:\n            self.idf_array = np.minimum(self.idf_array, Config.idf_cap)\n        self.idf_array = np.power(np.maximum(self.idf_array, 0.0), Config.idf_power)\n\n        # Keep norm consistent with retrieval_score: concave verbosity normalization.\n        eps = Config.epsilon\n        dl_ratio = (self.doc_lengths + eps) / (max(self.avgdl, eps) + eps)\n        self.norm_array = (1.0 - Config.b) + Config.b * (dl_ratio ** Config.dl_power)\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For coordination: either count matched terms or sum matched idf mass.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idf_total = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            idf_matched += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0 and scores.size > 0:\n            if Config.use_idf_coord and idf_total > 0.0:\n                coverage = idf_matched / (idf_total + eps)\n            else:\n                coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n        qtf = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n\n    # Coordination as a bounded reward (>=1), not a penalty (<1).\n    # Helps nDCG@10 while protecting recall@100.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Use IDF-mass coverage (informative terms matter more).\n    use_idf_coord: bool = True\n\n    # Concave verbosity normalization: reduces over-penalization of long docs.\n    dl_power: float = 0.65\n\n    use_log_qtf: bool = True\n    query_length_norm: bool = True\n\n    # Rarity shaping: cap extreme IDF and mildly sharpen.\n    idf_cap: float = 8.0\n    idf_power: float = 1.1\n\n    epsilon: float = 1e-9\n=======\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n\n    # Coordination as a bounded reward (>=1), not a penalty (<1).\n    # Helps nDCG@10 while protecting recall@100.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Use IDF-mass coverage (informative terms matter more).\n    use_idf_coord: bool = True\n\n    # Concave verbosity normalization: reduces over-penalization of long docs.\n    dl_power: float = 0.65\n\n    use_log_qtf: bool = True\n    query_length_norm: bool = True\n\n    # Rarity shaping: cap extreme IDF and mildly sharpen.\n    idf_cap: float = 8.0\n    idf_power: float = 1.1\n\n    # NEW: \"anchor\" (elite) term reward.\n    # Intuition: for many queries, a few highest-IDF terms are intent anchors.\n    # Rewarding their coverage improves early precision (nDCG@10) but is recall-safe\n    # because it's a bounded multiplier (never < 1).\n    elite_frac: float = 0.45     # fraction of query terms considered anchors\n    elite_alpha: float = 0.25    # reward strength\n    elite_gamma: float = 0.7     # curvature (<1 is forgiving)\n\n    epsilon: float = 1e-9\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    BM25-like evidence + IDF-mass coordination as a bounded reward (>=1).\n\n    Key fix: never multiply evidence by a factor < 1 due to missing terms.\n    Instead, reward documents that cover more (informative) query mass:\n        score *= (1 + alpha * coverage)^beta,   coverage in [0,1].\n    This keeps recall@100 healthier while improving nDCG@10.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Concave verbosity normalization (robust on very long docs).\n    if avgdl > 0:\n        dl_ratio = (doc_length + eps) / (avgdl + eps)\n        norm = (1.0 - b) + b * (dl_ratio ** Config.dl_power)\n    else:\n        norm = 1.0\n\n    score = 0.0\n    qn = len(query_repr.terms)\n    if qn == 0:\n        return 0.0\n\n    idf_total = 0.0\n    idf_matched = 0.0\n    matched = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        tidf = float(idf(df, N))\n        if Config.idf_cap > 0:\n            tidf = min(tidf, Config.idf_cap)\n        tidf = float(max(tidf, 0.0) ** Config.idf_power)\n\n        idf_total += tidf\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        idf_matched += tidf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * tidf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    if Config.use_idf_coord and idf_total > 0.0:\n        coverage = idf_matched / (idf_total + eps)\n    else:\n        coverage = matched / (qn + eps)\n\n    return score * (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    BM25-like evidence + (1) IDF-mass coordination reward + (2) anchor-term reward.\n\n    (1) is the existing recall-safe soft-AND.\n    (2) rewards matching the highest-IDF query terms (intent anchors). This is a\n        bounded multiplier (>=1), so it tends to lift correct docs into top ranks\n        (nDCG@10) without suppressing partial matches (recall@100).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Concave verbosity normalization (robust on very long docs).\n    if avgdl > 0:\n        dl_ratio = (doc_length + eps) / (avgdl + eps)\n        norm = (1.0 - b) + b * (dl_ratio ** Config.dl_power)\n    else:\n        norm = 1.0\n\n    score = 0.0\n    qterms = query_repr.terms\n    qn = len(qterms)\n    if qn == 0:\n        return 0.0\n\n    # Precompute shaped (capped, powered) IDF for query terms (query is short).\n    qidf = []\n    for term in qterms:\n        df = float(corpus_df.get(term, 1))\n        v = float(idf(df, N))\n        if Config.idf_cap > 0:\n            v = min(v, Config.idf_cap)\n        qidf.append(float(max(v, 0.0) ** Config.idf_power))\n\n    idf_total = 0.0\n    idf_matched = 0.0\n    matched = 0.0\n\n    for term, tidf in zip(qterms, qidf):\n        idf_total += tidf\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        matched += 1.0\n        idf_matched += tidf\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * tidf * tf_part\n\n    if score <= 0.0 or qn <= 1:\n        return score\n\n    # (1) Coordination reward (optionally IDF-mass based)\n    if Config.use_idf_coord and idf_total > 0.0:\n        coverage = idf_matched / (idf_total + eps)\n    else:\n        coverage = matched / (qn + eps)\n    score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # (2) Anchor/elite reward: top-IDF query terms should be present for best ranks.\n    if Config.elite_alpha > 0.0:\n        m = int(max(1, math.ceil(Config.elite_frac * float(qn))))\n        elite_idx = np.argsort(-np.asarray(qidf, dtype=np.float64))[:m]\n        elite_total = float(np.sum(np.asarray(qidf, dtype=np.float64)[elite_idx]))\n        if elite_total > 0.0:\n            elite_hit = 0.0\n            for j in elite_idx.tolist():\n                if doc_tf.get(qterms[j], 0) > 0:\n                    elite_hit += qidf[j]\n            elite_cov = elite_hit / (elite_total + eps)\n            score *= 1.0 + Config.elite_alpha * (elite_cov ** Config.elite_gamma)\n\n    return score\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For coordination: either count matched terms or sum matched idf mass.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idf_total = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            idf_matched += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0 and scores.size > 0:\n            if Config.use_idf_coord and idf_total > 0.0:\n                coverage = idf_matched / (idf_total + eps)\n            else:\n                coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For coordination: either count matched terms or sum matched idf mass.\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        idf_total = 0.0\n\n        # Elite/anchor bookkeeping (top-m query terms by shaped IDF).\n        qidf = np.asarray([float(self.corpus.idf_array[t]) for t in query_term_ids], dtype=np.float64)\n        qn = float(len(query_term_ids))\n        m = int(max(1.0, math.ceil(Config.elite_frac * qn)))\n        elite_idx = np.argsort(-qidf)[:m]\n        elite_total = float(np.sum(qidf[elite_idx])) + eps\n        elite_hit = np.zeros(len(candidate_docs), dtype=np.float64)\n        elite_set = set(elite_idx.tolist())\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            idf_total += idf_val\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            matched += present\n            idf_matched += present * idf_val\n            if i in elite_set:\n                elite_hit += present * idf_val\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if qn > 1.0 and scores.size > 0:\n            if Config.use_idf_coord and idf_total > 0.0:\n                coverage = idf_matched / (idf_total + eps)\n            else:\n                coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n            if Config.elite_alpha > 0.0 and elite_total > 0.0:\n                elite_cov = elite_hit / elite_total\n                scores *= 1.0 + Config.elite_alpha * (elite_cov ** Config.elite_gamma)\n\n        return scores\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}