{"id": "223d0458-083a-4f48-9db9-fed2f19442c1", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n    # Soften over-saturation for rare terms (idf-aware TF saturation).\n    tf_idf_gamma: float = 0.22   # 0 disables; higher => less saturation for rare terms\n\n    # Discriminativity shaping: normalize IDF by corpus-average IDF (\"lift\").\n    idf_lift_power: float = 0.45  # 0 disables\n\n    # Query DF dropout (only for long/noisy queries).\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # NEW: query-length dampening for coordination.\n    # Intuition: for long queries, \"match many terms\" is often less meaningful (many are modifiers),\n    # so we reduce coordination strength smoothly as query length grows.\n    q_len_dampen: float = 0.08\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: token-mass salience (\"aboutness\") reward, bounded >=1.\n    # Prefer documents where query terms constitute a larger share of the text,\n    # reducing long-doc false positives and often improving nDCG@10.\n    salience_alpha: float = 0.14\n    salience_power: float = 0.50\n\n    # Anchor-first mixing (rare intent terms drive top ranks; keep recall via full channel).\n    anchor_mix_alpha: float = 0.35\n    anchor_mix_power: float = 1.6\n    anchor_residual: float = 0.55\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    # NEW: for long queries, drop ultra-common tokens (df/N above threshold).\n    # NEW: apply an IDF \"lift\" normalization vs mean IDF to stabilize importance across corpora.\n    q_idfs: list[float] = []\n    q_terms: list[str] = []\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Approx mean idf: compute from corpus_df on the fly if corpus doesn't provide it.\n    # (Corpus path will provide mean_idf; this fallback keeps function standalone.)\n    mean_idf = 1.0\n    if hasattr(corpus_df, \"_mean_idf_hint\"):\n        mean_idf = float(getattr(corpus_df, \"_mean_idf_hint\"))\n    # If no hint, keep mean_idf=1.0 (lift becomes near-no-op).\n\n    long_query = len(query_repr.terms) >= Config.q_drop_min_len\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        if long_query and Config.q_drop_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.q_drop_df_ratio:\n                continue\n\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        if Config.idf_lift_power > 0.0:\n            lift = q_idf / (mean_idf + eps)\n            q_idf *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n        q_terms.append(term)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    # For salience/aboutness: how many doc tokens are query tokens.\n    q_mass = 0.0\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        q_mass += tf\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        # IDF-aware TF saturation: rarer terms saturate less.\n        k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (term_idf / (q_idf_max + eps)))\n        tf_part = tf / (tf + k1_eff * norm + eps)\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        if term_idf >= anchor_thr:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(term_idf >= anchor_thr)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        # Dampen coordination for long queries (often noisy/modifier-heavy).\n        q_d = 1.0 / (1.0 + Config.q_len_dampen * max(0.0, qn - 1.0))\n        full_score *= (1.0 + (Config.coord_alpha * and_gate * q_d) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Salience/aboutness multiplier (bounded >= 1): fraction of doc occupied by query tokens.\n    if Config.salience_alpha > 0.0 and doc_length > 0.0 and q_mass > 0.0:\n        frac = q_mass / (doc_length + eps)\n        sal = 1.0 + Config.salience_alpha * (frac ** Config.salience_power)\n        full_score *= sal\n        anchor_score *= sal\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # NEW: collection mean IDF (after base BM25 idf, before power is already applied above).\n        # Used for \"idf lift\" normalization: terms matter insofar as they are more\n        # discriminative than the average term in this corpus.\n        self.mean_idf = float(np.mean(np.maximum(self.idf_array, 0.0))) if self.vocab_size > 0 else 1.0\n        if self.mean_idf <= 0.0:\n            self.mean_idf = 1.0\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        mean_idf = float(getattr(self.corpus, \"mean_idf\", 1.0))\n        if mean_idf <= 0.0:\n            mean_idf = 1.0\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        q_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        long_query = len(query_term_ids) >= Config.q_drop_min_len\n\n        for i, term_id in enumerate(query_term_ids):\n            df = float(self.corpus._df[term_id])\n\n            if long_query and Config.q_drop_df_ratio > 0.0 and self.corpus.N > 0:\n                if (df / float(self.corpus.N)) >= Config.q_drop_df_ratio:\n                    continue\n\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            if Config.idf_lift_power > 0.0:\n                lift = idf_val / (mean_idf + eps)\n                idf_val *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            q_mass += tf_row\n            # IDF-aware TF saturation (vectorized mirror of retrieval_score()).\n            k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (idf_val / (q_idf_max + eps)))\n            tf_part = tf_row / (tf_row + k1_eff * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            q_d = 1.0 / (1.0 + Config.q_len_dampen * max(0.0, qn - 1.0))\n            scores *= (1.0 + (Config.coord_alpha * and_gate * q_d) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Salience/aboutness multiplier (must match retrieval_score()).\n        sal = None\n        if Config.salience_alpha > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            frac = q_mass / (dl + eps)\n            sal = 1.0 + Config.salience_alpha * np.power(frac, Config.salience_power)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            if sal is not None:\n                scores *= sal\n                anchor_scores *= sal\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n        else:\n            if sal is not None:\n                scores *= sal\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "a338b65e-b4bf-481c-a82f-70465c09e8d5", "generation": 11, "timestamp": 1770137410.0318139, "iteration_found": 198, "metrics": {"beir_nfcorpus_ndcg@10": 0.3152506731839931, "beir_nfcorpus_recall@100": 0.2538697127538831, "beir_nfcorpus_index_time_ms": 4179.230709007243, "beir_nfcorpus_query_time_ms": 110.7199169928208, "bright_pony_ndcg@10": 0.087561318727587, "bright_pony_recall@100": 0.2954987203134576, "bright_pony_index_time_ms": 1567.7973339916207, "bright_pony_query_time_ms": 596.8085000058636, "beir_scifact_ndcg@10": 0.6797702638711871, "beir_scifact_recall@100": 0.93, "beir_scifact_index_time_ms": 5415.49837499042, "beir_scifact_query_time_ms": 348.4532079892233, "beir_arguana_ndcg@10": 0.3033593459895595, "beir_arguana_recall@100": 0.9386152748037117, "beir_arguana_index_time_ms": 6461.766916007036, "beir_arguana_query_time_ms": 22683.759624982486, "bright_theoremqa_theorems_ndcg@10": 0.031752001545580895, "bright_theoremqa_theorems_recall@100": 0.16228070175438594, "bright_theoremqa_theorems_index_time_ms": 13081.453375023557, "bright_theoremqa_theorems_query_time_ms": 1128.1652080069762, "beir_scidocs_ndcg@10": 0.14279188164406054, "beir_scidocs_recall@100": 0.3435666666666667, "beir_scidocs_index_time_ms": 26090.95820802031, "beir_scidocs_query_time_ms": 4502.804917021422, "bright_economics_ndcg@10": 0.13154429480701577, "bright_economics_recall@100": 0.4107549992941734, "bright_economics_index_time_ms": 35160.36675000214, "bright_economics_query_time_ms": 5837.276957987342, "bright_biology_ndcg@10": 0.22611061930461376, "bright_biology_recall@100": 0.5125176957828976, "bright_biology_index_time_ms": 37545.41754198726, "bright_biology_query_time_ms": 6732.059041998582, "bright_earth_science_ndcg@10": 0.3419605001353784, "bright_earth_science_recall@100": 0.6729335175058859, "bright_earth_science_index_time_ms": 68702.23608301603, "bright_earth_science_query_time_ms": 8543.624167010421, "beir_fiqa_ndcg@10": 0.2330684751808366, "beir_fiqa_recall@100": 0.533607155019192, "beir_fiqa_index_time_ms": 88115.84516699077, "beir_fiqa_query_time_ms": 8614.619916013908, "bright_stackoverflow_ndcg@10": 0.18004562839763186, "bright_stackoverflow_recall@100": 0.49623096959495316, "bright_stackoverflow_index_time_ms": 159419.53849999118, "bright_stackoverflow_query_time_ms": 9725.284082989674, "beir_trec-covid_ndcg@10": 0.5793676810667201, "beir_trec-covid_recall@100": 0.098692962539572, "beir_trec-covid_index_time_ms": 207542.649540992, "beir_trec-covid_query_time_ms": 1461.6781669901684, "avg_ndcg@10": 0.2710485569878471, "avg_recall@100": 0.47071403133573164, "combined_score": 0.43078093646615473, "total_index_time_ms": 653282.7585000196, "total_query_time_ms": 70285.25370798889, "total_time_ms": 723568.0122080084, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 64 lines with 62 lines\nChange 2: Replace 28 lines with 29 lines\nChange 3: Replace 3 lines with 5 lines\nChange 4: Replace 5 lines with 11 lines\nChange 5: Replace 2 lines with 3 lines\nChange 6: Replace 7 lines with 8 lines\nChange 7: Replace 3 lines with 4 lines\nChange 8: Replace 15 lines with 28 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.31455332785941365, "beir_nfcorpus_recall@100": 0.25357946507586454, "beir_nfcorpus_index_time_ms": 3891.452042007586, "beir_nfcorpus_query_time_ms": 106.86941700987518, "bright_pony_ndcg@10": 0.09054787533594644, "bright_pony_recall@100": 0.3034001575327376, "bright_pony_index_time_ms": 1796.6660000092816, "bright_pony_query_time_ms": 507.5915830093436, "beir_scifact_ndcg@10": 0.678734003566366, "beir_scifact_recall@100": 0.93, "beir_scifact_index_time_ms": 5302.923624985851, "beir_scifact_query_time_ms": 338.58816599240527, "beir_arguana_ndcg@10": 0.2970988626964138, "beir_arguana_recall@100": 0.9350463954318344, "beir_arguana_index_time_ms": 6271.612500015181, "beir_arguana_query_time_ms": 21574.147000006633, "bright_theoremqa_theorems_ndcg@10": 0.03199924201862211, "bright_theoremqa_theorems_recall@100": 0.16228070175438594, "bright_theoremqa_theorems_index_time_ms": 11000.047458015615, "bright_theoremqa_theorems_query_time_ms": 907.3647079931106, "beir_scidocs_ndcg@10": 0.14152130583470668, "beir_scidocs_recall@100": 0.34156666666666674, "beir_scidocs_index_time_ms": 20508.025166986044, "beir_scidocs_query_time_ms": 3352.525666996371, "bright_biology_ndcg@10": 0.24904823360902437, "bright_biology_recall@100": 0.5240063689220562, "bright_biology_index_time_ms": 20628.174542012857, "bright_biology_query_time_ms": 2843.1253749877214, "bright_economics_ndcg@10": 0.1334345615579863, "bright_economics_recall@100": 0.40367433360940186, "bright_economics_index_time_ms": 22458.651124994503, "bright_economics_query_time_ms": 3045.109708007658, "beir_fiqa_ndcg@10": 0.23005034587382547, "beir_fiqa_recall@100": 0.5286737824932269, "beir_fiqa_index_time_ms": 46117.59012500988, "beir_fiqa_query_time_ms": 6520.755125005962, "bright_earth_science_ndcg@10": 0.35012406720356026, "bright_earth_science_recall@100": 0.6711701946218734, "bright_earth_science_index_time_ms": 46931.507667002734, "bright_earth_science_query_time_ms": 4830.224542005453, "bright_stackoverflow_ndcg@10": 0.177991455694105, "bright_stackoverflow_recall@100": 0.49175923317372117, "bright_stackoverflow_index_time_ms": 113192.57854201715, "bright_stackoverflow_query_time_ms": 9860.730082989903, "beir_trec-covid_ndcg@10": 0.5612041941054499, "beir_trec-covid_recall@100": 0.09794951548654779, "beir_trec-covid_index_time_ms": 158043.81091697724, "beir_trec-covid_query_time_ms": 1377.007125003729, "avg_ndcg@10": 0.2713589562796183, "avg_recall@100": 0.470258901230693, "combined_score": 0.43047891224047813, "total_index_time_ms": 456143.0397100339, "total_query_time_ms": 55264.038499008166, "total_time_ms": 511407.0782090421, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4305\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.4300 \u2192 0.4305\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace 3 lines with 16 lines\nChange 2: Replace 3 lines with 10 lines\nChange 3: Replace 8 lines with 8 lines\nChange 4: Replace 14 lines with 39 lines\nChange 5: 'for term, q_idf in zip(query_repr.terms, q_idfs):' to 'for term, q_idf in zip(q_terms, q_idfs):'\nChange 6: 'qn = float(len(query_repr.terms))' to 'qn = float(len(q_terms))'\nChange 7: Replace 2 lines with 5 lines\nChange 8: Replace 9 lines with 20 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3174, beir_nfcorpus_recall@100: 0.2543, beir_nfcorpus_index_time_ms: 3869.4080, beir_nfcorpus_query_time_ms: 157.9924, bright_pony_ndcg@10: 0.0883, bright_pony_recall@100: 0.2945, bright_pony_index_time_ms: 1693.3145, bright_pony_query_time_ms: 565.2502, beir_scifact_ndcg@10: 0.6787, beir_scifact_recall@100: 0.9333, beir_scifact_index_time_ms: 5183.6793, beir_scifact_query_time_ms: 347.8468, beir_arguana_ndcg@10: 0.2971, beir_arguana_recall@100: 0.9350, beir_arguana_index_time_ms: 6363.7125, beir_arguana_query_time_ms: 21581.7105, bright_theoremqa_theorems_ndcg@10: 0.0321, bright_theoremqa_theorems_recall@100: 0.1623, bright_theoremqa_theorems_index_time_ms: 10975.1620, bright_theoremqa_theorems_query_time_ms: 903.9419, beir_scidocs_ndcg@10: 0.1425, beir_scidocs_recall@100: 0.3438, beir_scidocs_index_time_ms: 19887.3635, beir_scidocs_query_time_ms: 3393.2400, bright_economics_ndcg@10: 0.1379, bright_economics_recall@100: 0.4014, bright_economics_index_time_ms: 18808.2286, bright_economics_query_time_ms: 2988.6834, bright_biology_ndcg@10: 0.2412, bright_biology_recall@100: 0.5192, bright_biology_index_time_ms: 21750.6373, bright_biology_query_time_ms: 2690.7042, beir_fiqa_ndcg@10: 0.2316, beir_fiqa_recall@100: 0.5333, beir_fiqa_index_time_ms: 42870.3833, beir_fiqa_query_time_ms: 6278.1854, bright_earth_science_ndcg@10: 0.3553, bright_earth_science_recall@100: 0.6647, bright_earth_science_index_time_ms: 46361.9532, bright_earth_science_query_time_ms: 4651.9268, bright_stackoverflow_ndcg@10: 0.1795, bright_stackoverflow_recall@100: 0.4879, bright_stackoverflow_index_time_ms: 110902.2974, bright_stackoverflow_query_time_ms: 9752.5676, beir_trec-covid_ndcg@10: 0.5789, beir_trec-covid_recall@100: 0.1003, beir_trec-covid_index_time_ms: 154719.7607, beir_trec-covid_query_time_ms: 1408.5600, avg_ndcg@10: 0.2734, avg_recall@100: 0.4692, combined_score: 0.4300, total_index_time_ms: 443385.9003, total_query_time_ms: 54720.6093, total_time_ms: 498106.5096, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace 3 lines with 13 lines\nChange 2: Replace 27 lines with 41 lines\nChange 3: Replace 25 lines with 34 lines\nChange 4: Replace 7 lines with 11 lines\nChange 5: Replace 6 lines with 8 lines\nChange 6: Replace 34 lines with 50 lines\n- Metrics: bright_pony_ndcg@10: 0.0880, bright_pony_recall@100: 0.2938, bright_pony_index_time_ms: 1913.5841, bright_pony_query_time_ms: 813.6110, beir_nfcorpus_ndcg@10: 0.3171, beir_nfcorpus_recall@100: 0.2542, beir_nfcorpus_index_time_ms: 4813.5023, beir_nfcorpus_query_time_ms: 118.7018, beir_scifact_ndcg@10: 0.6782, beir_scifact_recall@100: 0.9333, beir_scifact_index_time_ms: 6410.2006, beir_scifact_query_time_ms: 387.2371, beir_arguana_ndcg@10: 0.2972, beir_arguana_recall@100: 0.9350, beir_arguana_index_time_ms: 7606.4096, beir_arguana_query_time_ms: 26431.2717, bright_theoremqa_theorems_ndcg@10: 0.0320, bright_theoremqa_theorems_recall@100: 0.1623, bright_theoremqa_theorems_index_time_ms: 14244.6443, bright_theoremqa_theorems_query_time_ms: 997.1782, beir_scidocs_ndcg@10: 0.1413, beir_scidocs_recall@100: 0.3440, beir_scidocs_index_time_ms: 24505.7325, beir_scidocs_query_time_ms: 3600.2916, bright_biology_ndcg@10: 0.2419, bright_biology_recall@100: 0.5192, bright_biology_index_time_ms: 25036.9875, bright_biology_query_time_ms: 3193.0206, bright_economics_ndcg@10: 0.1380, bright_economics_recall@100: 0.4059, bright_economics_index_time_ms: 25425.9478, bright_economics_query_time_ms: 3406.6878, bright_earth_science_ndcg@10: 0.3527, bright_earth_science_recall@100: 0.6647, bright_earth_science_index_time_ms: 47050.7473, bright_earth_science_query_time_ms: 5375.4656, beir_fiqa_ndcg@10: 0.2315, beir_fiqa_recall@100: 0.5319, beir_fiqa_index_time_ms: 53469.2711, beir_fiqa_query_time_ms: 6829.4814, bright_stackoverflow_ndcg@10: 0.1779, bright_stackoverflow_recall@100: 0.4922, bright_stackoverflow_index_time_ms: 121007.2165, bright_stackoverflow_query_time_ms: 13312.6367, beir_trec-covid_ndcg@10: 0.5776, beir_trec-covid_recall@100: 0.0996, beir_trec-covid_index_time_ms: 182943.7644, beir_trec-covid_query_time_ms: 1562.0551, avg_ndcg@10: 0.2728, avg_recall@100: 0.4697, combined_score: 0.4303, total_index_time_ms: 514428.0080, total_query_time_ms: 66027.6388, total_time_ms: 580455.6467, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 3 lines with 7 lines\nChange 2: Replace 27 lines with 31 lines\nChange 3: Replace 8 lines with 10 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3146, beir_nfcorpus_recall@100: 0.2536, beir_nfcorpus_index_time_ms: 3891.4520, beir_nfcorpus_query_time_ms: 106.8694, bright_pony_ndcg@10: 0.0905, bright_pony_recall@100: 0.3034, bright_pony_index_time_ms: 1796.6660, bright_pony_query_time_ms: 507.5916, beir_scifact_ndcg@10: 0.6787, beir_scifact_recall@100: 0.9300, beir_scifact_index_time_ms: 5302.9236, beir_scifact_query_time_ms: 338.5882, beir_arguana_ndcg@10: 0.2971, beir_arguana_recall@100: 0.9350, beir_arguana_index_time_ms: 6271.6125, beir_arguana_query_time_ms: 21574.1470, bright_theoremqa_theorems_ndcg@10: 0.0320, bright_theoremqa_theorems_recall@100: 0.1623, bright_theoremqa_theorems_index_time_ms: 11000.0475, bright_theoremqa_theorems_query_time_ms: 907.3647, beir_scidocs_ndcg@10: 0.1415, beir_scidocs_recall@100: 0.3416, beir_scidocs_index_time_ms: 20508.0252, beir_scidocs_query_time_ms: 3352.5257, bright_biology_ndcg@10: 0.2490, bright_biology_recall@100: 0.5240, bright_biology_index_time_ms: 20628.1745, bright_biology_query_time_ms: 2843.1254, bright_economics_ndcg@10: 0.1334, bright_economics_recall@100: 0.4037, bright_economics_index_time_ms: 22458.6511, bright_economics_query_time_ms: 3045.1097, beir_fiqa_ndcg@10: 0.2301, beir_fiqa_recall@100: 0.5287, beir_fiqa_index_time_ms: 46117.5901, beir_fiqa_query_time_ms: 6520.7551, bright_earth_science_ndcg@10: 0.3501, bright_earth_science_recall@100: 0.6712, bright_earth_science_index_time_ms: 46931.5077, bright_earth_science_query_time_ms: 4830.2245, bright_stackoverflow_ndcg@10: 0.1780, bright_stackoverflow_recall@100: 0.4918, bright_stackoverflow_index_time_ms: 113192.5785, bright_stackoverflow_query_time_ms: 9860.7301, beir_trec-covid_ndcg@10: 0.5612, beir_trec-covid_recall@100: 0.0979, beir_trec-covid_index_time_ms: 158043.8109, beir_trec-covid_query_time_ms: 1377.0071, avg_ndcg@10: 0.2714, avg_recall@100: 0.4703, combined_score: 0.4305, total_index_time_ms: 456143.0397, total_query_time_ms: 55264.0385, total_time_ms: 511407.0782, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4311)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n    # --- NEW: soften over-saturation for rare terms ---\n    # Idea: when a term is very rare/discriminative, even small TF differences matter.\n    # Use a slightly smaller k1 for high-idf terms by scaling the saturation denominator.\n    tf_idf_gamma: float = 0.22   # 0 disables; higher => less saturation for rare terms\n\n    # NEW: \"aboutness\"/salience reward (bounded multiplier >= 1).\n    # Intuition: among candidates matching the same rare anchors, prefer documents where\n    # query terms make up a meaningful fraction of the text (reduces long-doc false positives),\n    # typically improving nDCG@10 while being recall-safe because it's only a multiplier.\n    salience_alpha: float = 0.18   # 0 disables; keep small\n    salience_power: float = 0.50   # sqrt: gentle, avoids overfitting\n\n    # --- NEW: discriminativity shaping ---\n    # Convert raw IDF into a *lift* over the collection's average IDF:\n    #   idf_lift = idf / mean_idf\n    # This makes \"important\" mean \"more discriminative than average\" and reduces\n    # dataset-to-dataset drift where absolute idf scale differs.\n    idf_lift_power: float = 0.45  # 0 disables lift; small power keeps it gentle\n\n    # --- NEW: query DF dropout (only for long/noisy queries) ---\n    # For long queries, extremely common tokens behave like glue and increase false positives.\n    # We drop terms with df/N above threshold, but only when query length >= q_drop_min_len.\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    # NEW: for long queries, drop ultra-common tokens (df/N above threshold).\n    # NEW: apply an IDF \"lift\" normalization vs mean IDF to stabilize importance across corpora.\n    q_idfs: list[float] = []\n    q_terms: list[str] = []\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Approx mean idf: compute from corpus_df on the fly if corpus doesn't provide it.\n    # (Corpus path will provide mean_idf; this fallback keeps function standalone.)\n    mean_idf = 1.0\n    if hasattr(corpus_df, \"_mean_idf_hint\"):\n        mean_idf = float(getattr(corpus_df, \"_mean_idf_hint\"))\n    # If no hint, keep mean_idf=1.0 (lift becomes near-no-op).\n\n    long_query = len(query_repr.terms) >= Config.q_drop_min_len\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        if long_query and Config.q_drop_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.q_drop_df_ratio:\n                continue\n\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        if Config.idf_lift_power > 0.0:\n            lift = q_idf / (mean_idf + eps)\n            q_idf *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n        q_terms.append(term)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    # Token-mass of (kept) query terms inside this document (for salience/aboutness).\n    q_mass = 0.0\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        q_mass += tf\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        # IDF-aware TF saturation:\n        # rarer terms get a smaller effective denominator => less TF saturation.\n        k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (term_idf / (q_idf_max + eps)))\n        tf_part = tf / (tf + k1_eff * norm + eps)\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Query-term salience (aboutness): fraction of doc tokens that are query terms.\n    # Apply as a gentle bounded reward to BOTH channels (it is query-dependent evidence).\n    if Config.salience_alpha > 0.0 and doc_length > 0.0 and q_mass > 0.0:\n        mass_frac = q_mass / (doc_length + eps)\n        sal = 1.0 + Config.salience_alpha * (mass_frac ** Config.salience_power)\n        full_score *= sal\n        anchor_score *= sal\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # NEW: collection mean IDF (after base BM25 idf, before power is already applied above).\n        # Used for \"idf lift\" normalization: terms matter insofar as they are more\n        # discriminative than the average term in this corpus.\n        self.mean_idf = float(np.mean(np.maximum(self.idf_array, 0.0))) if self.vocab_size > 0 else 1.0\n        if self.mean_idf <= 0.0:\n            self.mean_idf = 1.0\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        mean_idf = float(getattr(self.corpus, \"mean_idf\", 1.0))\n        if mean_idf <= 0.0:\n            mean_idf = 1.0\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        q_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        long_query = len(query_term_ids) >= Config.q_drop_min_len\n\n        for i, term_id in enumerate(query_term_ids):\n            df = float(self.corpus._df[term_id])\n\n            if long_query and Config.q_drop_df_ratio > 0.0 and self.corpus.N > 0:\n                if (df / float(self.corpus.N)) >= Config.q_drop_df_ratio:\n                    continue\n\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            if Config.idf_lift_power > 0.0:\n                lift = idf_val / (mean_idf + eps)\n                idf_val *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            q_mass += tf_row\n            # IDF-aware TF saturation (vectorized mirror of retrieval_score()).\n            k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (idf_val / (q_idf_max + eps)))\n            tf_part = tf_row / (tf_row + k1_eff * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Salience/aboutness multiplier (must mirror retrieval_score()).\n        if Config.salience_alpha > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            mass_frac = q_mass / (dl + eps)\n            sal = 1.0 + Config.salience_alpha * np.power(mass_frac, Config.salience_power)\n        else:\n            sal = None\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            if sal is not None:\n                scores *= sal\n                anchor_scores *= sal\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n        else:\n            if sal is not None:\n                scores *= sal\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3156), Performs well on beir_nfcorpus_recall@100 (0.2540), Performs well on beir_nfcorpus_index_time_ms (3832.8140), Performs well on beir_nfcorpus_query_time_ms (111.6095), Performs well on bright_pony_ndcg@10 (0.0931), Performs well on bright_pony_recall@100 (0.2970), Performs well on bright_pony_index_time_ms (1459.1534), Performs well on bright_pony_query_time_ms (516.9328), Performs well on beir_scifact_ndcg@10 (0.6808), Performs well on beir_scifact_recall@100 (0.9300), Performs well on beir_scifact_index_time_ms (4995.1130), Performs well on beir_scifact_query_time_ms (362.9218), Performs well on beir_arguana_ndcg@10 (0.3033), Performs well on beir_arguana_recall@100 (0.9393), Performs well on beir_arguana_index_time_ms (6262.3883), Performs well on beir_arguana_query_time_ms (21982.5742), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0318), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (11120.3139), Performs well on bright_theoremqa_theorems_query_time_ms (933.2900), Performs well on beir_scidocs_ndcg@10 (0.1430), Performs well on beir_scidocs_recall@100 (0.3438), Performs well on beir_scidocs_index_time_ms (20409.1543), Performs well on beir_scidocs_query_time_ms (3501.2599), Performs well on bright_economics_ndcg@10 (0.1363), Performs well on bright_economics_recall@100 (0.4052), Performs well on bright_economics_index_time_ms (21323.6072), Performs well on bright_economics_query_time_ms (2999.4342), Performs well on bright_biology_ndcg@10 (0.2337), Performs well on bright_biology_recall@100 (0.5185), Performs well on bright_biology_index_time_ms (21992.2133), Performs well on bright_biology_query_time_ms (2864.0176), Performs well on bright_earth_science_ndcg@10 (0.3454), Performs well on bright_earth_science_recall@100 (0.6729), Performs well on bright_earth_science_index_time_ms (45718.5758), Performs well on bright_earth_science_query_time_ms (4998.3019), Performs well on beir_fiqa_ndcg@10 (0.2348), Performs well on beir_fiqa_recall@100 (0.5343), Performs well on beir_fiqa_index_time_ms (48840.2106), Performs well on beir_fiqa_query_time_ms (6370.9475), Performs well on bright_stackoverflow_ndcg@10 (0.1766), Performs well on bright_stackoverflow_recall@100 (0.4922), Performs well on bright_stackoverflow_index_time_ms (110734.3736), Performs well on bright_stackoverflow_query_time_ms (10404.3024), Performs well on beir_trec-covid_ndcg@10 (0.5787), Performs well on beir_trec-covid_recall@100 (0.0995), Performs well on beir_trec-covid_index_time_ms (156805.5743), Performs well on beir_trec-covid_query_time_ms (1379.3762), Performs well on avg_ndcg@10 (0.2728), Performs well on avg_recall@100 (0.4707), Performs well on combined_score (0.4311), Performs well on total_index_time_ms (453493.4917), Performs well on total_query_time_ms (56424.9680), Performs well on total_time_ms (509918.4597), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4305)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n    # --- NEW: soften over-saturation for rare terms ---\n    # Idea: when a term is very rare/discriminative, even small TF differences matter.\n    # Use a slightly smaller k1 for high-idf terms by scaling the saturation denominator.\n    tf_idf_gamma: float = 0.22   # 0 disables; higher => less saturation for rare terms\n\n    # --- NEW: discriminativity shaping ---\n    # Convert raw IDF into a *lift* over the collection's average IDF:\n    #   idf_lift = idf / mean_idf\n    # This makes \"important\" mean \"more discriminative than average\" and reduces\n    # dataset-to-dataset drift where absolute idf scale differs.\n    idf_lift_power: float = 0.45  # 0 disables lift; small power keeps it gentle\n\n    # --- NEW: query DF dropout (only for long/noisy queries) ---\n    # For long queries, extremely common tokens behave like glue and increase false positives.\n    # We drop terms with df/N above threshold, but only when query length >= q_drop_min_len.\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    # NEW: for long queries, drop ultra-common tokens (df/N above threshold).\n    # NEW: apply an IDF \"lift\" normalization vs mean IDF to stabilize importance across corpora.\n    q_idfs: list[float] = []\n    q_terms: list[str] = []\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Approx mean idf: compute from corpus_df on the fly if corpus doesn't provide it.\n    # (Corpus path will provide mean_idf; this fallback keeps function standalone.)\n    mean_idf = 1.0\n    if hasattr(corpus_df, \"_mean_idf_hint\"):\n        mean_idf = float(getattr(corpus_df, \"_mean_idf_hint\"))\n    # If no hint, keep mean_idf=1.0 (lift becomes near-no-op).\n\n    long_query = len(query_repr.terms) >= Config.q_drop_min_len\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        if long_query and Config.q_drop_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.q_drop_df_ratio:\n                continue\n\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        if Config.idf_lift_power > 0.0:\n            lift = q_idf / (mean_idf + eps)\n            q_idf *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n        q_terms.append(term)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        # IDF-aware TF saturation:\n        # rarer terms get a smaller effective denominator => less TF saturation.\n        k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (term_idf / (q_idf_max + eps)))\n        tf_part = tf / (tf + k1_eff * norm + eps)\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # NEW: collection mean IDF (after base BM25 idf, before power is already applied above).\n        # Used for \"idf lift\" normalization: terms matter insofar as they are more\n        # discriminative than the average term in this corpus.\n        self.mean_idf = float(np.mean(np.maximum(self.idf_array, 0.0))) if self.vocab_size > 0 else 1.0\n        if self.mean_idf <= 0.0:\n            self.mean_idf = 1.0\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        mean_idf = float(getattr(self.corpus, \"mean_idf\", 1.0))\n        if mean_idf <= 0.0:\n            mean_idf = 1.0\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        long_query = len(query_term_ids) >= Config.q_drop_min_len\n\n        for i, term_id in enumerate(query_term_ids):\n            df = float(self.corpus._df[term_id])\n\n            if long_query and Config.q_drop_df_ratio > 0.0 and self.corpus.N > 0:\n                if (df / float(self.corpus.N)) >= Config.q_drop_df_ratio:\n                    continue\n\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            if Config.idf_lift_power > 0.0:\n                lift = idf_val / (mean_idf + eps)\n                idf_val *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            # IDF-aware TF saturation (vectorized mirror of retrieval_score()).\n            k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (idf_val / (q_idf_max + eps)))\n            tf_part = tf_row / (tf_row + k1_eff * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3146), Performs well on beir_nfcorpus_recall@100 (0.2536), Performs well on beir_nfcorpus_index_time_ms (3891.4520), Performs well on beir_nfcorpus_query_time_ms (106.8694), Performs well on bright_pony_ndcg@10 (0.0905), Performs well on bright_pony_recall@100 (0.3034), Performs well on bright_pony_index_time_ms (1796.6660), Performs well on bright_pony_query_time_ms (507.5916), Performs well on beir_scifact_ndcg@10 (0.6787), Performs well on beir_scifact_recall@100 (0.9300), Performs well on beir_scifact_index_time_ms (5302.9236), Performs well on beir_scifact_query_time_ms (338.5882), Performs well on beir_arguana_ndcg@10 (0.2971), Performs well on beir_arguana_recall@100 (0.9350), Performs well on beir_arguana_index_time_ms (6271.6125), Performs well on beir_arguana_query_time_ms (21574.1470), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0320), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (11000.0475), Performs well on bright_theoremqa_theorems_query_time_ms (907.3647), Performs well on beir_scidocs_ndcg@10 (0.1415), Performs well on beir_scidocs_recall@100 (0.3416), Performs well on beir_scidocs_index_time_ms (20508.0252), Performs well on beir_scidocs_query_time_ms (3352.5257), Performs well on bright_biology_ndcg@10 (0.2490), Performs well on bright_biology_recall@100 (0.5240), Performs well on bright_biology_index_time_ms (20628.1745), Performs well on bright_biology_query_time_ms (2843.1254), Performs well on bright_economics_ndcg@10 (0.1334), Performs well on bright_economics_recall@100 (0.4037), Performs well on bright_economics_index_time_ms (22458.6511), Performs well on bright_economics_query_time_ms (3045.1097), Performs well on beir_fiqa_ndcg@10 (0.2301), Performs well on beir_fiqa_recall@100 (0.5287), Performs well on beir_fiqa_index_time_ms (46117.5901), Performs well on beir_fiqa_query_time_ms (6520.7551), Performs well on bright_earth_science_ndcg@10 (0.3501), Performs well on bright_earth_science_recall@100 (0.6712), Performs well on bright_earth_science_index_time_ms (46931.5077), Performs well on bright_earth_science_query_time_ms (4830.2245), Performs well on bright_stackoverflow_ndcg@10 (0.1780), Performs well on bright_stackoverflow_recall@100 (0.4918), Performs well on bright_stackoverflow_index_time_ms (113192.5785), Performs well on bright_stackoverflow_query_time_ms (9860.7301), Performs well on beir_trec-covid_ndcg@10 (0.5612), Performs well on beir_trec-covid_recall@100 (0.0979), Performs well on beir_trec-covid_index_time_ms (158043.8109), Performs well on beir_trec-covid_query_time_ms (1377.0071), Performs well on avg_ndcg@10 (0.2714), Performs well on avg_recall@100 (0.4703), Performs well on combined_score (0.4305), Performs well on total_index_time_ms (456143.0397), Performs well on total_query_time_ms (55264.0385), Performs well on total_time_ms (511407.0782), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4303)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # NEW: query-tail \"must match\" soft gate (recall-safe multiplier).\n    # Intuition: many datasets have queries with a rare intent term + many modifiers.\n    # Additive evidence already captures this, but top ranks can be polluted by docs that\n    # match only broad terms. We compute a rare-tail coverage mass and apply a bounded\n    # reward >= 1 for covering that tail with some evidence.\n    tail_tau: float = 0.60          # tail threshold as fraction of query max-idf\n    tail_alpha: float = 0.10        # multiplier strength (small)\n    tail_tf_gamma: float = 0.60     # soften evidence inside tail mass (concave)\n    tail_sharp: float = 2.0         # diminishing returns: 1-exp(-sharp*mass)\n\n    # --- NEW: discriminativity shaping ---\n    # Convert raw IDF into a *lift* over the collection's average IDF:\n    #   idf_lift = idf / mean_idf\n    # This makes \"important\" mean \"more discriminative than average\" and reduces\n    # dataset-to-dataset drift where absolute idf scale differs.\n    idf_lift_power: float = 0.45  # 0 disables lift; small power keeps it gentle\n\n    # --- NEW: query DF dropout (only for long/noisy queries) ---\n    # For long queries, extremely common tokens behave like glue and increase false positives.\n    # We drop terms with df/N above threshold, but only when query length >= q_drop_min_len.\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    # NEW: for long queries, drop ultra-common tokens (df/N above threshold).\n    # NEW: apply an IDF \"lift\" normalization vs mean IDF to stabilize importance across corpora.\n    q_idfs: list[float] = []\n    q_terms: list[str] = []\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Approx mean idf: compute from corpus_df on the fly if corpus doesn't provide it.\n    # (Corpus path will provide mean_idf; this fallback keeps function standalone.)\n    mean_idf = 1.0\n    if hasattr(corpus_df, \"_mean_idf_hint\"):\n        mean_idf = float(getattr(corpus_df, \"_mean_idf_hint\"))\n    # If no hint, keep mean_idf=1.0 (lift becomes near-no-op).\n\n    long_query = len(query_repr.terms) >= Config.q_drop_min_len\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        if long_query and Config.q_drop_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.q_drop_df_ratio:\n                continue\n\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        if Config.idf_lift_power > 0.0:\n            lift = q_idf / (mean_idf + eps)\n            q_idf *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n        q_terms.append(term)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    # NEW: rare-tail coverage bookkeeping (doc-independent threshold, doc-dependent mass).\n    tail_thr = Config.tail_tau * q_idf_max\n    tail_den = 0.0\n    if q_idf_max > 0.0 and Config.tail_alpha > 0.0:\n        for v in q_idfs:\n            if v >= tail_thr:\n                tail_den += v\n\n    tail_num = 0.0\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        # NEW: accumulate rare-tail mass only when the term is in the tail.\n        if tail_den > 0.0 and term_idf >= tail_thr:\n            tail_num += term_idf * (tf_part ** Config.tail_tf_gamma)\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # NEW: rare-tail coverage reward (bounded multiplier >= 1, gated by and_gate).\n    # Helps nDCG@10 by preferring docs that cover the informative subset rather than\n    # many broad modifiers; should be recall-safe because it's only a reward.\n    if Config.tail_alpha > 0.0 and tail_den > 0.0 and tail_num > 0.0:\n        mass = min(1.0, max(0.0, tail_num / (tail_den + eps)))\n        sat = 1.0 - math.exp(-Config.tail_sharp * mass)\n        full_score *= 1.0 + (Config.tail_alpha * and_gate) * sat\n        anchor_score *= 1.0 + (Config.tail_alpha * and_gate) * sat\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # NEW: collection mean IDF (after base BM25 idf, before power is already applied above).\n        # Used for \"idf lift\" normalization: terms matter insofar as they are more\n        # discriminative than the average term in this corpus.\n        self.mean_idf = float(np.mean(np.maximum(self.idf_array, 0.0))) if self.vocab_size > 0 else 1.0\n        if self.mean_idf <= 0.0:\n            self.mean_idf = 1.0\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        mean_idf = float(getattr(self.corpus, \"mean_idf\", 1.0))\n        if mean_idf <= 0.0:\n            mean_idf = 1.0\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        # NEW: rare-tail coverage bookkeeping (vectorized mirror of retrieval_score()).\n        tail_den = 0.0\n        tail_nums: list[tuple[float, NDArray[np.float64], NDArray[np.float64]]] = []  # (idf, tf_part, present)\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        long_query = len(query_term_ids) >= Config.q_drop_min_len\n\n        for i, term_id in enumerate(query_term_ids):\n            df = float(self.corpus._df[term_id])\n\n            if long_query and Config.q_drop_df_ratio > 0.0 and self.corpus.N > 0:\n                if (df / float(self.corpus.N)) >= Config.q_drop_df_ratio:\n                    continue\n\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            if Config.idf_lift_power > 0.0:\n                lift = idf_val / (mean_idf + eps)\n                idf_val *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n            tail_nums.append((idf_val, tf_part, present))\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # NEW: rare-tail coverage reward (bounded, recall-safe). Compute tail denom from q_idf_max.\n        if Config.tail_alpha > 0.0 and q_idf_max > 0.0 and len(tail_nums) > 0:\n            tail_thr = Config.tail_tau * q_idf_max\n            tail_den = 0.0\n            for v, _, _ in tail_nums:\n                if v >= tail_thr:\n                    tail_den += v\n            if tail_den > 0.0:\n                tail_num = np.zeros(len(candidate_docs), dtype=np.float64)\n                for v, tfp, pres in tail_nums:\n                    if v >= tail_thr:\n                        tail_num += v * np.power(tfp, Config.tail_tf_gamma) * pres\n                mass = np.minimum(1.0, np.maximum(0.0, tail_num / (tail_den + eps)))\n                sat = 1.0 - np.exp(-Config.tail_sharp * mass)\n                scores *= 1.0 + (Config.tail_alpha * and_gate) * sat\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.0880), Performs well on bright_pony_recall@100 (0.2938), Performs well on bright_pony_index_time_ms (1913.5841), Performs well on bright_pony_query_time_ms (813.6110), Performs well on beir_nfcorpus_ndcg@10 (0.3171), Performs well on beir_nfcorpus_recall@100 (0.2542), Performs well on beir_nfcorpus_index_time_ms (4813.5023), Performs well on beir_nfcorpus_query_time_ms (118.7018), Performs well on beir_scifact_ndcg@10 (0.6782), Performs well on beir_scifact_recall@100 (0.9333), Performs well on beir_scifact_index_time_ms (6410.2006), Performs well on beir_scifact_query_time_ms (387.2371), Performs well on beir_arguana_ndcg@10 (0.2972), Performs well on beir_arguana_recall@100 (0.9350), Performs well on beir_arguana_index_time_ms (7606.4096), Performs well on beir_arguana_query_time_ms (26431.2717), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0320), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (14244.6443), Performs well on bright_theoremqa_theorems_query_time_ms (997.1782), Performs well on beir_scidocs_ndcg@10 (0.1413), Performs well on beir_scidocs_recall@100 (0.3440), Performs well on beir_scidocs_index_time_ms (24505.7325), Performs well on beir_scidocs_query_time_ms (3600.2916), Performs well on bright_biology_ndcg@10 (0.2419), Performs well on bright_biology_recall@100 (0.5192), Performs well on bright_biology_index_time_ms (25036.9875), Performs well on bright_biology_query_time_ms (3193.0206), Performs well on bright_economics_ndcg@10 (0.1380), Performs well on bright_economics_recall@100 (0.4059), Performs well on bright_economics_index_time_ms (25425.9478), Performs well on bright_economics_query_time_ms (3406.6878), Performs well on bright_earth_science_ndcg@10 (0.3527), Performs well on bright_earth_science_recall@100 (0.6647), Performs well on bright_earth_science_index_time_ms (47050.7473), Performs well on bright_earth_science_query_time_ms (5375.4656), Performs well on beir_fiqa_ndcg@10 (0.2315), Performs well on beir_fiqa_recall@100 (0.5319), Performs well on beir_fiqa_index_time_ms (53469.2711), Performs well on beir_fiqa_query_time_ms (6829.4814), Performs well on bright_stackoverflow_ndcg@10 (0.1779), Performs well on bright_stackoverflow_recall@100 (0.4922), Performs well on bright_stackoverflow_index_time_ms (121007.2165), Performs well on bright_stackoverflow_query_time_ms (13312.6367), Performs well on beir_trec-covid_ndcg@10 (0.5776), Performs well on beir_trec-covid_recall@100 (0.0996), Performs well on beir_trec-covid_index_time_ms (182943.7644), Performs well on beir_trec-covid_query_time_ms (1562.0551), Performs well on avg_ndcg@10 (0.2728), Performs well on avg_recall@100 (0.4697), Performs well on combined_score (0.4303), Performs well on total_index_time_ms (514428.0080), Performs well on total_query_time_ms (66027.6388), Performs well on total_time_ms (580455.6467), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4300)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # --- NEW: discriminativity shaping ---\n    # Convert raw IDF into a *lift* over the collection's average IDF:\n    #   idf_lift = idf / mean_idf\n    # This makes \"important\" mean \"more discriminative than average\" and reduces\n    # dataset-to-dataset drift where absolute idf scale differs.\n    idf_lift_power: float = 0.45  # 0 disables lift; small power keeps it gentle\n\n    # --- NEW: query DF dropout (only for long/noisy queries) ---\n    # For long queries, extremely common tokens behave like glue and increase false positives.\n    # We drop terms with df/N above threshold, but only when query length >= q_drop_min_len.\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    # NEW: for long queries, drop ultra-common tokens (df/N above threshold).\n    # NEW: apply an IDF \"lift\" normalization vs mean IDF to stabilize importance across corpora.\n    q_idfs: list[float] = []\n    q_terms: list[str] = []\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Approx mean idf: compute from corpus_df on the fly if corpus doesn't provide it.\n    # (Corpus path will provide mean_idf; this fallback keeps function standalone.)\n    mean_idf = 1.0\n    if hasattr(corpus_df, \"_mean_idf_hint\"):\n        mean_idf = float(getattr(corpus_df, \"_mean_idf_hint\"))\n    # If no hint, keep mean_idf=1.0 (lift becomes near-no-op).\n\n    long_query = len(query_repr.terms) >= Config.q_drop_min_len\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        if long_query and Config.q_drop_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.q_drop_df_ratio:\n                continue\n\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        if Config.idf_lift_power > 0.0:\n            lift = q_idf / (mean_idf + eps)\n            q_idf *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n        q_terms.append(term)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # NEW: collection mean IDF (after base BM25 idf, before power is already applied above).\n        # Used for \"idf lift\" normalization: terms matter insofar as they are more\n        # discriminative than the average term in this corpus.\n        self.mean_idf = float(np.mean(np.maximum(self.idf_array, 0.0))) if self.vocab_size > 0 else 1.0\n        if self.mean_idf <= 0.0:\n            self.mean_idf = 1.0\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        mean_idf = float(getattr(self.corpus, \"mean_idf\", 1.0))\n        if mean_idf <= 0.0:\n            mean_idf = 1.0\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        long_query = len(query_term_ids) >= Config.q_drop_min_len\n\n        for i, term_id in enumerate(query_term_ids):\n            df = float(self.corpus._df[term_id])\n\n            if long_query and Config.q_drop_df_ratio > 0.0 and self.corpus.N > 0:\n                if (df / float(self.corpus.N)) >= Config.q_drop_df_ratio:\n                    continue\n\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            if Config.idf_lift_power > 0.0:\n                lift = idf_val / (mean_idf + eps)\n                idf_val *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3174), Performs well on beir_nfcorpus_recall@100 (0.2543), Performs well on beir_nfcorpus_index_time_ms (3869.4080), Performs well on beir_nfcorpus_query_time_ms (157.9924), Performs well on bright_pony_ndcg@10 (0.0883), Performs well on bright_pony_recall@100 (0.2945), Performs well on bright_pony_index_time_ms (1693.3145), Performs well on bright_pony_query_time_ms (565.2502), Performs well on beir_scifact_ndcg@10 (0.6787), Performs well on beir_scifact_recall@100 (0.9333), Performs well on beir_scifact_index_time_ms (5183.6793), Performs well on beir_scifact_query_time_ms (347.8468), Performs well on beir_arguana_ndcg@10 (0.2971), Performs well on beir_arguana_recall@100 (0.9350), Performs well on beir_arguana_index_time_ms (6363.7125), Performs well on beir_arguana_query_time_ms (21581.7105), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0321), Performs well on bright_theoremqa_theorems_recall@100 (0.1623), Performs well on bright_theoremqa_theorems_index_time_ms (10975.1620), Performs well on bright_theoremqa_theorems_query_time_ms (903.9419), Performs well on beir_scidocs_ndcg@10 (0.1425), Performs well on beir_scidocs_recall@100 (0.3438), Performs well on beir_scidocs_index_time_ms (19887.3635), Performs well on beir_scidocs_query_time_ms (3393.2400), Performs well on bright_economics_ndcg@10 (0.1379), Performs well on bright_economics_recall@100 (0.4014), Performs well on bright_economics_index_time_ms (18808.2286), Performs well on bright_economics_query_time_ms (2988.6834), Performs well on bright_biology_ndcg@10 (0.2412), Performs well on bright_biology_recall@100 (0.5192), Performs well on bright_biology_index_time_ms (21750.6373), Performs well on bright_biology_query_time_ms (2690.7042), Performs well on beir_fiqa_ndcg@10 (0.2316), Performs well on beir_fiqa_recall@100 (0.5333), Performs well on beir_fiqa_index_time_ms (42870.3833), Performs well on beir_fiqa_query_time_ms (6278.1854), Performs well on bright_earth_science_ndcg@10 (0.3553), Performs well on bright_earth_science_recall@100 (0.6647), Performs well on bright_earth_science_index_time_ms (46361.9532), Performs well on bright_earth_science_query_time_ms (4651.9268), Performs well on bright_stackoverflow_ndcg@10 (0.1795), Performs well on bright_stackoverflow_recall@100 (0.4879), Performs well on bright_stackoverflow_index_time_ms (110902.2974), Performs well on bright_stackoverflow_query_time_ms (9752.5676), Performs well on beir_trec-covid_ndcg@10 (0.5789), Performs well on beir_trec-covid_recall@100 (0.1003), Performs well on beir_trec-covid_index_time_ms (154719.7607), Performs well on beir_trec-covid_query_time_ms (1408.5600), Performs well on avg_ndcg@10 (0.2734), Performs well on avg_recall@100 (0.4692), Performs well on combined_score (0.4300), Performs well on total_index_time_ms (443385.9003), Performs well on total_query_time_ms (54720.6093), Performs well on total_time_ms (498106.5096), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4295)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept modest; mainly helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms tend to be \"glue\"; softly downweight instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query-specificity gate for AND-like rewards (coordination + pair synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: Weighted Surprisal Satisfaction (WSS)\n    # Reward covering the *rare-term tail* of the query (surprisal mass) with evidence.\n    # Use as a bounded multiplier >= 1 (recall-safe), and gate it by spec_gate so peaky\n    # queries don't over-amplify a single anchor term.\n    sat_alpha: float = 0.12       # strength of the bounded reward\n    sat_power: float = 1.25       # emphasize rare terms (idf^power)\n    sat_sharp: float = 2.0        # diminishing returns: 1-exp(-sharp*mass)\n    sat_tf_gamma: float = 0.60    # evidence softness inside satisfaction (concave)\n    sat_tail_tau: float = 0.50    # keep only idf mass above tau*max_idf (rare-tail focus)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + bounded coordination reward + light co-occurrence synergy.\n\n    Refinement: gate the \"AND-like\" rewards (coordination + pair synergy) by a\n    doc-independent query-specificity proxy.\n\n    NEW: information-mass coverage (soft-AND over *surprisal mass*).\n    Treat shaped IDF as a proxy for surprisal; reward docs that cover more of the\n    query's total surprisal mass with non-trivial evidence (tf saturation), but as a\n    bounded multiplier >= 1 to avoid harming recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query-specificity gate (computed with the same common-term penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Tail-satisfaction denominator: surprisal mass budget on the rare tail of the query.\n    sat_den = 0.0\n\n    # Cache shaped query-idfs (doc-independent) to avoid recomputing them twice.\n    q_terms = query_repr.terms\n    q_idfs: list[float] = []\n\n    for t in q_terms:\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0  # higher => peakier\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Define \"rare tail\" relative to the query's max-idf.\n    tail_thr = Config.sat_tail_tau * q_idf_max\n    if Config.sat_alpha > 0.0 and q_idf_max > 0.0:\n        for v in q_idfs:\n            if v >= tail_thr:\n                sat_den += v ** Config.sat_power\n\n    sat_num = 0.0  # evidence-weighted covered tail mass\n\n    for term, term_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n\n        # Satisfaction: only count rare-tail mass, and require non-trivial evidence.\n        if sat_den > 0.0 and term_idf >= tail_thr:\n            sat_num += (term_idf ** Config.sat_power) * (tf_part ** Config.sat_tf_gamma)\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # NEW: weighted surprisal satisfaction (bounded, recall-safe) gated by query specificity.\n    if Config.sat_alpha > 0.0 and sat_den > 0.0 and sat_num > 0.0:\n        mass = min(1.0, max(0.0, sat_num / (sat_den + eps)))\n        sat = 1.0 - math.exp(-Config.sat_sharp * mass)\n        score *= 1.0 + (Config.sat_alpha * spec_gate) * sat\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Keep matched-term components for synergy (doc-major arrays per term).\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Tail satisfaction (vectorized mirror of retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        # Keep per-term penalized idf to define the tail threshold after we know max.\n        term_idfs: list[float] = []\n        term_tfps: list[NDArray[np.float64]] = []\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n            term_idfs.append(idf_val)\n            term_tfps.append(tf_part)\n\n        # Query-specificity gate (must match retrieval_score()).\n        qn_eff = float(len(term_idfs))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn_eff > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn_eff > 1.0:\n            coverage = matched / (qn_eff + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Co-occurrence synergy (same as before).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Tail satisfaction multiplier (bounded), gated by spec_gate.\n        if Config.sat_alpha > 0.0 and q_idf_max > 0.0:\n            tail_thr = Config.sat_tail_tau * q_idf_max\n            idf_arr = np.asarray(term_idfs, dtype=np.float64)\n\n            tail_mask = idf_arr >= tail_thr\n            if np.any(tail_mask):\n                sat_den = float(np.sum(np.power(idf_arr[tail_mask], Config.sat_power)))\n                if sat_den > 0.0:\n                    sat_num = np.zeros(len(candidate_docs), dtype=np.float64)\n                    for v, tfp, pres in zip(term_idfs, term_tfps, presents):\n                        if v >= tail_thr:\n                            sat_num += (v ** Config.sat_power) * np.power(tfp, Config.sat_tf_gamma) * pres\n                    mass = np.minimum(1.0, np.maximum(0.0, sat_num / (sat_den + eps)))\n                    sat = 1.0 - np.exp(-Config.sat_sharp * mass)\n                    scores *= 1.0 + (Config.sat_alpha * spec_gate) * sat\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4297)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # NEW: self-information gating for very common terms.\n    # Even after IDF, extremely high-df tokens can still create false positives (esp. long docs).\n    # Use an information-theoretic view: effective evidence should shrink as p(t)=df/N grows.\n    # We implement a smooth multiplicative shrink: idf_eff = idf * (1 - p)^p_df_power.\n    # This is 1 for rare terms and ->0 as p->1, without hard cutoffs.\n    p_df_power: float = 0.65\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    q_idfs: list[float] = []\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        # Smooth df-based shrink (information content): penalize as df/N grows.\n        if N > 0 and Config.p_df_power > 0.0:\n            p = min(1.0, max(0.0, df / (float(N) + eps)))\n            q_idf *= (max(0.0, 1.0 - p) ** Config.p_df_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    for term, q_idf in zip(query_repr.terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n\n            # Mirror retrieval_score(): smooth df-based shrink (information content).\n            if self.corpus.N > 0 and Config.p_df_power > 0.0:\n                p = min(1.0, max(0.0, df / (float(self.corpus.N) + eps)))\n                idf_val *= (max(0.0, 1.0 - p) ** Config.p_df_power)\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4298)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Query-term token-mass salience (aboutness) boost:\n    # Reward documents where matched query terms occupy a larger share of the document.\n    # This is lexical and query-dependent: it helps distinguish \"about the topic\"\n    # from \"mentions many terms once\", improving early precision on long documents.\n    salience_alpha: float = 0.22   # 0 disables; keep small/bounded\n    salience_power: float = 0.5    # sqrt by default (gentle)\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    q_idfs: list[float] = []\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    # Token-mass of query terms inside this document (for aboutness/salience).\n    q_mass = 0.0\n\n    for term, q_idf in zip(query_repr.terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        q_mass += tf\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Query-term salience (aboutness): fraction of doc tokens that are query terms.\n    # Apply as a gentle bounded reward to BOTH channels (it is query-dependent evidence).\n    if Config.salience_alpha > 0.0 and doc_length > 0.0 and q_mass > 0.0:\n        mass_frac = q_mass / (doc_length + eps)\n        sal = 1.0 + Config.salience_alpha * (mass_frac ** Config.salience_power)\n        full_score *= sal\n        anchor_score *= sal\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        q_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            q_mass += tf_row\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Query-term salience (aboutness) \u2014 must mirror retrieval_score().\n        if Config.salience_alpha > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            mass_frac = q_mass / (dl + eps)\n            sal = 1.0 + Config.salience_alpha * np.power(mass_frac, Config.salience_power)\n        else:\n            sal = None\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            # Apply salience to both channels before mixing (mirrors retrieval_score()).\n            if sal is not None:\n                scores *= sal\n                anchor_scores *= sal\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n        else:\n            if sal is not None:\n                scores *= sal\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4297)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # NEW: query-length-aware coverage reward.\n    # Long queries (esp. in BEIR/BRIGHT) often contain many low-value modifiers; rewarding\n    # \"matched many query terms\" can then over-promote generic docs.\n    # We damp the coordination reward by 1/(1 + q_len_dampen*(qn-1)).\n    q_len_dampen: float = 0.10\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixture.\n    # Rationale: many queries have 1\u20132 intent-defining rare terms; broad modifiers can\n    # cause false positives at top ranks. Blend full evidence with an anchor-only score\n    # (anchors = terms close to max IDF in the query), with weight increasing in query peakiness.\n    anchor_mix_alpha: float = 0.30   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # mapping from peakiness -> weight\n    anchor_residual: float = 0.55    # anchor threshold as fraction of query max-idf\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + gated \"soft AND\", plus an anchor-first mixture.\n\n    Anchor-first idea:\n    - Compute full_score from all query terms (recall-friendly).\n    - Compute anchor_score from only the most discriminative query terms\n      (idf close to the query max), then mix based on query peakiness.\n    This tends to improve nDCG@10 by preventing broad modifiers from outranking\n    the doc that best matches the \"intent\" term(s), while keeping recall@100 via full_score.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates (doc-independent), computed under the same \"common term\" penalty.\n    q_idfs: list[float] = []\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n\n    # (A) Peakiness gate: high for balanced, low for peaky.\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # (B) Entropy gate.\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor definition + mixture weight (both doc-independent)\n    anchor_thr = Config.anchor_residual * q_idf_max\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    for term, q_idf in zip(query_repr.terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n        if term_idf >= anchor_thr:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        # Length-aware coordination: keep the helpful \"soft AND\" for short queries,\n        # but reduce its dominance for long/noisy queries to protect nDCG@10.\n        q_d = 1.0 / (1.0 + Config.q_len_dampen * max(0.0, qn - 1.0))\n        full_score *= (1.0 + (Config.coord_alpha * and_gate * q_d) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Mixture: keep a floor so multi-term good matches don't collapse when anchors are absent.\n    if w_anchor > 0.0 and q_idf_max > 0.0:\n        anchor_score = max(anchor_score, 0.15 * full_score)\n        return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n    return full_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query specificity stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        # Entropy gate (must mirror retrieval_score()) using query-term idfs after penalty.\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            q_d = 1.0 / (1.0 + Config.q_len_dampen * max(0.0, qn - 1.0))\n            scores *= (1.0 + (Config.coord_alpha * and_gate * q_d) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4293, Type: Migrant)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept modest; mainly helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms tend to be \"glue\"; softly downweight instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query-specificity gate for AND-like rewards (coordination + pair synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: \"information mass coverage\" multiplier (soft-AND over surprisal mass).\n    # Intuition: nDCG@10 often improves when top results cover a larger fraction of the\n    # query's *informative* terms. Keep bounded (multiplier >= 1) to be recall-safe.\n    info_cov_alpha: float = 0.10   # small, precision-oriented\n    info_cov_gamma: float = 0.70   # concave: early informative matches help most\n    info_cov_tf_gamma: float = 0.50  # evidence softening inside coverage (sqrt(tf_sat))\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + bounded coordination reward + light co-occurrence synergy.\n\n    Refinement: gate the \"AND-like\" rewards (coordination + pair synergy) by a\n    doc-independent query-specificity proxy.\n\n    NEW: information-mass coverage (soft-AND over *surprisal mass*).\n    Treat shaped IDF as a proxy for surprisal; reward docs that cover more of the\n    query's total surprisal mass with non-trivial evidence (tf saturation), but as a\n    bounded multiplier >= 1 to avoid harming recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query-specificity gate (computed with the same common-term penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_info = 0.0  # total penalized IDF mass (surprisal budget)\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        q_info += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0  # higher => peakier\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    match_info = 0.0  # evidence-weighted matched surprisal mass\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n\n        # Soft evidence: prevents one-off mentions of rare terms from dominating \"coverage\".\n        match_info += term_idf * (tf_part ** Config.info_cov_tf_gamma)\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # NEW: information-mass coverage multiplier (bounded, recall-safe).\n    if Config.info_cov_alpha > 0.0 and q_info > 0.0 and match_info > 0.0:\n        cov = match_info / (q_info + eps)  # ~[0,1]\n        cov = min(1.0, max(0.0, cov))\n        score *= 1.0 + Config.info_cov_alpha * (cov ** Config.info_cov_gamma)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Keep matched-term components for synergy (doc-major arrays per term).\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # For info-mass coverage multiplier.\n        q_info = 0.0\n        match_info = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_info += idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            # Evidence-weighted matched surprisal mass (softened by sqrt(tf_sat)).\n            match_info += idf_val * np.power(tf_part, Config.info_cov_tf_gamma) * present\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query-specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Co-occurrence synergy: emphasize distinctive pairs (residualized vs max-idf),\n        # but gate it by query specificity to avoid over-rewarding broad modifiers.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Mirror retrieval_score(): info-mass coverage multiplier (bounded).\n        if Config.info_cov_alpha > 0.0 and q_info > 0.0:\n            cov = match_info / (q_info + eps)\n            cov = np.minimum(1.0, np.maximum(0.0, cov))\n            scores *= 1.0 + Config.info_cov_alpha * np.power(cov, Config.info_cov_gamma)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4292, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    # (A bounded \"AND-ish\" effect that usually helps nDCG@10 without killing recall.)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards.\n\n    Design:\n    - Base evidence: BM25-like saturating TF evidence (diminishing returns).\n    - Length prior: pivoted normalization on a blend of token length and unique-term length\n      (prefers focused docs without crushing long informative ones).\n    - Collection prior: smoothly downweight ultra-common (high-df) terms.\n    - Coordination + distinctive pair synergy: multiplicative rewards gated by query specificity\n      (avoid over-rewarding generic multi-matches when a query is dominated by one sharp term).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms.\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, gated by query specificity.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4300, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # --- NEW: discriminativity shaping ---\n    # Convert raw IDF into a *lift* over the collection's average IDF:\n    #   idf_lift = idf / mean_idf\n    # This makes \"important\" mean \"more discriminative than average\" and reduces\n    # dataset-to-dataset drift where absolute idf scale differs.\n    idf_lift_power: float = 0.45  # 0 disables lift; small power keeps it gentle\n\n    # --- NEW: query DF dropout (only for long/noisy queries) ---\n    # For long queries, extremely common tokens behave like glue and increase false positives.\n    # We drop terms with df/N above threshold, but only when query length >= q_drop_min_len.\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    # NEW: for long queries, drop ultra-common tokens (df/N above threshold).\n    # NEW: apply an IDF \"lift\" normalization vs mean IDF to stabilize importance across corpora.\n    q_idfs: list[float] = []\n    q_terms: list[str] = []\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Approx mean idf: compute from corpus_df on the fly if corpus doesn't provide it.\n    # (Corpus path will provide mean_idf; this fallback keeps function standalone.)\n    mean_idf = 1.0\n    if hasattr(corpus_df, \"_mean_idf_hint\"):\n        mean_idf = float(getattr(corpus_df, \"_mean_idf_hint\"))\n    # If no hint, keep mean_idf=1.0 (lift becomes near-no-op).\n\n    long_query = len(query_repr.terms) >= Config.q_drop_min_len\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        if long_query and Config.q_drop_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.q_drop_df_ratio:\n                continue\n\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        if Config.idf_lift_power > 0.0:\n            lift = q_idf / (mean_idf + eps)\n            q_idf *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n        q_terms.append(term)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # NEW: collection mean IDF (after base BM25 idf, before power is already applied above).\n        # Used for \"idf lift\" normalization: terms matter insofar as they are more\n        # discriminative than the average term in this corpus.\n        self.mean_idf = float(np.mean(np.maximum(self.idf_array, 0.0))) if self.vocab_size > 0 else 1.0\n        if self.mean_idf <= 0.0:\n            self.mean_idf = 1.0\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        mean_idf = float(getattr(self.corpus, \"mean_idf\", 1.0))\n        if mean_idf <= 0.0:\n            mean_idf = 1.0\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        long_query = len(query_term_ids) >= Config.q_drop_min_len\n\n        for i, term_id in enumerate(query_term_ids):\n            df = float(self.corpus._df[term_id])\n\n            if long_query and Config.q_drop_df_ratio > 0.0 and self.corpus.N > 0:\n                if (df / float(self.corpus.N)) >= Config.q_drop_df_ratio:\n                    continue\n\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            if Config.idf_lift_power > 0.0:\n                lift = idf_val / (mean_idf + eps)\n                idf_val *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4292, Type: Migrant)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    k1: float = 0.9\n    b: float = 0.35\n    focus_mix: float = 0.65\n    idf_power: float = 1.12\n\n    # Soft-AND reward; keep as-is but gate by query \"peakiness\"\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    use_log_qtf: bool = True\n\n    # Common-term soft penalty\n    common_df_cut: float = 0.12\n    common_penalty: float = 0.35\n\n    # Pair synergy; also gated by query peakiness\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # NEW: peakiness gate for \"AND-like\" effects (coord + pair).\n    # spec = max_idf / sum_idf; peaky queries (one dominant term) => smaller gate.\n    spec_floor: float = 0.55\n    spec_power: float = 1.2\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base model, with a *query-peakiness gate* on coordination + pair synergy.\n\n    Why: many queries contain one very distinctive anchor term plus several broad modifiers.\n    Raw coordination/pair boosts can over-promote generic docs matching many broad terms.\n    Gate those \"AND-like\" effects by spec = max_idf/sum_idf (peaky => smaller gate).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query peakiness (doc-independent) using same shaped IDF + common penalty.\n    q_sum = 0.0\n    q_max = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        v = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        q_sum += v\n        if v > q_max:\n            q_max = v\n    spec = (q_max / (q_sum + eps)) if q_sum > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Peakiness gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n    # --- NEW: soften over-saturation for rare terms ---\n    # Idea: when a term is very rare/discriminative, even small TF differences matter.\n    # Use a slightly smaller k1 for high-idf terms by scaling the saturation denominator.\n    tf_idf_gamma: float = 0.22   # 0 disables; higher => less saturation for rare terms\n\n    # --- NEW: discriminativity shaping ---\n    # Convert raw IDF into a *lift* over the collection's average IDF:\n    #   idf_lift = idf / mean_idf\n    # This makes \"important\" mean \"more discriminative than average\" and reduces\n    # dataset-to-dataset drift where absolute idf scale differs.\n    idf_lift_power: float = 0.45  # 0 disables lift; small power keeps it gentle\n\n    # --- NEW: query DF dropout (only for long/noisy queries) ---\n    # For long queries, extremely common tokens behave like glue and increase false positives.\n    # We drop terms with df/N above threshold, but only when query length >= q_drop_min_len.\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Two-channel lexical scoring:\n\n    1) full_score: additive saturated evidence + gated \"soft AND\" (coord + pair)\n    2) anchor_score: same evidence but only for \"anchor-like\" query terms\n       (terms whose shaped-idf is close to the query max-idf).\n\n    Final score is a query-dependent mixture. For peaky queries we trust anchors more\n    (improves nDCG@10 by reducing modifier-driven false positives), while full_score\n    maintains recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    full_score = 0.0\n    anchor_score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n    m_is_anchor: list[bool] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates + anchor set (doc-independent).\n    # NEW: for long queries, drop ultra-common tokens (df/N above threshold).\n    # NEW: apply an IDF \"lift\" normalization vs mean IDF to stabilize importance across corpora.\n    q_idfs: list[float] = []\n    q_terms: list[str] = []\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n\n    # Approx mean idf: compute from corpus_df on the fly if corpus doesn't provide it.\n    # (Corpus path will provide mean_idf; this fallback keeps function standalone.)\n    mean_idf = 1.0\n    if hasattr(corpus_df, \"_mean_idf_hint\"):\n        mean_idf = float(getattr(corpus_df, \"_mean_idf_hint\"))\n    # If no hint, keep mean_idf=1.0 (lift becomes near-no-op).\n\n    long_query = len(query_repr.terms) >= Config.q_drop_min_len\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        if long_query and Config.q_drop_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.q_drop_df_ratio:\n                continue\n\n        q_idf = float(max(float(idf(df, N)), 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        if Config.idf_lift_power > 0.0:\n            lift = q_idf / (mean_idf + eps)\n            q_idf *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n        q_terms.append(term)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(q_terms))\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    # Anchor mixture weight: larger when query is peaky (spec large).\n    # Use a smooth monotone mapping of spec to [0, anchor_mix_alpha].\n    w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n\n    # Anchor threshold in shaped-idf space.\n    anchor_thr = Config.anchor_residual * q_idf_max\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        # IDF-aware TF saturation:\n        # rarer terms get a smaller effective denominator => less TF saturation.\n        k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (term_idf / (q_idf_max + eps)))\n        tf_part = tf / (tf + k1_eff * norm + eps)\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n\n    if full_score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    # Pair synergy stays on full_score (anchors are meant to be conservative).\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        full_score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # NEW: collection mean IDF (after base BM25 idf, before power is already applied above).\n        # Used for \"idf lift\" normalization: terms matter insofar as they are more\n        # discriminative than the average term in this corpus.\n        self.mean_idf = float(np.mean(np.maximum(self.idf_array, 0.0))) if self.vocab_size > 0 else 1.0\n        if self.mean_idf <= 0.0:\n            self.mean_idf = 1.0\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        mean_idf = float(getattr(self.corpus, \"mean_idf\", 1.0))\n        if mean_idf <= 0.0:\n            mean_idf = 1.0\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query gate stats (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        long_query = len(query_term_ids) >= Config.q_drop_min_len\n\n        for i, term_id in enumerate(query_term_ids):\n            df = float(self.corpus._df[term_id])\n\n            if long_query and Config.q_drop_df_ratio > 0.0 and self.corpus.N > 0:\n                if (df / float(self.corpus.N)) >= Config.q_drop_df_ratio:\n                    continue\n\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            if Config.idf_lift_power > 0.0:\n                lift = idf_val / (mean_idf + eps)\n                idf_val *= float(max(lift, 0.0) ** Config.idf_lift_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            # IDF-aware TF saturation (vectorized mirror of retrieval_score()).\n            k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (idf_val / (q_idf_max + eps)))\n            tf_part = tf_row / (tf_row + k1_eff * norms + eps)\n\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        # Pair synergy on full score.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n    # --- NEW: soften over-saturation for rare terms ---\n    # Idea: when a term is very rare/discriminative, even small TF differences matter.\n    # Use a slightly smaller k1 for high-idf terms by scaling the saturation denominator.\n    tf_idf_gamma: float = 0.22   # 0 disables; higher => less saturation for rare terms\n\n    # --- NEW: discriminativity shaping ---\n    # Convert raw IDF into a *lift* over the collection's average IDF:\n    #   idf_lift = idf / mean_idf\n    # This makes \"important\" mean \"more discriminative than average\" and reduces\n    # dataset-to-dataset drift where absolute idf scale differs.\n    idf_lift_power: float = 0.45  # 0 disables lift; small power keeps it gentle\n\n    # --- NEW: query DF dropout (only for long/noisy queries) ---\n    # For long queries, extremely common tokens behave like glue and increase false positives.\n    # We drop terms with df/N above threshold, but only when query length >= q_drop_min_len.\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: \"anchor-first\" mixing. Many tasks have 1\u20132 intent-defining rare terms.\n    # We blend a pure-anchor score with the full evidence score:\n    #   final = (1-w)*full + w*anchor\n    # where w is high when the query is peaky (max-idf dominates sum-idf).\n    # This often improves nDCG@10 by preventing broad modifiers from outranking\n    # the document that best matches the anchor, while recall@100 is kept by full.\n    anchor_mix_alpha: float = 0.35   # maximum mixture weight\n    anchor_mix_power: float = 1.6    # sharpness vs peakiness\n    anchor_residual: float = 0.55    # which terms count as \"anchor-like\" (as fraction of max_idf)\n\n    epsilon: float = 1e-9\n=======\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n    # Soften over-saturation for rare terms (idf-aware TF saturation).\n    tf_idf_gamma: float = 0.22   # 0 disables; higher => less saturation for rare terms\n\n    # Discriminativity shaping: normalize IDF by corpus-average IDF (\"lift\").\n    idf_lift_power: float = 0.45  # 0 disables\n\n    # Query DF dropout (only for long/noisy queries).\n    q_drop_min_len: int = 8\n    q_drop_df_ratio: float = 0.22\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # NEW: query-length dampening for coordination.\n    # Intuition: for long queries, \"match many terms\" is often less meaningful (many are modifiers),\n    # so we reduce coordination strength smoothly as query length grows.\n    q_len_dampen: float = 0.08\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Query entropy gate (complements \"peaky vs balanced\").\n    entropy_floor: float = 0.35\n    entropy_power: float = 0.9\n\n    # NEW: token-mass salience (\"aboutness\") reward, bounded >=1.\n    # Prefer documents where query terms constitute a larger share of the text,\n    # reducing long-doc false positives and often improving nDCG@10.\n    salience_alpha: float = 0.14\n    salience_power: float = 0.50\n\n    # Anchor-first mixing (rare intent terms drive top ranks; keep recall via full channel).\n    anchor_mix_alpha: float = 0.35\n    anchor_mix_power: float = 1.6\n    anchor_residual: float = 0.55\n\n    epsilon: float = 1e-9\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        # IDF-aware TF saturation:\n        # rarer terms get a smaller effective denominator => less TF saturation.\n        k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (term_idf / (q_idf_max + eps)))\n        tf_part = tf / (tf + k1_eff * norm + eps)\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        is_anchor = term_idf >= anchor_thr\n        if is_anchor:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(is_anchor)\n=======\n    # For salience/aboutness: how many doc tokens are query tokens.\n    q_mass = 0.0\n\n    for term, q_idf in zip(q_terms, q_idfs):\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n        q_mass += tf\n\n        term_idf = q_idf\n        if term_idf <= 0.0:\n            continue\n\n        # IDF-aware TF saturation: rarer terms saturate less.\n        k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (term_idf / (q_idf_max + eps)))\n        tf_part = tf / (tf + k1_eff * norm + eps)\n\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        add = wq * term_idf * tf_part\n        full_score += add\n\n        if term_idf >= anchor_thr:\n            anchor_score += add\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n        m_is_anchor.append(term_idf >= anchor_thr)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        full_score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n=======\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        # Dampen coordination for long queries (often noisy/modifier-heavy).\n        q_d = 1.0 / (1.0 + Config.q_len_dampen * max(0.0, qn - 1.0))\n        full_score *= (1.0 + (Config.coord_alpha * and_gate * q_d) * coverage) ** Config.coord_beta\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    # keep a tiny floor fraction of full_score in the anchor channel.\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n=======\n    # Salience/aboutness multiplier (bounded >= 1): fraction of doc occupied by query tokens.\n    if Config.salience_alpha > 0.0 and doc_length > 0.0 and q_mass > 0.0:\n        frac = q_mass / (doc_length + eps)\n        sal = 1.0 + Config.salience_alpha * (frac ** Config.salience_power)\n        full_score *= sal\n        anchor_score *= sal\n\n    # Mixture. Ensure anchor_score doesn't go to 0 for multi-term matches that lack the max term:\n    anchor_score = max(anchor_score, 0.15 * full_score)\n\n    return (1.0 - w_anchor) * full_score + w_anchor * anchor_score\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n=======\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        q_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            # IDF-aware TF saturation (vectorized mirror of retrieval_score()).\n            k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (idf_val / (q_idf_max + eps)))\n            tf_part = tf_row / (tf_row + k1_eff * norms + eps)\n\n            scores += w * idf_val * tf_part\n=======\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            q_mass += tf_row\n            # IDF-aware TF saturation (vectorized mirror of retrieval_score()).\n            k1_eff = k1 / (1.0 + Config.tf_idf_gamma * (idf_val / (q_idf_max + eps)))\n            tf_part = tf_row / (tf_row + k1_eff * norms + eps)\n\n            scores += w * idf_val * tf_part\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n=======\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            q_d = 1.0 / (1.0 + Config.q_len_dampen * max(0.0, qn - 1.0))\n            scores *= (1.0 + (Config.coord_alpha * and_gate * q_d) * coverage) ** Config.coord_beta\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            # anchor_scores: sum only for anchor-like terms, but computed from already-built pieces.\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n\n        return scores\n=======\n        # Salience/aboutness multiplier (must match retrieval_score()).\n        sal = None\n        if Config.salience_alpha > 0.0:\n            dl = self.corpus.doc_lengths[candidate_docs]\n            frac = q_mass / (dl + eps)\n            sal = 1.0 + Config.salience_alpha * np.power(frac, Config.salience_power)\n\n        # Anchor-first mixing (must match retrieval_score()).\n        if q_idf_max > 0.0 and Config.anchor_mix_alpha > 0.0:\n            w_anchor = Config.anchor_mix_alpha * (spec ** Config.anchor_mix_power)\n            anchor_thr = Config.anchor_residual * q_idf_max\n\n            anchor_scores = np.zeros(len(candidate_docs), dtype=np.float64)\n            for idf_val, w, tfp in zip(idfs, ws, tfparts):\n                if idf_val >= anchor_thr:\n                    anchor_scores += w * idf_val * tfp\n\n            if sal is not None:\n                scores *= sal\n                anchor_scores *= sal\n\n            anchor_scores = np.maximum(anchor_scores, 0.15 * scores)\n            scores = (1.0 - w_anchor) * scores + w_anchor * anchor_scores\n        else:\n            if sal is not None:\n                scores *= sal\n\n        return scores\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}