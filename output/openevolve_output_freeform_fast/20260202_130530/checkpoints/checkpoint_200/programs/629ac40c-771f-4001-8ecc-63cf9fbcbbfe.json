{"id": "629ac40c-771f-4001-8ecc-63cf9fbcbbfe", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    # (A bounded \"AND-ish\" effect that usually helps nDCG@10 without killing recall.)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: soft-AND over *information mass* (idf-budget coverage).\n    # Rewards docs that cover a larger fraction of the query's discriminative mass,\n    # reducing the \"many weak modifiers beat one strong anchor\" failure mode.\n    info_cov_alpha: float = 0.18\n    info_cov_beta: float = 0.85\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards.\n\n    Additions:\n    - Anchor coverage reward: many queries have 1\u20132 \"anchor\" terms (entities / key concepts).\n      We compute the IDF-mass of the top anchors and reward documents that cover them\n      (with TF saturation). This is a bounded multiplicative bonus, so recall@100 is\n      largely preserved while nDCG@10 often improves on entity+modifier queries.\n    - Implementation: merge query-stat loops to keep overhead small.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # One pass: query stats for specificity, positive budget, and anchor selection.\n    q_weights: list[tuple[str, float]] = []\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_pos_budget = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        w = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            w *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += w\n        if w > q_idf_max:\n            q_idf_max = w\n        if w > 0.0:\n            q_pos_budget += w\n        q_weights.append((term, w))\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Anchor definition: top-2 query terms by shaped weight (after common penalty).\n    # Small A keeps it stable and cheap; reward is bounded and mild.\n    ANCHOR_K = 2\n    q_weights.sort(key=lambda x: x[1], reverse=True)\n    anchor_mass = 0.0\n    anchor_terms: list[str] = []\n    for t, w in q_weights[:ANCHOR_K]:\n        if w > 0.0:\n            anchor_terms.append(t)\n            anchor_mass += w\n\n    matched_info_mass = 0.0\n    matched_anchor_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        if term_idf > 0.0:\n            matched_info_mass += term_idf * tf_part\n            if term in anchor_terms:\n                matched_anchor_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        if Config.info_cov_alpha > 0.0 and q_pos_budget > 0.0:\n            info_cov = min(1.0, matched_info_mass / (q_pos_budget + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    # Anchor reward (bounded): prefer docs that cover the anchor terms of the query.\n    # The bonus is intentionally weak to avoid harming recall in noisy corpora.\n    if anchor_mass > 0.0:\n        anchor_cov = min(1.0, matched_anchor_mass / (anchor_mass + eps))\n        # Slightly stronger when the query is peaky (spec high -> spec_gate low), since then\n        # anchors matter even more. Use (1 - spec_gate) as a simple complement.\n        score *= 1.0 + 0.10 * (0.5 + 0.5 * (1.0 - spec_gate)) * anchor_cov\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # Info-mass coverage reward (mirrors retrieval_score()).\n            tfp = None\n            if Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.maximum(0.0, np.array(idfs, dtype=np.float64))\n                budget = float(np.sum(idf_arr))\n                if budget > 0.0:\n                    tfp = np.vstack(tfparts)\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    info_cov = np.minimum(1.0, matched_mass / (budget + eps))\n                    scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n            # Anchor coverage reward: top-2 idf terms in the query.\n            # Uses tfp if available; otherwise stack once here (still small: |q| x |cand|).\n            if len(idfs) > 0 and len(tfparts) > 0:\n                if tfp is None:\n                    tfp = np.vstack(tfparts)\n                idf_pos = np.maximum(0.0, np.array(idfs, dtype=np.float64))\n                if idf_pos.size >= 1:\n                    # indices of top-2 anchors\n                    topk = 2 if idf_pos.size >= 2 else 1\n                    anchor_idx = np.argpartition(-idf_pos, topk - 1)[:topk]\n                    anchor_mass = float(np.sum(idf_pos[anchor_idx]))\n                    if anchor_mass > 0.0:\n                        matched_anchor = (idf_pos[anchor_idx, None] * tfp[anchor_idx]).sum(axis=0)\n                        anchor_cov = np.minimum(1.0, matched_anchor / (anchor_mass + eps))\n                        scores *= 1.0 + 0.10 * (0.5 + 0.5 * (1.0 - spec_gate)) * anchor_cov\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "7182d8a1-e859-45e6-a46b-de690ed0d0c6", "generation": 10, "timestamp": 1770127227.654363, "iteration_found": 160, "metrics": {"beir_nfcorpus_ndcg@10": 0.32064238148661833, "beir_nfcorpus_recall@100": 0.2557442339663631, "beir_nfcorpus_index_time_ms": 3871.06775000575, "beir_nfcorpus_query_time_ms": 114.23174999072216, "bright_pony_ndcg@10": 0.10222180368768481, "bright_pony_recall@100": 0.291767568331805, "bright_pony_index_time_ms": 1588.3336669940036, "bright_pony_query_time_ms": 633.0193339963444, "beir_scifact_ndcg@10": 0.6747816209503706, "beir_scifact_recall@100": 0.932, "beir_scifact_index_time_ms": 5066.489541000919, "beir_scifact_query_time_ms": 476.280584000051, "beir_arguana_ndcg@10": 0.27929586532345313, "beir_arguana_recall@100": 0.9093504639543183, "beir_arguana_index_time_ms": 6221.663457981776, "beir_arguana_query_time_ms": 30874.302625015844, "bright_theoremqa_theorems_ndcg@10": 0.040423220229772364, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 10847.59383299388, "bright_theoremqa_theorems_query_time_ms": 1281.67858402594, "beir_scidocs_ndcg@10": 0.142772953201875, "beir_scidocs_recall@100": 0.3438833333333333, "beir_scidocs_index_time_ms": 19930.23104200256, "beir_scidocs_query_time_ms": 3945.467625017045, "bright_economics_ndcg@10": 0.14108320258335294, "bright_economics_recall@100": 0.390907487552556, "bright_economics_index_time_ms": 16774.048333987594, "bright_economics_query_time_ms": 4153.916958981426, "bright_biology_ndcg@10": 0.24280557645581854, "bright_biology_recall@100": 0.545365592223027, "bright_biology_index_time_ms": 25023.92191698891, "bright_biology_query_time_ms": 3961.8951250158716, "beir_fiqa_ndcg@10": 0.22624719245831906, "beir_fiqa_recall@100": 0.5259039657187804, "beir_fiqa_index_time_ms": 44791.731250006706, "beir_fiqa_query_time_ms": 7601.140917016892, "bright_earth_science_ndcg@10": 0.35042235709961883, "bright_earth_science_recall@100": 0.6647502376631234, "bright_earth_science_index_time_ms": 47489.652708987705, "bright_earth_science_query_time_ms": 7001.524084014818, "bright_stackoverflow_ndcg@10": 0.1718779537426295, "bright_stackoverflow_recall@100": 0.49322448689886184, "bright_stackoverflow_index_time_ms": 111664.82783301035, "bright_stackoverflow_query_time_ms": 13362.719124997966, "beir_trec-covid_ndcg@10": 0.623433568094502, "beir_trec-covid_recall@100": 0.10516512425463263, "beir_trec-covid_index_time_ms": 158329.16737502092, "beir_trec-covid_query_time_ms": 1680.4559589945711, "avg_ndcg@10": 0.27633397460950127, "avg_recall@100": 0.4676309388188855, "combined_score": 0.42937154597700866, "total_index_time_ms": 451598.72870898107, "total_query_time_ms": 75086.63267106749, "total_time_ms": 526685.3613800486, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 129 lines with 137 lines\nChange 2: Replace 13 lines with 30 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3200346893590633, "beir_nfcorpus_recall@100": 0.2557235967090566, "beir_nfcorpus_index_time_ms": 3791.1299580009654, "beir_nfcorpus_query_time_ms": 111.76299999351613, "bright_pony_ndcg@10": 0.10222180368768481, "bright_pony_recall@100": 0.291767568331805, "bright_pony_index_time_ms": 1729.5180409855675, "bright_pony_query_time_ms": 620.5731249938253, "beir_scifact_ndcg@10": 0.6749297840095178, "beir_scifact_recall@100": 0.932, "beir_scifact_index_time_ms": 5281.246458005626, "beir_scifact_query_time_ms": 392.0116670196876, "beir_arguana_ndcg@10": 0.2793675460267056, "beir_arguana_recall@100": 0.9093504639543183, "beir_arguana_index_time_ms": 6360.271667013876, "beir_arguana_query_time_ms": 30635.90987501084, "bright_theoremqa_theorems_ndcg@10": 0.040423220229772364, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 10920.756750012515, "bright_theoremqa_theorems_query_time_ms": 1240.6940829823725, "beir_scidocs_ndcg@10": 0.14332374085011565, "beir_scidocs_recall@100": 0.34298333333333336, "beir_scidocs_index_time_ms": 19929.2093749973, "beir_scidocs_query_time_ms": 3876.103416987462, "bright_economics_ndcg@10": 0.14096703589067244, "bright_economics_recall@100": 0.390907487552556, "bright_economics_index_time_ms": 19777.73170801811, "bright_economics_query_time_ms": 4510.514415975194, "bright_biology_ndcg@10": 0.24387517154645277, "bright_biology_recall@100": 0.545365592223027, "bright_biology_index_time_ms": 25694.005500001367, "bright_biology_query_time_ms": 3872.335916996235, "beir_fiqa_ndcg@10": 0.22616336417162872, "beir_fiqa_recall@100": 0.5253013790050827, "beir_fiqa_index_time_ms": 46333.54008299648, "beir_fiqa_query_time_ms": 7347.472708002897, "bright_earth_science_ndcg@10": 0.35091147249717675, "bright_earth_science_recall@100": 0.6647502376631234, "bright_earth_science_index_time_ms": 47762.676249985816, "bright_earth_science_query_time_ms": 7052.3392079921905, "bright_stackoverflow_ndcg@10": 0.17311598887616347, "bright_stackoverflow_recall@100": 0.49296548663986156, "bright_stackoverflow_index_time_ms": 112576.41341601266, "bright_stackoverflow_query_time_ms": 12017.1706249821, "beir_trec-covid_ndcg@10": 0.6202101683745842, "beir_trec-covid_recall@100": 0.10507560313136684, "beir_trec-covid_index_time_ms": 153940.7327079971, "beir_trec-covid_query_time_ms": 1575.5466250120662, "avg_ndcg@10": 0.27629533212662816, "avg_recall@100": 0.4674749600394463, "combined_score": 0.42923903445688266, "total_index_time_ms": 454097.23191402736, "total_query_time_ms": 73252.43466594839, "total_time_ms": 527349.6665799757, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 2}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4292\n- Feature coordinates: \n- Focus areas: - Fitness declined: 0.4293 \u2192 0.4292. Consider revising recent changes.\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace 42 lines with 41 lines\nChange 2: Replace 5 lines with 9 lines\nChange 3: Replace 8 lines with 8 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3200, beir_nfcorpus_recall@100: 0.2557, beir_nfcorpus_index_time_ms: 4009.3776, beir_nfcorpus_query_time_ms: 110.7289, bright_pony_ndcg@10: 0.1030, bright_pony_recall@100: 0.2918, bright_pony_index_time_ms: 1541.7650, bright_pony_query_time_ms: 629.3389, beir_scifact_ndcg@10: 0.6749, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5138.7260, beir_scifact_query_time_ms: 395.8099, beir_arguana_ndcg@10: 0.2791, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 6117.7285, beir_arguana_query_time_ms: 30750.5995, bright_theoremqa_theorems_ndcg@10: 0.0404, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10881.6147, bright_theoremqa_theorems_query_time_ms: 1232.8631, beir_scidocs_ndcg@10: 0.1434, beir_scidocs_recall@100: 0.3430, beir_scidocs_index_time_ms: 19932.3964, beir_scidocs_query_time_ms: 3937.1430, bright_economics_ndcg@10: 0.1409, bright_economics_recall@100: 0.3909, bright_economics_index_time_ms: 21308.0099, bright_economics_query_time_ms: 4377.8429, bright_biology_ndcg@10: 0.2442, bright_biology_recall@100: 0.5454, bright_biology_index_time_ms: 23399.2889, bright_biology_query_time_ms: 4159.4700, beir_fiqa_ndcg@10: 0.2263, beir_fiqa_recall@100: 0.5253, beir_fiqa_index_time_ms: 47799.4584, beir_fiqa_query_time_ms: 7461.4083, bright_earth_science_ndcg@10: 0.3520, bright_earth_science_recall@100: 0.6648, bright_earth_science_index_time_ms: 47630.1852, bright_earth_science_query_time_ms: 7185.4406, bright_stackoverflow_ndcg@10: 0.1724, bright_stackoverflow_recall@100: 0.4930, bright_stackoverflow_index_time_ms: 114334.3224, bright_stackoverflow_query_time_ms: 13875.4572, beir_trec-covid_ndcg@10: 0.6198, beir_trec-covid_recall@100: 0.1051, beir_trec-covid_index_time_ms: 159737.9588, beir_trec-covid_query_time_ms: 1646.5994, avg_ndcg@10: 0.2764, avg_recall@100: 0.4675, combined_score: 0.4293, total_index_time_ms: 461830.8318, total_query_time_ms: 75762.7016, total_time_ms: 537593.5334, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace 39 lines with 46 lines\nChange 2: Replace 14 lines with 26 lines\nChange 3: Replace 8 lines with 9 lines\nChange 4: Replace norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0 with 17 lines\nChange 5: Replace return score with 12 lines\nChange 6: Replace 10 lines with 11 lines\nChange 7: Replace 7 lines with 8 lines\nChange 8: Replace return scores with 18 lines\nChange 9: Replace 12 lines with 14 lines\nChange 10: Replace candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf) with 7 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3207, beir_nfcorpus_recall@100: 0.2557, beir_nfcorpus_index_time_ms: 3954.5953, beir_nfcorpus_query_time_ms: 191.4022, bright_pony_ndcg@10: 0.1038, bright_pony_recall@100: 0.2917, bright_pony_index_time_ms: 1882.0535, bright_pony_query_time_ms: 1277.2320, beir_scifact_ndcg@10: 0.6738, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5394.9763, beir_scifact_query_time_ms: 1072.1182, beir_arguana_ndcg@10: 0.2792, beir_arguana_recall@100: 0.9094, beir_arguana_index_time_ms: 6479.7305, beir_arguana_query_time_ms: 60890.8035, bright_theoremqa_theorems_ndcg@10: 0.0404, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 11341.1465, bright_theoremqa_theorems_query_time_ms: 2983.4386, beir_scidocs_ndcg@10: 0.1438, beir_scidocs_recall@100: 0.3428, beir_scidocs_index_time_ms: 20882.1934, beir_scidocs_query_time_ms: 11084.4901, bright_economics_ndcg@10: 0.1410, bright_economics_recall@100: 0.3909, bright_economics_index_time_ms: 17321.4414, bright_economics_query_time_ms: 9057.8690, bright_biology_ndcg@10: 0.2443, bright_biology_recall@100: 0.5454, bright_biology_index_time_ms: 22975.5218, bright_biology_query_time_ms: 8830.0593, bright_earth_science_ndcg@10: 0.3521, bright_earth_science_recall@100: 0.6665, bright_earth_science_index_time_ms: 50463.3499, bright_earth_science_query_time_ms: 15168.3426, beir_fiqa_ndcg@10: 0.2264, beir_fiqa_recall@100: 0.5266, beir_fiqa_index_time_ms: 51113.9434, beir_fiqa_query_time_ms: 20565.9507, bright_stackoverflow_ndcg@10: 0.1731, bright_stackoverflow_recall@100: 0.4930, bright_stackoverflow_index_time_ms: 121336.4571, bright_stackoverflow_query_time_ms: 43301.2054, beir_trec-covid_ndcg@10: 0.6215, beir_trec-covid_recall@100: 0.1054, beir_trec-covid_index_time_ms: 185836.0722, beir_trec-covid_query_time_ms: 5146.1392, avg_ndcg@10: 0.2767, avg_recall@100: 0.4677, combined_score: 0.4295, total_index_time_ms: 498981.4815, total_query_time_ms: 179569.0509, total_time_ms: 678550.5324, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 38 lines with 52 lines\nChange 2: Replace 108 lines with 126 lines\nChange 3: Replace 24 lines with 34 lines\nChange 4: Replace 26 lines with 31 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3204, beir_nfcorpus_recall@100: 0.2556, beir_nfcorpus_index_time_ms: 3849.8675, beir_nfcorpus_query_time_ms: 124.6751, bright_pony_ndcg@10: 0.1031, bright_pony_recall@100: 0.2924, bright_pony_index_time_ms: 1446.4430, bright_pony_query_time_ms: 743.3286, beir_scifact_ndcg@10: 0.6750, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5236.1868, beir_scifact_query_time_ms: 392.7914, beir_arguana_ndcg@10: 0.2797, beir_arguana_recall@100: 0.9129, beir_arguana_index_time_ms: 6210.9679, beir_arguana_query_time_ms: 30543.7940, bright_theoremqa_theorems_ndcg@10: 0.0404, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10995.8755, bright_theoremqa_theorems_query_time_ms: 1255.9586, beir_scidocs_ndcg@10: 0.1432, beir_scidocs_recall@100: 0.3432, beir_scidocs_index_time_ms: 20338.6954, beir_scidocs_query_time_ms: 3863.8864, bright_economics_ndcg@10: 0.1410, bright_economics_recall@100: 0.3909, bright_economics_index_time_ms: 17139.9562, bright_economics_query_time_ms: 4102.2913, bright_biology_ndcg@10: 0.2438, bright_biology_recall@100: 0.5454, bright_biology_index_time_ms: 24300.3506, bright_biology_query_time_ms: 3706.5546, beir_fiqa_ndcg@10: 0.2259, beir_fiqa_recall@100: 0.5274, beir_fiqa_index_time_ms: 46383.7951, beir_fiqa_query_time_ms: 7871.6607, bright_earth_science_ndcg@10: 0.3511, bright_earth_science_recall@100: 0.6648, bright_earth_science_index_time_ms: 47509.1306, bright_earth_science_query_time_ms: 7379.4591, bright_stackoverflow_ndcg@10: 0.1733, bright_stackoverflow_recall@100: 0.4930, bright_stackoverflow_index_time_ms: 113946.6597, bright_stackoverflow_query_time_ms: 12715.2238, beir_trec-covid_ndcg@10: 0.6177, beir_trec-covid_recall@100: 0.1050, beir_trec-covid_index_time_ms: 157636.6640, beir_trec-covid_query_time_ms: 1703.6822, avg_ndcg@10: 0.2762, avg_recall@100: 0.4680, combined_score: 0.4296, total_index_time_ms: 454994.5922, total_query_time_ms: 74403.3057, total_time_ms: 529397.8979, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4296)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Let ultra-common terms become *negative* evidence (repulsion).\n    use_signed_common_idf: bool = True\n    common_neg_strength: float = 0.18\n    common_neg_power: float = 1.25\n    # NEW: only apply negative evidence if the term is present without enough\n    # other positive evidence (avoid hurting recall@100 on entity+modifier queries).\n    common_neg_min_pos_terms: int = 1\n\n    # Bounded IDF-mass coverage reward (soft-AND over information).\n    info_cov_alpha: float = 0.18\n    info_cov_beta: float = 0.85\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: very cheap \"exactness\" evidence via adjacent query bigrams.\n    # Helps StackOverflow / TheoremQA / ArguAna style queries where token order matters,\n    # but is bounded so recall is largely preserved.\n    bigram_alpha: float = 0.05\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    doc_bigrams: set[int] | None = None,\n) -> float:\n    \"\"\"\n    Same base signals as before, but with a *query-specificity gate* on the\n    \"AND-like\" effects (coordination + pair synergy).\n\n    Why this can help:\n    - Many BEIR/BRIGHT queries contain one sharp entity/term + several broad modifiers.\n      Coordination/pair boosts can then over-promote generic docs that match many broad terms.\n    - Use a simple, doc-independent specificity proxy:\n        spec = max_idf / sum_idf  (peaky queries -> larger spec)\n      We down-gate coordination/synergy for peaky queries, keeping recall from the\n      main additive evidence but improving nDCG@10 by reducing noisy \"multi-match\" wins.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    # Also compute query \"idf budget\" for info-mass coverage.\n    #\n    # Change: do NOT include signed-negative mass in the gating stats; otherwise\n    # a query with many common terms can get artificially low sum and unstable spec.\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # For info-mass coverage we want *nonnegative* budget (avoid sign issues).\n    q_pos_budget = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        v = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        if v > 0.0:\n            q_pos_budget += v\n\n    matched_info_mass = 0.0\n    pos_matched_terms = 0  # used to gate signed-negative common evidence\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        # Coverage should only count positive (discriminative) mass.\n        if term_idf > 0.0:\n            matched_info_mass += term_idf * tf_part\n            pos_matched_terms += 1\n\n        # Signed common-term evidence, but only if the doc doesn't already have\n        # enough positive matches (protect recall).\n        if Config.use_signed_common_idf and df >= common_thr and pos_matched_terms <= Config.common_neg_min_pos_terms:\n            u = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf -= Config.common_neg_strength * (u ** Config.common_neg_power)\n\n        score += wq * term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        # Term-count coverage (kept) + IDF-mass coverage (new), both gated by spec_gate.\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        if q_pos_budget > 0.0 and Config.info_cov_alpha > 0.0:\n            info_cov = min(1.0, matched_info_mass / (q_pos_budget + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, but gate by query specificity too.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # NEW: adjacent-bigram reward (weak phrase proxy).\n    # We only apply it when the document provides a bigram set (score() path),\n    # keeping rank() fast unless Corpus chooses to store bigrams.\n    if doc_bigrams is not None and Config.bigram_alpha > 0.0 and len(query_repr.terms) > 1:\n        hits = 0.0\n        total = 0.0\n        for a, bterm in zip(query_repr.terms, query_repr.terms[1:]):\n            total += 1.0\n            if hash((a, bterm)) in doc_bigrams:\n                hits += 1.0\n        if hits > 0.0 and total > 0.0:\n            score *= 1.0 + Config.bigram_alpha * (hits / total)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    doc_bigrams = corpus.doc_bigrams[doc_idx] if getattr(corpus, \"doc_bigrams\", None) is not None else None\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency, doc_bigrams)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Optional per-doc hashed adjacent bigrams (cheap phrase proxy for score()).\n        # Stored only as Python sets; rank() stays unchanged/fast.\n        self.doc_bigrams: list[set[int]] = []\n        for doc in documents:\n            bg: set[int] = set()\n            if len(doc) > 1:\n                p = doc[0]\n                for t in doc[1:]:\n                    bg.add(hash((p, t)))\n                    p = t\n            self.doc_bigrams.append(bg)\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms + optional signed repulsion.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n                if Config.use_signed_common_idf:\n                    u = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                    idf_val -= Config.common_neg_strength * (u ** Config.common_neg_power)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # Info-mass coverage reward (vectorized), counting only positive term idf.\n            if Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                pos_idf = np.maximum(0.0, idf_arr)\n                budget = float(np.sum(pos_idf))\n                if budget > 0.0:\n                    tfp = np.vstack(tfparts)\n                    matched_mass = (pos_idf[:, None] * tfp).sum(axis=0)\n                    info_cov = np.minimum(1.0, matched_mass / (budget + eps))\n                    scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3204), Performs well on beir_nfcorpus_recall@100 (0.2556), Performs well on beir_nfcorpus_index_time_ms (4002.4356), Performs well on beir_nfcorpus_query_time_ms (111.7073), Performs well on bright_pony_ndcg@10 (0.1031), Performs well on bright_pony_recall@100 (0.2924), Performs well on bright_pony_index_time_ms (1825.6060), Performs well on bright_pony_query_time_ms (632.8104), Performs well on beir_scifact_ndcg@10 (0.6750), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5380.3460), Performs well on beir_scifact_query_time_ms (411.2025), Performs well on beir_arguana_ndcg@10 (0.2797), Performs well on beir_arguana_recall@100 (0.9129), Performs well on beir_arguana_index_time_ms (6441.8335), Performs well on beir_arguana_query_time_ms (30826.6664), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11361.3969), Performs well on bright_theoremqa_theorems_query_time_ms (1230.2351), Performs well on beir_scidocs_ndcg@10 (0.1432), Performs well on beir_scidocs_recall@100 (0.3432), Performs well on beir_scidocs_index_time_ms (20678.5818), Performs well on beir_scidocs_query_time_ms (3896.8082), Performs well on bright_economics_ndcg@10 (0.1410), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (18257.5565), Performs well on bright_economics_query_time_ms (4288.2190), Performs well on bright_biology_ndcg@10 (0.2438), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (26614.9767), Performs well on bright_biology_query_time_ms (3683.6242), Performs well on beir_fiqa_ndcg@10 (0.2259), Performs well on beir_fiqa_recall@100 (0.5274), Performs well on beir_fiqa_index_time_ms (49128.2802), Performs well on beir_fiqa_query_time_ms (7472.7737), Performs well on bright_earth_science_ndcg@10 (0.3511), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (50162.7465), Performs well on bright_earth_science_query_time_ms (7180.1572), Performs well on bright_stackoverflow_ndcg@10 (0.1733), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (121412.6903), Performs well on bright_stackoverflow_query_time_ms (15610.2907), Performs well on beir_trec-covid_ndcg@10 (0.6177), Performs well on beir_trec-covid_recall@100 (0.1050), Performs well on beir_trec-covid_index_time_ms (174066.8210), Performs well on beir_trec-covid_query_time_ms (1633.1988), Performs well on avg_ndcg@10 (0.2762), Performs well on avg_recall@100 (0.4680), Performs well on combined_score (0.4296), Performs well on total_index_time_ms (489333.2710), Performs well on total_query_time_ms (76977.6935), Performs well on total_time_ms (566310.9646), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4296)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (as in BM25, but we will change the \"norm\" definition)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # How much to trust \"lexical focus\" (unique terms) vs raw length.\n    # 0 -> pure length, 1 -> pure unique length.\n    focus_mix: float = 0.65\n\n    # Sharpen rarity/discriminativity (idf^power). power>1 penalizes common terms more.\n    idf_power: float = 1.12\n\n    # Coordination reward (>=1): favors docs covering more distinct query aspects.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query repetition is often noise; sublinear qtf improves robustness.\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    # Goal: improve nDCG@10 by reducing noisy ties while preserving recall.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # NEW: extremely common terms can be *negative* evidence (repulsion).\n    # Intuition: boilerplate-heavy documents tend to contain many of these; making them\n    # slightly repulsive helps nDCG@10 while the additive positive evidence preserves recall.\n    use_signed_common_idf: bool = True\n    common_neg_strength: float = 0.18\n    common_neg_power: float = 1.25\n\n    # NEW: Soft-AND over *information mass* (IDF-budget coverage).\n    # Treat the query as an \"idf budget\" and reward documents that cover a larger fraction\n    # of that budget with TF-saturated support. This helps verbose queries where term-count\n    # coordination is noisy (BRIGHT, StackOverflow), generally improving nDCG@10.\n    info_cov_alpha: float = 0.18\n    info_cov_beta: float = 0.85\n\n    # Light co-occurrence synergy: super-additive evidence when multiple distinctive\n    # query terms appear in the same doc (proxy for topical coherence without positions).\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query-specificity gate for AND-like effects (coordination + synergy).\n    # spec = max_idf/sum_idf: \"peaky\" queries (entity + modifiers) should downweight\n    # coordination/synergy to avoid promoting generic docs matching many modifiers.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use distinct query terms + sublinear query-TF weights.\n\n        Rationale:\n        - Coordination/facet coverage is only meaningful over distinct terms.\n        - Repetition in queries often comes from tokenization artifacts / verbosity;\n          (1+log qtf) is a stable, training-free dampener.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + (gated) coordination reward + distinctive\n    co-occurrence synergy + soft downweighting of ultra-common terms.\n\n    Additions:\n    - IDF-budget coverage reward: a bounded soft-AND over *information mass*.\n    - Optional signed common-term contribution: ultra-common terms become weakly repulsive,\n      improving top-rank precision in boilerplate-heavy corpora.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Approximate unique-length from the tf dict (cheap).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    # Keep per-matched-term components for synergy (m is usually small).\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query-specificity gate (doc-independent; computed with same shaping as scoring).\n    # Also compute a nonnegative \"idf budget\" for information-coverage.\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_pos_budget = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n        if q_idf > 0.0:\n            q_pos_budget += q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    matched_info_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms (keeps recall better than hard removal).\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n            # Optionally make extremely common terms weakly repulsive.\n            if Config.use_signed_common_idf:\n                u = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n                term_idf -= Config.common_neg_strength * (u ** Config.common_neg_power)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n\n        # Coverage should only count positive (discriminative) mass.\n        if term_idf > 0.0:\n            matched_info_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: IDF-budget coverage reward (soft-AND over information).\n        if Config.info_cov_alpha > 0.0 and q_pos_budget > 0.0:\n            info_cov = min(1.0, matched_info_mass / (q_pos_budget + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    # Distinctive-pair synergy: emphasize multiple *distinctive* matches.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        # Residualize vs max-idf to avoid boosting when only one term is distinctive.\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # For query-specificity gate, accumulate query-level idf stats *after* common penalty.\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n                # Mirror retrieval_score(): optional signed-negative evidence for ultra-common terms.\n                if Config.use_signed_common_idf:\n                    u = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                    idf_val -= Config.common_neg_strength * (u ** Config.common_neg_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # NEW: vectorized IDF-budget coverage reward (must match retrieval_score()).\n            if Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.maximum(0.0, np.array(idfs, dtype=np.float64))\n                budget = float(np.sum(idf_arr))\n                if budget > 0.0:\n                    tfp = np.vstack(tfparts)\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    info_cov = np.minimum(1.0, matched_mass / (budget + eps))\n                    scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(\n                self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist()\n            )\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3204), Performs well on beir_nfcorpus_recall@100 (0.2556), Performs well on beir_nfcorpus_index_time_ms (3849.8675), Performs well on beir_nfcorpus_query_time_ms (124.6751), Performs well on bright_pony_ndcg@10 (0.1031), Performs well on bright_pony_recall@100 (0.2924), Performs well on bright_pony_index_time_ms (1446.4430), Performs well on bright_pony_query_time_ms (743.3286), Performs well on beir_scifact_ndcg@10 (0.6750), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5236.1868), Performs well on beir_scifact_query_time_ms (392.7914), Performs well on beir_arguana_ndcg@10 (0.2797), Performs well on beir_arguana_recall@100 (0.9129), Performs well on beir_arguana_index_time_ms (6210.9679), Performs well on beir_arguana_query_time_ms (30543.7940), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10995.8755), Performs well on bright_theoremqa_theorems_query_time_ms (1255.9586), Performs well on beir_scidocs_ndcg@10 (0.1432), Performs well on beir_scidocs_recall@100 (0.3432), Performs well on beir_scidocs_index_time_ms (20338.6954), Performs well on beir_scidocs_query_time_ms (3863.8864), Performs well on bright_economics_ndcg@10 (0.1410), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (17139.9562), Performs well on bright_economics_query_time_ms (4102.2913), Performs well on bright_biology_ndcg@10 (0.2438), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (24300.3506), Performs well on bright_biology_query_time_ms (3706.5546), Performs well on beir_fiqa_ndcg@10 (0.2259), Performs well on beir_fiqa_recall@100 (0.5274), Performs well on beir_fiqa_index_time_ms (46383.7951), Performs well on beir_fiqa_query_time_ms (7871.6607), Performs well on bright_earth_science_ndcg@10 (0.3511), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (47509.1306), Performs well on bright_earth_science_query_time_ms (7379.4591), Performs well on bright_stackoverflow_ndcg@10 (0.1733), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (113946.6597), Performs well on bright_stackoverflow_query_time_ms (12715.2238), Performs well on beir_trec-covid_ndcg@10 (0.6177), Performs well on beir_trec-covid_recall@100 (0.1050), Performs well on beir_trec-covid_index_time_ms (157636.6640), Performs well on beir_trec-covid_query_time_ms (1703.6822), Performs well on avg_ndcg@10 (0.2762), Performs well on avg_recall@100 (0.4680), Performs well on combined_score (0.4296), Performs well on total_index_time_ms (454994.5922), Performs well on total_query_time_ms (74403.3057), Performs well on total_time_ms (529397.8979), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4295)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept modest; mainly helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms tend to be \"glue\"; softly downweight instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query-specificity gate for AND-like rewards (coordination + pair synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # IDF-budget coverage reward (soft-AND over *information mass*).\n    info_cov_alpha: float = 0.18\n    info_cov_beta: float = 0.85\n\n    # NEW: \"informativeness pivot\" normalization.\n    # Idea: length alone is a poor verbosity proxy; boilerplate-heavy docs are long but\n    # low-information. Normalize TF saturation by a cheap IDF-weighted doc mass.\n    info_norm_mix: float = 0.20   # 0 => ignore info pivot; 1 => only info pivot\n    info_norm_power: float = 0.55 # exponent on idf in the doc-mass accumulator\n\n    # NEW: adjacent-bigram reward (cheap phrase proxy).\n    bigram_alpha: float = 0.055\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    doc_bigrams: set[int] | None = None,\n) -> float:\n    \"\"\"\n    Additive lexical evidence + bounded coordination reward + light co-occurrence synergy.\n\n    Refinement: gate the \"AND-like\" rewards (coordination + pair synergy) by a\n    doc-independent query-specificity proxy.\n    - Balanced multi-term queries: rewarding coverage helps nDCG@10.\n    - Peaky queries (one rare anchor + broad modifiers): too much coverage reward\n      promotes generic docs; gating protects nDCG@10 while preserving recall via\n      the base additive evidence.\n\n    NEW: \"IDF budget coverage\" reward (soft-AND over information mass).\n    This counteracts the failure mode where matching many low-IDF modifiers outranks\n    matching the few high-IDF anchors (common in BRIGHT/StackOverflow-like queries).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # NEW: informativeness pivot (doc-dependent, but computed from the doc TF dict we already have).\n    # Accumulate an IDF-weighted \"mass\" of the document: docs heavy in common terms get a smaller\n    # effective normalization benefit than docs heavy in rare terms.\n    info_mass = 0.0\n    ip = Config.info_norm_power\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        if base_t <= 0.0:\n            continue\n        info_mass += float(tfv) * (base_t ** ip)\n\n    # Pivoted normalization of the info-mass. Uses avgdl as a stable scale proxy.\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_norm_mix) * len_norm + Config.info_norm_mix * info_norm\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query-specificity gate + query idf-budget (computed with the same common-term penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0  # higher => peakier\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Evidence-weighted matched info mass for idf-budget coverage.\n    info_matched_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        info_matched_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Soft-AND over information mass (bounded in [0,1]), gated like coordination.\n        if q_idf_sum > 0.0 and Config.info_cov_alpha > 0.0:\n            info_cov = min(1.0, info_matched_mass / (q_idf_sum + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # NEW: adjacent-bigram reward (weak phrase evidence; bounded).\n    if doc_bigrams is not None and Config.bigram_alpha > 0.0 and len(query_repr.terms) > 1:\n        hits = 0.0\n        total = 0.0\n        for a, bterm in zip(query_repr.terms, query_repr.terms[1:]):\n            total += 1.0\n            if hash((a, bterm)) in doc_bigrams:\n                hits += 1.0\n        if total > 0.0 and hits > 0.0:\n            score *= 1.0 + Config.bigram_alpha * (hits / total)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    doc_bigrams = corpus.doc_bigrams[doc_idx] if getattr(corpus, \"doc_bigrams\", None) is not None else None\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency, doc_bigrams)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        # NEW: hashed adjacent bigrams per document (cheap phrase proxy).\n        # Using python sets keeps memory moderate and enables a bounded phrase bonus.\n        self.doc_bigrams: list[set[int]] = []\n        for doc in documents:\n            bg: set[int] = set()\n            if len(doc) > 1:\n                p = doc[0]\n                for t in doc[1:]:\n                    bg.add(hash((p, t)))\n                    p = t\n            self.doc_bigrams.append(bg)\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n        query_bigram_hashes: list[int] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Keep matched-term components for synergy (doc-major arrays per term).\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query-specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # NEW: vectorized IDF-budget coverage reward (must mirror retrieval_score()).\n            if Config.info_cov_alpha > 0.0 and q_idf_sum > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                tfp = np.vstack(tfparts)\n                info_matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                info_cov = np.minimum(1.0, info_matched_mass / (q_idf_sum + eps))\n                scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n        # Co-occurrence synergy: emphasize distinctive pairs (residualized vs max-idf),\n        # but gate it by query specificity to avoid over-rewarding broad modifiers.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # NEW: bigram bonus for rank() (loop over candidate docs; bounded and usually cheap\n        # because candidate set is already filtered by postings).\n        if query_bigram_hashes is not None and Config.bigram_alpha > 0.0 and len(query_bigram_hashes) > 0:\n            denom = float(len(query_bigram_hashes))\n            if denom > 0.0:\n                hit = np.zeros(len(candidate_docs), dtype=np.float64)\n                # Python-level loop but only over candidates; typically acceptable for top-k retrieval.\n                for j, d in enumerate(candidate_docs.tolist()):\n                    bg = self.corpus.doc_bigrams[int(d)]\n                    c = 0\n                    for h in query_bigram_hashes:\n                        if h in bg:\n                            c += 1\n                    if c:\n                        hit[j] = float(c)\n                scores *= 1.0 + Config.bigram_alpha * (hit / denom)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        # Precompute query bigram hashes once (using query order as a weak phrase hypothesis).\n        q_bigrams: list[int] = []\n        if Config.bigram_alpha > 0.0 and len(query_terms) > 1:\n            for a, b in zip(query_terms, query_terms[1:]):\n                q_bigrams.append(hash((a, b)))\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf, q_bigrams)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3207), Performs well on beir_nfcorpus_recall@100 (0.2557), Performs well on beir_nfcorpus_index_time_ms (3954.5953), Performs well on beir_nfcorpus_query_time_ms (191.4022), Performs well on bright_pony_ndcg@10 (0.1038), Performs well on bright_pony_recall@100 (0.2917), Performs well on bright_pony_index_time_ms (1882.0535), Performs well on bright_pony_query_time_ms (1277.2320), Performs well on beir_scifact_ndcg@10 (0.6738), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5394.9763), Performs well on beir_scifact_query_time_ms (1072.1182), Performs well on beir_arguana_ndcg@10 (0.2792), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (6479.7305), Performs well on beir_arguana_query_time_ms (60890.8035), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11341.1465), Performs well on bright_theoremqa_theorems_query_time_ms (2983.4386), Performs well on beir_scidocs_ndcg@10 (0.1438), Performs well on beir_scidocs_recall@100 (0.3428), Performs well on beir_scidocs_index_time_ms (20882.1934), Performs well on beir_scidocs_query_time_ms (11084.4901), Performs well on bright_economics_ndcg@10 (0.1410), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (17321.4414), Performs well on bright_economics_query_time_ms (9057.8690), Performs well on bright_biology_ndcg@10 (0.2443), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (22975.5218), Performs well on bright_biology_query_time_ms (8830.0593), Performs well on bright_earth_science_ndcg@10 (0.3521), Performs well on bright_earth_science_recall@100 (0.6665), Performs well on bright_earth_science_index_time_ms (50463.3499), Performs well on bright_earth_science_query_time_ms (15168.3426), Performs well on beir_fiqa_ndcg@10 (0.2264), Performs well on beir_fiqa_recall@100 (0.5266), Performs well on beir_fiqa_index_time_ms (51113.9434), Performs well on beir_fiqa_query_time_ms (20565.9507), Performs well on bright_stackoverflow_ndcg@10 (0.1731), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (121336.4571), Performs well on bright_stackoverflow_query_time_ms (43301.2054), Performs well on beir_trec-covid_ndcg@10 (0.6215), Performs well on beir_trec-covid_recall@100 (0.1054), Performs well on beir_trec-covid_index_time_ms (185836.0722), Performs well on beir_trec-covid_query_time_ms (5146.1392), Performs well on avg_ndcg@10 (0.2767), Performs well on avg_recall@100 (0.4677), Performs well on combined_score (0.4295), Performs well on total_index_time_ms (498981.4815), Performs well on total_query_time_ms (179569.0509), Performs well on total_time_ms (678550.5324), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4293)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: super-additive evidence when multiple distinctive\n    # query terms are present in the same doc (a proxy for topical coherence).\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query-specificity gate for \"AND-like\" effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # \"Information coverage\" reward (soft-AND over IDF-mass).\n    # Change: compute coverage over *evidence-weighted* matched mass (idf * tf_part),\n    # not just idf(presence). Presence-only tends to over-reward one-off mentions of rare terms.\n    cov_alpha: float = 0.20\n    cov_beta: float = 0.90\n    cov_power: float = 1.00\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with a \"focus prior\" + (gated) coordination reward,\n    plus two precision-oriented refinements:\n\n    (1) common-term soft penalty: ultra-high-df terms behave like glue and create noisy ties.\n    (2) distinctive co-occurrence synergy: weakly reward multi-term matches when the matched\n        terms are distinctive (a proxy for topical coherence without positions).\n\n    Extra: IDF-mass coverage reward.\n    Term-count coverage can be misleading for verbose queries; instead reward the fraction\n    of the query's *information mass* (sum of query-term IDFs) that the document covers.\n    This is still purely lexical and training-free.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Precompute query specificity + query IDF mass (doc-independent).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        # (A) term-count coverage: good for short factoid queries\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # (B) Evidence-weighted IDF-mass coverage: soft-AND over *supported* information.\n        # matched_mass = sum_{t in q\u2229d} idf(t) * tf_part(t,d)\n        # This reduces false boosts from single, low-support occurrences of rare terms.\n        if q_idf_sum > 0.0 and m_idf:\n            matched_mass = 0.0\n            for v, tp in zip(m_idf, m_tfpart):\n                matched_mass += v * tp\n            cov_mass = (matched_mass / (q_idf_sum + eps)) ** Config.cov_power\n            score *= (1.0 + (Config.cov_alpha * spec_gate) * cov_mass) ** Config.cov_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs by residualizing vs max idf.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # For the query-specificity gate we need query-level idf stats (after common penalty).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            # Term-count coverage (as before)\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # Evidence-weighted IDF-mass coverage (must match retrieval_score()).\n            if q_idf_sum > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                tfp = np.vstack(tfparts) if len(tfparts) > 0 else None\n                if tfp is not None:\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    cov_mass = np.power(matched_mass / (q_idf_sum + eps), Config.cov_power)\n                    scores *= (1.0 + (Config.cov_alpha * spec_gate) * cov_mass) ** Config.cov_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (gated by specificity).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3200), Performs well on beir_nfcorpus_recall@100 (0.2557), Performs well on beir_nfcorpus_index_time_ms (4009.3776), Performs well on beir_nfcorpus_query_time_ms (110.7289), Performs well on bright_pony_ndcg@10 (0.1030), Performs well on bright_pony_recall@100 (0.2918), Performs well on bright_pony_index_time_ms (1541.7650), Performs well on bright_pony_query_time_ms (629.3389), Performs well on beir_scifact_ndcg@10 (0.6749), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5138.7260), Performs well on beir_scifact_query_time_ms (395.8099), Performs well on beir_arguana_ndcg@10 (0.2791), Performs well on beir_arguana_recall@100 (0.9094), Performs well on beir_arguana_index_time_ms (6117.7285), Performs well on beir_arguana_query_time_ms (30750.5995), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10881.6147), Performs well on bright_theoremqa_theorems_query_time_ms (1232.8631), Performs well on beir_scidocs_ndcg@10 (0.1434), Performs well on beir_scidocs_recall@100 (0.3430), Performs well on beir_scidocs_index_time_ms (19932.3964), Performs well on beir_scidocs_query_time_ms (3937.1430), Performs well on bright_economics_ndcg@10 (0.1409), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (21308.0099), Performs well on bright_economics_query_time_ms (4377.8429), Performs well on bright_biology_ndcg@10 (0.2442), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (23399.2889), Performs well on bright_biology_query_time_ms (4159.4700), Performs well on beir_fiqa_ndcg@10 (0.2263), Performs well on beir_fiqa_recall@100 (0.5253), Performs well on beir_fiqa_index_time_ms (47799.4584), Performs well on beir_fiqa_query_time_ms (7461.4083), Performs well on bright_earth_science_ndcg@10 (0.3520), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (47630.1852), Performs well on bright_earth_science_query_time_ms (7185.4406), Performs well on bright_stackoverflow_ndcg@10 (0.1724), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (114334.3224), Performs well on bright_stackoverflow_query_time_ms (13875.4572), Performs well on beir_trec-covid_ndcg@10 (0.6198), Performs well on beir_trec-covid_recall@100 (0.1051), Performs well on beir_trec-covid_index_time_ms (159737.9588), Performs well on beir_trec-covid_query_time_ms (1646.5994), Performs well on avg_ndcg@10 (0.2764), Performs well on avg_recall@100 (0.4675), Performs well on combined_score (0.4293), Performs well on total_index_time_ms (461830.8318), Performs well on total_query_time_ms (75762.7016), Performs well on total_time_ms (537593.5334), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like rewards.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: bounded \"information coverage\" reward (soft-AND over IDF mass).\n    # Intuition: coordination over *term count* is noisy for verbose queries; instead,\n    # reward the fraction of the query's discriminative mass actually supported by the doc.\n    info_cov_alpha: float = 0.18\n    info_cov_beta: float = 0.85\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + query-specificity\u2013gated \"soft AND\", plus a bounded\n    *information coverage* reward.\n\n    Key idea (novel vs plain BM25 knobs):\n    - Coordination over term *count* is a crude proxy for \"covering the query\".\n      For verbose queries, matching many low-IDF modifiers can beat matching the\n      few high-IDF anchors, hurting nDCG@10.\n    - Instead, treat the query as an \"IDF budget\" and reward the fraction of that\n      budget the document supports, with TF saturation included:\n          info_cov = sum_{t in q\u2229d} idf(t)*tf_part(t,d) / sum_{t in q} idf(t)\n      This is naturally bounded in [0,1] and multiplicative, so it tends to\n      improve top-rank ordering without damaging recall@100 much.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate + query idf-budget (doc-independent).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    info_matched_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        info_matched_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: soft-AND over information mass (idf budget coverage), gated like coordination.\n        if q_idf_sum > 0.0 and Config.info_cov_alpha > 0.0:\n            info_cov = min(1.0, info_matched_mass / (q_idf_sum + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query specificity gate (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        # Mirror retrieval_score(): down-gate AND-like rewards for peaky queries.\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # NEW: vectorized info-mass coverage reward (must match retrieval_score()).\n            if q_idf_sum > 0.0 and Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                tfp = np.vstack(tfparts)\n                info_matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                info_cov = np.minimum(1.0, info_matched_mass / (q_idf_sum + eps))\n                scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept modest; mainly helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms tend to be \"glue\"; softly downweight instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query-specificity gate for AND-like rewards (coordination + pair synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: Soft-AND over *information mass* (not term count).\n    # Reward documents that cover a large fraction of the query's *idf budget*.\n    # This helps verbose queries where matching the key terms matters most for nDCG@10,\n    # while being bounded/multiplicative so recall@100 is mostly preserved by base evidence.\n    info_cov_alpha: float = 0.22\n    info_cov_beta: float = 0.85\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + bounded coordination reward + light co-occurrence synergy,\n    plus an \"idf-budget coverage\" reward.\n\n    Key addition: instead of only rewarding matching many query terms (coordination),\n    also reward covering a large fraction of the query's *information mass*:\n        info_cov(doc) = sum_{t in q\u2229d} idf(t)*tf_part(t,doc) / sum_{t in q} idf(t)\n\n    This acts like a soft-AND over *important* terms and is naturally bounded in [0,1],\n    typically improving nDCG@10 on verbose/complex queries without sacrificing recall@100\n    (base additive evidence still dominates candidate inclusion).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query-specificity gate + query idf-budget (computed with same common-term penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0  # higher => peakier\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    info_matched_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        info_matched_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: idf-budget coverage reward (soft-AND over information).\n        if q_idf_sum > 0.0 and Config.info_cov_alpha > 0.0:\n            info_cov = min(1.0, info_matched_mass / (q_idf_sum + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Keep matched-term components for synergy (doc-major arrays per term).\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query-specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # NEW: vectorized idf-budget coverage reward (must match retrieval_score()).\n            if q_idf_sum > 0.0 and Config.info_cov_alpha > 0.0 and len(idfs) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                tfp = np.vstack(tfparts) if len(tfparts) > 0 else None\n                if tfp is not None:\n                    info_matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    info_cov = np.minimum(1.0, info_matched_mass / (q_idf_sum + eps))\n                    scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n        # Co-occurrence synergy: emphasize distinctive pairs (residualized vs max-idf),\n        # but gate it by query specificity to avoid over-rewarding broad modifiers.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: super-additive evidence when multiple distinctive\n    # query terms are present in the same doc (a proxy for topical coherence).\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query-specificity gate for \"AND-like\" effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: idf-mass coverage reward (soft-AND over *information*, not term count).\n    # Helps nDCG@10 on verbose queries where matching the \"important\" terms matters most.\n    cov_alpha: float = 0.18\n    cov_beta: float = 0.85\n    cov_idf_power: float = 1.0\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with a \"focus prior\" + bounded coordination reward,\n    plus two precision-oriented refinements:\n\n    (1) common-term soft penalty: ultra-high-df terms behave like glue and create noisy ties.\n    (2) distinctive co-occurrence synergy: weakly reward multi-term matches when the matched\n        terms are distinctive (a proxy for topical coherence without positions).\n\n    NEW: query-specificity gating for the AND-like effects (coord + synergy).\n    Intuition: queries with one very distinctive term + several broad modifiers are common;\n    naive coordination/synergy then over-rewards generic docs that match many modifiers.\n    Gate these effects down for \"peaky\" queries using spec = max_idf/sum_idf.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Precompute query specificity gate (doc-independent).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        # Two complementary \"AND-like\" notions:\n        # (A) term-count coverage (good for short factoid queries)\n        # (B) Evidence-weighted IDF-mass coverage:\n        #     cover \"important\" terms *with strength* (tf saturation), not just presence.\n        cov_count = matched / (qn + eps)\n\n        cov_idf = 0.0\n        if q_idf_sum > 0.0 and m_idf:\n            # Evidence-weighted matched info mass: sum(idf(term) * tf_part(term, doc)).\n            # This is a soft-AND over *information actually supported by the document*,\n            # reducing over-reward for one-off mentions of rare terms.\n            matched_mass = 0.0\n            for v, tp in zip(m_idf, m_tfpart):\n                matched_mass += v * tp\n            cov_idf = (matched_mass / (q_idf_sum + eps)) ** Config.cov_idf_power\n\n        coord = (1.0 + (Config.coord_alpha * spec_gate) * cov_count) ** Config.coord_beta\n        covr = (1.0 + (Config.cov_alpha * spec_gate) * cov_idf) ** Config.cov_beta\n        score *= coord * covr\n\n    # Co-occurrence synergy: emphasize distinctive pairs by residualizing vs max idf.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # For the query-specificity gate we need query-level idf stats (after common penalty).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            cov_count = matched / (qn + eps)\n\n            # Evidence-weighted IDF-mass coverage in vectorized form.\n            # matched_mass(doc) = sum(idf(term) * tf_part(term, doc))\n            cov_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n            if q_idf_sum > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                tfp = np.vstack(tfparts) if len(tfparts) > 0 else None\n                if tfp is not None:\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    cov_idf = np.power(matched_mass / (q_idf_sum + eps), Config.cov_idf_power)\n\n            coord = np.power(1.0 + (Config.coord_alpha * spec_gate) * cov_count, Config.coord_beta)\n            covr = np.power(1.0 + (Config.cov_alpha * spec_gate) * cov_idf, Config.cov_beta)\n            scores *= coord * covr\n\n        # Mirror retrieval_score(): distinctive-pair synergy (gated by specificity).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: Query entropy gate (complements \"peaky vs balanced\").\n    # max/sum only detects \"one dominant rare term\". Another failure mode is\n    # \"all terms are bland/common\" -> coordination/pairs add noise. Use normalized\n    # entropy of query-term idf-masses to gate AND-like rewards.\n    entropy_floor: float = 0.35   # never gate below this\n    entropy_power: float = 0.9    # concavity: <1 makes mid-entropy queries less penalized\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + gated \"soft AND\".\n\n    Upgrade: gate AND-like rewards not only by \"peakiness\" (max/sum) but also by\n    *query information dispersion* (entropy of query-term idf-masses).\n\n    Intuition:\n    - Peakiness gate prevents broad-modifier coordination from dominating when there is\n      one strong anchor term.\n    - Entropy gate prevents coordination/synergy from adding noise when *all* terms are\n      similarly common (flat/low-information queries).\n    Both gates are doc-independent, keeping recall from the base additive evidence.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf = []\n    m_w = []\n    m_tfpart = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query gates (doc-independent), computed under the same \"common term\" penalty.\n    q_idfs: list[float] = []\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idfs.append(q_idf)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n\n    # (A) Peakiness gate: high for balanced, low for peaky.\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # (B) Entropy gate: high when idf-mass is dispersed (informative multi-aspect query),\n    # low when distribution is too flat (all terms similarly common -> coordination noisy).\n    ent_gate = 1.0\n    if qn > 1.0 and q_idf_sum > 0.0:\n        H = 0.0\n        for v in q_idfs:\n            p = v / (q_idf_sum + eps)\n            if p > 0.0:\n                H -= p * math.log(p + eps)\n        Hn = H / (math.log(qn + eps) + eps)  # normalized to [0,1]\n        ent_gate = max(Config.entropy_floor, Hn ** Config.entropy_power)\n\n    and_gate = spec_gate * ent_gate\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query specificity gate (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        # Entropy gate (must mirror retrieval_score()) using query-term idfs after penalty.\n        ent_gate = 1.0\n        if qn > 1.0 and q_idf_sum > 0.0 and len(idfs) == len(query_term_ids):\n            p = np.maximum(0.0, np.array(idfs, dtype=np.float64)) / (q_idf_sum + eps)\n            # -sum p log p, normalized by log(qn)\n            H = -float(np.sum(np.where(p > 0.0, p * np.log(p + eps), 0.0)))\n            Hn = H / (math.log(qn + eps) + eps)\n            ent_gate = max(Config.entropy_floor, float(Hn ** Config.entropy_power))\n\n        and_gate = spec_gate * ent_gate\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * and_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * and_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4296, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (as in BM25, but we will change the \"norm\" definition)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # How much to trust \"lexical focus\" (unique terms) vs raw length.\n    # 0 -> pure length, 1 -> pure unique length.\n    focus_mix: float = 0.65\n\n    # Sharpen rarity/discriminativity (idf^power). power>1 penalizes common terms more.\n    idf_power: float = 1.12\n\n    # Coordination reward (>=1): favors docs covering more distinct query aspects.\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query repetition is often noise; sublinear qtf improves robustness.\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    # Goal: improve nDCG@10 by reducing noisy ties while preserving recall.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # NEW: extremely common terms can be *negative* evidence (repulsion).\n    # Intuition: boilerplate-heavy documents tend to contain many of these; making them\n    # slightly repulsive helps nDCG@10 while the additive positive evidence preserves recall.\n    use_signed_common_idf: bool = True\n    common_neg_strength: float = 0.18\n    common_neg_power: float = 1.25\n\n    # NEW: Soft-AND over *information mass* (IDF-budget coverage).\n    # Treat the query as an \"idf budget\" and reward documents that cover a larger fraction\n    # of that budget with TF-saturated support. This helps verbose queries where term-count\n    # coordination is noisy (BRIGHT, StackOverflow), generally improving nDCG@10.\n    info_cov_alpha: float = 0.18\n    info_cov_beta: float = 0.85\n\n    # Light co-occurrence synergy: super-additive evidence when multiple distinctive\n    # query terms appear in the same doc (proxy for topical coherence without positions).\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query-specificity gate for AND-like effects (coordination + synergy).\n    # spec = max_idf/sum_idf: \"peaky\" queries (entity + modifiers) should downweight\n    # coordination/synergy to avoid promoting generic docs matching many modifiers.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use distinct query terms + sublinear query-TF weights.\n\n        Rationale:\n        - Coordination/facet coverage is only meaningful over distinct terms.\n        - Repetition in queries often comes from tokenization artifacts / verbosity;\n          (1+log qtf) is a stable, training-free dampener.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + (gated) coordination reward + distinctive\n    co-occurrence synergy + soft downweighting of ultra-common terms.\n\n    Additions:\n    - IDF-budget coverage reward: a bounded soft-AND over *information mass*.\n    - Optional signed common-term contribution: ultra-common terms become weakly repulsive,\n      improving top-rank precision in boilerplate-heavy corpora.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Approximate unique-length from the tf dict (cheap).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    # Keep per-matched-term components for synergy (m is usually small).\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query-specificity gate (doc-independent; computed with same shaping as scoring).\n    # Also compute a nonnegative \"idf budget\" for information-coverage.\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_pos_budget = 0.0\n    for t in query_repr.terms:\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n        if q_idf > 0.0:\n            q_pos_budget += q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    matched_info_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms (keeps recall better than hard removal).\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n            # Optionally make extremely common terms weakly repulsive.\n            if Config.use_signed_common_idf:\n                u = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n                term_idf -= Config.common_neg_strength * (u ** Config.common_neg_power)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n\n        # Coverage should only count positive (discriminative) mass.\n        if term_idf > 0.0:\n            matched_info_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: IDF-budget coverage reward (soft-AND over information).\n        if Config.info_cov_alpha > 0.0 and q_pos_budget > 0.0:\n            info_cov = min(1.0, matched_info_mass / (q_pos_budget + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    # Distinctive-pair synergy: emphasize multiple *distinctive* matches.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        # Residualize vs max-idf to avoid boosting when only one term is distinctive.\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # For query-specificity gate, accumulate query-level idf stats *after* common penalty.\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n                # Mirror retrieval_score(): optional signed-negative evidence for ultra-common terms.\n                if Config.use_signed_common_idf:\n                    u = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                    idf_val -= Config.common_neg_strength * (u ** Config.common_neg_power)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # NEW: vectorized IDF-budget coverage reward (must match retrieval_score()).\n            if Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.maximum(0.0, np.array(idfs, dtype=np.float64))\n                budget = float(np.sum(idf_arr))\n                if budget > 0.0:\n                    tfp = np.vstack(tfparts)\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    info_cov = np.minimum(1.0, matched_mass / (budget + eps))\n                    scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(\n                self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist()\n            )\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4292, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: super-additive evidence when multiple distinctive\n    # query terms are present in the same doc (a proxy for topical coherence).\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query-specificity gate for \"AND-like\" effects (coordination + synergy).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: idf-mass coverage reward (soft-AND over *information*, not term count).\n    # Helps nDCG@10 on verbose queries where matching the \"important\" terms matters most.\n    cov_alpha: float = 0.18\n    cov_beta: float = 0.85\n    cov_idf_power: float = 1.0\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with a \"focus prior\" + bounded coordination reward,\n    plus two precision-oriented refinements:\n\n    (1) common-term soft penalty: ultra-high-df terms behave like glue and create noisy ties.\n    (2) distinctive co-occurrence synergy: weakly reward multi-term matches when the matched\n        terms are distinctive (a proxy for topical coherence without positions).\n\n    NEW: query-specificity gating for the AND-like effects (coord + synergy).\n    Intuition: queries with one very distinctive term + several broad modifiers are common;\n    naive coordination/synergy then over-rewards generic docs that match many modifiers.\n    Gate these effects down for \"peaky\" queries using spec = max_idf/sum_idf.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Precompute query specificity gate (doc-independent).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        # Two complementary \"AND-like\" notions:\n        # (A) term-count coverage (good for short factoid queries)\n        # (B) Evidence-weighted IDF-mass coverage:\n        #     cover \"important\" terms *with strength* (tf saturation), not just presence.\n        cov_count = matched / (qn + eps)\n\n        cov_idf = 0.0\n        if q_idf_sum > 0.0 and m_idf:\n            # Evidence-weighted matched info mass: sum(idf(term) * tf_part(term, doc)).\n            # This is a soft-AND over *information actually supported by the document*,\n            # reducing over-reward for one-off mentions of rare terms.\n            matched_mass = 0.0\n            for v, tp in zip(m_idf, m_tfpart):\n                matched_mass += v * tp\n            cov_idf = (matched_mass / (q_idf_sum + eps)) ** Config.cov_idf_power\n\n        coord = (1.0 + (Config.coord_alpha * spec_gate) * cov_count) ** Config.coord_beta\n        covr = (1.0 + (Config.cov_alpha * spec_gate) * cov_idf) ** Config.cov_beta\n        score *= coord * covr\n\n    # Co-occurrence synergy: emphasize distinctive pairs by residualizing vs max idf.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # For the query-specificity gate we need query-level idf stats (after common penalty).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            cov_count = matched / (qn + eps)\n\n            # Evidence-weighted IDF-mass coverage in vectorized form.\n            # matched_mass(doc) = sum(idf(term) * tf_part(term, doc))\n            cov_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n            if q_idf_sum > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                tfp = np.vstack(tfparts) if len(tfparts) > 0 else None\n                if tfp is not None:\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    cov_idf = np.power(matched_mass / (q_idf_sum + eps), Config.cov_idf_power)\n\n            coord = np.power(1.0 + (Config.coord_alpha * spec_gate) * cov_count, Config.coord_beta)\n            covr = np.power(1.0 + (Config.cov_alpha * spec_gate) * cov_idf, Config.cov_beta)\n            scores *= coord * covr\n\n        # Mirror retrieval_score(): distinctive-pair synergy (gated by specificity).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: Modification:, [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4292, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # NEW: query specificity gating.\n    # Intuition: if a query is dominated by common terms, we should avoid rewarding\n    # coverage/pairs too much (they become noisy). If the query has rare terms,\n    # reward multi-term matches more aggressively (helps nDCG@10).\n    spec_floor: float = 0.55   # minimum gate on coordination/synergy\n    spec_power: float = 1.20   # >1 emphasizes differences in specificity\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + query-specificity\u2013gated \"soft AND\".\n\n    Same base as before (BM25-ish tf saturation + focus-normalization + common-term penalty),\n    but we *gate* coordination/synergy by query specificity:\n\n    - If the query's evidence is mostly from common terms, multi-term rewards are noisy,\n      so reduce them (protect recall, improve nDCG on broad queries).\n    - If the query contains rare terms, reward covering multiple aspects more (boost nDCG@10).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf = []\n    m_w = []\n    m_tfpart = []\n\n    # Compute a query-level specificity gate from the query itself (doc-independent).\n    # Uses the same common-term penalty so \"glue-heavy\" queries down-gate AND-like rewards.\n    common_thr = Config.common_df_cut * float(N)\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    # spec = max/sum is HIGH for \"peaky\" queries (one strong anchor + many broad terms).\n    # For AND-like rewards (coordination/pairs), we want the opposite:\n    #   - balanced multi-term queries -> stronger gate\n    #   - peaky queries -> weaker gate (avoid promoting generic modifier matches)\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        # Gate coordination by query specificity\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        # Gate synergy by query specificity as well\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query specificity gate (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        # Mirror retrieval_score(): down-gate AND-like rewards for peaky queries.\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4292, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Co-occurrence synergy (kept small; primarily helps top ranks)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # High-df terms are \"glue words\"; softly downweight instead of hard stopwording\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Query specificity gating for AND-like rewards.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: bounded \"information coverage\" reward (soft-AND over IDF mass).\n    # Intuition: coordination over *term count* is noisy for verbose queries; instead,\n    # reward the fraction of the query's discriminative mass actually supported by the doc.\n    info_cov_alpha: float = 0.18\n    info_cov_beta: float = 0.85\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive lexical evidence + query-specificity\u2013gated \"soft AND\", plus a bounded\n    *information coverage* reward.\n\n    Key idea (novel vs plain BM25 knobs):\n    - Coordination over term *count* is a crude proxy for \"covering the query\".\n      For verbose queries, matching many low-IDF modifiers can beat matching the\n      few high-IDF anchors, hurting nDCG@10.\n    - Instead, treat the query as an \"IDF budget\" and reward the fraction of that\n      budget the document supports, with TF saturation included:\n          info_cov = sum_{t in q\u2229d} idf(t)*tf_part(t,d) / sum_{t in q} idf(t)\n      This is naturally bounded in [0,1] and multiplicative, so it tends to\n      improve top-rank ordering without damaging recall@100 much.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate + query idf-budget (doc-independent).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    info_matched_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        info_matched_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: soft-AND over information mass (idf budget coverage), gated like coordination.\n        if q_idf_sum > 0.0 and Config.info_cov_alpha > 0.0:\n            info_cov = min(1.0, info_matched_mass / (q_idf_sum + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs = []\n        ws = []\n        tfparts = []\n        presents = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Query specificity gate (must match retrieval_score()).\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            q_idf_sum += idf_val\n            if idf_val > q_idf_max:\n                q_idf_max = idf_val\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        # Mirror retrieval_score(): down-gate AND-like rewards for peaky queries.\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # NEW: vectorized info-mass coverage reward (must match retrieval_score()).\n            if q_idf_sum > 0.0 and Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                tfp = np.vstack(tfparts)\n                info_matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                info_cov = np.minimum(1.0, info_matched_mass / (q_idf_sum + eps))\n                scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    # (A bounded \"AND-ish\" effect that usually helps nDCG@10 without killing recall.)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: soft-AND over *information mass* (idf-budget coverage).\n    # Rewards docs that cover a larger fraction of the query's discriminative mass,\n    # reducing the \"many weak modifiers beat one strong anchor\" failure mode.\n    info_cov_alpha: float = 0.18\n    info_cov_beta: float = 0.85\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards.\n\n    Design:\n    - Base evidence: BM25-like saturating TF evidence (diminishing returns).\n    - Length prior: pivoted normalization on a blend of token length and unique-term length\n      (prefers focused docs without crushing long informative ones).\n    - Collection prior: smoothly downweight ultra-common (high-df) terms.\n    - Coordination + distinctive pair synergy: multiplicative rewards gated by query specificity\n      (avoid over-rewarding generic multi-matches when a query is dominated by one sharp term).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Query positive IDF budget (after common-term penalty, consistent with scoring).\n    q_pos_budget = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        v = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        if v > 0.0:\n            q_pos_budget += v\n\n    matched_info_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms.\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        if term_idf > 0.0:\n            matched_info_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: bounded info-mass coverage reward (soft-AND over discriminative mass).\n        if Config.info_cov_alpha > 0.0 and q_pos_budget > 0.0:\n            info_cov = min(1.0, matched_info_mass / (q_pos_budget + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, gated by query specificity.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # NEW: vectorized info-mass coverage reward (must mirror retrieval_score()).\n            if Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.maximum(0.0, np.array(idfs, dtype=np.float64))\n                budget = float(np.sum(idf_arr))\n                if budget > 0.0:\n                    tfp = np.vstack(tfparts)\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    info_cov = np.minimum(1.0, matched_mass / (budget + eps))\n                    scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Below are two targeted changes aimed at improving *ranking quality* (nDCG@10) while keeping recall stable, and also simplifying/shortening code a bit.\n\nMain idea:\n1. **Add \u201canchor term\u201d dominance**: many queries have one or two highly-discriminative terms (entities, key concepts). We reward documents that match these anchors, but do it *smoothly* and *bounded* so we don\u2019t crush recall.\n2. **Avoid double query loops**: compute query stats (specificity + budget) in one pass.\n\n---\n\n## Change 1 \u2014 Add anchor coverage reward + merge query-stat loops (retrieval_score)\n\nThis introduces a new bounded multiplier:\n- Compute the top-`A` query terms by discriminative weight (after common penalty), where `A` is small (e.g., 2).\n- Compute the fraction of this \u201canchor mass\u201d covered by the document (with TF saturation), and apply a mild multiplicative bonus.\n- This tends to improve top-rank ordering when a document matches many generic modifiers but misses the key entity.\n\nAlso merges the two query loops into one.\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards.\n\n    Design:\n    - Base evidence: BM25-like saturating TF evidence (diminishing returns).\n    - Length prior: pivoted normalization on a blend of token length and unique-term length\n      (prefers focused docs without crushing long informative ones).\n    - Collection prior: smoothly downweight ultra-common (high-df) terms.\n    - Coordination + distinctive pair synergy: multiplicative rewards gated by query specificity\n      (avoid over-rewarding generic multi-matches when a query is dominated by one sharp term).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Query positive IDF budget (after common-term penalty, consistent with scoring).\n    q_pos_budget = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        v = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            v *= (1.0 - Config.common_penalty * frac)\n        if v > 0.0:\n            q_pos_budget += v\n\n    matched_info_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms.\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        if term_idf > 0.0:\n            matched_info_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: bounded info-mass coverage reward (soft-AND over discriminative mass).\n        if Config.info_cov_alpha > 0.0 and q_pos_budget > 0.0:\n            info_cov = min(1.0, matched_info_mass / (q_pos_budget + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, gated by query specificity.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards.\n\n    Additions:\n    - Anchor coverage reward: many queries have 1\u20132 \"anchor\" terms (entities / key concepts).\n      We compute the IDF-mass of the top anchors and reward documents that cover them\n      (with TF saturation). This is a bounded multiplicative bonus, so recall@100 is\n      largely preserved while nDCG@10 often improves on entity+modifier queries.\n    - Implementation: merge query-stat loops to keep overhead small.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # One pass: query stats for specificity, positive budget, and anchor selection.\n    q_weights: list[tuple[str, float]] = []\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_pos_budget = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        w = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            w *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += w\n        if w > q_idf_max:\n            q_idf_max = w\n        if w > 0.0:\n            q_pos_budget += w\n        q_weights.append((term, w))\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Anchor definition: top-2 query terms by shaped weight (after common penalty).\n    # Small A keeps it stable and cheap; reward is bounded and mild.\n    ANCHOR_K = 2\n    q_weights.sort(key=lambda x: x[1], reverse=True)\n    anchor_mass = 0.0\n    anchor_terms: list[str] = []\n    for t, w in q_weights[:ANCHOR_K]:\n        if w > 0.0:\n            anchor_terms.append(t)\n            anchor_mass += w\n\n    matched_info_mass = 0.0\n    matched_anchor_mass = 0.0\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        if term_idf > 0.0:\n            matched_info_mass += term_idf * tf_part\n            if term in anchor_terms:\n                matched_anchor_mass += term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        if Config.info_cov_alpha > 0.0 and q_pos_budget > 0.0:\n            info_cov = min(1.0, matched_info_mass / (q_pos_budget + eps))\n            score *= (1.0 + (Config.info_cov_alpha * spec_gate) * info_cov) ** Config.info_cov_beta\n\n    # Anchor reward (bounded): prefer docs that cover the anchor terms of the query.\n    # The bonus is intentionally weak to avoid harming recall in noisy corpora.\n    if anchor_mass > 0.0:\n        anchor_cov = min(1.0, matched_anchor_mass / (anchor_mass + eps))\n        # Slightly stronger when the query is peaky (spec high -> spec_gate low), since then\n        # anchors matter even more. Use (1 - spec_gate) as a simple complement.\n        score *= 1.0 + 0.10 * (0.5 + 0.5 * (1.0 - spec_gate)) * anchor_cov\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n>>>>>>> REPLACE\n\n---\n\n## Change 2 \u2014 Mirror anchor reward in vectorized rank() scoring\n\nWe must keep `rank()` consistent with `retrieval_score()`. This adds a cheap anchor term computation at query-time (from `idfs` already computed) and a vectorized \u201canchor mass matched\u201d using the already-built `tfparts`.\n\nThis is designed to be minimal overhead (small number of anchors).\n\n<<<<<<< SEARCH\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # NEW: vectorized info-mass coverage reward (must mirror retrieval_score()).\n            if Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.maximum(0.0, np.array(idfs, dtype=np.float64))\n                budget = float(np.sum(idf_arr))\n                if budget > 0.0:\n                    tfp = np.vstack(tfparts)\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    info_cov = np.minimum(1.0, matched_mass / (budget + eps))\n                    scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n=======\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # Info-mass coverage reward (mirrors retrieval_score()).\n            tfp = None\n            if Config.info_cov_alpha > 0.0 and len(idfs) > 0 and len(tfparts) > 0:\n                idf_arr = np.maximum(0.0, np.array(idfs, dtype=np.float64))\n                budget = float(np.sum(idf_arr))\n                if budget > 0.0:\n                    tfp = np.vstack(tfparts)\n                    matched_mass = (idf_arr[:, None] * tfp).sum(axis=0)\n                    info_cov = np.minimum(1.0, matched_mass / (budget + eps))\n                    scores *= np.power(1.0 + (Config.info_cov_alpha * spec_gate) * info_cov, Config.info_cov_beta)\n\n            # Anchor coverage reward: top-2 idf terms in the query.\n            # Uses tfp if available; otherwise stack once here (still small: |q| x |cand|).\n            if len(idfs) > 0 and len(tfparts) > 0:\n                if tfp is None:\n                    tfp = np.vstack(tfparts)\n                idf_pos = np.maximum(0.0, np.array(idfs, dtype=np.float64))\n                if idf_pos.size >= 1:\n                    # indices of top-2 anchors\n                    topk = 2 if idf_pos.size >= 2 else 1\n                    anchor_idx = np.argpartition(-idf_pos, topk - 1)[:topk]\n                    anchor_mass = float(np.sum(idf_pos[anchor_idx]))\n                    if anchor_mass > 0.0:\n                        matched_anchor = (idf_pos[anchor_idx, None] * tfp[anchor_idx]).sum(axis=0)\n                        anchor_cov = np.minimum(1.0, matched_anchor / (anchor_mass + eps))\n                        scores *= 1.0 + 0.10 * (0.5 + 0.5 * (1.0 - spec_gate)) * anchor_cov\n>>>>>>> REPLACE\n\n---\n\n### Why this might improve FITNESS\n- Improves **nDCG@10** on datasets with entity-heavy or \u201cone key term + modifiers\u201d queries (common in ArguAna, StackOverflow, some BRIGHT tasks) by preventing modifier-only matches from floating to the top.\n- Keeps **recall@100** stable because:\n  - base additive evidence is unchanged,\n  - anchor reward is bounded and mild (multiplier close to 1),\n  - it only reorders candidates already retrieved by postings union.\n\nIf you want to push diversity further in a later iteration, the next orthogonal direction would be *term proximity via hashed window co-occurrence* (not just adjacent bigrams), but the above is intentionally minimal and contract-safe."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}