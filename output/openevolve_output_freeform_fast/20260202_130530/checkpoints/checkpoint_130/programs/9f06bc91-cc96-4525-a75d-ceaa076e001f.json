{"id": "9f06bc91-cc96-4525-a75d-ceaa076e001f", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # NEW: \"IDF-mass satisfaction\" reward.\n    # Intuition: early precision improves when we reward covering the query's informative mass,\n    # not just counting matched terms. This is bounded and query-dependent.\n    sat_alpha: float = 0.16\n    sat_power: float = 1.6\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base signals as before, but with a *query-specificity gate* on the\n    \"AND-like\" effects (coordination + pair synergy).\n\n    Why this can help:\n    - Many BEIR/BRIGHT queries contain one sharp entity/term + several broad modifiers.\n      Coordination/pair boosts can then over-promote generic docs that match many broad terms.\n    - Use a simple, doc-independent specificity proxy:\n        spec = max_idf / sum_idf  (peaky queries -> larger spec)\n      We down-gate coordination/synergy for peaky queries, keeping recall from the\n      main additive evidence but improving nDCG@10 by reducing noisy \"multi-match\" wins.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    # Also compute satisfaction denominator: total \"informative mass\" in the query.\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    sat_den = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n        sat_den += q_idf ** Config.sat_power\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    sat_num = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n        sat_num += term_idf ** Config.sat_power\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # IDF-mass satisfaction (bounded in [0,1]), also gated by specificity.\n        if Config.sat_alpha > 0.0 and sat_den > 0.0:\n            sat = sat_num / (sat_den + eps)\n            score *= 1.0 + (Config.sat_alpha * spec_gate) * sat\n\n    # Co-occurrence synergy: emphasize distinctive pairs, but gate by query specificity too.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # Mirror retrieval_score(): per-doc IDF-mass satisfaction multiplier.\n            if Config.sat_alpha > 0.0 and idfs:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                idf_pow = np.power(idf_arr, Config.sat_power)\n                sat_den = float(np.sum(idf_pow))\n                if sat_den > 0.0 and presents:\n                    sat_num = np.zeros(len(candidate_docs), dtype=np.float64)\n                    for ti, pres in enumerate(presents):\n                        w = float(idf_pow[ti])\n                        if w > 0.0:\n                            sat_num += pres * w\n                    scores *= 1.0 + (Config.sat_alpha * spec_gate) * (sat_num / (sat_den + eps))\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "d6da60db-e991-4e2a-8a89-b9559e4ac0f0", "generation": 5, "timestamp": 1770113272.4765031, "iteration_found": 111, "metrics": {"beir_nfcorpus_ndcg@10": 0.3193867750203054, "beir_nfcorpus_recall@100": 0.25565492215930236, "beir_nfcorpus_index_time_ms": 3768.9228340168484, "beir_nfcorpus_query_time_ms": 112.14287497568876, "bright_pony_ndcg@10": 0.10414094652709362, "bright_pony_recall@100": 0.29253287445425397, "bright_pony_index_time_ms": 1728.718750004191, "bright_pony_query_time_ms": 631.5483330108691, "beir_scifact_ndcg@10": 0.6740001438760891, "beir_scifact_recall@100": 0.932, "beir_scifact_index_time_ms": 5195.948499982478, "beir_scifact_query_time_ms": 394.29324999218807, "beir_arguana_ndcg@10": 0.2783352075076407, "beir_arguana_recall@100": 0.9093504639543183, "beir_arguana_index_time_ms": 6306.3886250020005, "beir_arguana_query_time_ms": 30043.33279200364, "bright_theoremqa_theorems_ndcg@10": 0.040423220229772364, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 11085.6226249889, "bright_theoremqa_theorems_query_time_ms": 1199.6998339891434, "beir_scidocs_ndcg@10": 0.1426490821104922, "beir_scidocs_recall@100": 0.3431833333333334, "beir_scidocs_index_time_ms": 20283.502291014884, "beir_scidocs_query_time_ms": 3724.0320409764536, "bright_economics_ndcg@10": 0.13665500417817997, "bright_economics_recall@100": 0.3905838629570868, "bright_economics_index_time_ms": 21526.091375009855, "bright_economics_query_time_ms": 3844.018874980975, "bright_biology_ndcg@10": 0.24535846706594805, "bright_biology_recall@100": 0.545365592223027, "bright_biology_index_time_ms": 23990.5623330269, "bright_biology_query_time_ms": 3572.4504580139183, "beir_fiqa_ndcg@10": 0.2252031942493102, "beir_fiqa_recall@100": 0.5229975871179574, "beir_fiqa_index_time_ms": 45158.623124996666, "beir_fiqa_query_time_ms": 7344.847457978176, "bright_earth_science_ndcg@10": 0.3432311414936158, "bright_earth_science_recall@100": 0.6649984090319844, "bright_earth_science_index_time_ms": 48442.26516599883, "bright_earth_science_query_time_ms": 6559.600167005556, "bright_stackoverflow_ndcg@10": 0.17216679652835598, "bright_stackoverflow_recall@100": 0.49296548663986156, "bright_stackoverflow_index_time_ms": 114270.81620800891, "bright_stackoverflow_query_time_ms": 11240.357999980915, "beir_trec-covid_ndcg@10": 0.5977978022442849, "beir_trec-covid_recall@100": 0.10412695686958598, "beir_trec-covid_index_time_ms": 157266.14183399943, "beir_trec-covid_query_time_ms": 1620.2659159898758, "avg_ndcg@10": 0.2732789817525907, "avg_recall@100": 0.46727235505587794, "combined_score": 0.42847368039522055, "total_index_time_ms": 459023.6036660499, "total_query_time_ms": 70286.5899988974, "total_time_ms": 529310.1936649473, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 36 lines with 39 lines\nChange 2: Replace 40 lines with 46 lines\nChange 3: Replace 3 lines with 8 lines\nChange 4: Replace 3 lines with 16 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3200468649013623, "beir_nfcorpus_recall@100": 0.25527972929225395, "beir_nfcorpus_index_time_ms": 3730.9816670021974, "beir_nfcorpus_query_time_ms": 104.74454198265448, "bright_pony_ndcg@10": 0.10206184664180705, "bright_pony_recall@100": 0.2906975569939365, "bright_pony_index_time_ms": 1504.448666004464, "bright_pony_query_time_ms": 586.403541994514, "beir_scifact_ndcg@10": 0.6749924182626882, "beir_scifact_recall@100": 0.932, "beir_scifact_index_time_ms": 5022.029166022548, "beir_scifact_query_time_ms": 374.3780410150066, "beir_arguana_ndcg@10": 0.27897096168745067, "beir_arguana_recall@100": 0.9114917915774446, "beir_arguana_index_time_ms": 6018.225208012154, "beir_arguana_query_time_ms": 29252.28425001842, "bright_theoremqa_theorems_ndcg@10": 0.040423220229772364, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 10863.940458017169, "bright_theoremqa_theorems_query_time_ms": 1203.9313330024015, "beir_scidocs_ndcg@10": 0.14317869851971954, "beir_scidocs_recall@100": 0.34298333333333336, "beir_scidocs_index_time_ms": 19813.92529202276, "beir_scidocs_query_time_ms": 3647.5089589948766, "bright_economics_ndcg@10": 0.14096139848729317, "bright_economics_recall@100": 0.390907487552556, "bright_economics_index_time_ms": 17039.452458993765, "bright_economics_query_time_ms": 3852.5785419915337, "bright_biology_ndcg@10": 0.24327326048284922, "bright_biology_recall@100": 0.545365592223027, "bright_biology_index_time_ms": 20535.623042000225, "bright_biology_query_time_ms": 3441.96137500694, "beir_fiqa_ndcg@10": 0.22599243658671295, "beir_fiqa_recall@100": 0.5253013790050827, "beir_fiqa_index_time_ms": 43185.21604099078, "beir_fiqa_query_time_ms": 7016.158082988113, "bright_earth_science_ndcg@10": 0.3513546941933217, "bright_earth_science_recall@100": 0.6647502376631234, "bright_earth_science_index_time_ms": 45724.76408298826, "bright_earth_science_query_time_ms": 6465.890708990628, "bright_stackoverflow_ndcg@10": 0.17266920793762008, "bright_stackoverflow_recall@100": 0.49296548663986156, "bright_stackoverflow_index_time_ms": 111039.5411659847, "bright_stackoverflow_query_time_ms": 10289.653624990024, "beir_trec-covid_ndcg@10": 0.6185599747268605, "beir_trec-covid_recall@100": 0.10472873117470223, "beir_trec-covid_index_time_ms": 150803.10637500952, "beir_trec-covid_query_time_ms": 1551.8009999941569, "avg_ndcg@10": 0.2760404152214548, "avg_recall@100": 0.4674983414487622, "combined_score": 0.42920675620330073, "total_index_time_ms": 435281.25362304854, "total_query_time_ms": 67787.29400096927, "total_time_ms": 503068.5476240178, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4292\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.4288 \u2192 0.4292\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 4\n- Changes: Change 1: Replace 42 lines with 48 lines\nChange 2: Replace 3 lines with 8 lines\nChange 3: Replace 2 lines with 7 lines\nChange 4: Replace 5 lines with 31 lines\nChange 5: Replace scores = np.zeros(len(candidate_docs), dtype=np.float64) with 7 lines\nChange 6: Replace 2 lines with 7 lines\nChange 7: Replace 26 lines with 37 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3182, beir_nfcorpus_recall@100: 0.2551, beir_nfcorpus_index_time_ms: 4785.3834, beir_nfcorpus_query_time_ms: 170.2771, bright_pony_ndcg@10: 0.1141, bright_pony_recall@100: 0.3035, bright_pony_index_time_ms: 2091.7668, bright_pony_query_time_ms: 809.7659, beir_scifact_ndcg@10: 0.6728, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 6169.2489, beir_scifact_query_time_ms: 471.0450, beir_arguana_ndcg@10: 0.2737, beir_arguana_recall@100: 0.9008, beir_arguana_index_time_ms: 6979.6276, beir_arguana_query_time_ms: 33088.6636, bright_theoremqa_theorems_ndcg@10: 0.0347, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 11115.4245, bright_theoremqa_theorems_query_time_ms: 1398.3117, beir_scidocs_ndcg@10: 0.1401, beir_scidocs_recall@100: 0.3398, beir_scidocs_index_time_ms: 20121.5033, beir_scidocs_query_time_ms: 4908.7378, bright_economics_ndcg@10: 0.1359, bright_economics_recall@100: 0.3882, bright_economics_index_time_ms: 21408.1888, bright_economics_query_time_ms: 4693.0406, bright_biology_ndcg@10: 0.2628, bright_biology_recall@100: 0.5624, bright_biology_index_time_ms: 23411.3782, bright_biology_query_time_ms: 4316.5700, bright_earth_science_ndcg@10: 0.3472, bright_earth_science_recall@100: 0.6689, bright_earth_science_index_time_ms: 45799.3406, bright_earth_science_query_time_ms: 8743.3879, beir_fiqa_ndcg@10: 0.2194, beir_fiqa_recall@100: 0.5162, beir_fiqa_index_time_ms: 49157.7050, beir_fiqa_query_time_ms: 10229.5536, bright_stackoverflow_ndcg@10: 0.1722, bright_stackoverflow_recall@100: 0.4931, bright_stackoverflow_index_time_ms: 117703.1995, bright_stackoverflow_query_time_ms: 13355.1454, beir_trec-covid_ndcg@10: 0.5753, beir_trec-covid_recall@100: 0.1017, beir_trec-covid_index_time_ms: 163427.4311, beir_trec-covid_query_time_ms: 2127.6443, avg_ndcg@10: 0.2722, avg_recall@100: 0.4679, combined_score: 0.4288, total_index_time_ms: 472170.1975, total_query_time_ms: 84312.1427, total_time_ms: 556482.3403, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 3\n- Changes: Change 1: Replace 34 lines with 40 lines\nChange 2: Replace 107 lines with 112 lines\nChange 3: Replace 9 lines with 20 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3200, beir_nfcorpus_recall@100: 0.2553, beir_nfcorpus_index_time_ms: 3856.8961, beir_nfcorpus_query_time_ms: 111.5035, bright_pony_ndcg@10: 0.1021, bright_pony_recall@100: 0.2907, bright_pony_index_time_ms: 1558.0094, bright_pony_query_time_ms: 597.4185, beir_scifact_ndcg@10: 0.6750, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5175.9376, beir_scifact_query_time_ms: 380.2010, beir_arguana_ndcg@10: 0.2790, beir_arguana_recall@100: 0.9115, beir_arguana_index_time_ms: 6410.2790, beir_arguana_query_time_ms: 29386.0143, bright_theoremqa_theorems_ndcg@10: 0.0404, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10786.8476, bright_theoremqa_theorems_query_time_ms: 1187.0875, beir_scidocs_ndcg@10: 0.1432, beir_scidocs_recall@100: 0.3430, beir_scidocs_index_time_ms: 19956.8390, beir_scidocs_query_time_ms: 3644.8884, bright_economics_ndcg@10: 0.1410, bright_economics_recall@100: 0.3909, bright_economics_index_time_ms: 21620.8496, bright_economics_query_time_ms: 3798.6068, bright_biology_ndcg@10: 0.2433, bright_biology_recall@100: 0.5454, bright_biology_index_time_ms: 23987.4105, bright_biology_query_time_ms: 3501.4967, beir_fiqa_ndcg@10: 0.2260, beir_fiqa_recall@100: 0.5253, beir_fiqa_index_time_ms: 46954.9682, beir_fiqa_query_time_ms: 6883.6242, bright_earth_science_ndcg@10: 0.3514, bright_earth_science_recall@100: 0.6648, bright_earth_science_index_time_ms: 46103.5097, bright_earth_science_query_time_ms: 6693.3751, bright_stackoverflow_ndcg@10: 0.1727, bright_stackoverflow_recall@100: 0.4930, bright_stackoverflow_index_time_ms: 109288.0844, bright_stackoverflow_query_time_ms: 11057.4444, beir_trec-covid_ndcg@10: 0.6186, beir_trec-covid_recall@100: 0.1047, beir_trec-covid_index_time_ms: 153711.9914, beir_trec-covid_query_time_ms: 1517.6864, avg_ndcg@10: 0.2760, avg_recall@100: 0.4675, combined_score: 0.4292, total_index_time_ms: 449411.6225, total_query_time_ms: 68759.3469, total_time_ms: 518170.9694, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 48 lines with 34 lines\nChange 2: Replace 18 lines with 10 lines\nChange 3: Replace 106 lines with 107 lines\nChange 4: Replace 9 lines with 8 lines\nChange 5: Replace 80 lines with 74 lines\nChange 6: Replace 5 lines with # No query bigram features (keeps ranking stable and cheap).\nChange 7: 'candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf, q_bigrams)' to 'candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)'\n- Metrics: beir_nfcorpus_ndcg@10: 0.3200, beir_nfcorpus_recall@100: 0.2553, beir_nfcorpus_index_time_ms: 4402.0911, beir_nfcorpus_query_time_ms: 107.9049, bright_pony_ndcg@10: 0.1021, bright_pony_recall@100: 0.2907, bright_pony_index_time_ms: 1714.2250, bright_pony_query_time_ms: 623.5102, beir_scifact_ndcg@10: 0.6750, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5699.8717, beir_scifact_query_time_ms: 429.1488, beir_arguana_ndcg@10: 0.2790, beir_arguana_recall@100: 0.9115, beir_arguana_index_time_ms: 6735.6733, beir_arguana_query_time_ms: 30903.6009, bright_theoremqa_theorems_ndcg@10: 0.0404, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 11148.4606, bright_theoremqa_theorems_query_time_ms: 1170.0600, beir_scidocs_ndcg@10: 0.1432, beir_scidocs_recall@100: 0.3430, beir_scidocs_index_time_ms: 20587.3181, beir_scidocs_query_time_ms: 3752.0252, bright_economics_ndcg@10: 0.1410, bright_economics_recall@100: 0.3909, bright_economics_index_time_ms: 18704.2499, bright_economics_query_time_ms: 3781.9327, bright_biology_ndcg@10: 0.2433, bright_biology_recall@100: 0.5454, bright_biology_index_time_ms: 23874.3709, bright_biology_query_time_ms: 3607.8255, beir_fiqa_ndcg@10: 0.2260, beir_fiqa_recall@100: 0.5253, beir_fiqa_index_time_ms: 47226.3965, beir_fiqa_query_time_ms: 7493.4136, bright_earth_science_ndcg@10: 0.3514, bright_earth_science_recall@100: 0.6648, bright_earth_science_index_time_ms: 47167.7351, bright_earth_science_query_time_ms: 6856.8051, bright_stackoverflow_ndcg@10: 0.1727, bright_stackoverflow_recall@100: 0.4930, bright_stackoverflow_index_time_ms: 109775.0935, bright_stackoverflow_query_time_ms: 11464.5012, beir_trec-covid_ndcg@10: 0.6186, beir_trec-covid_recall@100: 0.1047, beir_trec-covid_index_time_ms: 157027.3320, beir_trec-covid_query_time_ms: 1666.0685, avg_ndcg@10: 0.2760, avg_recall@100: 0.4675, combined_score: 0.4292, total_index_time_ms: 454062.8177, total_query_time_ms: 71856.7966, total_time_ms: 525919.6143, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # NEW: query specificity gating for \"AND-like\" effects.\n    # Intuition: when a query is dominated by one highly distinctive term + many vague terms,\n    # coordination/pairs can over-reward noisy matches. Gate those effects by specificity.\n    # spec = max_idf / sum_idf in [~1/|q|, 1]. Higher => more \"peaky\" query.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base signals as before, but with a *query-specificity gate* on the\n    \"AND-like\" effects (coordination + pair synergy).\n\n    Why this can help:\n    - Many BEIR/BRIGHT queries contain one sharp entity/term + several broad modifiers.\n      Coordination/pair boosts can then over-promote generic docs that match many broad terms.\n    - Use a simple, doc-independent specificity proxy:\n        spec = max_idf / sum_idf  (peaky queries -> larger spec)\n      We down-gate coordination/synergy for peaky queries, keeping recall from the\n      main additive evidence but improving nDCG@10 by reducing noisy \"multi-match\" wins.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, but gate by query specificity too.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3200), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (3730.9817), Performs well on beir_nfcorpus_query_time_ms (104.7445), Performs well on bright_pony_ndcg@10 (0.1021), Performs well on bright_pony_recall@100 (0.2907), Performs well on bright_pony_index_time_ms (1504.4487), Performs well on bright_pony_query_time_ms (586.4035), Performs well on beir_scifact_ndcg@10 (0.6750), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5022.0292), Performs well on beir_scifact_query_time_ms (374.3780), Performs well on beir_arguana_ndcg@10 (0.2790), Performs well on beir_arguana_recall@100 (0.9115), Performs well on beir_arguana_index_time_ms (6018.2252), Performs well on beir_arguana_query_time_ms (29252.2843), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10863.9405), Performs well on bright_theoremqa_theorems_query_time_ms (1203.9313), Performs well on beir_scidocs_ndcg@10 (0.1432), Performs well on beir_scidocs_recall@100 (0.3430), Performs well on beir_scidocs_index_time_ms (19813.9253), Performs well on beir_scidocs_query_time_ms (3647.5090), Performs well on bright_economics_ndcg@10 (0.1410), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (17039.4525), Performs well on bright_economics_query_time_ms (3852.5785), Performs well on bright_biology_ndcg@10 (0.2433), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (20535.6230), Performs well on bright_biology_query_time_ms (3441.9614), Performs well on beir_fiqa_ndcg@10 (0.2260), Performs well on beir_fiqa_recall@100 (0.5253), Performs well on beir_fiqa_index_time_ms (43185.2160), Performs well on beir_fiqa_query_time_ms (7016.1581), Performs well on bright_earth_science_ndcg@10 (0.3514), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (45724.7641), Performs well on bright_earth_science_query_time_ms (6465.8907), Performs well on bright_stackoverflow_ndcg@10 (0.1727), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (111039.5412), Performs well on bright_stackoverflow_query_time_ms (10289.6536), Performs well on beir_trec-covid_ndcg@10 (0.6186), Performs well on beir_trec-covid_recall@100 (0.1047), Performs well on beir_trec-covid_index_time_ms (150803.1064), Performs well on beir_trec-covid_query_time_ms (1551.8010), Performs well on avg_ndcg@10 (0.2760), Performs well on avg_recall@100 (0.4675), Performs well on combined_score (0.4292), Performs well on total_index_time_ms (435281.2536), Performs well on total_query_time_ms (67787.2940), Performs well on total_time_ms (503068.5476), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    # (A bounded \"AND-ish\" effect that usually helps nDCG@10 without killing recall.)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards.\n\n    Design:\n    - Base evidence: BM25-like saturating TF evidence (diminishing returns).\n    - Length prior: pivoted normalization on a blend of token length and unique-term length\n      (prefers focused docs without crushing long informative ones).\n    - Collection prior: smoothly downweight ultra-common (high-df) terms.\n    - Coordination + distinctive pair synergy: multiplicative rewards gated by query specificity\n      (avoid over-rewarding generic multi-matches when a query is dominated by one sharp term).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms.\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, gated by query specificity.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3200), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (4402.0911), Performs well on beir_nfcorpus_query_time_ms (107.9049), Performs well on bright_pony_ndcg@10 (0.1021), Performs well on bright_pony_recall@100 (0.2907), Performs well on bright_pony_index_time_ms (1714.2250), Performs well on bright_pony_query_time_ms (623.5102), Performs well on beir_scifact_ndcg@10 (0.6750), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5699.8717), Performs well on beir_scifact_query_time_ms (429.1488), Performs well on beir_arguana_ndcg@10 (0.2790), Performs well on beir_arguana_recall@100 (0.9115), Performs well on beir_arguana_index_time_ms (6735.6733), Performs well on beir_arguana_query_time_ms (30903.6009), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11148.4606), Performs well on bright_theoremqa_theorems_query_time_ms (1170.0600), Performs well on beir_scidocs_ndcg@10 (0.1432), Performs well on beir_scidocs_recall@100 (0.3430), Performs well on beir_scidocs_index_time_ms (20587.3181), Performs well on beir_scidocs_query_time_ms (3752.0252), Performs well on bright_economics_ndcg@10 (0.1410), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (18704.2499), Performs well on bright_economics_query_time_ms (3781.9327), Performs well on bright_biology_ndcg@10 (0.2433), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (23874.3709), Performs well on bright_biology_query_time_ms (3607.8255), Performs well on beir_fiqa_ndcg@10 (0.2260), Performs well on beir_fiqa_recall@100 (0.5253), Performs well on beir_fiqa_index_time_ms (47226.3965), Performs well on beir_fiqa_query_time_ms (7493.4136), Performs well on bright_earth_science_ndcg@10 (0.3514), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (47167.7351), Performs well on bright_earth_science_query_time_ms (6856.8051), Performs well on bright_stackoverflow_ndcg@10 (0.1727), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (109775.0935), Performs well on bright_stackoverflow_query_time_ms (11464.5012), Performs well on beir_trec-covid_ndcg@10 (0.6186), Performs well on beir_trec-covid_recall@100 (0.1047), Performs well on beir_trec-covid_index_time_ms (157027.3320), Performs well on beir_trec-covid_query_time_ms (1666.0685), Performs well on avg_ndcg@10 (0.2760), Performs well on avg_recall@100 (0.4675), Performs well on combined_score (0.4292), Performs well on total_index_time_ms (454062.8177), Performs well on total_query_time_ms (71856.7966), Performs well on total_time_ms (525919.6143), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4292)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # NEW: \"information satisfaction\" reward.\n    # Intuition: early precision improves when we reward covering the query's *IDF mass*,\n    # not just counting matched terms. This is a soft, bounded multiplier.\n    sat_alpha: float = 0.18\n    sat_power: float = 1.6\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    # (A bounded \"AND-ish\" effect that usually helps nDCG@10 without killing recall.)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards\n    + NEW \"IDF-mass satisfaction\".\n\n    Satisfaction intuition (doc-independent query prior, doc-dependent match):\n    - Some queries have many modifiers; matching the *most informative* subset is a strong\n      relevance signal for early ranks.\n    - Use a soft fraction of query IDF-mass that is matched:\n        sat = sum(matched idf^sat_power) / sum(query idf^sat_power)\n      then apply a small bounded multiplier (also gated by query specificity).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    sat_den = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n        sat_den += q_idf ** Config.sat_power\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    sat_num = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n        sat_num += term_idf ** Config.sat_power\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: satisfaction multiplier (bounded), gated by specificity.\n        if Config.sat_alpha > 0.0 and sat_den > 0.0:\n            sat = sat_num / (sat_den + eps)\n            score *= 1.0 + (Config.sat_alpha * spec_gate) * sat\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        idfs_arr0 = np.array(idfs, dtype=np.float64) if idfs else np.array([], dtype=np.float64)\n        q_idf_sum = float(np.sum(idfs_arr0)) if idfs else 0.0\n        q_idf_max = float(np.max(idfs_arr0)) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n            if Config.sat_alpha > 0.0 and idfs_arr0.size > 0:\n                sat_den = float(np.sum(np.power(idfs_arr0, Config.sat_power)))\n                if sat_den > 0.0:\n                    sat_num = float(np.sum(np.power(idfs_arr0, Config.sat_power)))  # presence-filtered below\n                    # presence-filter: keep only terms that match (any tf>0)\n                    # (we approximate with matched counts via presents, which is exact for presence)\n                    sat_num = 0.0\n                    for t_i in range(len(presents)):\n                        # presents[t_i] is per-doc; we need per-doc sat_num, so accumulate vector\n                        pass\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3200), Performs well on beir_nfcorpus_recall@100 (0.2553), Performs well on beir_nfcorpus_index_time_ms (3856.8961), Performs well on beir_nfcorpus_query_time_ms (111.5035), Performs well on bright_pony_ndcg@10 (0.1021), Performs well on bright_pony_recall@100 (0.2907), Performs well on bright_pony_index_time_ms (1558.0094), Performs well on bright_pony_query_time_ms (597.4185), Performs well on beir_scifact_ndcg@10 (0.6750), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5175.9376), Performs well on beir_scifact_query_time_ms (380.2010), Performs well on beir_arguana_ndcg@10 (0.2790), Performs well on beir_arguana_recall@100 (0.9115), Performs well on beir_arguana_index_time_ms (6410.2790), Performs well on beir_arguana_query_time_ms (29386.0143), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0404), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10786.8476), Performs well on bright_theoremqa_theorems_query_time_ms (1187.0875), Performs well on beir_scidocs_ndcg@10 (0.1432), Performs well on beir_scidocs_recall@100 (0.3430), Performs well on beir_scidocs_index_time_ms (19956.8390), Performs well on beir_scidocs_query_time_ms (3644.8884), Performs well on bright_economics_ndcg@10 (0.1410), Performs well on bright_economics_recall@100 (0.3909), Performs well on bright_economics_index_time_ms (21620.8496), Performs well on bright_economics_query_time_ms (3798.6068), Performs well on bright_biology_ndcg@10 (0.2433), Performs well on bright_biology_recall@100 (0.5454), Performs well on bright_biology_index_time_ms (23987.4105), Performs well on bright_biology_query_time_ms (3501.4967), Performs well on beir_fiqa_ndcg@10 (0.2260), Performs well on beir_fiqa_recall@100 (0.5253), Performs well on beir_fiqa_index_time_ms (46954.9682), Performs well on beir_fiqa_query_time_ms (6883.6242), Performs well on bright_earth_science_ndcg@10 (0.3514), Performs well on bright_earth_science_recall@100 (0.6648), Performs well on bright_earth_science_index_time_ms (46103.5097), Performs well on bright_earth_science_query_time_ms (6693.3751), Performs well on bright_stackoverflow_ndcg@10 (0.1727), Performs well on bright_stackoverflow_recall@100 (0.4930), Performs well on bright_stackoverflow_index_time_ms (109288.0844), Performs well on bright_stackoverflow_query_time_ms (11057.4444), Performs well on beir_trec-covid_ndcg@10 (0.6186), Performs well on beir_trec-covid_recall@100 (0.1047), Performs well on beir_trec-covid_index_time_ms (153711.9914), Performs well on beir_trec-covid_query_time_ms (1517.6864), Performs well on avg_ndcg@10 (0.2760), Performs well on avg_recall@100 (0.4675), Performs well on combined_score (0.4292), Performs well on total_index_time_ms (449411.6225), Performs well on total_query_time_ms (68759.3469), Performs well on total_time_ms (518170.9694), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 4 (Score: 0.4288)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\", not term count\n    cov_idf_power: float = 1.0\n\n    # \"Informativeness pivot\": normalize by an IDF-weighted document mass.\n    info_power: float = 0.65\n    info_mix: float = 0.25\n\n    # Soft penalty for ultra-common terms (\"glue\") instead of hard stopwording.\n    common_df_cut: float = 0.12   # df/N above this begins downweighting\n    common_penalty: float = 0.35  # maximum relative downweight at df=N\n\n    # Query-specificity gate for \"AND-ish\" rewards (coordination + focus prior).\n    # spec = max_idf/sum_idf; peaky queries should be less AND-like.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Distinctive pair synergy: super-additive reward for matching multiple informative terms.\n    # Bounded multiplicative factor; helps nDCG@10 while base additive evidence protects recall.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Entropy-based focus reward (doc prior), but it can overfit on short queries.\n    # Gate it by query length: long/noisy queries benefit more from a focus prior.\n    focus_entropy_alpha: float = 0.12\n    focus_entropy_power: float = 1.25\n    focus_query_len_gate: float = 3.5  # gate = 1 - exp(-|q|/gate)\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float, norm_entropy: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n        # Normalized entropy in [0,1] over the term distribution within the doc.\n        # 0 => highly concentrated (focused), 1 => uniform-ish (diffuse).\n        self.norm_entropy = norm_entropy\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length + a cached focus statistic.\"\"\"\n        tf = Counter(tokens)\n        L = float(len(tokens))\n        if L <= 0.0 or len(tf) <= 1:\n            ne = 0.0\n        else:\n            # H = -sum p log p, normalized by log(|V_d|)\n            H = 0.0\n            invL = 1.0 / L\n            for c in tf.values():\n                p = float(c) * invL\n                H -= p * math.log(p)\n            ne = float(H / max(math.log(float(len(tf))), Config.epsilon))\n            ne = min(max(ne, 0.0), 1.0)\n        return cls(term_frequencies=tf, length=L, norm_entropy=ne)\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    doc_norm_entropy: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with two normalizers + (gated) entropy-based focus prior.\n\n    Fixes vs current:\n    1) Add a df-only \"common term\" prior: ultra-common terms create many false positives.\n       We downweight them smoothly (never drop), which tends to improve nDCG@10.\n    2) Gate AND-ish effects (coordination + focus prior) by query specificity:\n       if a query is dominated by one very distinctive term, coordination/focus rewards\n       can over-promote generic multi-match docs; gating keeps recall while improving ranking.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # Informativeness pivot mass: sum tf * idf^info_power\n    info_mass = 0.0\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        tidf = float(max(base_t, 0.0) ** (Config.idf_power * Config.info_power))\n        info_mass += float(tfv) * tidf\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Cache matched-term components for pair synergy (small: only matched query terms).\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    # Query specificity gate (doc-independent).\n    common_thr = Config.common_df_cut * float(max(N, 1))\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        tidf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            tidf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += tidf\n        if tidf > q_idf_max:\n            q_idf_max = tidf\n    spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        # df-only common-term penalty (collection prior).\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        score += wq * term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Distinctive pair synergy (bounded multiplicative reward), gated by query specificity.\n    # Residualize vs max-idf so pairs matter mainly when *both* terms are fairly distinctive.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                # Gate by (tf evidence) so accidental single hits don't explode.\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    # Entropy focus reward:\n    # additionally gate by query length to avoid over-preferring repetitive \"focused\" docs\n    # on short already-precise queries (common in some BEIR tasks).\n    qn = float(len(query_repr.terms))\n    qlen_gate = 1.0 - math.exp(-qn / max(Config.focus_query_len_gate, eps))\n    focus = max(0.0, 1.0 - float(doc_norm_entropy))\n    score *= 1.0 + (Config.focus_entropy_alpha * spec_gate * qlen_gate) * (focus ** Config.focus_entropy_power)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        d.norm_entropy,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build lightweight doc repr once (TF only; positions removed).\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring used by rank(); matches retrieval_score (except entropy focus,\n        which is doc-global and applied as a final lightweight multiplier).\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Matched-term caches for pair synergy (vectorized per term; pair accumulation over small m).\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        # Proxy for info_norm: IDF-mass of matched terms (presence-weighted).\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        common_thr = Config.common_df_cut * float(max(self.corpus.N, 1))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): softly suppress ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            mass = float(idf_val ** Config.cov_idf_power)\n            cov_den += mass\n\n            wq = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wq * idf_val * tf_part\n\n            idfs.append(float(idf_val))\n            ws.append(float(wq))\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        # Distinctive pair synergy (vectorized over candidates; loops only over |q|^2).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        # Apply cached entropy focus reward, but gate by query length (matches retrieval_score()).\n        qlen_gate = 1.0 - math.exp(-qn / max(Config.focus_query_len_gate, eps))\n        ent = np.array([self.corpus.doc_repr[i].norm_entropy for i in candidate_docs], dtype=np.float64)\n        focus = np.maximum(0.0, 1.0 - ent)\n        scores *= 1.0 + (Config.focus_entropy_alpha * spec_gate * qlen_gate) * np.power(focus, Config.focus_entropy_power)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3182), Performs well on beir_nfcorpus_recall@100 (0.2551), Performs well on beir_nfcorpus_index_time_ms (4785.3834), Performs well on beir_nfcorpus_query_time_ms (170.2771), Performs well on bright_pony_ndcg@10 (0.1141), Performs well on bright_pony_recall@100 (0.3035), Performs well on bright_pony_index_time_ms (2091.7668), Performs well on bright_pony_query_time_ms (809.7659), Performs well on beir_scifact_ndcg@10 (0.6728), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (6169.2489), Performs well on beir_scifact_query_time_ms (471.0450), Performs well on beir_arguana_ndcg@10 (0.2737), Performs well on beir_arguana_recall@100 (0.9008), Performs well on beir_arguana_index_time_ms (6979.6276), Performs well on beir_arguana_query_time_ms (33088.6636), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0347), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11115.4245), Performs well on bright_theoremqa_theorems_query_time_ms (1398.3117), Performs well on beir_scidocs_ndcg@10 (0.1401), Performs well on beir_scidocs_recall@100 (0.3398), Performs well on beir_scidocs_index_time_ms (20121.5033), Performs well on beir_scidocs_query_time_ms (4908.7378), Performs well on bright_economics_ndcg@10 (0.1359), Performs well on bright_economics_recall@100 (0.3882), Performs well on bright_economics_index_time_ms (21408.1888), Performs well on bright_economics_query_time_ms (4693.0406), Performs well on bright_biology_ndcg@10 (0.2628), Performs well on bright_biology_recall@100 (0.5624), Performs well on bright_biology_index_time_ms (23411.3782), Performs well on bright_biology_query_time_ms (4316.5700), Performs well on bright_earth_science_ndcg@10 (0.3472), Performs well on bright_earth_science_recall@100 (0.6689), Performs well on bright_earth_science_index_time_ms (45799.3406), Performs well on bright_earth_science_query_time_ms (8743.3879), Performs well on beir_fiqa_ndcg@10 (0.2194), Performs well on beir_fiqa_recall@100 (0.5162), Performs well on beir_fiqa_index_time_ms (49157.7050), Performs well on beir_fiqa_query_time_ms (10229.5536), Performs well on bright_stackoverflow_ndcg@10 (0.1722), Performs well on bright_stackoverflow_recall@100 (0.4931), Performs well on bright_stackoverflow_index_time_ms (117703.1995), Performs well on bright_stackoverflow_query_time_ms (13355.1454), Performs well on beir_trec-covid_ndcg@10 (0.5753), Performs well on beir_trec-covid_recall@100 (0.1017), Performs well on beir_trec-covid_index_time_ms (163427.4311), Performs well on beir_trec-covid_query_time_ms (2127.6443), Performs well on avg_ndcg@10 (0.2722), Performs well on avg_recall@100 (0.4679), Performs well on combined_score (0.4288), Performs well on total_index_time_ms (472170.1975), Performs well on total_query_time_ms (84312.1427), Performs well on total_time_ms (556482.3403), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4286)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # NEW: \"elitist\" query backbone.\n    # Intuition: many queries have 1\u20133 intent-defining rare terms + several broad modifiers.\n    # We softly upweight the rare backbone terms to improve early precision (nDCG@10)\n    # while keeping additive evidence so recall@100 usually stays stable.\n    backbone_alpha: float = 0.18        # extra weight multiplier for backbone terms\n    backbone_k: float = 0.75            # choose top ~ backbone_k*sqrt(|q|) terms by IDF\n    backbone_idf_power: float = 1.15    # make backbone selection favor rarer terms\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Additive saturated evidence + bounded \"AND-ish\" rewards, plus a *query backbone*.\n\n    Backbone idea (novel-ish but still lexical): treat retrieval as a mixture:\n      - \"backbone\" terms: a few rare, intent-defining tokens\n      - \"context\" terms: broad modifiers that should not dominate early ranks\n    We implement this by softly upweighting the top ~sqrt(|q|) terms by powered-IDF.\n    This usually helps nDCG@10 on noisy/multi-facet queries without harming recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Precompute query-term IDFs once (also used for specificity + backbone selection).\n    q_terms = query_repr.terms\n    qn = float(len(q_terms))\n\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    q_idf_map: dict[str, float] = {}\n    for term in q_terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        tidf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            tidf *= (1.0 - Config.common_penalty * frac)\n        q_idf_map[term] = tidf\n        q_idf_sum += tidf\n        if tidf > q_idf_max:\n            q_idf_max = tidf\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    # Choose backbone terms (doc-independent) by powered IDF.\n    if q_terms:\n        bb_n = int(max(1.0, round(Config.backbone_k * math.sqrt(float(len(q_terms))))))\n        bb_n = min(bb_n, len(q_terms))\n        backbone = set(\n            t\n            for t, _ in sorted(\n                q_idf_map.items(),\n                key=lambda kv: kv[1] ** Config.backbone_idf_power,\n                reverse=True,\n            )[:bb_n]\n        )\n    else:\n        backbone = set()\n\n    score = 0.0\n    matched = 0.0\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    for term in q_terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        term_idf = float(q_idf_map.get(term, 0.0))\n        if term_idf <= 0.0:\n            continue\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        if term in backbone:\n            wq *= (1.0 + Config.backbone_alpha)\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        # Backbone selection uses the same idf stream as scoring.\n        idf_stream: list[float] = []\n        for term_id in query_term_ids:\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                idf_stream.append(0.0)\n                continue\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n            idf_stream.append(float(idf_val))\n\n        if idf_stream:\n            bb_n = int(max(1.0, round(Config.backbone_k * math.sqrt(float(len(idf_stream))))))\n            bb_n = min(bb_n, len(idf_stream))\n            bb_idx = set(\n                np.argsort(-(np.power(np.array(idf_stream, dtype=np.float64), Config.backbone_idf_power)))[:bb_n].tolist()\n            )\n        else:\n            bb_idx = set()\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(idf_stream[i])\n            if idf_val <= 0.0:\n                continue\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            if i in bb_idx:\n                w *= (1.0 + Config.backbone_alpha)\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D2 (Score: 0.4286)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # IDF-mass coverage: coordination over information, not term count\n    cov_idf_power: float = 1.0\n\n    # \"Informativeness pivot\" normalization:\n    # penalize docs that are long mainly due to common terms; keep info-dense long docs.\n    info_power: float = 0.6\n    info_mix: float = 0.22\n\n    # Smooth collection prior: downweight ultra-common \"glue\" terms (df-only; no training).\n    common_df_cut: float = 0.12   # start downweighting when df/N exceeds this\n    common_penalty: float = 0.35  # max relative downweight as df->N\n\n    # Gate AND-like rewards by query specificity (peaky queries should be less AND-ish).\n    # spec = max_idf/sum_idf; higher means dominated by one rare term.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    # Bounded distinctive-pair synergy: reward co-occurrence of multiple informative terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Lightweight doc repr: TF + length (rank-compatible, fast to build).\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence = saturated TF-IDF under a dual normalization (length + informativeness),\n    plus bounded AND-like rewards that are *query-aware*.\n\n    Additions (all lexical, no training):\n    - common-term prior: smoothly downweight ultra-common df terms (\"glue\").\n    - query specificity gate: reduce AND-like multipliers for peaky queries.\n    - distinctive pair synergy: bounded reward for co-matching informative terms.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Focused length (token length blended with unique-term length)\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # IDF-weighted document information mass (computed on the fly)\n    info_mass = 0.0\n    ip = Config.idf_power * Config.info_power\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        if base_t <= 0.0:\n            continue\n        info_mass += float(tfv) * (base_t ** ip)\n\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Cache matched-term components for pair synergy (only over matched query terms).\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(max(N, 1))\n\n    # Query specificity gate (doc-independent) computed under the same collection prior.\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        tidf = float(max(base, 0.0) ** Config.idf_power)\n        if tidf <= 0.0:\n            continue\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            tidf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += tidf\n        if tidf > q_idf_max:\n            q_idf_max = tidf\n    spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        # Smoothly downweight ultra-common terms (keeps recall better than dropping).\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        score += wq * term_idf * tf_part\n\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Distinctive pair synergy (bounded multiplicative reward), gated by query specificity.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf)\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n        pair = 0.0\n        for i in range(m):\n            if r[i] <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                if r[j] <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (r[i] * r[j]) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build richer doc repr once (tf + positions) for proximity scoring in score().\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        # Proxy for info_mass: accumulate matched IDF-mass (presence-weighted)\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For query specificity gate + pair synergy\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(max(self.corpus.N, 1))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): smoothly downweight ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            mass = float(idf_val ** Config.cov_idf_power)\n            cov_den += mass\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(float(idf_val))\n            ws.append(float(w))\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (doc-independent; uses the same weighted idf stream).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if cov_den > 0.0 and qn > 1.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive pair synergy (vectorized over candidates).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D3 (Score: 0.4283)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    # Improves nDCG@10 on noisy corpora while keeping recall.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + bounded coordination reward,\n    plus two precision-oriented refinements:\n\n    (1) common-term soft penalty: very high-df terms are often \"glue\" and add noisy matches.\n    (2) light co-occurrence synergy: super-additive reward when multiple distinctive\n        query terms are present (helps nDCG@10) while base additive evidence protects recall.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    # Collect matched-term components for synergy (doc-local, typically small).\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        # Softly downweight ultra-common terms (keeps recall better than hard removal).\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    qn = float(len(query_repr.terms))\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs by residualizing vs max idf.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + Config.pair_boost * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy.\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + Config.pair_boost * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n### Program D4 (Score: 0.4285)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # NEW: \"information satisfaction\" reward.\n    # Intuition: early precision improves when we reward covering the query's *IDF mass*,\n    # not just counting matched terms. This is a soft, bounded multiplier.\n    sat_alpha: float = 0.18\n    sat_power: float = 1.6\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    # (A bounded \"AND-ish\" effect that usually helps nDCG@10 without killing recall.)\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects (coordination + pairs).\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length.\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Robust lexical relevance = additive saturated evidence + bounded \"AND-ish\" rewards\n    + NEW \"IDF-mass satisfaction\".\n\n    Satisfaction intuition (doc-independent query prior, doc-dependent match):\n    - Some queries have many modifiers; matching the *most informative* subset is a strong\n      relevance signal for early ranks.\n    - Use a soft fraction of query IDF-mass that is matched:\n        sat = sum(matched idf^sat_power) / sum(query idf^sat_power)\n      then apply a small bounded multiplier (also gated by query specificity).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    sat_den = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n        sat_den += q_idf ** Config.sat_power\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    sat_num = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n        sat_num += term_idf ** Config.sat_power\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # NEW: satisfaction multiplier (bounded), gated by specificity.\n        if Config.sat_alpha > 0.0 and sat_den > 0.0:\n            sat = sat_num / (sat_den + eps)\n            score *= 1.0 + (Config.sat_alpha * spec_gate) * sat\n\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(\n        q,\n        d.term_frequencies,\n        d.length,\n        corpus.N,\n        corpus.avgdl,\n        corpus.document_frequency,\n    )\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build doc representations once (TF + bigram set). This also lets score() stay fast.\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = self._doc_tf_dicts[doc_idx]\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score().\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        qn = float(len(query_term_ids))\n        idfs_arr0 = np.array(idfs, dtype=np.float64) if idfs else np.array([], dtype=np.float64)\n        q_idf_sum = float(np.sum(idfs_arr0)) if idfs else 0.0\n        q_idf_max = float(np.max(idfs_arr0)) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # Mirror retrieval_score(): per-doc IDF-mass satisfaction multiplier.\n            # sat_num(doc) = sum_{t matched} idf(t)^sat_power\n            # sat = sat_num / sat_den  in [0,1]\n            if Config.sat_alpha > 0.0 and idfs_arr0.size > 0:\n                idf_pow = np.power(idfs_arr0, Config.sat_power)\n                sat_den = float(np.sum(idf_pow))\n                if sat_den > 0.0 and presents:\n                    sat_num_vec = np.zeros(len(candidate_docs), dtype=np.float64)\n                    # Each presents[t_i] is {0,1} per doc for query term i in the scored stream.\n                    for t_i, pres in enumerate(presents):\n                        if idf_pow[t_i] > 0.0:\n                            sat_num_vec += pres * float(idf_pow[t_i])\n                    scores *= 1.0 + (Config.sat_alpha * spec_gate) * (sat_num_vec / (sat_den + eps))\n\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_terms = list(term_counts.keys())\n\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                query_term_weights.append((1.0 + math.log(float(count))) if Config.use_log_qtf else float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        # No query bigram features (keeps ranking stable and cheap).\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Alternative approach to beir_nfcorpus_ndcg@10, Alternative approach to beir_nfcorpus_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4268, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\", not term count\n    cov_idf_power: float = 1.0\n\n    # \"Informativeness pivot\": normalize by an IDF-weighted document mass.\n    info_power: float = 0.65\n    info_mix: float = 0.25\n\n    # Residual-IDF gating (RSJ-derivative)\n    use_residual_idf: bool = True\n    ridf_gamma: float = 0.55\n    ridf_p_power: float = 1.25\n    ridf_lambda_scale: float = 0.9\n\n    # NEW: Softly penalize ultra-common terms (glue) instead of relying on RIDF alone.\n    # Rationale: RIDF is doc-length dependent; a df-only penalty is a stable\n    # \"collection prior\" that improves early precision on noisy corpora.\n    common_df_cut: float = 0.12   # df/N above this begins downweighting\n    common_penalty: float = 0.35  # maximum relative downweight at df=N\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length (positions removed to reduce cost).\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with two normalizers + a *residual evidence* term weight.\n\n    Upgrade: add a df-only \"common-term prior\" downweight, which is orthogonal to RIDF.\n    Intuition: many false positives are driven by glue terms that RIDF may not fully\n    suppress (esp. when doc_length is moderate). A smooth penalty improves nDCG@10\n    while preserving recall because rare terms remain unchanged.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # Informativeness pivot mass: sum tf * idf^info_power\n    info_mass = 0.0\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        tidf = float(max(base_t, 0.0) ** (Config.idf_power * Config.info_power))\n        info_mass += float(tfv) * tidf\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    common_thr = Config.common_df_cut * float(max(N, 1))\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        base = max(base, 0.0)\n\n        # Sharpen base idf (as before)\n        term_idf = float(base ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        # Smoothly downweight ultra-common terms (collection prior; doc-independent).\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        # Residual-IDF: evidence above chance, but *softened* to avoid recall loss.\n        if Config.use_residual_idf and doc_length > 0:\n            p = min(max(df / max(float(N), 1.0), eps), 1.0 - eps)\n            log_p_present = math.log1p(-math.exp(doc_length * math.log1p(-p)))\n            ridf_raw = max(term_idf + log_p_present, 0.0)\n\n            lam = doc_length * p\n            lam_gate = 1.0 / (1.0 + Config.ridf_lambda_scale * lam)\n            p_gate = p ** Config.ridf_p_power\n\n            ridf = (1.0 - p_gate) * term_idf + p_gate * ((1.0 - lam_gate) * term_idf + lam_gate * ridf_raw)\n            term_w = (1.0 - Config.ridf_gamma) * term_idf + Config.ridf_gamma * ridf\n        else:\n            term_w = term_idf\n\n        mass = (term_w if term_w > 0 else term_idf) ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_w * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build lightweight doc repr once (TF only; positions removed).\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring used by rank(); matches retrieval_score including RIDF.\n\n        For RIDF we need per-candidate doc length and df/N. We apply RIDF only as a\n        per-(term,doc) weight; info-pivot remains the matched-mass approximation.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        dlen = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        N = max(float(self.corpus.N), 1.0)\n\n        common_thr = Config.common_df_cut * float(max(self.corpus.N, 1))\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n\n            # Mirror retrieval_score(): smooth downweighting of ultra-common terms.\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            # RIDF term weight per candidate doc (vectorized) with the same softening\n            # as retrieval_score to keep rank() aligned with score().\n            if Config.use_residual_idf:\n                p = min(max(df / N, eps), 1.0 - eps)\n                log1m_p = math.log1p(-p)\n                log_p_present = np.log1p(-np.exp(dlen * log1m_p))\n                ridf_raw = np.maximum(idf_val + log_p_present, 0.0)\n\n                lam = dlen * p\n                lam_gate = 1.0 / (1.0 + Config.ridf_lambda_scale * lam)\n                p_gate = p ** Config.ridf_p_power\n\n                ridf = (1.0 - p_gate) * idf_val + p_gate * ((1.0 - lam_gate) * idf_val + lam_gate * ridf_raw)\n                term_w = (1.0 - Config.ridf_gamma) * idf_val + Config.ridf_gamma * ridf\n            else:\n                term_w = idf_val\n\n            # coverage uses the same weight notion\n            mass = float(idf_val ** Config.cov_idf_power)\n            cov_den += mass\n\n            wq = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wq * term_w * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\", not term count\n    cov_idf_power: float = 1.0\n\n    # \"Informativeness pivot\": normalize by an IDF-weighted document mass.\n    info_power: float = 0.65\n    info_mix: float = 0.25\n\n    # Residual-IDF gating (RSJ-derivative)\n    use_residual_idf: bool = True\n    ridf_gamma: float = 0.55\n    ridf_p_power: float = 1.25\n    ridf_lambda_scale: float = 0.9\n\n    # NEW: df-only \"collection prior\" against ultra-common terms.\n    # Intuition: many false positives are driven by glue terms; RIDF is doc-length dependent,\n    # so add a stable df-only downweight that improves nDCG@10 without killing recall.\n    common_df_cut: float = 0.12   # start downweighting when df/N exceeds this\n    common_penalty: float = 0.35  # maximum relative downweight as df->N\n\n    # NEW: gate RIDF by query specificity to avoid over-correcting peaky queries\n    # (one very distinctive term + broad modifiers). This tends to protect recall@100.\n    spec_floor: float = 0.55\n    spec_power: float = 1.2\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length (positions removed to reduce cost).\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with two normalizers + residual evidence (RIDF),\n    plus a df-only collection prior and a query-specificity gate.\n\n    Why these changes:\n    - RIDF is doc-length dependent and can be too harsh on long documents, hurting recall.\n    - A df-only common-term penalty is a stable *collection prior* that suppresses glue terms.\n    - Query-specificity gating reduces \"over-correction\" for peaky queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # Informativeness pivot mass: sum tf * idf^info_power\n    info_mass = 0.0\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        tidf = float(max(base_t, 0.0) ** (Config.idf_power * Config.info_power))\n        info_mass += float(tfv) * tidf\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Query specificity gate computed from the same df-weighting used in scoring.\n    common_thr = Config.common_df_cut * float(max(N, 1))\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        tidf = float(max(base, 0.0) ** Config.idf_power)\n        if tidf <= 0.0:\n            continue\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            tidf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += tidf\n        q_idf_max = max(q_idf_max, tidf)\n\n    spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n    # higher spec (peakier) => smaller gate\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        base = max(base, 0.0)\n\n        term_idf = float(base ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        # df-only common-term downweight (collection prior)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        # RIDF, but gated by query specificity (protect peaky-query recall)\n        if Config.use_residual_idf and doc_length > 0:\n            p = min(max(df / max(float(N), 1.0), eps), 1.0 - eps)\n            log_p_present = math.log1p(-math.exp(doc_length * math.log1p(-p)))\n            ridf_raw = max(term_idf + log_p_present, 0.0)\n\n            lam = doc_length * p\n            lam_gate = 1.0 / (1.0 + Config.ridf_lambda_scale * lam)\n            p_gate = p ** Config.ridf_p_power\n\n            ridf = (1.0 - p_gate) * term_idf + p_gate * ((1.0 - lam_gate) * term_idf + lam_gate * ridf_raw)\n            term_w = (1.0 - Config.ridf_gamma * spec_gate) * term_idf + (Config.ridf_gamma * spec_gate) * ridf\n        else:\n            term_w = term_idf\n\n        mass = (term_w if term_w > 0 else term_idf) ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_w * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build lightweight doc repr once (TF only; positions removed).\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring used by rank(); matches retrieval_score including RIDF.\n\n        For RIDF we need per-candidate doc length and df/N. We apply RIDF only as a\n        per-(term,doc) weight; info-pivot remains the matched-mass approximation.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        dlen = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        N = max(float(self.corpus.N), 1.0)\n\n        # Query specificity gate (must mirror retrieval_score()).\n        common_thr = Config.common_df_cut * float(max(self.corpus.N, 1))\n        q_idf_sum = 0.0\n        q_idf_max = 0.0\n        for term_id in query_term_ids:\n            idf_q = float(self.corpus.idf_array[term_id])\n            if idf_q <= 0.0:\n                continue\n            df_q = float(self.corpus._df[term_id])\n            if df_q >= common_thr:\n                frac = min(1.0, (df_q - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_q *= (1.0 - Config.common_penalty * frac)\n            q_idf_sum += idf_q\n            q_idf_max = max(q_idf_max, idf_q)\n\n        spec = (q_idf_max / (q_idf_sum + eps)) if q_idf_sum > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            df = float(self.corpus._df[term_id])\n\n            # df-only common-term prior (mirror retrieval_score()).\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            # RIDF term weight per candidate doc, gated by query specificity (mirror retrieval_score()).\n            if Config.use_residual_idf:\n                p = min(max(df / N, eps), 1.0 - eps)\n                log1m_p = math.log1p(-p)\n                log_p_present = np.log1p(-np.exp(dlen * log1m_p))\n                ridf_raw = np.maximum(idf_val + log_p_present, 0.0)\n\n                lam = dlen * p\n                lam_gate = 1.0 / (1.0 + Config.ridf_lambda_scale * lam)\n                p_gate = p ** Config.ridf_p_power\n\n                ridf = (1.0 - p_gate) * idf_val + p_gate * ((1.0 - lam_gate) * idf_val + lam_gate * ridf_raw)\n                g = Config.ridf_gamma * spec_gate\n                term_w = (1.0 - g) * idf_val + g * ridf\n            else:\n                term_w = idf_val\n\n            # coverage uses the same weight notion\n            mass = float(term_w ** Config.cov_idf_power)\n            cov_den += mass\n\n            wq = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wq * term_w * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.4266, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # IDF-weighted coverage (coordination over \"information mass\", not term count)\n    cov_idf_power: float = 1.0\n\n    # NEW: IDF-mass \"satisfaction\" gate.\n    # Intuition: queries often contain a few highly discriminative terms; matching those\n    # should be rewarded more than matching many weak terms.\n    # We compute a soft fraction of the query's IDF-mass that is matched, and boost it.\n    sat_alpha: float = 0.22     # strength of satisfaction reward\n    sat_power: float = 1.6      # emphasize covering the top-IDF chunk\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # NEW: drop ultra-common query terms (df/N above threshold) as \"near-stopwords\".\n    # This is a lexical, collection-statistics filter; it mainly helps early precision.\n    drop_query_df_ratio: float = 0.0  # 0 disables; typical useful values: 0.15-0.30\n\n    # \"Informativeness pivot\" normalization:\n    # normalize TF saturation by an IDF-weighted document mass so documents\n    # that are long due to common/low-information terms don't dominate.\n    info_power: float = 0.6   # how strongly rare terms contribute to \"mass\"\n    info_mix: float = 0.22    # blend between length-norm and info-norm\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Additionally: optionally drop ultra-common tokens (collection-statistics\n        near-stopwords). This reduces false positives and tends to improve nDCG@10.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + IDF-mass coverage reward\n    + NEW \"informativeness pivot\" normalization.\n\n    Informativeness pivot intuition:\n    - Two docs can have same length, but one spends its mass on generic words while\n      the other contains many rare/discriminative terms.\n    - We softly normalize TF saturation by an IDF-weighted doc mass so \"common-term\n      verbosity\" is discounted without harshly penalizing long but information-dense docs.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Focused length (token length blended with unique-term length)\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # IDF-weighted document \"information mass\" (computed on the fly; no extra storage)\n    info_mass = 0.0\n    ip = Config.idf_power * Config.info_power\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        if base_t <= 0.0:\n            continue\n        info_mass += float(tfv) * (base_t ** ip)\n\n    # Pivot against avgdl (monotone pivot; doesn't need exact expectation)\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n\n    # Blend norms: when info_mix=0 => pure length norm\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # IDF-mass satisfaction: emphasize matching the informative \"core\" of the query.\n    sat_num = 0.0\n    sat_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n\n        # Optional near-stopword filter (query-only): drop extremely common terms.\n        if Config.drop_query_df_ratio > 0.0 and N > 0:\n            if (df / float(N)) >= Config.drop_query_df_ratio:\n                continue\n\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        # Satisfaction mass is sharper than coverage mass to focus on the key terms.\n        sat_mass = mass ** Config.sat_power\n        sat_den += sat_mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n        sat_num += sat_mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # Classic (soft) coordination on IDF-mass coverage\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # New: satisfaction reward (bounded, multiplicative).\n    # If a doc matches the most informative terms, sat approaches 1 quickly.\n    if sat_den > 0.0 and Config.sat_alpha > 0.0 and len(query_repr.terms) > 1:\n        sat = sat_num / (sat_den + eps)\n        score *= 1.0 + Config.sat_alpha * sat\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring for rank(); mirrors retrieval_score.\n\n        Full info_mass would require iterating all doc terms; instead we use a cheap\n        proxy: matched IDF-mass (presence-weighted). It still suppresses \"common-term\n        verbosity\" while keeping rank() fast.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            mass = float(idf_val ** Config.cov_idf_power)\n            cov_den += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            cov_num += present * mass\n\n            # Proxy for document info_mass: only the matched terms contribute (presence)\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 4 (Score: 0.4217, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\"\n    cov_idf_power: float = 1.0\n\n    # \"Informativeness pivot\" normalization:\n    # normalize TF saturation by an IDF-weighted document mass so common-term verbosity\n    # is discounted without over-penalizing long but information-dense docs.\n    info_power: float = 0.6\n    info_mix: float = 0.22\n\n    # Cheap phrase/proximity reward. Only uses adjacent query term pairs.\n    prox_window: int = 8\n    prox_alpha: float = 0.08\n\n    # Query-dependent gating for proximity.\n    prox_gate_alpha: float = 0.7\n    prox_gate_floor: float = 0.35\n\n    # NEW: \"query backbone\" weighting.\n    # Intuition: many queries have a small set of highly discriminative terms that define intent.\n    # We softly upweight those terms (not a hard filter) which tends to improve nDCG@10 while\n    # keeping recall@100 stable.\n    backbone_alpha: float = 0.35   # strength of upweighting\n    backbone_k: float = 0.55       # controls how many terms count as backbone: k*sqrt(|q|)\n    backbone_idf_power: float = 1.2  # makes backbone prefer rarer terms\n\n    # NEW: bounded \"AND-synergy\" when multiple query terms co-occur.\n    # This is not BM25's coord; it rewards joint satisfaction of multiple constraints.\n    pair_alpha: float = 0.04  # keep small to avoid recall loss\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float, positions: dict[str, list[int]]):\n        self.term_frequencies = term_frequencies\n        self.length = length\n        self.positions = positions  # for lightweight proximity/phrase evidence\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Store positions for proximity reward (kept minimal: only per-term position lists).\"\"\"\n        tf = Counter(tokens)\n        pos: dict[str, list[int]] = {}\n        for i, t in enumerate(tokens):\n            pos.setdefault(t, []).append(i)\n        return cls(term_frequencies=tf, length=float(len(tokens)), positions=pos)\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Additionally apply a *query-backbone* reweighting:\n        - pick the top ~k*sqrt(|q|) terms by a rarity proxy (token length / IDF unknown here),\n          but compute the true rarity inside retrieval_score where df is known.\n        Here we only produce baseline qtf weights; backbone is applied in retrieval_score/rank.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    doc_pos: dict[str, list[int]] | None = None,\n) -> float:\n    \"\"\"\n    Evidence accumulation with:\n      (1) focus+informativeness normalization (as before),\n      (2) IDF-mass coverage reward (as before),\n      (3) NEW query-backbone reweighting (upweights the most discriminative query terms),\n      (4) NEW small pairwise \"AND-synergy\" for multi-term satisfaction,\n      (5) bounded proximity reward gated by query specificity (as before).\n\n    Backbone intuition:\n      Retrieval is often decided by 1\u20133 intent-defining terms; treating all query terms\n      equally makes early ranks drift toward generic matches. We therefore upweight a\n      small backbone set chosen by powered IDF.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    info_mass = 0.0\n    ip = Config.idf_power * Config.info_power\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        if base_t > 0.0:\n            info_mass += float(tfv) * (base_t ** ip)\n\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    # Precompute powered-IDF per query term to:\n    #  - gate proximity (specificity),\n    #  - compute coverage,\n    #  - select backbone terms.\n    term_idf_map: dict[str, float] = {}\n    q_idf_sum = 0.0\n    q_idf_n = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        tidf = float(max(base, 0.0) ** Config.idf_power)\n        if tidf > 0.0:\n            term_idf_map[term] = tidf\n            q_idf_sum += tidf\n            q_idf_n += 1.0\n\n    if not term_idf_map:\n        return 0.0\n\n    # Backbone selection: top ~k*sqrt(|q|) by (idf^backbone_idf_power).\n    qn = max(1.0, float(len(term_idf_map)))\n    bb_n = int(max(1.0, round(Config.backbone_k * math.sqrt(qn))))\n    backbone = set(\n        t for t, _ in sorted(\n            term_idf_map.items(),\n            key=lambda kv: kv[1] ** Config.backbone_idf_power,\n            reverse=True,\n        )[:bb_n]\n    )\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n    matched_terms: list[str] = []\n\n    for term in query_repr.terms:\n        term_idf = float(term_idf_map.get(term, 0.0))\n        if term_idf <= 0.0:\n            continue\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += mass\n        matched_terms.append(term)\n\n        # Backbone reweighting is multiplicative on query TF weight (keeps lexical nature).\n        wq = float(query_repr.term_weights.get(term, 1.0))\n        if term in backbone:\n            wq *= (1.0 + Config.backbone_alpha)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += wq * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # Pairwise AND-synergy: if multiple query terms match, reward joint constraint satisfaction.\n    # Use backbone-weighted average IDF for stability; keep bounded and small.\n    if Config.pair_alpha > 0.0 and len(matched_terms) >= 2:\n        m = float(len(matched_terms))\n        idf_avg = 0.0\n        for t in matched_terms:\n            idf_avg += term_idf_map.get(t, 0.0)\n        idf_avg /= (m + eps)\n        # grows like number of satisfied pairs, but damped\n        pair_gain = (m * (m - 1.0) / 2.0) / (1.0 + m)\n        score *= 1.0 + Config.pair_alpha * pair_gain * (idf_avg / (idf_avg + 1.0))\n\n    # Proximity reward (bounded) gated by specificity (as before).\n    if doc_pos is not None and len(query_repr.terms) > 1 and Config.prox_alpha > 0.0:\n        q_spec = (q_idf_sum / (q_idf_n + eps)) if q_idf_n > 0.0 else 0.0\n        q_gate = q_spec / (q_spec + 1.0)\n        prox_scale = Config.prox_gate_floor + Config.prox_gate_alpha * q_gate\n\n        hits = 0.0\n        pairs = 0.0\n        w = int(max(1, Config.prox_window))\n        for a, bterm in zip(query_repr.terms, query_repr.terms[1:]):\n            pa = doc_pos.get(a)\n            pb = doc_pos.get(bterm)\n            pairs += 1.0\n            if not pa or not pb:\n                continue\n            i = j = 0\n            best = 1_000_000\n            while i < len(pa) and j < len(pb):\n                da = pa[i] - pb[j]\n                ad = da if da >= 0 else -da\n                if ad < best:\n                    best = ad\n                    if best == 0:\n                        break\n                if da < 0:\n                    i += 1\n                else:\n                    j += 1\n            if best <= w:\n                hits += 1.0\n        if pairs > 0.0 and hits > 0.0:\n            score *= 1.0 + (Config.prox_alpha * prox_scale) * (hits / pairs)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency, d.positions)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build richer doc repr once (tf + positions) for proximity scoring in score().\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized core score for rank(); mirrors retrieval_score except proximity.\n\n        Added here too:\n          - query-backbone upweighting (computed from query term IDFs),\n          - small pairwise AND-synergy based on number of matched query terms.\n\n        Keep everything bounded to preserve recall@100.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_cnt = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf_sum = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Backbone: select top ~k*sqrt(|q|) terms by idf^p\n        idf_vals = np.array([float(self.corpus.idf_array[tid]) for tid in query_term_ids], dtype=np.float64)\n        if len(idf_vals) > 0:\n            bb_n = int(max(1.0, round(Config.backbone_k * math.sqrt(float(len(idf_vals))))))\n            bb_n = min(bb_n, len(idf_vals))\n            bb_idx = set(np.argsort(-(idf_vals ** Config.backbone_idf_power))[:bb_n].tolist())\n        else:\n            bb_idx = set()\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            mass = float(idf_val ** Config.cov_idf_power)\n            cov_den += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            if i in bb_idx:\n                w *= (1.0 + Config.backbone_alpha)\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            matched_cnt += present\n            matched_idf_sum += present * idf_val\n\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        # Pairwise AND-synergy (vectorized):\n        if Config.pair_alpha > 0.0:\n            m = matched_cnt\n            idf_avg = matched_idf_sum / (m + eps)\n            pair_gain = (m * (m - 1.0) / 2.0) / (1.0 + m)\n            scores *= 1.0 + Config.pair_alpha * pair_gain * (idf_avg / (idf_avg + 1.0))\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # NEW: query specificity gating for \"AND-like\" effects.\n    # Intuition: when a query is dominated by one highly distinctive term + many vague terms,\n    # coordination/pairs can over-reward noisy matches. Gate those effects by specificity.\n    # spec = max_idf / sum_idf in [~1/|q|, 1]. Higher => more \"peaky\" query.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Same base signals as before, but with a *query-specificity gate* on the\n    \"AND-like\" effects (coordination + pair synergy).\n\n    Why this can help:\n    - Many BEIR/BRIGHT queries contain one sharp entity/term + several broad modifiers.\n      Coordination/pair boosts can then over-promote generic docs that match many broad terms.\n    - Use a simple, doc-independent specificity proxy:\n        spec = max_idf / sum_idf  (peaky queries -> larger spec)\n      We down-gate coordination/synergy for peaky queries, keeping recall from the\n      main additive evidence but improving nDCG@10 by reducing noisy \"multi-match\" wins.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Effective length mixes verbosity (tokens) with lexical breadth (unique terms).\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    matched = 0.0\n\n    m_idf: list[float] = []\n    m_w: list[float] = []\n    m_tfpart: list[float] = []\n\n    common_thr = Config.common_df_cut * float(N)\n\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n    if score <= 0.0:\n        return 0.0\n\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n    # Co-occurrence synergy: emphasize distinctive pairs, but gate by query specificity too.\n    m = len(m_idf)\n    if m >= 2 and Config.pair_boost > 0.0:\n        max_idf = max(m_idf) if m_idf else 0.0\n        r = [max(0.0, v - 0.5 * max_idf) for v in m_idf]\n\n        pair = 0.0\n        for i in range(m):\n            ri = r[i]\n            if ri <= 0.0:\n                continue\n            for j in range(i + 1, m):\n                rj = r[j]\n                if rj <= 0.0:\n                    continue\n                gate = (m_tfpart[i] * m_tfpart[j]) ** 0.5\n                pair += (ri * rj) ** Config.pair_power * (m_w[i] * m_w[j]) ** 0.5 * gate\n\n        score *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        idfs: list[float] = []\n        ws: list[float] = []\n        tfparts: list[NDArray[np.float64]] = []\n        presents: list[NDArray[np.float64]] = []\n\n        common_thr = Config.common_df_cut * float(self.corpus.N)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # Mirror retrieval_score(): soften ultra-common terms.\n            df = float(self.corpus._df[term_id])\n            if df >= common_thr:\n                frac = min(1.0, (df - common_thr) / (float(self.corpus.N) - common_thr + eps))\n                idf_val *= (1.0 - Config.common_penalty * frac)\n\n            w = float(query_term_weights[i] if query_term_weights is not None else 1.0)\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n            idfs.append(idf_val)\n            ws.append(w)\n            tfparts.append(tf_part)\n            presents.append(present)\n\n        # Query specificity gate (must match retrieval_score()).\n        qn = float(len(query_term_ids))\n        q_idf_sum = float(np.sum(idfs)) if idfs else 0.0\n        q_idf_max = float(np.max(np.array(idfs, dtype=np.float64))) if idfs else 0.0\n        spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n        spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # Mirror retrieval_score(): distinctive-pair synergy (also gated).\n        m = len(idfs)\n        if m >= 2 and Config.pair_boost > 0.0:\n            idfs_arr = np.array(idfs, dtype=np.float64)\n            max_idf = float(np.max(idfs_arr))\n            r = np.maximum(0.0, idfs_arr - 0.5 * max_idf)\n\n            pair = np.zeros(len(candidate_docs), dtype=np.float64)\n            for i in range(m):\n                if r[i] <= 0.0:\n                    continue\n                for j in range(i + 1, m):\n                    if r[j] <= 0.0:\n                        continue\n                    gate = np.sqrt(tfparts[i] * tfparts[j])\n                    pair += (r[i] * r[j]) ** Config.pair_power * math.sqrt(ws[i] * ws[j]) * gate * (presents[i] * presents[j])\n\n            scores *= (1.0 + (Config.pair_boost * spec_gate) * pair)\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Below are **two targeted changes** that introduce a new, principled signal while keeping the code relatively small.\n\n### Rationale (brief but fundamental)\n1. **IDF-mass satisfaction (coverage over information, not term count)**:  \n   A doc that matches the *most discriminative* subset of query terms is often more relevant than one matching many weak terms. We compute a bounded fraction of the query\u2019s IDF-mass that is matched and apply a small multiplier. This tends to help **nDCG@10** without hurting **recall@100** because the base additive evidence remains unchanged.\n\n2. **Vectorized parity**:  \n   We mirror the same satisfaction multiplier in `_score_candidates_vectorized` so `rank()` stays correct.\n\n---\n\n## Change 1: Add satisfaction parameters to `Config`\n<<<<<<< SEARCH\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # NEW: query specificity gating for \"AND-like\" effects.\n    # Intuition: when a query is dominated by one highly distinctive term + many vague terms,\n    # coordination/pairs can over-reward noisy matches. Gate those effects by specificity.\n    # spec = max_idf / sum_idf in [~1/|q|, 1]. Higher => more \"peaky\" query.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n=======\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Soft penalty for ultra-common terms (glue words) instead of hard stopwording.\n    common_df_cut: float = 0.12   # fraction of corpus considered \"common\"\n    common_penalty: float = 0.35  # max downweight for very common terms\n\n    # NEW: \"IDF-mass satisfaction\" reward.\n    # Intuition: early precision improves when we reward covering the query's informative mass,\n    # not just counting matched terms. This is bounded and query-dependent.\n    sat_alpha: float = 0.16\n    sat_power: float = 1.6\n\n    # Light co-occurrence synergy: reward multi-term matches with distinctive terms.\n    pair_boost: float = 0.06\n    pair_power: float = 1.0\n\n    # Query specificity gating for \"AND-like\" effects.\n    spec_floor: float = 0.55\n    spec_power: float = 1.20\n\n    epsilon: float = 1e-9\n>>>>>>> REPLACE\n\n---\n\n## Change 2: Implement satisfaction multiplier in `retrieval_score` (gated by specificity)\n<<<<<<< SEARCH\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n=======\n    # Query specificity gate (doc-independent; uses same idf + common penalty).\n    # Also compute satisfaction denominator: total \"informative mass\" in the query.\n    q_idf_sum = 0.0\n    q_idf_max = 0.0\n    sat_den = 0.0\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        q_idf = float(max(base, 0.0) ** Config.idf_power)\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            q_idf *= (1.0 - Config.common_penalty * frac)\n        q_idf_sum += q_idf\n        if q_idf > q_idf_max:\n            q_idf_max = q_idf\n        sat_den += q_idf ** Config.sat_power\n\n    qn = float(len(query_repr.terms))\n    spec = (q_idf_max / (q_idf_sum + eps)) if qn > 0.0 else 0.0\n    # Higher spec (peakier) -> smaller gate; lower spec (balanced) -> larger gate.\n    spec_gate = max(Config.spec_floor, (1.0 - spec) ** Config.spec_power)\n\n    sat_num = 0.0\n    for term in query_repr.terms:\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        matched += 1.0\n\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n\n        if df >= common_thr:\n            frac = min(1.0, (df - common_thr) / (float(N) - common_thr + eps))\n            term_idf *= (1.0 - Config.common_penalty * frac)\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        wq = float(query_repr.term_weights.get(term, 1.0))\n\n        score += wq * term_idf * tf_part\n        m_idf.append(term_idf)\n        m_w.append(wq)\n        m_tfpart.append(tf_part)\n\n        sat_num += term_idf ** Config.sat_power\n>>>>>>> REPLACE\n\nNow apply the multiplier after coordination (still inside `retrieval_score`):\n\n<<<<<<< SEARCH\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n=======\n    if qn > 1.0:\n        coverage = matched / (qn + eps)\n        score *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n        # IDF-mass satisfaction (bounded in [0,1]), also gated by specificity.\n        if Config.sat_alpha > 0.0 and sat_den > 0.0:\n            sat = sat_num / (sat_den + eps)\n            score *= 1.0 + (Config.sat_alpha * spec_gate) * sat\n>>>>>>> REPLACE\n\n---\n\n## Change 3: Mirror satisfaction in `_score_candidates_vectorized`\nAdd satisfaction computation right after coordination multiplier:\n\n<<<<<<< SEARCH\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n=======\n        if qn > 1.0:\n            coverage = matched / (qn + eps)\n            scores *= (1.0 + (Config.coord_alpha * spec_gate) * coverage) ** Config.coord_beta\n\n            # Mirror retrieval_score(): per-doc IDF-mass satisfaction multiplier.\n            if Config.sat_alpha > 0.0 and idfs:\n                idf_arr = np.array(idfs, dtype=np.float64)\n                idf_pow = np.power(idf_arr, Config.sat_power)\n                sat_den = float(np.sum(idf_pow))\n                if sat_den > 0.0 and presents:\n                    sat_num = np.zeros(len(candidate_docs), dtype=np.float64)\n                    for ti, pres in enumerate(presents):\n                        w = float(idf_pow[ti])\n                        if w > 0.0:\n                            sat_num += pres * w\n                    scores *= 1.0 + (Config.sat_alpha * spec_gate) * (sat_num / (sat_den + eps))\n>>>>>>> REPLACE\n\n---\n\n### Why this is likely to help your current weak spots\n- You already have **coordination** and **pair synergy**; satisfaction adds a *different* \u201cAND-ish\u201d signal: it rewards covering the query\u2019s **informative core**, not just more terms/pairs.  \n- This often improves **nDCG@10** on corpora with verbose docs and queries with many modifiers (common in BRIGHT), while keeping recall stable because the base BM25-like sum remains the main driver.\n\nIf you want a follow-up simplification pass (your note about code length), we can fold some loops and remove unused locals while keeping behavior identical."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}