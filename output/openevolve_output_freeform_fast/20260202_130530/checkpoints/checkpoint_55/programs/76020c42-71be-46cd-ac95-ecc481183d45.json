{"id": "76020c42-71be-46cd-ac95-ecc481183d45", "code": "\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\", not term count\n    cov_idf_power: float = 1.0\n\n    # \"Informativeness pivot\": normalize by an IDF-weighted document mass.\n    info_power: float = 0.65\n    info_mix: float = 0.25\n\n    # NEW: Residual-IDF gating (RSJ-derivative)\n    # Intuition: an *observed match* is evidence above what we'd expect by chance\n    # given document length and term prevalence. This helps short, precise docs\n    # and reduces common-term false positives (often improves nDCG@10).\n    use_residual_idf: bool = True\n    ridf_gamma: float = 0.55  # mix between base idf and residual idf\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length (positions removed to reduce cost).\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with two normalizers + a *residual evidence* term weight.\n\n    Residual-IDF (RIDF) idea: a match is more meaningful if it is less likely to\n    happen \"by chance\" in a random document of length |d| given collection term\n    prevalence p=df/N. We approximate chance by P(t appears)\u22481-(1-p)^|d|.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # Informativeness pivot mass: sum tf * idf^info_power\n    info_mass = 0.0\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        tidf = float(max(base_t, 0.0) ** (Config.idf_power * Config.info_power))\n        info_mass += float(tfv) * tidf\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        base = max(base, 0.0)\n\n        # Sharpen base idf (as before)\n        term_idf = float(base ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        # Residual-IDF: downweight terms that are expected to appear in a doc of this length\n        if Config.use_residual_idf and doc_length > 0:\n            p = min(max(df / max(float(N), 1.0), eps), 1.0 - eps)\n            # log chance that term appears at least once in a length-|d| sample\n            log_p_present = math.log1p(-math.exp(doc_length * math.log1p(-p)))\n            ridf = max(term_idf + log_p_present, 0.0)\n            term_w = (1.0 - Config.ridf_gamma) * term_idf + Config.ridf_gamma * ridf\n        else:\n            term_w = term_idf\n\n        mass = (term_w if term_w > 0 else term_idf) ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_w * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build lightweight doc repr once (TF only; positions removed).\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring used by rank(); matches retrieval_score including RIDF.\n\n        For RIDF we need per-candidate doc length and df/N. We apply RIDF only as a\n        per-(term,doc) weight; info-pivot remains the matched-mass approximation.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        dlen = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        N = max(float(self.corpus.N), 1.0)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # RIDF term weight per candidate doc: idf + log P_present(d)\n            if Config.use_residual_idf:\n                p = min(max(float(self.corpus._df[term_id]) / N, eps), 1.0 - eps)\n                log1m_p = math.log1p(-p)\n                # log(1 - (1-p)^|d|)\n                log_p_present = np.log1p(-np.exp(dlen * log1m_p))\n                ridf = np.maximum(idf_val + log_p_present, 0.0)\n                term_w = (1.0 - Config.ridf_gamma) * idf_val + Config.ridf_gamma * ridf\n            else:\n                term_w = idf_val\n\n            # coverage uses the same weight notion\n            mass = float(idf_val ** Config.cov_idf_power)\n            cov_den += mass\n\n            wq = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wq * term_w * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n", "language": "python", "parent_id": "6516a13c-5c8a-476e-816b-daa6ccdb0b6c", "generation": 5, "timestamp": 1770074382.063566, "iteration_found": 29, "metrics": {"beir_nfcorpus_ndcg@10": 0.31876078784051615, "beir_nfcorpus_recall@100": 0.2547316218742289, "beir_nfcorpus_index_time_ms": 3760.323334019631, "beir_nfcorpus_query_time_ms": 120.97712501417845, "bright_pony_ndcg@10": 0.11687800595021021, "bright_pony_recall@100": 0.31657080952801575, "bright_pony_index_time_ms": 1531.0084170196205, "bright_pony_query_time_ms": 813.6481670080684, "beir_scifact_ndcg@10": 0.668870638348328, "beir_scifact_recall@100": 0.9253333333333335, "beir_scifact_index_time_ms": 5095.015958999284, "beir_scifact_query_time_ms": 465.2172499918379, "beir_arguana_ndcg@10": 0.2529783358169046, "beir_arguana_recall@100": 0.8850820842255531, "beir_arguana_index_time_ms": 6119.739334011683, "beir_arguana_query_time_ms": 34947.46266701259, "bright_theoremqa_theorems_ndcg@10": 0.03461693015746919, "bright_theoremqa_theorems_recall@100": 0.14035087719298245, "bright_theoremqa_theorems_index_time_ms": 11016.26970799407, "bright_theoremqa_theorems_query_time_ms": 1374.4167920085602, "beir_scidocs_ndcg@10": 0.1394506518063606, "beir_scidocs_recall@100": 0.33478333333333343, "beir_scidocs_index_time_ms": 20135.603833012283, "beir_scidocs_query_time_ms": 4608.853874990018, "bright_economics_ndcg@10": 0.14709819203701957, "bright_economics_recall@100": 0.36762625724951276, "bright_economics_index_time_ms": 18650.307750009233, "bright_economics_query_time_ms": 4624.1842090094, "bright_biology_ndcg@10": 0.2881048503787563, "bright_biology_recall@100": 0.5574986757408833, "bright_biology_index_time_ms": 23945.366458996432, "bright_biology_query_time_ms": 4083.4584579861257, "beir_fiqa_ndcg@10": 0.20782494877310032, "beir_fiqa_recall@100": 0.5007478075765113, "beir_fiqa_index_time_ms": 46550.793208007235, "beir_fiqa_query_time_ms": 9610.44720801874, "bright_earth_science_ndcg@10": 0.3381267791851095, "bright_earth_science_recall@100": 0.6642307260444739, "bright_earth_science_index_time_ms": 47094.97174998978, "bright_earth_science_query_time_ms": 7156.261333002476, "bright_stackoverflow_ndcg@10": 0.1894100037583118, "bright_stackoverflow_recall@100": 0.4540426612466229, "bright_stackoverflow_index_time_ms": 110422.20608299249, "bright_stackoverflow_query_time_ms": 15349.956166988704, "beir_trec-covid_ndcg@10": 0.6408838645450825, "beir_trec-covid_recall@100": 0.10515674586220637, "beir_trec-covid_index_time_ms": 154082.09995899233, "beir_trec-covid_query_time_ms": 1950.1550839922857, "avg_ndcg@10": 0.27858366571643073, "avg_recall@100": 0.45884624443397154, "combined_score": 0.4227937286904634, "total_index_time_ms": 448403.7057940441, "total_query_time_ms": 85105.03833502298, "total_time_ms": 533508.744129067, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 31 lines with 35 lines\nChange 2: Replace 72 lines with 76 lines\nChange 3: Replace 59 lines with 65 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3185855538696177, "beir_nfcorpus_recall@100": 0.25471824833110923, "beir_nfcorpus_index_time_ms": 3740.122249990236, "beir_nfcorpus_query_time_ms": 110.89162499411032, "bright_pony_ndcg@10": 0.11027381364018421, "bright_pony_recall@100": 0.3024248182516816, "bright_pony_index_time_ms": 1481.6161250055302, "bright_pony_query_time_ms": 551.498916989658, "beir_scifact_ndcg@10": 0.6767607926951988, "beir_scifact_recall@100": 0.9313333333333332, "beir_scifact_index_time_ms": 5064.4481659983285, "beir_scifact_query_time_ms": 384.9434159928933, "beir_arguana_ndcg@10": 0.26630038306970044, "beir_arguana_recall@100": 0.8986438258386866, "beir_arguana_index_time_ms": 6163.4193749923725, "beir_arguana_query_time_ms": 23176.013958000112, "bright_theoremqa_theorems_ndcg@10": 0.03159562804513474, "bright_theoremqa_theorems_recall@100": 0.15350877192982457, "bright_theoremqa_theorems_index_time_ms": 11116.086958005326, "bright_theoremqa_theorems_query_time_ms": 923.6545840103645, "beir_scidocs_ndcg@10": 0.14394245843337378, "beir_scidocs_recall@100": 0.34191666666666665, "beir_scidocs_index_time_ms": 20387.940916989464, "beir_scidocs_query_time_ms": 3611.99137501535, "bright_economics_ndcg@10": 0.14482121430825098, "bright_economics_recall@100": 0.3811608405337128, "bright_economics_index_time_ms": 18183.44320799224, "bright_economics_query_time_ms": 2968.9875000040047, "bright_biology_ndcg@10": 0.2745854528055333, "bright_biology_recall@100": 0.54919514993608, "bright_biology_index_time_ms": 21801.2593750027, "bright_biology_query_time_ms": 2676.543459005188, "beir_fiqa_ndcg@10": 0.2181951765639589, "beir_fiqa_recall@100": 0.5159227046958528, "beir_fiqa_index_time_ms": 48150.33245898667, "beir_fiqa_query_time_ms": 7054.773917014245, "bright_earth_science_ndcg@10": 0.35047694676955604, "bright_earth_science_recall@100": 0.658066248759307, "bright_earth_science_index_time_ms": 46839.188459009165, "bright_earth_science_query_time_ms": 4630.281042016577, "bright_stackoverflow_ndcg@10": 0.18902857998004846, "bright_stackoverflow_recall@100": 0.46167776601340954, "bright_stackoverflow_index_time_ms": 112289.15879098349, "bright_stackoverflow_query_time_ms": 9310.334333014907, "beir_trec-covid_ndcg@10": 0.6421231395267699, "beir_trec-covid_recall@100": 0.10706703195752366, "beir_trec-covid_index_time_ms": 151911.8769580091, "beir_trec-covid_query_time_ms": 1530.9229170088656, "avg_ndcg@10": 0.2805574283089439, "avg_recall@100": 0.4629696171872657, "combined_score": 0.4264871794116014, "total_index_time_ms": 447128.8930409646, "total_query_time_ms": 56930.837043066276, "total_time_ms": 504059.7300840309, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 1}, "prompts": {"diff_user": {"system": "You are discovering a **new lexical retrieval method**. The seed program is a minimal skeleton: document representation, query representation, and a scoring function. Your job is to propose formulations that are **novel, deep, and intuitively justified**\u2014not just BM25 with extra knobs. Be creative and exploratory.\n\n## Goal\n\n- **Optimize**: per-dataset recall, nDCG@10, and a combined_score = 0.8 \u00d7 avg_recall@100 + 0.2 \u00d7 avg_ndcg@10 (higher is better).\n- **Design**: Invent or refine the relevance formula and representations with clear, fundamental reasoning (e.g. information-theoretic, probabilistic, or geometric). We want ideas that could plausibly generalize and that have a coherent story, not ad-hoc constants.\n\n## What you can change (evolve)\n\n1. **Config** \u2014 Add or change parameters (k1, b, epsilon are only the default; you can replace or extend them).\n2. **idf(df, N)** \u2014 How term importance depends on document frequency. EVOLVE: try other notions of rarity/discriminativity.\n3. **DocumentRepr** \u2014 What we store per document (e.g. term freqs, length; you can add positions, fields, etc.). Evolve `from_tokens` and any new fields.\n4. **QueryRepr** \u2014 How the query is represented (terms, weights; you can add expansion, dedup, weighting). Evolve `from_tokens`.\n5. **retrieval_score(...)** \u2014 **The core retrieval method.** This function scores one document for one query. EVOLVE: design a formula with a clear, intuitive justification. You can use multiple sub-signals and combine them, or a single unified formula; the seed is BM25 only as a starting point.\n6. **score_document(query, doc_idx, corpus)** \u2014 Top-level entry; you can change the pipeline (e.g. different reprs, preprocessing) as long as the final score is returned.\n7. **BM25._score_candidates_vectorized** \u2014 Used by rank() for speed. If you change the scoring formula, keep this in sync with retrieval_score so rank() remains correct and fast (or document that you accept a slower path).\n\nUse **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit.\n\nUse **per-dataset metrics** to see where the method is weak and target those benchmarks.\n\n## What you must keep (evaluator contract)\n\n- The module must expose: **BM25**, **Corpus**, **tokenize**, **LuceneTokenizer**.\n- **BM25** must have **rank(query, top_k=None)** returning (indices, scores) and **score(query, index)** returning a float.\n- **Corpus** is constructed with (documents, ids); the evaluator uses it and BM25.rank() / BM25.score(). Do not remove or rename these public APIs.\n- Avoid division by zero and NaNs (use Config.epsilon or similar).\n\n## Guidelines\n\n- Prefer one or a few coherent ideas per edit rather than many unrelated tweaks.\n- Explain in comments or structure *why* a formulation is reasonable (e.g. \"saturate TF because repeated terms matter less\" or \"penalize length to favor focused docs\").\n- If you add new parameters or signals, give them meaningful names and clear roles.\n- Novel formulations (e.g. different IDF, length norms, or multi-term interactions) are encouraged; stay within lexical retrieval (no external APIs or learned weights that require training data).\n", "user": "# Current Program Information\n- Fitness: 0.4265\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.4262 \u2192 0.4265\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 21 lines with 30 lines\nChange 2: Replace 9 lines with 14 lines\nChange 3: Replace 48 lines with 90 lines\nChange 4: Replace 10 lines with 9 lines\nChange 5: Replace 60 lines with 62 lines\nChange 6: Replace 32 lines with 40 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3205, beir_nfcorpus_recall@100: 0.2559, beir_nfcorpus_index_time_ms: 4013.5049, beir_nfcorpus_query_time_ms: 106.3459, bright_pony_ndcg@10: 0.0975, bright_pony_recall@100: 0.2907, bright_pony_index_time_ms: 1727.8343, bright_pony_query_time_ms: 506.1092, beir_scifact_ndcg@10: 0.6773, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5645.9492, beir_scifact_query_time_ms: 347.6350, beir_arguana_ndcg@10: 0.2760, beir_arguana_recall@100: 0.9108, beir_arguana_index_time_ms: 6570.3110, beir_arguana_query_time_ms: 20548.8009, bright_theoremqa_theorems_ndcg@10: 0.0329, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 11959.5706, bright_theoremqa_theorems_query_time_ms: 823.6215, beir_scidocs_ndcg@10: 0.1466, beir_scidocs_recall@100: 0.3445, beir_scidocs_index_time_ms: 21511.2980, beir_scidocs_query_time_ms: 3395.4754, bright_economics_ndcg@10: 0.1622, bright_economics_recall@100: 0.3882, bright_economics_index_time_ms: 22331.4429, bright_economics_query_time_ms: 2575.4495, bright_biology_ndcg@10: 0.2618, bright_biology_recall@100: 0.5199, bright_biology_index_time_ms: 23688.1132, bright_biology_query_time_ms: 2420.3535, beir_fiqa_ndcg@10: 0.2263, beir_fiqa_recall@100: 0.5246, beir_fiqa_index_time_ms: 47030.7875, beir_fiqa_query_time_ms: 6907.6567, bright_earth_science_ndcg@10: 0.3466, bright_earth_science_recall@100: 0.6559, bright_earth_science_index_time_ms: 50242.5387, bright_earth_science_query_time_ms: 3864.6330, bright_stackoverflow_ndcg@10: 0.1914, bright_stackoverflow_recall@100: 0.4603, bright_stackoverflow_index_time_ms: 123168.6981, bright_stackoverflow_query_time_ms: 9681.2816, beir_trec-covid_ndcg@10: 0.6552, beir_trec-covid_recall@100: 0.1085, beir_trec-covid_index_time_ms: 171148.4577, beir_trec-covid_query_time_ms: 1453.9748, avg_ndcg@10: 0.2829, avg_recall@100: 0.4621, combined_score: 0.4262, total_index_time_ms: 489038.5060, total_query_time_ms: 52631.3370, total_time_ms: 541669.8431, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 21 lines with 24 lines\nChange 2: Replace 48 lines with 53 lines\nChange 3: Replace 32 lines with 38 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3205, beir_nfcorpus_recall@100: 0.2559, beir_nfcorpus_index_time_ms: 3786.3495, beir_nfcorpus_query_time_ms: 99.0347, bright_pony_ndcg@10: 0.0975, bright_pony_recall@100: 0.2907, bright_pony_index_time_ms: 1521.9191, bright_pony_query_time_ms: 492.7793, beir_scifact_ndcg@10: 0.6773, beir_scifact_recall@100: 0.9320, beir_scifact_index_time_ms: 5136.1763, beir_scifact_query_time_ms: 346.7202, beir_arguana_ndcg@10: 0.2760, beir_arguana_recall@100: 0.9108, beir_arguana_index_time_ms: 6100.4585, beir_arguana_query_time_ms: 20390.8756, bright_theoremqa_theorems_ndcg@10: 0.0329, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 10875.3425, bright_theoremqa_theorems_query_time_ms: 820.7274, beir_scidocs_ndcg@10: 0.1466, beir_scidocs_recall@100: 0.3445, beir_scidocs_index_time_ms: 19992.2679, beir_scidocs_query_time_ms: 3388.7657, bright_economics_ndcg@10: 0.1622, bright_economics_recall@100: 0.3882, bright_economics_index_time_ms: 19571.8049, bright_economics_query_time_ms: 2543.2227, bright_biology_ndcg@10: 0.2618, bright_biology_recall@100: 0.5199, bright_biology_index_time_ms: 24504.7122, bright_biology_query_time_ms: 2438.8401, bright_earth_science_ndcg@10: 0.3466, bright_earth_science_recall@100: 0.6559, bright_earth_science_index_time_ms: 42995.2224, bright_earth_science_query_time_ms: 4255.6002, beir_fiqa_ndcg@10: 0.2263, beir_fiqa_recall@100: 0.5246, beir_fiqa_index_time_ms: 46354.7861, beir_fiqa_query_time_ms: 6820.2796, bright_stackoverflow_ndcg@10: 0.1914, bright_stackoverflow_recall@100: 0.4603, bright_stackoverflow_index_time_ms: 109257.0425, bright_stackoverflow_query_time_ms: 8434.5640, beir_trec-covid_ndcg@10: 0.6552, beir_trec-covid_recall@100: 0.1085, beir_trec-covid_index_time_ms: 151128.7131, beir_trec-covid_query_time_ms: 1487.2132, avg_ndcg@10: 0.2829, avg_recall@100: 0.4621, combined_score: 0.4262, total_index_time_ms: 441224.7949, total_query_time_ms: 51518.6227, total_time_ms: 492743.4176, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace 30 lines with 31 lines\nChange 2: Replace 14 lines with 10 lines\nChange 3: Replace 90 lines with 72 lines\nChange 4: Replace 9 lines with 9 lines\nChange 5: Replace 2 lines with 2 lines\nChange 6: Replace 40 lines with 59 lines\n- Metrics: beir_nfcorpus_ndcg@10: 0.3186, beir_nfcorpus_recall@100: 0.2547, beir_nfcorpus_index_time_ms: 3740.1222, beir_nfcorpus_query_time_ms: 110.8916, bright_pony_ndcg@10: 0.1103, bright_pony_recall@100: 0.3024, bright_pony_index_time_ms: 1481.6161, bright_pony_query_time_ms: 551.4989, beir_scifact_ndcg@10: 0.6768, beir_scifact_recall@100: 0.9313, beir_scifact_index_time_ms: 5064.4482, beir_scifact_query_time_ms: 384.9434, beir_arguana_ndcg@10: 0.2663, beir_arguana_recall@100: 0.8986, beir_arguana_index_time_ms: 6163.4194, beir_arguana_query_time_ms: 23176.0140, bright_theoremqa_theorems_ndcg@10: 0.0316, bright_theoremqa_theorems_recall@100: 0.1535, bright_theoremqa_theorems_index_time_ms: 11116.0870, bright_theoremqa_theorems_query_time_ms: 923.6546, beir_scidocs_ndcg@10: 0.1439, beir_scidocs_recall@100: 0.3419, beir_scidocs_index_time_ms: 20387.9409, beir_scidocs_query_time_ms: 3611.9914, bright_economics_ndcg@10: 0.1448, bright_economics_recall@100: 0.3812, bright_economics_index_time_ms: 18183.4432, bright_economics_query_time_ms: 2968.9875, bright_biology_ndcg@10: 0.2746, bright_biology_recall@100: 0.5492, bright_biology_index_time_ms: 21801.2594, bright_biology_query_time_ms: 2676.5435, beir_fiqa_ndcg@10: 0.2182, beir_fiqa_recall@100: 0.5159, beir_fiqa_index_time_ms: 48150.3325, beir_fiqa_query_time_ms: 7054.7739, bright_earth_science_ndcg@10: 0.3505, bright_earth_science_recall@100: 0.6581, bright_earth_science_index_time_ms: 46839.1885, bright_earth_science_query_time_ms: 4630.2810, bright_stackoverflow_ndcg@10: 0.1890, bright_stackoverflow_recall@100: 0.4617, bright_stackoverflow_index_time_ms: 112289.1588, bright_stackoverflow_query_time_ms: 9310.3343, beir_trec-covid_ndcg@10: 0.6421, beir_trec-covid_recall@100: 0.1071, beir_trec-covid_index_time_ms: 151911.8770, beir_trec-covid_query_time_ms: 1530.9229, avg_ndcg@10: 0.2806, avg_recall@100: 0.4630, combined_score: 0.4265, total_index_time_ms: 447128.8930, total_query_time_ms: 56930.8370, total_time_ms: 504059.7301, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4265)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\", not term count\n    cov_idf_power: float = 1.0\n\n    # NEW: \"informativeness pivot\" normalization.\n    # Idea: normalize by an IDF-weighted document mass so verbose docs that mostly\n    # repeat common terms don't dominate; long but information-dense docs aren't\n    # punished as harshly as pure length norms.\n    info_power: float = 0.65     # how strongly to count rare terms as \"mass\"\n    info_mix: float = 0.25       # blend between length-norm and info-norm\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length (positions removed to reduce cost).\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with two coupled normalizers:\n\n    (1) Focus-length norm (tokens + uniq mix): favors focused docs.\n    (2) NEW \"informativeness pivot\": normalize TF-saturation by an IDF-weighted\n        document mass so verbosity in common terms is discounted.\n\n    Plus: IDF-mass coverage reward (soft AND over information, not term count).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Focused length (cheap proxy for topical breadth)\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # Informativeness mass: sum_t tf(t,d) * idf(t)^info_power\n    # (computed on the fly from doc_tf; avoids storing extra arrays)\n    info_mass = 0.0\n    for t, tfv in doc_tf.items():\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        tidf = float(max(base, 0.0) ** (Config.idf_power * Config.info_power))\n        info_mass += float(tfv) * tidf\n\n    # Normalize against corpus average: approximate by scaling with avgdl (stable, cheap).\n    # We only need a monotone pivot; exact expectation not required.\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n\n    # Blend: when info_mix=0 -> pure length norm; when 1 -> pure info norm.\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build lightweight doc repr once (TF only; positions removed).\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring used by rank().\n\n        Mirrors retrieval_score's main parts:\n          - TF saturation with blended normalization\n          - IDF-mass coverage reward\n\n        Info-pivot is approximated using *matched* IDF-mass (cheap) rather than full\n        document IDF-mass (expensive). This still discourages \"common-term verbosity\"\n        while keeping rank() fast.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        # Proxy for info_norm: IDF-mass of matched terms (presence-weighted).\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            mass = idf_val ** Config.cov_idf_power\n            cov_den += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n\n            # matched_mass uses a slightly softer power to avoid over-favoring ultra-rare typos\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            # blended normalization: length norm + matched-mass norm\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3186), Performs well on beir_nfcorpus_recall@100 (0.2547), Performs well on beir_nfcorpus_index_time_ms (3740.1222), Performs well on beir_nfcorpus_query_time_ms (110.8916), Performs well on bright_pony_ndcg@10 (0.1103), Performs well on bright_pony_recall@100 (0.3024), Performs well on bright_pony_index_time_ms (1481.6161), Performs well on bright_pony_query_time_ms (551.4989), Performs well on beir_scifact_ndcg@10 (0.6768), Performs well on beir_scifact_recall@100 (0.9313), Performs well on beir_scifact_index_time_ms (5064.4482), Performs well on beir_scifact_query_time_ms (384.9434), Performs well on beir_arguana_ndcg@10 (0.2663), Performs well on beir_arguana_recall@100 (0.8986), Performs well on beir_arguana_index_time_ms (6163.4194), Performs well on beir_arguana_query_time_ms (23176.0140), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0316), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11116.0870), Performs well on bright_theoremqa_theorems_query_time_ms (923.6546), Performs well on beir_scidocs_ndcg@10 (0.1439), Performs well on beir_scidocs_recall@100 (0.3419), Performs well on beir_scidocs_index_time_ms (20387.9409), Performs well on beir_scidocs_query_time_ms (3611.9914), Performs well on bright_economics_ndcg@10 (0.1448), Performs well on bright_economics_recall@100 (0.3812), Performs well on bright_economics_index_time_ms (18183.4432), Performs well on bright_economics_query_time_ms (2968.9875), Performs well on bright_biology_ndcg@10 (0.2746), Performs well on bright_biology_recall@100 (0.5492), Performs well on bright_biology_index_time_ms (21801.2594), Performs well on bright_biology_query_time_ms (2676.5435), Performs well on beir_fiqa_ndcg@10 (0.2182), Performs well on beir_fiqa_recall@100 (0.5159), Performs well on beir_fiqa_index_time_ms (48150.3325), Performs well on beir_fiqa_query_time_ms (7054.7739), Performs well on bright_earth_science_ndcg@10 (0.3505), Performs well on bright_earth_science_recall@100 (0.6581), Performs well on bright_earth_science_index_time_ms (46839.1885), Performs well on bright_earth_science_query_time_ms (4630.2810), Performs well on bright_stackoverflow_ndcg@10 (0.1890), Performs well on bright_stackoverflow_recall@100 (0.4617), Performs well on bright_stackoverflow_index_time_ms (112289.1588), Performs well on bright_stackoverflow_query_time_ms (9310.3343), Performs well on beir_trec-covid_ndcg@10 (0.6421), Performs well on beir_trec-covid_recall@100 (0.1071), Performs well on beir_trec-covid_index_time_ms (151911.8770), Performs well on beir_trec-covid_query_time_ms (1530.9229), Performs well on avg_ndcg@10 (0.2806), Performs well on avg_recall@100 (0.4630), Performs well on combined_score (0.4265), Performs well on total_index_time_ms (447128.8930), Performs well on total_query_time_ms (56930.8370), Performs well on total_time_ms (504059.7301), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4262)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # IDF-weighted coverage (coordination over \"information mass\", not term count)\n    cov_idf_power: float = 1.0\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + *IDF-mass* coverage reward.\n\n    Intuition: coordination should measure how much of the query's *information*\n    is covered (rare terms matter more than common ones). This tends to improve\n    early precision/nDCG while keeping partial matches viable for recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            mass = idf_val ** Config.cov_idf_power\n            cov_den += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            cov_num += present * mass\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3205), Performs well on beir_nfcorpus_recall@100 (0.2559), Performs well on beir_nfcorpus_index_time_ms (3786.3495), Performs well on beir_nfcorpus_query_time_ms (99.0347), Performs well on bright_pony_ndcg@10 (0.0975), Performs well on bright_pony_recall@100 (0.2907), Performs well on bright_pony_index_time_ms (1521.9191), Performs well on bright_pony_query_time_ms (492.7793), Performs well on beir_scifact_ndcg@10 (0.6773), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5136.1763), Performs well on beir_scifact_query_time_ms (346.7202), Performs well on beir_arguana_ndcg@10 (0.2760), Performs well on beir_arguana_recall@100 (0.9108), Performs well on beir_arguana_index_time_ms (6100.4585), Performs well on beir_arguana_query_time_ms (20390.8756), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0329), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (10875.3425), Performs well on bright_theoremqa_theorems_query_time_ms (820.7274), Performs well on beir_scidocs_ndcg@10 (0.1466), Performs well on beir_scidocs_recall@100 (0.3445), Performs well on beir_scidocs_index_time_ms (19992.2679), Performs well on beir_scidocs_query_time_ms (3388.7657), Performs well on bright_economics_ndcg@10 (0.1622), Performs well on bright_economics_recall@100 (0.3882), Performs well on bright_economics_index_time_ms (19571.8049), Performs well on bright_economics_query_time_ms (2543.2227), Performs well on bright_biology_ndcg@10 (0.2618), Performs well on bright_biology_recall@100 (0.5199), Performs well on bright_biology_index_time_ms (24504.7122), Performs well on bright_biology_query_time_ms (2438.8401), Performs well on bright_earth_science_ndcg@10 (0.3466), Performs well on bright_earth_science_recall@100 (0.6559), Performs well on bright_earth_science_index_time_ms (42995.2224), Performs well on bright_earth_science_query_time_ms (4255.6002), Performs well on beir_fiqa_ndcg@10 (0.2263), Performs well on beir_fiqa_recall@100 (0.5246), Performs well on beir_fiqa_index_time_ms (46354.7861), Performs well on beir_fiqa_query_time_ms (6820.2796), Performs well on bright_stackoverflow_ndcg@10 (0.1914), Performs well on bright_stackoverflow_recall@100 (0.4603), Performs well on bright_stackoverflow_index_time_ms (109257.0425), Performs well on bright_stackoverflow_query_time_ms (8434.5640), Performs well on beir_trec-covid_ndcg@10 (0.6552), Performs well on beir_trec-covid_recall@100 (0.1085), Performs well on beir_trec-covid_index_time_ms (151128.7131), Performs well on beir_trec-covid_query_time_ms (1487.2132), Performs well on avg_ndcg@10 (0.2829), Performs well on avg_recall@100 (0.4621), Performs well on combined_score (0.4262), Performs well on total_index_time_ms (441224.7949), Performs well on total_query_time_ms (51518.6227), Performs well on total_time_ms (492743.4176), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4262)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Novel signal: \"informativeness-weighted coverage\".\n    # Intuition: matching rare/discriminative terms should count more toward coordination\n    # than matching ubiquitous terms (improves early precision/nDCG without killing recall).\n    cov_idf_power: float = 1.0\n\n    # Soft phrase/proximity reward. Only uses adjacent query term pairs (cheap).\n    prox_window: int = 8\n    prox_alpha: float = 0.12\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float, positions: dict[str, list[int]]):\n        self.term_frequencies = term_frequencies\n        self.length = length\n        self.positions = positions  # for lightweight proximity/phrase evidence\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Store positions for proximity reward (kept minimal: only per-term position lists).\"\"\"\n        tf = Counter(tokens)\n        pos: dict[str, list[int]] = {}\n        for i, t in enumerate(tokens):\n            pos.setdefault(t, []).append(i)\n        return cls(term_frequencies=tf, length=float(len(tokens)), positions=pos)\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    doc_pos: dict[str, list[int]] | None = None,\n) -> float:\n    \"\"\"\n    Two coupled ideas:\n\n    (1) IDF-weighted coverage: coordination should be about *information covered*,\n        not just #tokens matched. This tends to lift docs that hit the rare \"spine\"\n        of the query (better nDCG@10) while leaving partial matches competitive (recall).\n\n    (2) Cheap proximity reward: if adjacent query terms appear within a small window,\n        add a bounded boost. This helps theorem/stackoverflow-ish \"phrasey\" queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Term evidence + accumulate IDF mass for coverage\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        cov_den += term_idf ** Config.cov_idf_power\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += term_idf ** Config.cov_idf_power\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # IDF-weighted coordination (bounded reward >= 1)\n    if cov_den > 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # Proximity / soft-phrase reward from adjacent query term pairs\n    if doc_pos is not None and len(query_repr.terms) > 1 and Config.prox_alpha > 0:\n        hits = 0.0\n        pairs = 0.0\n        w = int(max(1, Config.prox_window))\n        for a, bterm in zip(query_repr.terms, query_repr.terms[1:]):\n            pa = doc_pos.get(a)\n            pb = doc_pos.get(bterm)\n            if not pa or not pb:\n                pairs += 1.0\n                continue\n            # two-pointer min distance\n            i = j = 0\n            best = 1_000_000\n            while i < len(pa) and j < len(pb):\n                da = pa[i] - pb[j]\n                ad = da if da >= 0 else -da\n                if ad < best:\n                    best = ad\n                    if best == 0:\n                        break\n                if da < 0:\n                    i += 1\n                else:\n                    j += 1\n            if best <= w:\n                hits += 1.0\n            pairs += 1.0\n        if pairs > 0.0:\n            score *= (1.0 + Config.prox_alpha * (hits / pairs))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency, d.positions)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build richer doc repr once (tf + positions) for proximity scoring in score().\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized core score for rank(); matches retrieval_score *except* proximity\n        (which is applied only in score(), not rank()) to keep ranking fast.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            cov_den += idf_val ** Config.cov_idf_power\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            cov_num += present * (idf_val ** Config.cov_idf_power)\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3205), Performs well on beir_nfcorpus_recall@100 (0.2559), Performs well on beir_nfcorpus_index_time_ms (4013.5049), Performs well on beir_nfcorpus_query_time_ms (106.3459), Performs well on bright_pony_ndcg@10 (0.0975), Performs well on bright_pony_recall@100 (0.2907), Performs well on bright_pony_index_time_ms (1727.8343), Performs well on bright_pony_query_time_ms (506.1092), Performs well on beir_scifact_ndcg@10 (0.6773), Performs well on beir_scifact_recall@100 (0.9320), Performs well on beir_scifact_index_time_ms (5645.9492), Performs well on beir_scifact_query_time_ms (347.6350), Performs well on beir_arguana_ndcg@10 (0.2760), Performs well on beir_arguana_recall@100 (0.9108), Performs well on beir_arguana_index_time_ms (6570.3110), Performs well on beir_arguana_query_time_ms (20548.8009), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0329), Performs well on bright_theoremqa_theorems_recall@100 (0.1535), Performs well on bright_theoremqa_theorems_index_time_ms (11959.5706), Performs well on bright_theoremqa_theorems_query_time_ms (823.6215), Performs well on beir_scidocs_ndcg@10 (0.1466), Performs well on beir_scidocs_recall@100 (0.3445), Performs well on beir_scidocs_index_time_ms (21511.2980), Performs well on beir_scidocs_query_time_ms (3395.4754), Performs well on bright_economics_ndcg@10 (0.1622), Performs well on bright_economics_recall@100 (0.3882), Performs well on bright_economics_index_time_ms (22331.4429), Performs well on bright_economics_query_time_ms (2575.4495), Performs well on bright_biology_ndcg@10 (0.2618), Performs well on bright_biology_recall@100 (0.5199), Performs well on bright_biology_index_time_ms (23688.1132), Performs well on bright_biology_query_time_ms (2420.3535), Performs well on beir_fiqa_ndcg@10 (0.2263), Performs well on beir_fiqa_recall@100 (0.5246), Performs well on beir_fiqa_index_time_ms (47030.7875), Performs well on beir_fiqa_query_time_ms (6907.6567), Performs well on bright_earth_science_ndcg@10 (0.3466), Performs well on bright_earth_science_recall@100 (0.6559), Performs well on bright_earth_science_index_time_ms (50242.5387), Performs well on bright_earth_science_query_time_ms (3864.6330), Performs well on bright_stackoverflow_ndcg@10 (0.1914), Performs well on bright_stackoverflow_recall@100 (0.4603), Performs well on bright_stackoverflow_index_time_ms (123168.6981), Performs well on bright_stackoverflow_query_time_ms (9681.2816), Performs well on beir_trec-covid_ndcg@10 (0.6552), Performs well on beir_trec-covid_recall@100 (0.1085), Performs well on beir_trec-covid_index_time_ms (171148.4577), Performs well on beir_trec-covid_query_time_ms (1453.9748), Performs well on avg_ndcg@10 (0.2829), Performs well on avg_recall@100 (0.4621), Performs well on combined_score (0.4262), Performs well on total_index_time_ms (489038.5060), Performs well on total_query_time_ms (52631.3370), Performs well on total_time_ms (541669.8431), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.4262, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # IDF-weighted coverage (coordination over \"information mass\", not term count)\n    cov_idf_power: float = 1.0\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"EVOLVE: different document views (e.g. positions, fields).\"\"\"\n        return cls(term_frequencies=Counter(tokens), length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Focus-normalized evidence accumulation + *IDF-mass* coverage reward.\n\n    Intuition: coordination should measure how much of the query's *information*\n    is covered (rare terms matter more than common ones). This tends to improve\n    early precision/nDCG while keeping partial matches viable for recall@100.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score(). EVOLVE: change pipeline if needed.\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    doc_length = float(corpus.doc_lengths[doc_idx])\n    return retrieval_score(q, doc_tf, doc_length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        # Unique-term length as a proxy for topical breadth / verbosity.\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        # Discriminativity: classic BM25 idf, then sharpened (idf^power).\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        # Pivoted normalization on a mix of token length and unique-term length.\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"Vectorized scoring for rank(); must match retrieval_score formula.\"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            mass = idf_val ** Config.cov_idf_power\n            cov_den += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            cov_num += present * mass\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4262, Type: Experimental)\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Novel signal: \"informativeness-weighted coverage\".\n    # Intuition: matching rare/discriminative terms should count more toward coordination\n    # than matching ubiquitous terms (improves early precision/nDCG without killing recall).\n    cov_idf_power: float = 1.0\n\n    # Soft phrase/proximity reward. Only uses adjacent query term pairs (cheap).\n    prox_window: int = 8\n    prox_alpha: float = 0.12\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float, positions: dict[str, list[int]]):\n        self.term_frequencies = term_frequencies\n        self.length = length\n        self.positions = positions  # for lightweight proximity/phrase evidence\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Store positions for proximity reward (kept minimal: only per-term position lists).\"\"\"\n        tf = Counter(tokens)\n        pos: dict[str, list[int]] = {}\n        for i, t in enumerate(tokens):\n            pos.setdefault(t, []).append(i)\n        return cls(term_frequencies=tf, length=float(len(tokens)), positions=pos)\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n    doc_pos: dict[str, list[int]] | None = None,\n) -> float:\n    \"\"\"\n    Two coupled ideas:\n\n    (1) IDF-weighted coverage: coordination should be about *information covered*,\n        not just #tokens matched. This tends to lift docs that hit the rare \"spine\"\n        of the query (better nDCG@10) while leaving partial matches competitive (recall).\n\n    (2) Cheap proximity reward: if adjacent query terms appear within a small window,\n        add a bounded boost. This helps theorem/stackoverflow-ish \"phrasey\" queries.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    # Term evidence + accumulate IDF mass for coverage\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        cov_den += term_idf ** Config.cov_idf_power\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n\n        cov_num += term_idf ** Config.cov_idf_power\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    # IDF-weighted coordination (bounded reward >= 1)\n    if cov_den > 0.0:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    # Proximity / soft-phrase reward from adjacent query term pairs\n    if doc_pos is not None and len(query_repr.terms) > 1 and Config.prox_alpha > 0:\n        hits = 0.0\n        pairs = 0.0\n        w = int(max(1, Config.prox_window))\n        for a, bterm in zip(query_repr.terms, query_repr.terms[1:]):\n            pa = doc_pos.get(a)\n            pb = doc_pos.get(bterm)\n            if not pa or not pb:\n                pairs += 1.0\n                continue\n            # two-pointer min distance\n            i = j = 0\n            best = 1_000_000\n            while i < len(pa) and j < len(pb):\n                da = pa[i] - pb[j]\n                ad = da if da >= 0 else -da\n                if ad < best:\n                    best = ad\n                    if best == 0:\n                        break\n                if da < 0:\n                    i += 1\n                else:\n                    j += 1\n            if best <= w:\n                hits += 1.0\n            pairs += 1.0\n        if pairs > 0.0:\n            score *= (1.0 + Config.prox_alpha * (hits / pairs))\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency, d.positions)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build richer doc repr once (tf + positions) for proximity scoring in score().\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized core score for rank(); matches retrieval_score *except* proximity\n        (which is applied only in score(), not rank()) to keep ranking fast.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            cov_den += idf_val ** Config.cov_idf_power\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            cov_num += present * (idf_val ** Config.cov_idf_power)\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nFreeform lexical retrieval seed \u2014 maximum freedom for discovering a new retrieval method.\n\nCore idea: document representation + query representation + scoring method.\nThe evaluator requires: BM25, Corpus, tokenize, LuceneTokenizer; BM25 must have rank() and score().\nEverything else is evolvable. Default behavior: Lucene BM25 (same as current seed).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# -----------------------------------------------------------------------------\n# Config \u2014 EVOLVE: add parameters for your retrieval method\n# -----------------------------------------------------------------------------\n\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\", not term count\n    cov_idf_power: float = 1.0\n\n    # NEW: \"informativeness pivot\" normalization.\n    # Idea: normalize by an IDF-weighted document mass so verbose docs that mostly\n    # repeat common terms don't dominate; long but information-dense docs aren't\n    # punished as harshly as pure length norms.\n    info_power: float = 0.65     # how strongly to count rare terms as \"mass\"\n    info_mix: float = 0.25       # blend between length-norm and info-norm\n\n    epsilon: float = 1e-9\n\n\n# -----------------------------------------------------------------------------\n# IDF \u2014 EVOLVE: fundamental term importance (e.g. rarity, discriminativity)\n# -----------------------------------------------------------------------------\n\ndef idf(df: float | NDArray[np.float64], N: int) -> float | NDArray[np.float64]:\n    \"\"\"Term importance from document frequency. EVOLVE: try other formulations.\"\"\"\n    return np.log(1.0 + (N - df + 0.5) / (df + 0.5))\n\n\n# -----------------------------------------------------------------------------\n# Document representation \u2014 EVOLVE: what to store per document\n# -----------------------------------------------------------------------------\n\nclass DocumentRepr:\n    def __init__(self, term_frequencies: Counter[str], length: float):\n        self.term_frequencies = term_frequencies\n        self.length = length\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> DocumentRepr:\n        \"\"\"Keep doc repr light: TF + length (positions removed to reduce cost).\"\"\"\n        tf = Counter(tokens)\n        return cls(term_frequencies=tf, length=float(len(tokens)))\n\n\n# -----------------------------------------------------------------------------\n# Query representation \u2014 EVOLVE: how to represent the query\n# -----------------------------------------------------------------------------\n\nclass QueryRepr:\n    def __init__(self, terms: list[str], term_weights: dict[str, float] | None = None):\n        self.terms = terms\n        self.term_weights = term_weights or {t: 1.0 for t in terms}\n\n    @classmethod\n    def from_tokens(cls, tokens: list[str]) -> QueryRepr:\n        \"\"\"\n        Use unique query terms + sublinear query-TF.\n\n        Rationale: coordination becomes meaningful only over distinct terms, and\n        (1+log qtf) reduces the impact of repetition artifacts (esp. long/noisy queries),\n        usually improving nDCG@10 without harming recall.\n        \"\"\"\n        c = Counter(tokens)\n        terms = list(c.keys())\n        if Config.use_log_qtf:\n            tw = {t: 1.0 + math.log(float(q)) for t, q in c.items()}\n        else:\n            tw = {t: float(q) for t, q in c.items()}\n        return cls(terms=terms, term_weights=tw)\n\n\n# -----------------------------------------------------------------------------\n# Lexical retrieval score \u2014 EVOLVE: the core relevance formula\n# -----------------------------------------------------------------------------\n\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with two coupled normalizers:\n\n    (1) Focus-length norm (tokens + uniq mix): favors focused docs.\n    (2) NEW \"informativeness pivot\": normalize TF-saturation by an IDF-weighted\n        document mass so verbosity in common terms is discounted.\n\n    Plus: IDF-mass coverage reward (soft AND over information, not term count).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Focused length (cheap proxy for topical breadth)\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # Informativeness mass: sum_t tf(t,d) * idf(t)^info_power\n    # (computed on the fly from doc_tf; avoids storing extra arrays)\n    info_mass = 0.0\n    for t, tfv in doc_tf.items():\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        tidf = float(max(base, 0.0) ** (Config.idf_power * Config.info_power))\n        info_mass += float(tfv) * tidf\n\n    # Normalize against corpus average: approximate by scaling with avgdl (stable, cheap).\n    # We only need a monotone pivot; exact expectation not required.\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n\n    # Blend: when info_mix=0 -> pure length norm; when 1 -> pure info norm.\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n\n\ndef score_document(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Entry point used by BM25.score().\"\"\"\n    if not query:\n        return 0.0\n    q = QueryRepr.from_tokens(query)\n    if not q.terms:\n        return 0.0\n    d = corpus.doc_repr[doc_idx]\n    return retrieval_score(q, d.term_frequencies, d.length, corpus.N, corpus.avgdl, corpus.document_frequency)\n\n\n# -----------------------------------------------------------------------------\n# Tokenization (fixed for evaluator)\n# -----------------------------------------------------------------------------\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# -----------------------------------------------------------------------------\n# Corpus (interface fixed for evaluator; internals can evolve if needed)\n# -----------------------------------------------------------------------------\n\nclass Corpus:\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n\n        # Build lightweight doc repr once (TF only; positions removed).\n        self.doc_repr = [DocumentRepr.from_tokens(d) for d in documents]\n\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self.doc_uniq = np.array([len(set(d)) for d in documents], dtype=np.float64)\n        self.avguq = float(np.mean(self.doc_uniq)) if self.N > 0 else 1.0\n\n        self._vocab: dict[str, int] = {}\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = len(self._vocab)\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [d.term_frequencies for d in self.doc_repr]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen = set()\n            for term, count in term_counts.items():\n                tid = self._vocab[term]\n                tf_matrix_lil[tid, doc_idx] = count\n                if tid not in seen:\n                    self._inverted_index[tid].append(doc_idx)\n                    self._df[tid] += 1\n                    seen.add(tid)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n\n        base_idf = np.asarray(idf(self._df, self.N), dtype=np.float64)\n        self.idf_array = np.power(np.maximum(base_idf, 0.0), Config.idf_power)\n\n        b = Config.b\n        mix = Config.focus_mix\n        eff_len = (1.0 - mix) * self.doc_lengths + mix * self.doc_uniq\n        avg_eff = (1.0 - mix) * max(self.avgdl, 1.0) + mix * max(self.avguq, 1.0)\n        self.norm_array = 1.0 - b + b * (eff_len / max(avg_eff, 1.0))\n\n        self.document_frequency = Counter(\n            {term: max(1, int(self._df[tid])) for term, tid in self._vocab.items()}\n        )\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            tid: np.array(doc_ids, dtype=np.int64)\n            for tid, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        tid = self._vocab.get(term)\n        return max(1, int(self._df[tid])) if tid is not None else 1\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        tid = self._vocab.get(term)\n        return int(self.tf_matrix[tid, doc_idx]) if tid is not None else 0\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        tid = self._vocab.get(term)\n        return self._posting_lists.get(tid, np.array([], dtype=np.int64)) if tid is not None else np.array([], dtype=np.int64)\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[i] for i in ids if i in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n\n# -----------------------------------------------------------------------------\n# BM25 (interface fixed for evaluator)\n# -----------------------------------------------------------------------------\n\nclass BM25:\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_document(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring used by rank().\n\n        Mirrors retrieval_score's main parts:\n          - TF saturation with blended normalization\n          - IDF-mass coverage reward\n\n        Info-pivot is approximated using *matched* IDF-mass (cheap) rather than full\n        document IDF-mass (expensive). This still discourages \"common-term verbosity\"\n        while keeping rank() fast.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        # Proxy for info_norm: IDF-mass of matched terms (presence-weighted).\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            mass = idf_val ** Config.cov_idf_power\n            cov_den += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n\n            # matched_mass uses a slightly softer power to avoid over-favoring ultra-rare typos\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            # blended normalization: length norm + matched-mass norm\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        # Mirror QueryRepr: distinct terms + optional log-qtf\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            tid = self.corpus.get_term_id(term)\n            if tid is not None:\n                query_term_ids.append(tid)\n                if Config.use_log_qtf:\n                    query_term_weights.append(1.0 + math.log(float(count)))\n                else:\n                    query_term_weights.append(float(count))\n\n        if not query_term_ids:\n            return np.arange(self.corpus.N, dtype=np.int64), np.zeros(self.corpus.N, dtype=np.float64)\n\n        qtf = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for tid in query_term_ids:\n            candidate_set.update(self.corpus._posting_lists.get(tid, np.array([], dtype=np.int64)).tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n\n        candidate_scores = self._score_candidates_vectorized(query_term_ids, candidate_docs, qtf)\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices, sorted_scores = sorted_indices[:top_k], sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(q, top_k) for q in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as ex:\n            return list(ex.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"Config\",\n    \"DocumentRepr\",\n    \"QueryRepr\",\n    \"idf\",\n    \"retrieval_score\",\n    \"score_document\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["<<<<<<< SEARCH\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\", not term count\n    cov_idf_power: float = 1.0\n\n    # NEW: \"informativeness pivot\" normalization.\n    # Idea: normalize by an IDF-weighted document mass so verbose docs that mostly\n    # repeat common terms don't dominate; long but information-dense docs aren't\n    # punished as harshly as pure length norms.\n    info_power: float = 0.65     # how strongly to count rare terms as \"mass\"\n    info_mix: float = 0.25       # blend between length-norm and info-norm\n\n    epsilon: float = 1e-9\n=======\nclass Config:\n    # TF saturation (BM25-like)\n    k1: float = 0.9\n\n    # Pivoted length normalization strength\n    b: float = 0.35\n\n    # Mix of token length and unique-term length (focus prior)\n    focus_mix: float = 0.65\n\n    # Rarity shaping\n    idf_power: float = 1.12\n\n    # Soft-AND / coordination as a *reward* (never < 1)\n    coord_alpha: float = 0.25\n    coord_beta: float = 0.75\n\n    # Query TF dampening: repeated query tokens often come from artifacts/noise\n    use_log_qtf: bool = True\n\n    # Coordination measured over \"information mass\", not term count\n    cov_idf_power: float = 1.0\n\n    # \"Informativeness pivot\": normalize by an IDF-weighted document mass.\n    info_power: float = 0.65\n    info_mix: float = 0.25\n\n    # NEW: Residual-IDF gating (RSJ-derivative)\n    # Intuition: an *observed match* is evidence above what we'd expect by chance\n    # given document length and term prevalence. This helps short, precise docs\n    # and reduces common-term false positives (often improves nDCG@10).\n    use_residual_idf: bool = True\n    ridf_gamma: float = 0.55  # mix between base idf and residual idf\n\n    epsilon: float = 1e-9\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with two coupled normalizers:\n\n    (1) Focus-length norm (tokens + uniq mix): favors focused docs.\n    (2) NEW \"informativeness pivot\": normalize TF-saturation by an IDF-weighted\n        document mass so verbosity in common terms is discounted.\n\n    Plus: IDF-mass coverage reward (soft AND over information, not term count).\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    # Focused length (cheap proxy for topical breadth)\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # Informativeness mass: sum_t tf(t,d) * idf(t)^info_power\n    # (computed on the fly from doc_tf; avoids storing extra arrays)\n    info_mass = 0.0\n    for t, tfv in doc_tf.items():\n        df = float(corpus_df.get(t, 1))\n        base = float(idf(df, N))\n        tidf = float(max(base, 0.0) ** (Config.idf_power * Config.info_power))\n        info_mass += float(tfv) * tidf\n\n    # Normalize against corpus average: approximate by scaling with avgdl (stable, cheap).\n    # We only need a monotone pivot; exact expectation not required.\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n\n    # Blend: when info_mix=0 -> pure length norm; when 1 -> pure info norm.\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        term_idf = float(max(base, 0.0) ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        mass = term_idf ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_idf * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n=======\ndef retrieval_score(\n    query_repr: QueryRepr,\n    doc_tf: Counter[str],\n    doc_length: float,\n    N: int,\n    avgdl: float,\n    corpus_df: Counter[str],\n) -> float:\n    \"\"\"\n    Evidence accumulation with two normalizers + a *residual evidence* term weight.\n\n    Residual-IDF (RIDF) idea: a match is more meaningful if it is less likely to\n    happen \"by chance\" in a random document of length |d| given collection term\n    prevalence p=df/N. We approximate chance by P(t appears)\u22481-(1-p)^|d|.\n    \"\"\"\n    k1, b, eps = Config.k1, Config.b, Config.epsilon\n\n    doc_uniq = float(len(doc_tf))\n    mix = Config.focus_mix\n    eff_len = (1.0 - mix) * doc_length + mix * doc_uniq\n    avg_eff = max(avgdl, 1.0)\n    len_norm = 1.0 - b + b * (eff_len / (avg_eff + eps)) if avg_eff > 0 else 1.0\n\n    # Informativeness pivot mass: sum tf * idf^info_power\n    info_mass = 0.0\n    for t, tfv in doc_tf.items():\n        df_t = float(corpus_df.get(t, 1))\n        base_t = float(idf(df_t, N))\n        tidf = float(max(base_t, 0.0) ** (Config.idf_power * Config.info_power))\n        info_mass += float(tfv) * tidf\n    info_norm = 1.0 - b + b * (info_mass / (max(avgdl, 1.0) + eps))\n    norm = (1.0 - Config.info_mix) * len_norm + Config.info_mix * info_norm\n\n    score = 0.0\n    cov_num = 0.0\n    cov_den = 0.0\n\n    for term in query_repr.terms:\n        df = float(corpus_df.get(term, 1))\n        base = float(idf(df, N))\n        base = max(base, 0.0)\n\n        # Sharpen base idf (as before)\n        term_idf = float(base ** Config.idf_power)\n        if term_idf <= 0.0:\n            continue\n\n        # Residual-IDF: downweight terms that are expected to appear in a doc of this length\n        if Config.use_residual_idf and doc_length > 0:\n            p = min(max(df / max(float(N), 1.0), eps), 1.0 - eps)\n            # log chance that term appears at least once in a length-|d| sample\n            log_p_present = math.log1p(-math.exp(doc_length * math.log1p(-p)))\n            ridf = max(term_idf + log_p_present, 0.0)\n            term_w = (1.0 - Config.ridf_gamma) * term_idf + Config.ridf_gamma * ridf\n        else:\n            term_w = term_idf\n\n        mass = (term_w if term_w > 0 else term_idf) ** Config.cov_idf_power\n        cov_den += mass\n\n        tf = float(doc_tf.get(term, 0))\n        if tf <= 0.0:\n            continue\n        cov_num += mass\n\n        tf_part = tf / (tf + k1 * norm + eps)\n        score += query_repr.term_weights.get(term, 1.0) * term_w * tf_part\n\n    if score <= 0.0:\n        return 0.0\n\n    if cov_den > 0.0 and len(query_repr.terms) > 1:\n        coverage = cov_num / (cov_den + eps)\n        score *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n    return score\n>>>>>>> REPLACE\n\n<<<<<<< SEARCH\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring used by rank().\n\n        Mirrors retrieval_score's main parts:\n          - TF saturation with blended normalization\n          - IDF-mass coverage reward\n\n        Info-pivot is approximated using *matched* IDF-mass (cheap) rather than full\n        document IDF-mass (expensive). This still discourages \"common-term verbosity\"\n        while keeping rank() fast.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        # Proxy for info_norm: IDF-mass of matched terms (presence-weighted).\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            mass = idf_val ** Config.cov_idf_power\n            cov_den += mass\n\n            w = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n\n            # matched_mass uses a slightly softer power to avoid over-favoring ultra-rare typos\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            # blended normalization: length norm + matched-mass norm\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += w * idf_val * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n=======\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        \"\"\"\n        Vectorized scoring used by rank(); matches retrieval_score including RIDF.\n\n        For RIDF we need per-candidate doc length and df/N. We apply RIDF only as a\n        per-(term,doc) weight; info-pivot remains the matched-mass approximation.\n        \"\"\"\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        k1, eps = Config.k1, Config.epsilon\n        len_norms = self.corpus.norm_array[candidate_docs]\n        dlen = self.corpus.doc_lengths[candidate_docs]\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        cov_num = np.zeros(len(candidate_docs), dtype=np.float64)\n        cov_den = 0.0\n\n        matched_mass = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        N = max(float(self.corpus.N), 1.0)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf_val = float(self.corpus.idf_array[term_id])\n            if idf_val <= 0.0:\n                continue\n\n            # RIDF term weight per candidate doc: idf + log P_present(d)\n            if Config.use_residual_idf:\n                p = min(max(float(self.corpus._df[term_id]) / N, eps), 1.0 - eps)\n                log1m_p = math.log1p(-p)\n                # log(1 - (1-p)^|d|)\n                log_p_present = np.log1p(-np.exp(dlen * log1m_p))\n                ridf = np.maximum(idf_val + log_p_present, 0.0)\n                term_w = (1.0 - Config.ridf_gamma) * idf_val + Config.ridf_gamma * ridf\n            else:\n                term_w = idf_val\n\n            # coverage uses the same weight notion\n            mass = float(idf_val ** Config.cov_idf_power)\n            cov_den += mass\n\n            wq = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n\n            cov_num += present * mass\n            matched_mass += present * (idf_val ** (Config.info_power * 0.5))\n\n            info_norms = 1.0 - Config.b + Config.b * (matched_mass / (max(self.corpus.avgdl, 1.0) + eps))\n            norms = (1.0 - Config.info_mix) * len_norms + Config.info_mix * info_norms\n\n            tf_part = tf_row / (tf_row + k1 * norms + eps)\n            scores += wq * term_w * tf_part\n\n        if cov_den > 0.0 and len(query_term_ids) > 1:\n            coverage = cov_num / (cov_den + eps)\n            scores *= (1.0 + Config.coord_alpha * coverage) ** Config.coord_beta\n\n        return scores\n>>>>>>> REPLACE"]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}