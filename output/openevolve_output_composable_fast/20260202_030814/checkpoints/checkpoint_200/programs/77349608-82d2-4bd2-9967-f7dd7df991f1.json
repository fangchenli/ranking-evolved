{"id": "77349608-82d2-4bd2-9967-f7dd7df991f1", "code": "\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending weight (kept)\n    alpha: float = 0.6\n\n    # Replace harsh coordination with bounded rarity-mass soft-AND (recall-safe)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Additive priors (recall-friendly)\n    cov_gamma: float = 0.10       # small additive term-coverage bump\n    idf_match_gamma: float = 0.08 # additive matched-IDF-share bump\n\n    # New: nDCG-friendly \"one strong term is diagnostic\" prior (small, additive)\n    maxterm_gamma: float = 0.03\n\n    # Keep mild rare-term shaping but do it via IDF choice rather than tf multiplier\n    gamma: float = 0.0\n\n    # Keep experimental knobs defined (prevents runtime failure if referenced)\n    idf_pivot: float = 3.0\n    cov_power: float = 2.0\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n        \"\"\"Bounded query-IDF mass coverage in [0,1]. Used as additive recall-friendly prior.\"\"\"\n        t = max(total_idf, EvolvedParameters.epsilon)\n        return max(0.0, min(1.0, matched_idf / t))\n\n    @staticmethod\n    def matched_idf_share_vectorized(\n        matched_idf: NDArray[np.float64], total_idf: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_idf), EvolvedParameters.epsilon)\n        return np.clip(matched_idf / t, 0.0, 1.0)\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        \"\"\"\n        Bounded rarity-mass soft-AND multiplier:\n          floor + (1-floor) * (matched_rarity/total_rarity)^beta\n        \"\"\"\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coord_factor(matched_terms: int, total_query_terms: int, beta: float) -> float:\n        \"\"\"\n        Soft coordination factor: (matched/total)^beta. Multiplies score.\n        \"\"\"\n        if total_query_terms <= 0 or matched_terms <= 0:\n            return 0.0\n        return (matched_terms / total_query_terms) ** max(beta, 0.0)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def idf_mixed_compressed(df: float, N: int) -> float:\n        \"\"\"\n        New primitive: blend two IDFs then compress extremes.\n        Motivation: very rare terms can dominate nDCG@10; compression tends to improve\n        ranking robustness across heterogeneous BEIR/BRIGHT corpora while preserving recall.\n        \"\"\"\n        a = EvolvedParameters.alpha\n        idf_a = ScoringPrimitives.idf_lucene(df, N)\n        idf_b = ScoringPrimitives.idf_atire(df, N)\n        mixed = a * idf_a + (1.0 - a) * idf_b\n        # soft compression of very large idf values (keeps monotonicity)\n        p = EvolvedParameters.idf_pivot\n        beta = EvolvedParameters.beta\n        return mixed / (1.0 + beta * max(0.0, mixed - p))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_tflog_bm25(tf: float, k1: float, norm: float) -> float:\n        \"\"\"\n        New primitive: apply log-TF before BM25-style saturation.\n        Helps corpora with bursty term repetition (e.g., forum/stack traces) without\n        killing signals for single occurrences.\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        t = 1.0 + math.log(tf)\n        denom = t + k1 * norm + EvolvedParameters.epsilon\n        return (t * (k1 + 1.0)) / denom\n\n    @staticmethod\n    def saturate_idf_adaptive(tf: float, k1: float, norm: float, idf: float, pivot: float) -> float:\n        \"\"\"\n        New primitive: IDF-adaptive TF saturation.\n        - Common terms: lean toward log-TF saturation (anti-burst).\n        - Rare terms: lean toward standard Lucene saturation.\n        mix = idf/(idf+pivot) in [0,1]\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        p = max(float(pivot), EvolvedParameters.epsilon)\n        mix = max(0.0, float(idf)) / (max(0.0, float(idf)) + p)\n        tf_l = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_log = ScoringPrimitives.saturate_tflog_bm25(tf, k1, norm)\n        return mix * tf_l + (1.0 - mix) * tf_log\n\n    @staticmethod\n    def saturate_idf_adaptive_vectorized(\n        tf: NDArray[np.float64],\n        k1: float,\n        norm: NDArray[np.float64],\n        idf: float,\n        pivot: float,\n    ) -> NDArray[np.float64]:\n        p = max(float(pivot), EvolvedParameters.epsilon)\n        mix = max(0.0, float(idf)) / (max(0.0, float(idf)) + p)\n        tf_l = ScoringPrimitives.saturate_lucene_vectorized(tf, k1, norm)\n        t = np.where(tf > 0, 1.0 + np.log(np.maximum(tf, 1.0)), 0.0)\n        denom = t + float(k1) * norm + EvolvedParameters.epsilon\n        tf_log = (t * (float(k1) + 1.0)) / denom\n        return mix * tf_l + (1.0 - mix) * tf_log\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        # more tunable than fixed square; tends to help nDCG@10 by preferring fuller matches\n        return coverage ** max(1.0, EvolvedParameters.cov_power)\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Term evidence IDF: Lucene is a strong default across BEIR/BRIGHT\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # Blend: IDF-adaptive saturation (bursty robustness) + length-aware salience\n        tf_sat = ScoringPrimitives.saturate_idf_adaptive(\n            tf, k1, norm, idf, EvolvedParameters.idf_pivot\n        )\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        # Mild rare-term shaping (kept small)\n        if EvolvedParameters.gamma > 0:\n            tf_comp *= (1.0 + EvolvedParameters.gamma * math.tanh(idf))\n\n        return idf * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n        matched_idf: float = 0.0,\n        total_idf: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # nDCG-friendly: one strong term match can be highly diagnostic\n        if EvolvedParameters.maxterm_gamma > 0:\n            base += EvolvedParameters.maxterm_gamma * max(term_scores)\n\n        # Recall-friendly additive priors\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n\n        # Precision-friendly bounded soft-AND (doesn't zero out partial matches)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    # For bounded coordination + priors\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    total_idf = 0.0\n    matched_idf = 0.0\n\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n        total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n            matched_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n        matched_idf=matched_idf,\n        total_idf=total_idf,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        # Match TermScorer: lucene IDF for term evidence (balanced used only for coordination)\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For bounded coordination + IDF-share prior\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n\n        qids = np.array(query_term_ids, dtype=np.int64)\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N)\n            if df_q.size\n            else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else 0.0\n\n        max_contrib = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_idf_adaptive_vectorized(\n                tf_row, k1, norms, idf, EvolvedParameters.idf_pivot\n            )\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            contrib = weight * idf * tf_comp\n            scores += contrib\n            max_contrib = np.maximum(max_contrib, contrib)\n\n        # nDCG-friendly max-term prior (mirror DocumentScorer)\n        if EvolvedParameters.maxterm_gamma > 0:\n            scores += EvolvedParameters.maxterm_gamma * max_contrib\n\n        # Additive priors\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            share = ScoringPrimitives.matched_idf_share_vectorized(matched_idf, total_idf)\n            scores += EvolvedParameters.idf_match_gamma * share\n\n        # Bounded rarity-aware coordination\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        query_terms, query_weights = QueryProcessor.process(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, w in zip(query_terms, query_weights, strict=False):\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                query_term_weights.append(float(w))\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n", "language": "python", "parent_id": "753aa0a5-e570-409e-b6e9-deb42048a5c5", "generation": 5, "timestamp": 1770175160.2327142, "iteration_found": 178, "metrics": {"bright_pony_ndcg@10": 0.10475814151795608, "bright_pony_recall@100": 0.28918901054178103, "bright_pony_index_time_ms": 6519.353761803359, "bright_pony_query_time_ms": 1328.674172051251, "beir_nfcorpus_ndcg@10": 0.31733606321749935, "beir_nfcorpus_recall@100": 0.24829977347601934, "beir_nfcorpus_index_time_ms": 14863.672779873013, "beir_nfcorpus_query_time_ms": 260.3648160584271, "beir_scifact_ndcg@10": 0.6777348079694512, "beir_scifact_recall@100": 0.9213333333333332, "beir_scifact_index_time_ms": 18696.047118864954, "beir_scifact_query_time_ms": 881.0999388806522, "beir_arguana_ndcg@10": 0.27924378180377846, "beir_arguana_recall@100": 0.9129193433261956, "beir_arguana_index_time_ms": 24047.155261039734, "beir_arguana_query_time_ms": 57890.24295890704, "bright_theoremqa_theorems_ndcg@10": 0.02628233215669716, "bright_theoremqa_theorems_recall@100": 0.13157894736842105, "bright_theoremqa_theorems_index_time_ms": 56558.601337019354, "bright_theoremqa_theorems_query_time_ms": 1988.0118328146636, "beir_scidocs_ndcg@10": 0.14614168470648656, "beir_scidocs_recall@100": 0.34251666666666664, "beir_scidocs_index_time_ms": 95590.37789003924, "beir_scidocs_query_time_ms": 6254.21960093081, "bright_economics_ndcg@10": 0.16163930932937176, "bright_economics_recall@100": 0.3948375583884944, "bright_economics_index_time_ms": 57367.57849995047, "bright_economics_query_time_ms": 5042.300674133003, "bright_biology_ndcg@10": 0.2248591673965114, "bright_biology_recall@100": 0.45054358575053516, "bright_biology_index_time_ms": 62717.72127924487, "bright_biology_query_time_ms": 4909.273393917829, "bright_earth_science_ndcg@10": 0.3416069295090866, "bright_earth_science_recall@100": 0.6244447816249088, "bright_earth_science_index_time_ms": 126888.23625911027, "bright_earth_science_query_time_ms": 9091.93548001349, "beir_fiqa_ndcg@10": 0.23098030045314655, "beir_fiqa_recall@100": 0.5322978761636169, "beir_fiqa_index_time_ms": 139401.7638177611, "beir_fiqa_query_time_ms": 10470.21958231926, "bright_stackoverflow_ndcg@10": 0.17938484327131574, "bright_stackoverflow_recall@100": 0.45073415820573803, "bright_stackoverflow_index_time_ms": 468456.8605781533, "bright_stackoverflow_query_time_ms": 22038.39446697384, "beir_trec-covid_ndcg@10": 0.6392536111383139, "beir_trec-covid_recall@100": 0.10511377094034734, "beir_trec-covid_index_time_ms": 569380.562800914, "beir_trec-covid_query_time_ms": 2916.8163328431547, "avg_ndcg@10": 0.27743508103913456, "avg_recall@100": 0.4503174004821715, "combined_score": 0.41574093659356415, "total_index_time_ms": 1640487.9313837737, "total_query_time_ms": 123071.55324984342, "total_time_ms": 1763559.484633617, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 28 lines with 31 lines\nChange 2: Replace 18 lines with 50 lines\nChange 3: Replace 4 lines with 6 lines\nChange 4: Replace 11 lines with 15 lines\nChange 5: Replace 26 lines with 36 lines", "parent_metrics": {"beir_nfcorpus_ndcg@10": 0.3196325348218937, "beir_nfcorpus_recall@100": 0.25485868225194774, "beir_nfcorpus_index_time_ms": 3501.1177089763805, "beir_nfcorpus_query_time_ms": 108.87954098870978, "beir_scifact_ndcg@10": 0.6773610477433755, "beir_scifact_recall@100": 0.9286666666666668, "beir_scifact_index_time_ms": 4730.89341702871, "beir_scifact_query_time_ms": 389.59579198854044, "bright_pony_ndcg@10": 0.10316894736189276, "bright_pony_recall@100": 0.2887149506596221, "bright_pony_index_time_ms": 1346.201958018355, "bright_pony_query_time_ms": 505.2232500165701, "beir_arguana_ndcg@10": 0.278343630861194, "beir_arguana_recall@100": 0.9143468950749465, "beir_arguana_index_time_ms": 5773.531000013463, "beir_arguana_query_time_ms": 21994.287041015923, "bright_theoremqa_theorems_ndcg@10": 0.02630385922484444, "bright_theoremqa_theorems_recall@100": 0.1513157894736842, "bright_theoremqa_theorems_index_time_ms": 10366.210124979261, "bright_theoremqa_theorems_query_time_ms": 936.7610000190325, "beir_scidocs_ndcg@10": 0.14630709390374053, "beir_scidocs_recall@100": 0.3437166666666666, "beir_scidocs_index_time_ms": 19324.792625033297, "beir_scidocs_query_time_ms": 3548.745209001936, "bright_economics_ndcg@10": 0.1648657619218878, "bright_economics_recall@100": 0.3894991123304595, "bright_economics_index_time_ms": 13793.32895798143, "bright_economics_query_time_ms": 2551.6687920317054, "bright_biology_ndcg@10": 0.2458544054150909, "bright_biology_recall@100": 0.4808024854269106, "bright_biology_index_time_ms": 14427.09879099857, "bright_biology_query_time_ms": 2448.9384579937905, "beir_fiqa_ndcg@10": 0.2285223041472186, "beir_fiqa_recall@100": 0.529234727035653, "beir_fiqa_index_time_ms": 32786.42449999461, "beir_fiqa_query_time_ms": 6414.223333005793, "bright_earth_science_ndcg@10": 0.35847734364826556, "bright_earth_science_recall@100": 0.6542188150627352, "bright_earth_science_index_time_ms": 30939.42508404143, "bright_earth_science_query_time_ms": 4134.89466602914, "bright_stackoverflow_ndcg@10": 0.18157781615268354, "bright_stackoverflow_recall@100": 0.4631522271358498, "bright_stackoverflow_index_time_ms": 98265.89766703546, "bright_stackoverflow_query_time_ms": 9259.07583301887, "beir_trec-covid_ndcg@10": 0.651351321210786, "beir_trec-covid_recall@100": 0.10681631868652841, "beir_trec-covid_index_time_ms": 132160.35229200497, "beir_trec-covid_query_time_ms": 1542.3132500145584, "avg_ndcg@10": 0.2818138388677394, "avg_recall@100": 0.4587786113726391, "combined_score": 0.4233856568716592, "total_index_time_ms": 367415.27412610594, "total_query_time_ms": 53834.60616512457, "total_time_ms": 421249.8802912305, "datasets_evaluated": 12, "datasets_failed": 0, "error": 0.0}, "island": 3}, "prompts": {"diff_user": {"system": "You are evolving a LEXICAL RETRIEVAL algorithm. The goal is discovery of new IR methods\u2014invent new algorithms, not just recombine existing primitives.\n\n## View: Primitives + Structure\nLexical retrieval in this codebase is PRIMITIVES (atoms) + STRUCTURE (how they combine).\n- Primitives: IDF, TF, saturation, length norm, aggregation (weighted sum, means, etc.).\n- Structure: how term scores become doc scores, how the pipeline is organized.\nThe seed is one structure (BM25-like). You can change parameters, add or invent new primitives, change formulas, or change structure. Prefer inventing new formulas or new primitives over only swapping which existing primitive is used where.\n\n## Evaluation Metrics\nPer dataset: nDCG@10, Recall@100, index_time_ms, query_time_ms.\nAggregate: avg_ndcg@10, avg_recall@100, combined_score = 0.8*avg_recall@100 + 0.2*avg_ndcg@10 (used to rank solutions).\nUse per-dataset metrics to find weak spots.\n\n## Evolution Targets (bm25_composable_fast.py)\n\n### 1. EvolvedParameters\nNumeric constants: k1, b, k3, delta, alpha, beta, gamma, epsilon, max_idf, min_idf. Change or add parameters via SEARCH/REPLACE.\n\n### 2. ScoringPrimitives\nAtoms: idf_* (classic, lucene, atire, bm25plus, smooth), tf_* (raw, log, double_log, boolean, augmented), saturate_* (basic, lucene, bm25, bm25l, bm25plus, log), length_norm_* (bm25, pivot, log), multiply, add, weighted_sum, geometric_mean, harmonic_mean, soft_max, query_weight_*, coverage_bonus, rarity_boost.\nYou can add new primitives with new formulas (e.g. a new saturation curve, a new IDF variant). Invent when it might help; don\u2019t limit yourself to recombining only what\u2019s there.\n\n### 3. TermScorer.score(tf, df, N, dl, avgdl)\nOne term\u2019s contribution. Evolve the formula: new combination of primitives or new math (e.g. asymmetric term importance, different saturation).\n\n### 4. DocumentScorer.score(term_scores, query_weights, matched_count, total_query_terms)\nAggregate term scores into doc score. Evolve aggregation or add terms (e.g. coverage, rarity).\n\n### 5. QueryProcessor.process(query) \u2192 (terms, weights)\nEvolve how the query is interpreted (weighting, dedup).\n\n### 6. score_kernel(query, doc_idx, corpus)\nFull pipeline. You can restructure it (e.g. different flow, use DocumentScorer here, or a different orchestration).\n\n## Guidelines\n- Invent new formulas or new primitives when it might improve retrieval; recombination alone is not the goal.\n- Keep interface compatibility: BM25(corpus).rank(query) and the Corpus/BM25 API must still work for the evaluator.\n- Preserve numerical stability (epsilon, safe division).\n- Use SEARCH/REPLACE diffs; SEARCH must exactly match the current program.\n- If you add vectorized primitives (e.g. for Corpus.idf_array / norm_array), add a *_vectorized variant in ScoringPrimitives and use it in BM25._score_candidates_vectorized so batch ranking stays consistent with score_kernel.\n", "user": "# Current Program Information\n- Fitness: 0.4234\n- Feature coordinates: \n- Focus areas: - Fitness improved: 0.4232 \u2192 0.4234\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Change 1: Replace 31 lines with 32 lines\nChange 2: Replace 35 lines with 43 lines\nChange 3: Replace 35 lines with 43 lines\n- Metrics: bright_pony_ndcg@10: 0.1029, bright_pony_recall@100: 0.2866, bright_pony_index_time_ms: 7736.4938, bright_pony_query_time_ms: 1010.0360, beir_nfcorpus_ndcg@10: 0.3201, beir_nfcorpus_recall@100: 0.2484, beir_nfcorpus_index_time_ms: 17838.7611, beir_nfcorpus_query_time_ms: 233.9148, beir_scifact_ndcg@10: 0.6770, beir_scifact_recall@100: 0.9253, beir_scifact_index_time_ms: 18457.8106, beir_scifact_query_time_ms: 815.1087, beir_arguana_ndcg@10: 0.2783, beir_arguana_recall@100: 0.9165, beir_arguana_index_time_ms: 28340.8921, beir_arguana_query_time_ms: 42648.9010, bright_theoremqa_theorems_ndcg@10: 0.0263, bright_theoremqa_theorems_recall@100: 0.1513, bright_theoremqa_theorems_index_time_ms: 56425.8816, bright_theoremqa_theorems_query_time_ms: 1452.9304, beir_scidocs_ndcg@10: 0.1462, beir_scidocs_recall@100: 0.3446, beir_scidocs_index_time_ms: 95767.7338, beir_scidocs_query_time_ms: 5317.2982, bright_economics_ndcg@10: 0.1653, bright_economics_recall@100: 0.3896, bright_economics_index_time_ms: 41583.8852, bright_economics_query_time_ms: 3944.4038, bright_biology_ndcg@10: 0.2455, bright_biology_recall@100: 0.4840, bright_biology_index_time_ms: 47315.8499, bright_biology_query_time_ms: 3745.4774, bright_earth_science_ndcg@10: 0.3620, bright_earth_science_recall@100: 0.6542, bright_earth_science_index_time_ms: 103544.7347, bright_earth_science_query_time_ms: 6528.1151, beir_fiqa_ndcg@10: 0.2292, beir_fiqa_recall@100: 0.5301, beir_fiqa_index_time_ms: 147007.1474, beir_fiqa_query_time_ms: 9247.6499, bright_stackoverflow_ndcg@10: 0.1821, bright_stackoverflow_recall@100: 0.4632, bright_stackoverflow_index_time_ms: 411059.6188, bright_stackoverflow_query_time_ms: 15316.0306, beir_trec-covid_ndcg@10: 0.6549, beir_trec-covid_recall@100: 0.1071, beir_trec-covid_index_time_ms: 582798.7503, beir_trec-covid_query_time_ms: 2379.2085, avg_ndcg@10: 0.2825, avg_recall@100: 0.4584, combined_score: 0.4232, total_index_time_ms: 1557877.5593, total_query_time_ms: 92639.0745, total_time_ms: 1650516.6338, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 2\n- Changes: Change 1: Replace 31 lines with 28 lines\nChange 2: Replace 6 lines with 5 lines\nChange 3: Replace 35 lines with 43 lines\nChange 4: Replace 43 lines with 54 lines\n- Metrics: bright_pony_ndcg@10: 0.1029, bright_pony_recall@100: 0.2866, bright_pony_index_time_ms: 7254.9727, bright_pony_query_time_ms: 997.3392, beir_nfcorpus_ndcg@10: 0.3201, beir_nfcorpus_recall@100: 0.2484, beir_nfcorpus_index_time_ms: 16443.1161, beir_nfcorpus_query_time_ms: 228.7658, beir_scifact_ndcg@10: 0.6770, beir_scifact_recall@100: 0.9253, beir_scifact_index_time_ms: 20864.2206, beir_scifact_query_time_ms: 708.3540, beir_arguana_ndcg@10: 0.2783, beir_arguana_recall@100: 0.9165, beir_arguana_index_time_ms: 29037.4196, beir_arguana_query_time_ms: 41980.3634, bright_theoremqa_theorems_ndcg@10: 0.0263, bright_theoremqa_theorems_recall@100: 0.1513, bright_theoremqa_theorems_index_time_ms: 56710.2345, bright_theoremqa_theorems_query_time_ms: 1441.0293, beir_scidocs_ndcg@10: 0.1462, beir_scidocs_recall@100: 0.3446, beir_scidocs_index_time_ms: 94442.4502, beir_scidocs_query_time_ms: 4912.4534, bright_economics_ndcg@10: 0.1653, bright_economics_recall@100: 0.3896, bright_economics_index_time_ms: 57010.1037, bright_economics_query_time_ms: 3857.0202, bright_biology_ndcg@10: 0.2455, bright_biology_recall@100: 0.4840, bright_biology_index_time_ms: 65570.9057, bright_biology_query_time_ms: 3752.4672, beir_fiqa_ndcg@10: 0.2292, beir_fiqa_recall@100: 0.5301, beir_fiqa_index_time_ms: 138113.8350, beir_fiqa_query_time_ms: 8598.8800, bright_earth_science_ndcg@10: 0.3620, bright_earth_science_recall@100: 0.6542, bright_earth_science_index_time_ms: 137335.0319, bright_earth_science_query_time_ms: 6770.7841, bright_stackoverflow_ndcg@10: 0.1821, bright_stackoverflow_recall@100: 0.4632, bright_stackoverflow_index_time_ms: 442960.6839, bright_stackoverflow_query_time_ms: 15016.0061, beir_trec-covid_ndcg@10: 0.6549, beir_trec-covid_recall@100: 0.1071, beir_trec-covid_index_time_ms: 589577.5791, beir_trec-covid_query_time_ms: 2307.1250, avg_ndcg@10: 0.2825, avg_recall@100: 0.4584, combined_score: 0.4232, total_index_time_ms: 1655320.5529, total_query_time_ms: 90570.5875, total_time_ms: 1745891.1404, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n### Attempt 1\n- Changes: Change 1: Replace:\n  class EvolvedParameters:\n      \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n      k1: float = 0.9\n      b: float = 0.4\n      k3: float = 8.0\n      delta: float = 0.5\n  \n      # Restored high-performing knobs (Program 1/2 lineage)\n      alpha: float = 0.6   # TF blending weight\n      beta: float = 0.35   # coordination strength\n      gamma: float = 0.05  # mild rare-term shaping\n  \n      # Keep experimental knobs defined (prevents runtime failure if referenced)\n      idf_pivot: float = 3.0\n      cov_power: float = 2.0\n  \n      epsilon: float = 1e-9\n      max_idf: float = float(\"inf\")\n      min_idf: float = 0.0\nwith:\n  class EvolvedParameters:\n      \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n      k1: float = 0.9\n      b: float = 0.4\n      k3: float = 8.0\n      delta: float = 0.5\n  \n      # TF blending weight (kept)\n      alpha: float = 0.6\n  \n      # Replace harsh coordination with bounded rarity-mass soft-AND (recall-safe)\n      coord_beta: float = 0.55\n      coord_floor: float = 0.35\n  \n      # Additive priors (recall-friendly)\n      cov_gamma: float = 0.10       # small additive term-coverage bump\n      idf_match_gamma: float = 0.08 # additive matched-IDF-share bump\n  \n      # Keep mild rare-term shaping but do it via IDF choice rather than tf multiplier\n      gamma: float = 0.0\n  \n      # Keep experimental knobs defined (prevents runtime failure if referenced)\n      idf_pivot: float = 3.0\n      cov_power: float = 2.0\n  \n      epsilon: float = 1e-9\n      max_idf: float = float(\"inf\")\n      min_idf: float = 0.0\nChange 2: Replace:\n  class ScoringPrimitives:\n      \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n  \n      @staticmethod\n      def idf_classic(df: float, N: int) -> float:\n          return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\nwith:\n  class ScoringPrimitives:\n      \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n  \n      @staticmethod\n      def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n          \"\"\"Bounded query-IDF mass coverage in [0,1]. Used as additive recall-friendly prior.\"\"\"\n          t = max(total_idf, EvolvedParameters.epsilon)\n          return max(0.0, min(1.0, matched_idf / t))\n  \n      @staticmethod\n      def matched_idf_share_vectorized(\n          matched_idf: NDArray[np.float64], total_idf: float\n      ) -> NDArray[np.float64]:\n          t = max(float(total_idf), EvolvedParameters.epsilon)\n          return np.clip(matched_idf / t, 0.0, 1.0)\n  \n      @staticmethod\n      def coord_rarity_aware(\n          matched_rarity: float, total_rarity: float, beta: float, floor: float\n      ) -> float:\n          \"\"\"\n          Bounded rarity-mass soft-AND multiplier:\n            floor + (1-floor) * (matched_rarity/total_rarity)^beta\n          \"\"\"\n          t = max(total_rarity, EvolvedParameters.epsilon)\n          frac = max(0.0, matched_rarity) / t\n          f = float(max(0.0, min(1.0, floor)))\n          return f + (1.0 - f) * (frac ** max(0.0, beta))\n  \n      @staticmethod\n  ... (14 more lines)\nChange 3: Replace:\n  # Robust IDF\n          idf = ScoringPrimitives.idf_balanced(df, N)\n          idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\nwith:\n  # Term evidence IDF: Lucene is a strong default across BEIR/BRIGHT\n          idf = ScoringPrimitives.idf_lucene(df, N)\n          idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\nChange 4: Replace:\n  class DocumentScorer:\n      \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n  \n      @staticmethod\n      def score(\n          term_scores: list[float],\n          query_weights: list[float],\n          matched_count: int,\n          total_query_terms: int,\n      ) -> float:\n          if not term_scores:\n              return 0.0\n  \n          base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n  \n          coord = ScoringPrimitives.coord_factor(\n              matched_count, total_query_terms, EvolvedParameters.beta\n          )\n          return base * coord\nwith:\n  class DocumentScorer:\n      \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n  \n      @staticmethod\n      def score(\n          term_scores: list[float],\n          query_weights: list[float],\n          matched_count: int,\n          total_query_terms: int,\n          matched_rarity: float = 0.0,\n          total_rarity: float = 0.0,\n          matched_idf: float = 0.0,\n          total_idf: float = 0.0,\n      ) -> float:\n          if not term_scores:\n              return 0.0\n  \n          base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n  \n          # Recall-friendly additive priors\n          if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n              c = float(matched_count) / max(1.0, float(total_query_terms))\n              base += EvolvedParameters.cov_gamma * (c * c)\n  \n          if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n              base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                  matched_idf, total_idf\n              )\n  \n          # Precision-friendly bounded soft-AND (doesn't zero out partial matches)\n  ... (9 more lines)\nChange 5: Replace:\n  def score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n      if not query:\n          return 0.0\n      query_terms, query_weights = QueryProcessor.process(query)\n      if not query_terms:\n          return 0.0\n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      dl = corpus.doc_lengths[doc_idx]\n      avgdl = corpus.avgdl\n      N = corpus.N\n  \n      term_scores: list[float] = []\n      used_weights: list[float] = []\n      matched_count = 0\n  \n      for term, w in zip(query_terms, query_weights, strict=False):\n          tf = doc_tf.get(term, 0)\n          if tf > 0:\n              matched_count += 1\n              df = corpus.get_df(term)\n              term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n              used_weights.append(w)\n  \n      return DocumentScorer.score(term_scores, used_weights, matched_count, len(query_terms))\nwith:\n  def score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n      \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n      if not query:\n          return 0.0\n      query_terms, query_weights = QueryProcessor.process(query)\n      if not query_terms:\n          return 0.0\n      doc_tf = corpus.get_term_frequencies(doc_idx)\n      dl = corpus.doc_lengths[doc_idx]\n      avgdl = corpus.avgdl\n      N = corpus.N\n  \n      term_scores: list[float] = []\n      used_weights: list[float] = []\n      matched_count = 0\n  \n      # For bounded coordination + priors\n      total_rarity = 0.0\n      matched_rarity = 0.0\n      total_idf = 0.0\n      matched_idf = 0.0\n  \n      for term in query_terms:\n          df = corpus.get_df(term)\n          total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n          total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n  \n      for term, w in zip(query_terms, query_weights, strict=False):\n          tf = doc_tf.get(term, 0)\n          if tf > 0:\n  ... (18 more lines)\nChange 6: Replace:\n  # Match TermScorer: balanced IDF for robustness across datasets\n          self.idf_array = ScoringPrimitives.idf_balanced_vectorized(self._df, self.N)\nwith:\n  # Match TermScorer: lucene IDF for term evidence (balanced used only for coordination)\n          self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\nChange 7: Replace:\n  scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          k1 = EvolvedParameters.k1\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf = float(self.corpus.idf_array[term_id])\n              if idf <= 0:\n                  continue\n              idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n  \n              weight = query_term_weights[i] if query_term_weights is not None else 1.0\n  \n              tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n              present = tf_row > 0\n              matched += present.astype(np.float64)\n  \n              tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n              tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n              tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n  \n              if EvolvedParameters.gamma > 0:\n                  tf_comp = tf_comp * (1.0 + EvolvedParameters.gamma * np.tanh(idf))\n  \n              scores += weight * idf * tf_comp\n  \n          # Apply coordination multiplier (match DocumentScorer)\n          if EvolvedParameters.beta > 0 and len(query_term_ids) > 0:\n              total_q = float(len(query_term_ids))\n              scores *= np.power(np.maximum(matched / max(total_q, 1.0), 0.0), EvolvedParameters.beta)\n  ... (2 more lines)\nwith:\n  scores = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          # For bounded coordination + IDF-share prior\n          matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n          matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n  \n          k1 = EvolvedParameters.k1\n  \n          qids = np.array(query_term_ids, dtype=np.int64)\n          df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n  \n          rarity_q = (\n              ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N)\n              if df_q.size\n              else np.array([], dtype=np.float64)\n          )\n          rarity_q = np.maximum(rarity_q, 0.0)\n          total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n  \n          total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else...\n  \n          for i, term_id in enumerate(query_term_ids):\n              idf = float(self.corpus.idf_array[term_id])\n              if idf <= 0:\n                  continue\n              idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n  \n              weight = query_term_weights[i] if query_term_weights is not None else 1.0\n  \n  ... (33 more lines)\n- Metrics: beir_nfcorpus_ndcg@10: 0.3196, beir_nfcorpus_recall@100: 0.2549, beir_nfcorpus_index_time_ms: 3501.1177, beir_nfcorpus_query_time_ms: 108.8795, beir_scifact_ndcg@10: 0.6774, beir_scifact_recall@100: 0.9287, beir_scifact_index_time_ms: 4730.8934, beir_scifact_query_time_ms: 389.5958, bright_pony_ndcg@10: 0.1032, bright_pony_recall@100: 0.2887, bright_pony_index_time_ms: 1346.2020, bright_pony_query_time_ms: 505.2233, beir_arguana_ndcg@10: 0.2783, beir_arguana_recall@100: 0.9143, beir_arguana_index_time_ms: 5773.5310, beir_arguana_query_time_ms: 21994.2870, bright_theoremqa_theorems_ndcg@10: 0.0263, bright_theoremqa_theorems_recall@100: 0.1513, bright_theoremqa_theorems_index_time_ms: 10366.2101, bright_theoremqa_theorems_query_time_ms: 936.7610, beir_scidocs_ndcg@10: 0.1463, beir_scidocs_recall@100: 0.3437, beir_scidocs_index_time_ms: 19324.7926, beir_scidocs_query_time_ms: 3548.7452, bright_economics_ndcg@10: 0.1649, bright_economics_recall@100: 0.3895, bright_economics_index_time_ms: 13793.3290, bright_economics_query_time_ms: 2551.6688, bright_biology_ndcg@10: 0.2459, bright_biology_recall@100: 0.4808, bright_biology_index_time_ms: 14427.0988, bright_biology_query_time_ms: 2448.9385, beir_fiqa_ndcg@10: 0.2285, beir_fiqa_recall@100: 0.5292, beir_fiqa_index_time_ms: 32786.4245, beir_fiqa_query_time_ms: 6414.2233, bright_earth_science_ndcg@10: 0.3585, bright_earth_science_recall@100: 0.6542, bright_earth_science_index_time_ms: 30939.4251, bright_earth_science_query_time_ms: 4134.8947, bright_stackoverflow_ndcg@10: 0.1816, bright_stackoverflow_recall@100: 0.4632, bright_stackoverflow_index_time_ms: 98265.8977, bright_stackoverflow_query_time_ms: 9259.0758, beir_trec-covid_ndcg@10: 0.6514, beir_trec-covid_recall@100: 0.1068, beir_trec-covid_index_time_ms: 132160.3523, beir_trec-covid_query_time_ms: 1542.3133, avg_ndcg@10: 0.2818, avg_recall@100: 0.4588, combined_score: 0.4234, total_index_time_ms: 367415.2741, total_query_time_ms: 53834.6062, total_time_ms: 421249.8803, datasets_evaluated: 12.0000, datasets_failed: 0.0000, error: 0.0000\n- Outcome: Mixed results\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.4234)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending weight (kept)\n    alpha: float = 0.6\n\n    # Replace harsh coordination with bounded rarity-mass soft-AND (recall-safe)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Additive priors (recall-friendly)\n    cov_gamma: float = 0.10       # small additive term-coverage bump\n    idf_match_gamma: float = 0.08 # additive matched-IDF-share bump\n\n    # Keep mild rare-term shaping but do it via IDF choice rather than tf multiplier\n    gamma: float = 0.0\n\n    # Keep experimental knobs defined (prevents runtime failure if referenced)\n    idf_pivot: float = 3.0\n    cov_power: float = 2.0\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n        \"\"\"Bounded query-IDF mass coverage in [0,1]. Used as additive recall-friendly prior.\"\"\"\n        t = max(total_idf, EvolvedParameters.epsilon)\n        return max(0.0, min(1.0, matched_idf / t))\n\n    @staticmethod\n    def matched_idf_share_vectorized(\n        matched_idf: NDArray[np.float64], total_idf: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_idf), EvolvedParameters.epsilon)\n        return np.clip(matched_idf / t, 0.0, 1.0)\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        \"\"\"\n        Bounded rarity-mass soft-AND multiplier:\n          floor + (1-floor) * (matched_rarity/total_rarity)^beta\n        \"\"\"\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coord_factor(matched_terms: int, total_query_terms: int, beta: float) -> float:\n        \"\"\"\n        Soft coordination factor: (matched/total)^beta. Multiplies score.\n        \"\"\"\n        if total_query_terms <= 0 or matched_terms <= 0:\n            return 0.0\n        return (matched_terms / total_query_terms) ** max(beta, 0.0)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def idf_mixed_compressed(df: float, N: int) -> float:\n        \"\"\"\n        New primitive: blend two IDFs then compress extremes.\n        Motivation: very rare terms can dominate nDCG@10; compression tends to improve\n        ranking robustness across heterogeneous BEIR/BRIGHT corpora while preserving recall.\n        \"\"\"\n        a = EvolvedParameters.alpha\n        idf_a = ScoringPrimitives.idf_lucene(df, N)\n        idf_b = ScoringPrimitives.idf_atire(df, N)\n        mixed = a * idf_a + (1.0 - a) * idf_b\n        # soft compression of very large idf values (keeps monotonicity)\n        p = EvolvedParameters.idf_pivot\n        beta = EvolvedParameters.beta\n        return mixed / (1.0 + beta * max(0.0, mixed - p))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_tflog_bm25(tf: float, k1: float, norm: float) -> float:\n        \"\"\"\n        New primitive: apply log-TF before BM25-style saturation.\n        Helps corpora with bursty term repetition (e.g., forum/stack traces) without\n        killing signals for single occurrences.\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        t = 1.0 + math.log(tf)\n        denom = t + k1 * norm + EvolvedParameters.epsilon\n        return (t * (k1 + 1.0)) / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        # more tunable than fixed square; tends to help nDCG@10 by preferring fuller matches\n        return coverage ** max(1.0, EvolvedParameters.cov_power)\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Term evidence IDF: Lucene is a strong default across BEIR/BRIGHT\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # Blend: classic Lucene BM25-like sat + length-aware salience\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        # Mild rare-term shaping (kept small)\n        if EvolvedParameters.gamma > 0:\n            tf_comp *= (1.0 + EvolvedParameters.gamma * math.tanh(idf))\n\n        return idf * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n        matched_idf: float = 0.0,\n        total_idf: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # Recall-friendly additive priors\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n\n        # Precision-friendly bounded soft-AND (doesn't zero out partial matches)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    # For bounded coordination + priors\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    total_idf = 0.0\n    matched_idf = 0.0\n\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n        total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n            matched_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n        matched_idf=matched_idf,\n        total_idf=total_idf,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        # Match TermScorer: lucene IDF for term evidence (balanced used only for coordination)\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For bounded coordination + IDF-share prior\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n\n        qids = np.array(query_term_ids, dtype=np.int64)\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N)\n            if df_q.size\n            else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            scores += weight * idf * tf_comp\n\n        # Additive priors\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            share = ScoringPrimitives.matched_idf_share_vectorized(matched_idf, total_idf)\n            scores += EvolvedParameters.idf_match_gamma * share\n\n        # Bounded rarity-aware coordination\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        query_terms, query_weights = QueryProcessor.process(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, w in zip(query_terms, query_weights, strict=False):\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                query_term_weights.append(float(w))\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nKey features: Performs well on beir_nfcorpus_ndcg@10 (0.3196), Performs well on beir_nfcorpus_recall@100 (0.2549), Performs well on beir_nfcorpus_index_time_ms (3501.1177), Performs well on beir_nfcorpus_query_time_ms (108.8795), Performs well on beir_scifact_ndcg@10 (0.6774), Performs well on beir_scifact_recall@100 (0.9287), Performs well on beir_scifact_index_time_ms (4730.8934), Performs well on beir_scifact_query_time_ms (389.5958), Performs well on bright_pony_ndcg@10 (0.1032), Performs well on bright_pony_recall@100 (0.2887), Performs well on bright_pony_index_time_ms (1346.2020), Performs well on bright_pony_query_time_ms (505.2233), Performs well on beir_arguana_ndcg@10 (0.2783), Performs well on beir_arguana_recall@100 (0.9143), Performs well on beir_arguana_index_time_ms (5773.5310), Performs well on beir_arguana_query_time_ms (21994.2870), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0263), Performs well on bright_theoremqa_theorems_recall@100 (0.1513), Performs well on bright_theoremqa_theorems_index_time_ms (10366.2101), Performs well on bright_theoremqa_theorems_query_time_ms (936.7610), Performs well on beir_scidocs_ndcg@10 (0.1463), Performs well on beir_scidocs_recall@100 (0.3437), Performs well on beir_scidocs_index_time_ms (19324.7926), Performs well on beir_scidocs_query_time_ms (3548.7452), Performs well on bright_economics_ndcg@10 (0.1649), Performs well on bright_economics_recall@100 (0.3895), Performs well on bright_economics_index_time_ms (13793.3290), Performs well on bright_economics_query_time_ms (2551.6688), Performs well on bright_biology_ndcg@10 (0.2459), Performs well on bright_biology_recall@100 (0.4808), Performs well on bright_biology_index_time_ms (14427.0988), Performs well on bright_biology_query_time_ms (2448.9385), Performs well on beir_fiqa_ndcg@10 (0.2285), Performs well on beir_fiqa_recall@100 (0.5292), Performs well on beir_fiqa_index_time_ms (32786.4245), Performs well on beir_fiqa_query_time_ms (6414.2233), Performs well on bright_earth_science_ndcg@10 (0.3585), Performs well on bright_earth_science_recall@100 (0.6542), Performs well on bright_earth_science_index_time_ms (30939.4251), Performs well on bright_earth_science_query_time_ms (4134.8947), Performs well on bright_stackoverflow_ndcg@10 (0.1816), Performs well on bright_stackoverflow_recall@100 (0.4632), Performs well on bright_stackoverflow_index_time_ms (98265.8977), Performs well on bright_stackoverflow_query_time_ms (9259.0758), Performs well on beir_trec-covid_ndcg@10 (0.6514), Performs well on beir_trec-covid_recall@100 (0.1068), Performs well on beir_trec-covid_index_time_ms (132160.3523), Performs well on beir_trec-covid_query_time_ms (1542.3133), Performs well on avg_ndcg@10 (0.2818), Performs well on avg_recall@100 (0.4588), Performs well on combined_score (0.4234), Performs well on total_index_time_ms (367415.2741), Performs well on total_query_time_ms (53834.6062), Performs well on total_time_ms (421249.8803), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 2 (Score: 0.4232)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending (helps long/noisy docs)\n    alpha: float = 0.6\n\n    # Best-lineage additive priors (small, recall-safe)\n    cov_gamma: float = 0.10\n    idf_match_gamma: float = 0.08\n\n    # nDCG-friendly: reward a single very strong matching term (cheap, additive)\n    maxterm_gamma: float = 0.03\n\n    # Keep frequent-term rescue defined but disable by default (avoid regression)\n    common_strength: float = 0.0\n    common_pivot: float = 2.5\n\n    # Rarity-aware coordination (bounded soft-AND multiplier)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n        t = max(total_idf, EvolvedParameters.epsilon)\n        return max(0.0, min(1.0, matched_idf / t))\n\n    @staticmethod\n    def matched_idf_share_vectorized(\n        matched_idf: NDArray[np.float64], total_idf: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_idf), EvolvedParameters.epsilon)\n        return np.clip(matched_idf / t, 0.0, 1.0)\n\n    @staticmethod\n    def commonness_rescue(idf: float, strength: float, pivot: float) -> float:\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def commonness_rescue_vectorized(\n        idf: NDArray[np.float64], strength: float, pivot: float\n    ) -> NDArray[np.float64]:\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = max(0.0, min(1.0, floor))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare term spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def idf_mixed_compressed(df: float, N: int) -> float:\n        \"\"\"\n        New primitive: blend two IDFs then compress extremes.\n        Motivation: very rare terms can dominate nDCG@10; compression tends to improve\n        ranking robustness across heterogeneous BEIR/BRIGHT corpora while preserving recall.\n        \"\"\"\n        a = EvolvedParameters.alpha\n        idf_a = ScoringPrimitives.idf_lucene(df, N)\n        idf_b = ScoringPrimitives.idf_atire(df, N)\n        mixed = a * idf_a + (1.0 - a) * idf_b\n        # soft compression of very large idf values (keeps monotonicity)\n        p = EvolvedParameters.idf_pivot\n        beta = EvolvedParameters.beta\n        return mixed / (1.0 + beta * max(0.0, mixed - p))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_tflog_bm25(tf: float, k1: float, norm: float) -> float:\n        \"\"\"\n        New primitive: apply log-TF before BM25-style saturation.\n        Helps corpora with bursty term repetition (e.g., forum/stack traces) without\n        killing signals for single occurrences.\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        t = 1.0 + math.log(tf)\n        denom = t + k1 * norm + EvolvedParameters.epsilon\n        return (t * (k1 + 1.0)) / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        c = matched_terms / total_query_terms\n        return c * c\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Strong default IDF (better core than idf_balanced in your history)\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # TF: blend saturation with length-aware salience\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        rescue = ScoringPrimitives.commonness_rescue(\n            idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n        )\n        return idf * rescue * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n        matched_idf: float = 0.0,\n        total_idf: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # nDCG-friendly: one strong term match can be highly diagnostic\n        if EvolvedParameters.maxterm_gamma > 0:\n            base += EvolvedParameters.maxterm_gamma * max(term_scores)\n\n        # Recall-friendly additive priors\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n\n        # Precision-friendly bounded soft-AND (doesn't zero out partial matches)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    total_idf = 0.0\n    matched_idf = 0.0\n\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n        total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n            matched_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n        matched_idf=matched_idf,\n        total_idf=total_idf,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        # Keep idf_array aligned with TermScorer for consistency across scoring paths\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # rarity-mass tracking for bounded coordination\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # matched lucene-idf mass for additive idf-share prior\n        matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n        qids = np.array(query_term_ids, dtype=np.int64)\n\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N) if df_q.size else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else 0.0\n\n        max_contrib = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            rescue = ScoringPrimitives.commonness_rescue(\n                idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n            )\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            contrib = weight * idf * rescue * tf_comp\n            scores += contrib\n            max_contrib = np.maximum(max_contrib, contrib)\n\n        # nDCG-friendly max-term booster (mirror DocumentScorer)\n        if EvolvedParameters.maxterm_gamma > 0:\n            scores += EvolvedParameters.maxterm_gamma * max_contrib\n\n        # Additive coverage prior (mirror DocumentScorer)\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n\n        # Additive matched-idf-share prior\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            share = ScoringPrimitives.matched_idf_share_vectorized(matched_idf, total_idf)\n            scores += EvolvedParameters.idf_match_gamma * share\n\n        # Bounded rarity-aware coordination multiplier\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        query_terms, query_weights = QueryProcessor.process(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, w in zip(query_terms, query_weights, strict=False):\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                query_term_weights.append(float(w))\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1029), Performs well on bright_pony_recall@100 (0.2866), Performs well on bright_pony_index_time_ms (7254.9727), Performs well on bright_pony_query_time_ms (997.3392), Performs well on beir_nfcorpus_ndcg@10 (0.3201), Performs well on beir_nfcorpus_recall@100 (0.2484), Performs well on beir_nfcorpus_index_time_ms (16443.1161), Performs well on beir_nfcorpus_query_time_ms (228.7658), Performs well on beir_scifact_ndcg@10 (0.6770), Performs well on beir_scifact_recall@100 (0.9253), Performs well on beir_scifact_index_time_ms (20864.2206), Performs well on beir_scifact_query_time_ms (708.3540), Performs well on beir_arguana_ndcg@10 (0.2783), Performs well on beir_arguana_recall@100 (0.9165), Performs well on beir_arguana_index_time_ms (29037.4196), Performs well on beir_arguana_query_time_ms (41980.3634), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0263), Performs well on bright_theoremqa_theorems_recall@100 (0.1513), Performs well on bright_theoremqa_theorems_index_time_ms (56710.2345), Performs well on bright_theoremqa_theorems_query_time_ms (1441.0293), Performs well on beir_scidocs_ndcg@10 (0.1462), Performs well on beir_scidocs_recall@100 (0.3446), Performs well on beir_scidocs_index_time_ms (94442.4502), Performs well on beir_scidocs_query_time_ms (4912.4534), Performs well on bright_economics_ndcg@10 (0.1653), Performs well on bright_economics_recall@100 (0.3896), Performs well on bright_economics_index_time_ms (57010.1037), Performs well on bright_economics_query_time_ms (3857.0202), Performs well on bright_biology_ndcg@10 (0.2455), Performs well on bright_biology_recall@100 (0.4840), Performs well on bright_biology_index_time_ms (65570.9057), Performs well on bright_biology_query_time_ms (3752.4672), Performs well on beir_fiqa_ndcg@10 (0.2292), Performs well on beir_fiqa_recall@100 (0.5301), Performs well on beir_fiqa_index_time_ms (138113.8350), Performs well on beir_fiqa_query_time_ms (8598.8800), Performs well on bright_earth_science_ndcg@10 (0.3620), Performs well on bright_earth_science_recall@100 (0.6542), Performs well on bright_earth_science_index_time_ms (137335.0319), Performs well on bright_earth_science_query_time_ms (6770.7841), Performs well on bright_stackoverflow_ndcg@10 (0.1821), Performs well on bright_stackoverflow_recall@100 (0.4632), Performs well on bright_stackoverflow_index_time_ms (442960.6839), Performs well on bright_stackoverflow_query_time_ms (15016.0061), Performs well on beir_trec-covid_ndcg@10 (0.6549), Performs well on beir_trec-covid_recall@100 (0.1071), Performs well on beir_trec-covid_index_time_ms (589577.5791), Performs well on beir_trec-covid_query_time_ms (2307.1250), Performs well on avg_ndcg@10 (0.2825), Performs well on avg_recall@100 (0.4584), Performs well on combined_score (0.4232), Performs well on total_index_time_ms (1655320.5529), Performs well on total_query_time_ms (90570.5875), Performs well on total_time_ms (1745891.1404), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n### Program 3 (Score: 0.4232)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending (helps long/noisy docs)\n    alpha: float = 0.6\n\n    # Best-lineage additive priors (small, recall-safe)\n    cov_gamma: float = 0.10\n    idf_match_gamma: float = 0.08\n\n    # nDCG-friendly: reward a single very strong matching term (additive, cheap)\n    maxterm_gamma: float = 0.03\n\n    # Frequent-term rescue kept but disabled by default (often hurts early precision)\n    common_strength: float = 0.0\n    common_pivot: float = 2.5\n\n    # Rarity-aware coordination (bounded soft-AND multiplier)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Keep these to avoid runtime errors if referenced elsewhere\n    idf_pivot: float = 3.0\n    beta: float = 1.0  # used by idf_mixed_compressed if enabled later\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n        t = max(total_idf, EvolvedParameters.epsilon)\n        return max(0.0, min(1.0, matched_idf / t))\n\n    @staticmethod\n    def matched_idf_share_vectorized(\n        matched_idf: NDArray[np.float64], total_idf: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_idf), EvolvedParameters.epsilon)\n        return np.clip(matched_idf / t, 0.0, 1.0)\n\n    @staticmethod\n    def commonness_rescue(idf: float, strength: float, pivot: float) -> float:\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def commonness_rescue_vectorized(\n        idf: NDArray[np.float64], strength: float, pivot: float\n    ) -> NDArray[np.float64]:\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = max(0.0, min(1.0, floor))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare term spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def idf_mixed_compressed(df: float, N: int) -> float:\n        \"\"\"\n        New primitive: blend two IDFs then compress extremes.\n        Motivation: very rare terms can dominate nDCG@10; compression tends to improve\n        ranking robustness across heterogeneous BEIR/BRIGHT corpora while preserving recall.\n        \"\"\"\n        a = EvolvedParameters.alpha\n        idf_a = ScoringPrimitives.idf_lucene(df, N)\n        idf_b = ScoringPrimitives.idf_atire(df, N)\n        mixed = a * idf_a + (1.0 - a) * idf_b\n        # soft compression of very large idf values (keeps monotonicity)\n        p = EvolvedParameters.idf_pivot\n        beta = EvolvedParameters.beta\n        return mixed / (1.0 + beta * max(0.0, mixed - p))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_tflog_bm25(tf: float, k1: float, norm: float) -> float:\n        \"\"\"\n        New primitive: apply log-TF before BM25-style saturation.\n        Helps corpora with bursty term repetition (e.g., forum/stack traces) without\n        killing signals for single occurrences.\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        t = 1.0 + math.log(tf)\n        denom = t + k1 * norm + EvolvedParameters.epsilon\n        return (t * (k1 + 1.0)) / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        # more tunable than fixed square; tends to help nDCG@10 by preferring fuller matches\n        return coverage ** max(1.0, EvolvedParameters.cov_power)\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Strong default IDF (better core than idf_balanced in your history)\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # TF: blend saturation with length-aware salience\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        rescue = ScoringPrimitives.commonness_rescue(\n            idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n        )\n        return idf * rescue * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n        matched_idf: float = 0.0,\n        total_idf: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # nDCG-friendly: one strong term match can be highly diagnostic\n        if EvolvedParameters.maxterm_gamma > 0:\n            base += EvolvedParameters.maxterm_gamma * max(term_scores)\n\n        # Recall-friendly additive priors (stable across datasets)\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n\n        # Precision-friendly bounded soft-AND (doesn't zero out partial matches)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    total_idf = 0.0\n    matched_idf = 0.0\n\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n        total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n            matched_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n        matched_idf=matched_idf,\n        total_idf=total_idf,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        # Keep idf_array aligned with TermScorer for consistency across scoring paths\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # rarity-mass tracking for bounded coordination\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # matched lucene-idf mass for additive idf-share prior\n        matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n        qids = np.array(query_term_ids, dtype=np.int64)\n\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N) if df_q.size else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else 0.0\n\n        max_contrib = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            rescue = ScoringPrimitives.commonness_rescue(\n                idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n            )\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            contrib = weight * idf * rescue * tf_comp\n            scores += contrib\n            max_contrib = np.maximum(max_contrib, contrib)\n\n        # nDCG-friendly max-term booster (mirror DocumentScorer)\n        if EvolvedParameters.maxterm_gamma > 0:\n            scores += EvolvedParameters.maxterm_gamma * max_contrib\n\n        # Additive coverage prior (mirror DocumentScorer)\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n\n        # Additive matched-idf-share prior\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            share = ScoringPrimitives.matched_idf_share_vectorized(matched_idf, total_idf)\n            scores += EvolvedParameters.idf_match_gamma * share\n\n        # Bounded rarity-aware coordination multiplier\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity, total_rarity, EvolvedParameters.coord_beta, EvolvedParameters.coord_floor\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        query_terms, query_weights = QueryProcessor.process(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, w in zip(query_terms, query_weights, strict=False):\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                query_term_weights.append(float(w))\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nKey features: Performs well on bright_pony_ndcg@10 (0.1029), Performs well on bright_pony_recall@100 (0.2866), Performs well on bright_pony_index_time_ms (7736.4938), Performs well on bright_pony_query_time_ms (1010.0360), Performs well on beir_nfcorpus_ndcg@10 (0.3201), Performs well on beir_nfcorpus_recall@100 (0.2484), Performs well on beir_nfcorpus_index_time_ms (17838.7611), Performs well on beir_nfcorpus_query_time_ms (233.9148), Performs well on beir_scifact_ndcg@10 (0.6770), Performs well on beir_scifact_recall@100 (0.9253), Performs well on beir_scifact_index_time_ms (18457.8106), Performs well on beir_scifact_query_time_ms (815.1087), Performs well on beir_arguana_ndcg@10 (0.2783), Performs well on beir_arguana_recall@100 (0.9165), Performs well on beir_arguana_index_time_ms (28340.8921), Performs well on beir_arguana_query_time_ms (42648.9010), Performs well on bright_theoremqa_theorems_ndcg@10 (0.0263), Performs well on bright_theoremqa_theorems_recall@100 (0.1513), Performs well on bright_theoremqa_theorems_index_time_ms (56425.8816), Performs well on bright_theoremqa_theorems_query_time_ms (1452.9304), Performs well on beir_scidocs_ndcg@10 (0.1462), Performs well on beir_scidocs_recall@100 (0.3446), Performs well on beir_scidocs_index_time_ms (95767.7338), Performs well on beir_scidocs_query_time_ms (5317.2982), Performs well on bright_economics_ndcg@10 (0.1653), Performs well on bright_economics_recall@100 (0.3896), Performs well on bright_economics_index_time_ms (41583.8852), Performs well on bright_economics_query_time_ms (3944.4038), Performs well on bright_biology_ndcg@10 (0.2455), Performs well on bright_biology_recall@100 (0.4840), Performs well on bright_biology_index_time_ms (47315.8499), Performs well on bright_biology_query_time_ms (3745.4774), Performs well on bright_earth_science_ndcg@10 (0.3620), Performs well on bright_earth_science_recall@100 (0.6542), Performs well on bright_earth_science_index_time_ms (103544.7347), Performs well on bright_earth_science_query_time_ms (6528.1151), Performs well on beir_fiqa_ndcg@10 (0.2292), Performs well on beir_fiqa_recall@100 (0.5301), Performs well on beir_fiqa_index_time_ms (147007.1474), Performs well on beir_fiqa_query_time_ms (9247.6499), Performs well on bright_stackoverflow_ndcg@10 (0.1821), Performs well on bright_stackoverflow_recall@100 (0.4632), Performs well on bright_stackoverflow_index_time_ms (411059.6188), Performs well on bright_stackoverflow_query_time_ms (15316.0306), Performs well on beir_trec-covid_ndcg@10 (0.6549), Performs well on beir_trec-covid_recall@100 (0.1071), Performs well on beir_trec-covid_index_time_ms (582798.7503), Performs well on beir_trec-covid_query_time_ms (2379.2085), Performs well on avg_ndcg@10 (0.2825), Performs well on avg_recall@100 (0.4584), Performs well on combined_score (0.4232), Performs well on total_index_time_ms (1557877.5593), Performs well on total_query_time_ms (92639.0745), Performs well on total_time_ms (1650516.6338), Performs well on datasets_evaluated (12.0000), Performs well on datasets_failed (0.0000), Performs well on error (0.0000)\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 0.4231)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending (kept)\n    alpha: float = 0.6\n\n    # Bounded rarity-mass coordination (recall-safe soft-AND)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Additive priors (recall-friendly)\n    cov_gamma: float = 0.10\n    idf_match_gamma: float = 0.08\n\n    # nDCG-friendly: reward a single very strong matching term (cheap, additive)\n    maxterm_gamma: float = 0.03\n\n    # New: IDF-adaptive presence prior (boost docs that contain rare query terms at least once)\n    pres_gamma: float = 0.025   # overall strength (keep small)\n    pres_pivot: float = 2.5     # IDF scale where presence becomes meaningful\n    pres_tau: float = 4.0       # query-length gate pivot (short queries get more benefit)\n\n    # Keep mild rare-term shaping off (lucene idf already strong)\n    gamma: float = 0.0\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n        t = max(total_idf, EvolvedParameters.epsilon)\n        return max(0.0, min(1.0, matched_idf / t))\n\n    @staticmethod\n    def matched_idf_share_vectorized(\n        matched_idf: NDArray[np.float64], total_idf: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_idf), EvolvedParameters.epsilon)\n        return np.clip(matched_idf / t, 0.0, 1.0)\n\n    @staticmethod\n    def short_query_gate(q_len: int, tau: float) -> float:\n        \"\"\"\n        Smooth gate in (0,1]: ~1 for short queries, decays for longer queries.\n        \"\"\"\n        t = max(float(tau), 1.0)\n        q = max(1.0, float(q_len))\n        return t / (t + max(0.0, q - t))\n\n    @staticmethod\n    def rarity_presence_weight(idf: float, pivot: float) -> float:\n        \"\"\"\n        Bounded [0,1) presence weight that rises with IDF:\n          w = idf / (idf + pivot)\n        \"\"\"\n        p = max(float(pivot), EvolvedParameters.epsilon)\n        x = max(0.0, float(idf))\n        return x / (x + p)\n\n    @staticmethod\n    def rarity_presence_weight_vectorized(\n        idf: NDArray[np.float64], pivot: float\n    ) -> NDArray[np.float64]:\n        p = max(float(pivot), EvolvedParameters.epsilon)\n        x = np.maximum(idf, 0.0)\n        return x / (x + p)\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare term spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coord_factor(matched_terms: int, total_query_terms: int, beta: float) -> float:\n        \"\"\"\n        Soft coordination factor: (matched/total)^beta. Multiplies score (better than additive bonus).\n        \"\"\"\n        if total_query_terms <= 0 or matched_terms <= 0:\n            return 0.0\n        return (matched_terms / total_query_terms) ** max(beta, 0.0)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        return coverage * coverage\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Term evidence IDF: lucene is a strong default across datasets\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # Blend two TF views: classic saturation + length-aware salience\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        # Small sublinear rare-term shaping\n        if EvolvedParameters.gamma > 0:\n            tf_comp *= (1.0 + EvolvedParameters.gamma * math.tanh(idf))\n\n        return idf * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n        matched_idf: float = 0.0,\n        total_idf: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # nDCG-friendly: one strong term match can be highly diagnostic\n        if EvolvedParameters.maxterm_gamma > 0 and term_scores:\n            base += EvolvedParameters.maxterm_gamma * max(term_scores)\n\n        # Additive priors (recall-friendly)\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n\n        # Bounded rarity-aware coordination (precision-friendly without killing recall)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    total_idf = 0.0\n    matched_idf = 0.0\n\n    # New: presence-only rarity mass (for additive prior)\n    matched_pres = 0.0\n\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n        total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n            midf = max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n            matched_idf += midf\n            matched_pres += ScoringPrimitives.rarity_presence_weight(midf, EvolvedParameters.pres_pivot)\n\n    base_score = DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n        matched_idf=matched_idf,\n        total_idf=total_idf,\n    )\n\n    # Add gated presence prior (post-aggregation, recall-safe)\n    if EvolvedParameters.pres_gamma > 0 and matched_count > 0:\n        gate = ScoringPrimitives.short_query_gate(len(query_terms), EvolvedParameters.pres_tau)\n        base_score += EvolvedParameters.pres_gamma * gate * matched_pres\n\n    return base_score\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_pres = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n\n        qids = np.array(query_term_ids, dtype=np.int64)\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N)\n            if df_q.size\n            else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else 0.0\n\n        max_contrib = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            pos_idf = max(0.0, idf)\n            matched_idf += present * pos_idf\n            if EvolvedParameters.pres_gamma > 0:\n                matched_pres += present * ScoringPrimitives.rarity_presence_weight(pos_idf, EvolvedParameters.pres_pivot)\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            contrib = weight * idf * tf_comp\n            scores += contrib\n            max_contrib = np.maximum(max_contrib, contrib)\n\n        # nDCG-friendly max-term booster (mirror DocumentScorer)\n        if EvolvedParameters.maxterm_gamma > 0:\n            scores += EvolvedParameters.maxterm_gamma * max_contrib\n\n        # Additive priors (mirror DocumentScorer)\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            share = ScoringPrimitives.matched_idf_share_vectorized(matched_idf, total_idf)\n            scores += EvolvedParameters.idf_match_gamma * share\n\n        # New: gated presence prior (vectorized analogue of score_kernel)\n        if EvolvedParameters.pres_gamma > 0 and len(query_term_ids) > 0:\n            gate = ScoringPrimitives.short_query_gate(len(query_term_ids), EvolvedParameters.pres_tau)\n            scores += EvolvedParameters.pres_gamma * float(gate) * matched_pres\n\n        # Bounded rarity-aware coordination (mirror DocumentScorer)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                # Match QueryProcessor (saturated qtf weighting)\n                query_term_weights.append(\n                    ScoringPrimitives.query_weight_saturated(float(count), EvolvedParameters.k3)\n                )\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D2 (Score: 0.4232)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending (kept)\n    alpha: float = 0.6\n\n    # Bounded rarity-mass coordination (recall-safe soft-AND)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Additive priors (recall-friendly)\n    cov_gamma: float = 0.10\n    idf_match_gamma: float = 0.08\n\n    # nDCG-friendly: reward a single very strong matching term (cheap, additive)\n    maxterm_gamma: float = 0.03\n\n    # Keep mild rare-term shaping off (lucene idf already strong)\n    gamma: float = 0.0\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n        t = max(total_idf, EvolvedParameters.epsilon)\n        return max(0.0, min(1.0, matched_idf / t))\n\n    @staticmethod\n    def matched_idf_share_vectorized(\n        matched_idf: NDArray[np.float64], total_idf: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_idf), EvolvedParameters.epsilon)\n        return np.clip(matched_idf / t, 0.0, 1.0)\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare term spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coord_factor(matched_terms: int, total_query_terms: int, beta: float) -> float:\n        \"\"\"\n        Soft coordination factor: (matched/total)^beta. Multiplies score (better than additive bonus).\n        \"\"\"\n        if total_query_terms <= 0 or matched_terms <= 0:\n            return 0.0\n        return (matched_terms / total_query_terms) ** max(beta, 0.0)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        return coverage * coverage\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Term evidence IDF: lucene is a strong default across datasets\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # Blend two TF views: classic saturation + length-aware salience\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        # Small sublinear rare-term shaping\n        if EvolvedParameters.gamma > 0:\n            tf_comp *= (1.0 + EvolvedParameters.gamma * math.tanh(idf))\n\n        return idf * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n        matched_idf: float = 0.0,\n        total_idf: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # nDCG-friendly: one strong term match can be highly diagnostic\n        if EvolvedParameters.maxterm_gamma > 0 and term_scores:\n            base += EvolvedParameters.maxterm_gamma * max(term_scores)\n\n        # Additive priors (recall-friendly)\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n\n        # Bounded rarity-aware coordination (precision-friendly without killing recall)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    total_idf = 0.0\n    matched_idf = 0.0\n\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n        total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n            matched_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n        matched_idf=matched_idf,\n        total_idf=total_idf,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n\n        qids = np.array(query_term_ids, dtype=np.int64)\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N)\n            if df_q.size\n            else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else 0.0\n\n        max_contrib = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            contrib = weight * idf * tf_comp\n            scores += contrib\n            max_contrib = np.maximum(max_contrib, contrib)\n\n        # nDCG-friendly max-term booster (mirror DocumentScorer)\n        if EvolvedParameters.maxterm_gamma > 0:\n            scores += EvolvedParameters.maxterm_gamma * max_contrib\n\n        # Additive priors (mirror DocumentScorer)\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            share = ScoringPrimitives.matched_idf_share_vectorized(matched_idf, total_idf)\n            scores += EvolvedParameters.idf_match_gamma * share\n\n        # Bounded rarity-aware coordination (mirror DocumentScorer)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                # Match QueryProcessor (saturated qtf weighting)\n                query_term_weights.append(\n                    ScoringPrimitives.query_weight_saturated(float(count), EvolvedParameters.k3)\n                )\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n### Program D3 (Score: 0.4232)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending (kept)\n    alpha: float = 0.6\n\n    # Bounded rarity-mass coordination (recall-safe soft-AND)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Additive priors (recall-friendly)\n    cov_gamma: float = 0.10\n    idf_match_gamma: float = 0.08\n\n    # nDCG-friendly: reward a single very strong matching term (cheap, additive)\n    maxterm_gamma: float = 0.03\n\n    # Keep mild rare-term shaping off (lucene idf already strong)\n    gamma: float = 0.0\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n        t = max(total_idf, EvolvedParameters.epsilon)\n        return max(0.0, min(1.0, matched_idf / t))\n\n    @staticmethod\n    def matched_idf_share_vectorized(\n        matched_idf: NDArray[np.float64], total_idf: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_idf), EvolvedParameters.epsilon)\n        return np.clip(matched_idf / t, 0.0, 1.0)\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare term spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coord_factor(matched_terms: int, total_query_terms: int, beta: float) -> float:\n        \"\"\"\n        Soft coordination factor: (matched/total)^beta. Multiplies score (better than additive bonus).\n        \"\"\"\n        if total_query_terms <= 0 or matched_terms <= 0:\n            return 0.0\n        return (matched_terms / total_query_terms) ** max(beta, 0.0)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        return coverage * coverage\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Term evidence IDF: lucene is a strong default across datasets\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # Blend two TF views: lucene-style saturation + length-aware salience\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        # Small sublinear rare-term shaping\n        if EvolvedParameters.gamma > 0:\n            tf_comp *= (1.0 + EvolvedParameters.gamma * math.tanh(idf))\n\n        return idf * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n        matched_idf: float = 0.0,\n        total_idf: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # nDCG-friendly: one strong term match can be highly diagnostic\n        if EvolvedParameters.maxterm_gamma > 0 and term_scores:\n            base += EvolvedParameters.maxterm_gamma * max(term_scores)\n\n        # Additive priors (recall-friendly)\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n\n        # Bounded rarity-aware coordination (precision-friendly without killing recall)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    total_idf = 0.0\n    matched_idf = 0.0\n\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n        total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n            matched_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n        matched_idf=matched_idf,\n        total_idf=total_idf,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n\n        qids = np.array(query_term_ids, dtype=np.int64)\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N)\n            if df_q.size\n            else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else 0.0\n\n        max_contrib = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            contrib = weight * idf * tf_comp\n            scores += contrib\n            max_contrib = np.maximum(max_contrib, contrib)\n\n        # nDCG-friendly max-term booster (mirror DocumentScorer)\n        if EvolvedParameters.maxterm_gamma > 0:\n            scores += EvolvedParameters.maxterm_gamma * max_contrib\n\n        # Additive priors (mirror DocumentScorer)\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            share = ScoringPrimitives.matched_idf_share_vectorized(matched_idf, total_idf)\n            scores += EvolvedParameters.idf_match_gamma * share\n\n        # Bounded rarity-aware coordination (mirror DocumentScorer)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        term_counts = Counter(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, count in term_counts.items():\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                # Match QueryProcessor (saturated qtf weighting)\n                query_term_weights.append(\n                    ScoringPrimitives.query_weight_saturated(float(count), EvolvedParameters.k3)\n                )\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nKey features: Alternative approach to bright_pony_ndcg@10, Alternative approach to bright_pony_recall@100\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 0.3233, Type: Exploratory)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending (keep; tends to help long/noisy docs)\n    alpha: float = 0.6\n\n    # Additive recall-friendly coverage bonus (helps recall@100 without harsh AND)\n    gamma: float = 0.22\n\n    # Frequent-term recall rescue (bounded)\n    common_strength: float = 0.25\n    common_pivot: float = 2.5\n\n    # Rarity-aware coordination (soft AND), bounded to protect recall\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Fix: parameters referenced elsewhere (keep small defaults)\n    cov_power: float = 2.0\n    idf_pivot: float = 3.0\n    beta: float = 0.35\n\n    # New: idf-driven TF shaping (rarer terms tolerate more tf; common terms log-saturate sooner)\n    tf_idf_mix: float = 0.45\n\n    # New: candidate gating for speed on large/postings-heavy corpora\n    max_candidate_terms: int = 6\n    gate_min_query_terms: int = 10\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def commonness_rescue(idf: float, strength: float, pivot: float) -> float:\n        \"\"\"\n        Boost frequent terms a bit (low idf), bounded and smooth.\n        When idf << pivot => multiplier ~ 1 + strength\n        When idf >> pivot => multiplier ~ 1\n        \"\"\"\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def commonness_rescue_vectorized(\n        idf: NDArray[np.float64], strength: float, pivot: float\n    ) -> NDArray[np.float64]:\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    # (coverage_bonus defined later; keep a single definition for simplicity/consistency)\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        \"\"\"\n        Coordination based on rarity-mass coverage (soft AND), bounded for recall.\n        Returns: floor + (1-floor) * (matched_rarity/total_rarity)^beta\n        \"\"\"\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = max(0.0, min(1.0, floor))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_idf_adaptive(tf: float, k1: float, norm: float, idf: float, pivot: float) -> float:\n        \"\"\"\n        New primitive: rarity-adaptive TF saturation.\n        - For common terms (idf << pivot): behave more like log saturation (anti-burst).\n        - For rare terms (idf >> pivot): behave like standard BM25/Lucene saturation.\n        \"\"\"\n        # mix in [0,1]\n        mix = idf / (idf + pivot + EvolvedParameters.epsilon)\n        tf_l = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_log = ScoringPrimitives.saturate_tflog_bm25(tf, k1, norm)\n        return mix * tf_l + (1.0 - mix) * tf_log\n\n    @staticmethod\n    def saturate_idf_adaptive_vectorized(\n        tf: NDArray[np.float64],\n        k1: float,\n        norm: NDArray[np.float64],\n        idf: float,\n        pivot: float,\n    ) -> NDArray[np.float64]:\n        mix = float(idf) / (float(idf) + float(pivot) + EvolvedParameters.epsilon)\n        tf_l = ScoringPrimitives.saturate_lucene_vectorized(tf, k1, norm)\n        # vectorized version of saturate_tflog_bm25\n        t = np.where(tf > 0, 1.0 + np.log(np.maximum(tf, 1.0)), 0.0)\n        denom = t + float(k1) * norm + EvolvedParameters.epsilon\n        tf_log = (t * (float(k1) + 1.0)) / denom\n        return mix * tf_l + (1.0 - mix) * tf_log\n\n    @staticmethod\n    def coord_factor(matched_terms: int, total_query_terms: int, beta: float) -> float:\n        \"\"\"\n        Soft coordination factor: (matched/total)^beta. Multiplies score.\n        \"\"\"\n        if total_query_terms <= 0 or matched_terms <= 0:\n            return 0.0\n        return (matched_terms / total_query_terms) ** max(beta, 0.0)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def idf_mixed_compressed(df: float, N: int) -> float:\n        \"\"\"\n        New primitive: blend two IDFs then compress extremes.\n        Motivation: very rare terms can dominate nDCG@10; compression tends to improve\n        ranking robustness across heterogeneous BEIR/BRIGHT corpora while preserving recall.\n        \"\"\"\n        a = EvolvedParameters.alpha\n        idf_a = ScoringPrimitives.idf_lucene(df, N)\n        idf_b = ScoringPrimitives.idf_atire(df, N)\n        mixed = a * idf_a + (1.0 - a) * idf_b\n        # soft compression of very large idf values (keeps monotonicity)\n        p = EvolvedParameters.idf_pivot\n        beta = EvolvedParameters.beta\n        return mixed / (1.0 + beta * max(0.0, mixed - p))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_tflog_bm25(tf: float, k1: float, norm: float) -> float:\n        \"\"\"\n        New primitive: apply log-TF before BM25-style saturation.\n        Helps corpora with bursty term repetition (e.g., forum/stack traces) without\n        killing signals for single occurrences.\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        t = 1.0 + math.log(tf)\n        denom = t + k1 * norm + EvolvedParameters.epsilon\n        return (t * (k1 + 1.0)) / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        # more tunable than fixed square; tends to help nDCG@10 by preferring fuller matches\n        return coverage ** max(1.0, EvolvedParameters.cov_power)\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Strong default IDF\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # TF shaping: blend base Lucene saturation with IDF-adaptive saturation,\n        # then blend with length-aware salience.\n        tf_base = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_adapt = ScoringPrimitives.saturate_idf_adaptive(\n            tf, k1, norm, idf, EvolvedParameters.idf_pivot\n        )\n        m = max(0.0, min(1.0, float(EvolvedParameters.tf_idf_mix)))\n        tf_shape = (1.0 - m) * tf_base + m * tf_adapt\n\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        a = max(0.0, min(1.0, float(EvolvedParameters.alpha)))\n        tf_comp = (1.0 - a) * tf_shape + a * tf_sal\n\n        rescue = ScoringPrimitives.commonness_rescue(\n            idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n        )\n        return idf * tf_comp * rescue\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # Additive coverage bonus keeps recall strong\n        if EvolvedParameters.gamma > 0:\n            base += EvolvedParameters.gamma * ScoringPrimitives.coverage_bonus(\n                matched_count, total_query_terms\n            )\n\n        # Bounded rarity-aware coordination improves early precision without killing recall\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    # rarity-mass coverage tracking (bounded rarity; reduces ultra-rare dominance)\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        # Match TermScorer: lucene IDF array for fast vectorized scoring\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # rarity-mass tracking\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n        total_rarity = 0.0\n\n        # Precompute bounded rarity per query term (balanced idf) for coordination\n        df_q = self.corpus._df[np.array(query_term_ids, dtype=np.int64)] if query_term_ids else np.array([], dtype=np.float64)\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N) if df_q.size else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            rescue = ScoringPrimitives.commonness_rescue(\n                idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n            )\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n\n            tf_base = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_adapt = ScoringPrimitives.saturate_idf_adaptive_vectorized(\n                tf_row, k1, norms, idf, EvolvedParameters.idf_pivot\n            )\n            m = max(0.0, min(1.0, float(EvolvedParameters.tf_idf_mix)))\n            tf_shape = (1.0 - m) * tf_base + m * tf_adapt\n\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            a = max(0.0, min(1.0, float(EvolvedParameters.alpha)))\n            tf_comp = (1.0 - a) * tf_shape + a * tf_sal\n\n            scores += weight * idf * rescue * tf_comp\n\n        # Additive coverage bonus\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.gamma > 0 and qn > 0:\n            coverage = matched / qn\n            scores += EvolvedParameters.gamma * (coverage * coverage)\n\n        # Bounded rarity-aware coordination\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            coord = ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n            scores *= coord\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        query_terms, query_weights = QueryProcessor.process(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, w in zip(query_terms, query_weights, strict=False):\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                query_term_weights.append(float(w))\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n\n        # Candidate gating (recall-safe): only gate for sufficiently long queries.\n        if (\n            len(query_term_ids) >= EvolvedParameters.gate_min_query_terms\n            and len(query_term_ids) > EvolvedParameters.max_candidate_terms\n        ):\n            df_q = self.corpus._df[np.array(query_term_ids, dtype=np.int64)]\n            order = np.argsort(df_q)[: EvolvedParameters.max_candidate_terms]\n            gated_term_ids = [query_term_ids[i] for i in order.tolist()]\n        else:\n            gated_term_ids = query_term_ids\n\n        candidate_set: set[int] = set()\n        for term_id in gated_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 2 (Score: 0.4218, Type: Experimental)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blend: saturation vs length-aware salience (robust on long/noisy docs)\n    alpha: float = 0.6\n\n    # Light document-level priors\n    gamma: float = 0.22  # additive coverage prior\n\n    # Frequent-term recall rescue (bounded)\n    common_strength: float = 0.25\n    common_pivot: float = 2.5\n\n    # Bounded rarity-aware coordination (recall-safe soft-AND)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded rarity for coordination/priors: log1p((1-p)/p) where p=df/N.\n        Helps avoid ultra-rare terms fully dominating rarity-mass coverage.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"Length-aware TF salience: downweights inflated TF in long docs.\"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def commonness_rescue(idf: float, strength: float, pivot: float) -> float:\n        \"\"\"\n        Boost frequent terms a bit (low idf), bounded and smooth.\n        When idf << pivot => multiplier ~ 1 + strength\n        When idf >> pivot => multiplier ~ 1\n        \"\"\"\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def commonness_rescue_vectorized(\n        idf: NDArray[np.float64], strength: float, pivot: float\n    ) -> NDArray[np.float64]:\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def coord_rarity_aware(matched_rarity: float, total_rarity: float, beta: float, floor: float) -> float:\n        \"\"\"\n        Bounded rarity-mass coordination (soft AND, recall-safe):\n          floor + (1-floor) * (matched_rarity/total_rarity)^beta\n        \"\"\"\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64], total_rarity: float, beta: float, floor: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        return coverage * coverage\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        a = float(max(0.0, min(1.0, EvolvedParameters.alpha)))\n        tf_comp = (1.0 - a) * tf_sat + a * tf_sal\n\n        rescue = ScoringPrimitives.commonness_rescue(\n            idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n        )\n        return idf * rescue * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # Additive coverage bonus keeps recall strong.\n        if EvolvedParameters.gamma > 0:\n            base += EvolvedParameters.gamma * ScoringPrimitives.coverage_bonus(\n                matched_count, total_query_terms\n            )\n\n        # Bounded rarity-aware coordination (recall-safe soft-AND)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        # Deduplicate terms and use saturated qtf weighting (classic BM25 query term factor idea)\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    # rarity-mass coverage tracking: use balanced rarity (less spiky than lucene idf)\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # Track rarity-mass matched for bounded coordination\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n        a = float(np.clip(float(EvolvedParameters.alpha), 0.0, 1.0))\n\n        qids = np.array(query_term_ids, dtype=np.int64)\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N) if df_q.size else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            rescue = ScoringPrimitives.commonness_rescue(\n                idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n            )\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - a) * tf_sat + a * tf_sal\n\n            scores += weight * idf * rescue * tf_comp\n\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.gamma > 0 and qn > 0:\n            coverage = matched / qn\n            scores += EvolvedParameters.gamma * (coverage * coverage)\n\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n\n        # Keep rank() consistent with QueryProcessor: dedup + saturated qtf\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n        for term, count in term_counts.items():\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                query_term_weights.append(\n                    ScoringPrimitives.query_weight_saturated(float(count), EvolvedParameters.k3)\n                )\n\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n### Inspiration 3 (Score: 0.0000, Type: Exploratory)\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blend: restore strong-performing setting\n    alpha: float = 0.6\n    beta: float = 1.0\n\n    # Light document-level priors\n    gamma: float = 0.22\n\n    # Frequent-term recall rescue (bounded)\n    common_strength: float = 0.25\n    common_pivot: float = 2.5\n\n    # Rarity-aware coordination (soft AND), bounded to protect recall\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # New: best-term dominance (helps nDCG@10 on \u201cone-keyword\u201d queries)\n    top_term_gamma: float = 0.10\n    top_term_temp: float = 0.7\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded rarity for coordination/priors: log1p((1-p)/p) where p=df/N.\n        Helps avoid ultra-rare terms fully dominating rarity-mass coverage.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def commonness_rescue(idf: float, strength: float, pivot: float) -> float:\n        \"\"\"\n        Boost frequent terms a bit (low idf), bounded and smooth.\n        When idf << pivot => multiplier ~ 1 + strength\n        When idf >> pivot => multiplier ~ 1\n        \"\"\"\n        # 1 + strength * pivot/(pivot+idf)\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def commonness_rescue_vectorized(\n        idf: NDArray[np.float64], strength: float, pivot: float\n    ) -> NDArray[np.float64]:\n        return 1.0 + strength * (pivot / (pivot + idf + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        \"\"\"\n        Bounded rarity-mass coordination (soft AND, recall-safe).\n        Returns: floor + (1-floor) * (matched_rarity/total_rarity)^beta\n        \"\"\"\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = max(0.0, min(1.0, floor))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        return coverage * coverage\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        rescue = ScoringPrimitives.commonness_rescue(\n            idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n        )\n        return idf * tf_comp * rescue\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # Additive coverage bonus keeps recall strong.\n        if EvolvedParameters.gamma > 0:\n            base += EvolvedParameters.gamma * ScoringPrimitives.coverage_bonus(\n                matched_count, total_query_terms\n            )\n\n        # New: best-term dominance prior (smooth max)\n        if EvolvedParameters.top_term_gamma > 0:\n            base += EvolvedParameters.top_term_gamma * ScoringPrimitives.soft_max(\n                term_scores, temperature=max(EvolvedParameters.top_term_temp, 0.05)\n            )\n\n        # Bounded rarity-aware coordination improves early precision without killing recall.\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        # Deduplicate terms and use saturated qtf weighting (classic BM25 query term factor idea)\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    # rarity-mass coverage tracking: use balanced rarity (less spiky than lucene idf)\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # rarity-mass tracking for bounded coordination\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n\n        # Precompute bounded rarity per query term (balanced idf) for coordination\n        df_q = self.corpus._df[np.array(query_term_ids, dtype=np.int64)] if query_term_ids else np.array([], dtype=np.float64)\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N) if df_q.size else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            rescue = ScoringPrimitives.commonness_rescue(\n                idf, EvolvedParameters.common_strength, EvolvedParameters.common_pivot\n            )\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            scores += weight * idf * rescue * tf_comp\n\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.gamma > 0 and qn > 0:\n            coverage = matched / qn\n            scores += EvolvedParameters.gamma * (coverage * coverage)\n\n        # top-term dominance prior (vectorized approx: just add gamma * max term contribution proxy)\n        # We approximate using scores/q-weight mixture is hard post-aggregation; keep it only in scalar path.\n        # (Leaving it out here would create mismatch; so we add a lightweight proxy via max per-term contrib.)\n        if EvolvedParameters.top_term_gamma > 0 and qn > 0:\n            # recompute max per-term contribution cheaply (no extra sparse fetch: reuse tf_row loop not available here),\n            # so approximate with (scores/qn) scaled. This keeps directionality but is mild.\n            scores += EvolvedParameters.top_term_gamma * (scores / qn)\n\n        # Bounded rarity-aware coordination\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n\n        # Keep rank() consistent with QueryProcessor: dedup + saturated qtf\n        term_counts = Counter(query)\n        query_term_ids: list[int] = []\n        query_term_weights: list[float] = []\n        for term, count in term_counts.items():\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                query_term_weights.append(\n                    ScoringPrimitives.query_weight_saturated(float(count), EvolvedParameters.k3)\n                )\n\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\nUnique approach: [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name'], [Fragment formatting error: 'metric_name']\n\n# Current Program\n```python\n\"\"\"\nLexical retrieval as PRIMITIVES + STRUCTURE.\n- Primitives: atomic scoring pieces (IDF, TF, saturation, length norm, aggregation).\n- Structure: how they are combined (term score \u2192 doc score \u2192 ranking).\nThis seed is one structure (BM25-like); evolution can invent new primitives and new structure.\n\"\"\"\nfrom __future__ import annotations\n\nimport math\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import TYPE_CHECKING\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, lil_matrix\n\nfrom ranking_evolved.bm25 import (\n    ENGLISH_STOPWORDS,\n    LUCENE_STOPWORDS,\n    LuceneTokenizer as _BaseLuceneTokenizer,\n)\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\nNUM_QUERY_WORKERS = 32\nMIN_QUERIES_FOR_PARALLEL = 10\n\n\n# ----- EVOLVE: Parameters -----\n\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending weight (kept)\n    alpha: float = 0.6\n\n    # Replace harsh coordination with bounded rarity-mass soft-AND (recall-safe)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Additive priors (recall-friendly)\n    cov_gamma: float = 0.10       # small additive term-coverage bump\n    idf_match_gamma: float = 0.08 # additive matched-IDF-share bump\n\n    # Keep mild rare-term shaping but do it via IDF choice rather than tf multiplier\n    gamma: float = 0.0\n\n    # Keep experimental knobs defined (prevents runtime failure if referenced)\n    idf_pivot: float = 3.0\n    cov_power: float = 2.0\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n\n\n# ----- EVOLVE: Primitives (atoms). Add new ones or change formulas. -----\n\nclass ScoringPrimitives:\n    \"\"\"IDF, TF, saturation, length norm, aggregation. Invent new primitives or new formulas.\"\"\"\n\n    @staticmethod\n    def matched_idf_share(matched_idf: float, total_idf: float) -> float:\n        \"\"\"Bounded query-IDF mass coverage in [0,1]. Used as additive recall-friendly prior.\"\"\"\n        t = max(total_idf, EvolvedParameters.epsilon)\n        return max(0.0, min(1.0, matched_idf / t))\n\n    @staticmethod\n    def matched_idf_share_vectorized(\n        matched_idf: NDArray[np.float64], total_idf: float\n    ) -> NDArray[np.float64]:\n        t = max(float(total_idf), EvolvedParameters.epsilon)\n        return np.clip(matched_idf / t, 0.0, 1.0)\n\n    @staticmethod\n    def coord_rarity_aware(\n        matched_rarity: float, total_rarity: float, beta: float, floor: float\n    ) -> float:\n        \"\"\"\n        Bounded rarity-mass soft-AND multiplier:\n          floor + (1-floor) * (matched_rarity/total_rarity)^beta\n        \"\"\"\n        t = max(total_rarity, EvolvedParameters.epsilon)\n        frac = max(0.0, matched_rarity) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * (frac ** max(0.0, beta))\n\n    @staticmethod\n    def coord_rarity_aware_vectorized(\n        matched_rarity: NDArray[np.float64],\n        total_rarity: float,\n        beta: float,\n        floor: float,\n    ) -> NDArray[np.float64]:\n        t = max(float(total_rarity), EvolvedParameters.epsilon)\n        frac = np.maximum(matched_rarity, 0.0) / t\n        f = float(max(0.0, min(1.0, floor)))\n        return f + (1.0 - f) * np.power(frac, max(0.0, float(beta)))\n\n    @staticmethod\n    def idf_classic(df: float, N: int) -> float:\n        return math.log((N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced(df: float, N: int) -> float:\n        \"\"\"\n        Bounded, two-sided IDF: reduces ultra-rare spikes while still penalizing frequent terms.\n        \"\"\"\n        p = df / (N + EvolvedParameters.epsilon)\n        return math.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_balanced_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        p = df / (N + EvolvedParameters.epsilon)\n        return np.log1p((1.0 - p) / (p + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def tf_salience(tf: float, dl: float, avgdl: float) -> float:\n        \"\"\"\n        Length-aware TF salience: downweights inflated TF in long docs.\n        \"\"\"\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def tf_salience_vectorized(\n        tf: NDArray[np.float64], dl: NDArray[np.float64], avgdl: float\n    ) -> NDArray[np.float64]:\n        denom = tf + 0.5 * (dl / max(avgdl, 1.0)) + 1.0\n        return tf / (denom + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coord_factor(matched_terms: int, total_query_terms: int, beta: float) -> float:\n        \"\"\"\n        Soft coordination factor: (matched/total)^beta. Multiplies score.\n        \"\"\"\n        if total_query_terms <= 0 or matched_terms <= 0:\n            return 0.0\n        return (matched_terms / total_query_terms) ** max(beta, 0.0)\n\n    @staticmethod\n    def idf_lucene(df: float, N: int) -> float:\n        return math.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_lucene_vectorized(df: NDArray[np.float64], N: int) -> NDArray[np.float64]:\n        return np.log(1.0 + (N - df + 0.5) / (df + 0.5 + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_atire(df: float, N: int) -> float:\n        return math.log(N / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_bm25plus(df: float, N: int) -> float:\n        return math.log((N + 1) / (df + EvolvedParameters.epsilon))\n\n    @staticmethod\n    def idf_smooth(df: float, N: int) -> float:\n        return math.log((N + 0.5) / (df + 0.5))\n\n    @staticmethod\n    def idf_mixed_compressed(df: float, N: int) -> float:\n        \"\"\"\n        New primitive: blend two IDFs then compress extremes.\n        Motivation: very rare terms can dominate nDCG@10; compression tends to improve\n        ranking robustness across heterogeneous BEIR/BRIGHT corpora while preserving recall.\n        \"\"\"\n        a = EvolvedParameters.alpha\n        idf_a = ScoringPrimitives.idf_lucene(df, N)\n        idf_b = ScoringPrimitives.idf_atire(df, N)\n        mixed = a * idf_a + (1.0 - a) * idf_b\n        # soft compression of very large idf values (keeps monotonicity)\n        p = EvolvedParameters.idf_pivot\n        beta = EvolvedParameters.beta\n        return mixed / (1.0 + beta * max(0.0, mixed - p))\n\n    @staticmethod\n    def tf_raw(tf: float) -> float:\n        return tf\n\n    @staticmethod\n    def tf_log(tf: float) -> float:\n        return 1.0 + math.log(tf) if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_double_log(tf: float) -> float:\n        if tf <= 0:\n            return 0.0\n        return 1.0 + math.log(1.0 + math.log(tf + 1))\n\n    @staticmethod\n    def tf_boolean(tf: float) -> float:\n        return 1.0 if tf > 0 else 0.0\n\n    @staticmethod\n    def tf_augmented(tf: float, max_tf: float) -> float:\n        return 0.5 + 0.5 * (tf / max_tf) if max_tf > 0 else 0.5\n\n    @staticmethod\n    def saturate(x: float, k: float) -> float:\n        return x / (x + k + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return (tf * (k1 + 1)) / denom\n\n    @staticmethod\n    def saturate_lucene(tf: float, k1: float, norm: float) -> float:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_tflog_bm25(tf: float, k1: float, norm: float) -> float:\n        \"\"\"\n        New primitive: apply log-TF before BM25-style saturation.\n        Helps corpora with bursty term repetition (e.g., forum/stack traces) without\n        killing signals for single occurrences.\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        t = 1.0 + math.log(tf)\n        denom = t + k1 * norm + EvolvedParameters.epsilon\n        return (t * (k1 + 1.0)) / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n\n    @staticmethod\n    def saturate_bm25l(tf: float, k1: float, norm: float, delta: float) -> float:\n        c = tf / (norm + EvolvedParameters.epsilon)\n        c_delta = c + delta\n        return ((k1 + 1) * c_delta) / (k1 + c_delta + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def saturate_bm25plus(tf: float, k1: float, norm: float, delta: float) -> float:\n        base = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return base + delta if tf > 0 else base\n\n    @staticmethod\n    def saturate_log(tf: float, k1: float, norm: float) -> float:\n        bm25_sat = (tf * (k1 + 1)) / (tf + k1 * norm + EvolvedParameters.epsilon)\n        return math.log(1.0 + bm25_sat)\n\n    @staticmethod\n    def length_norm_bm25(dl: float, avgdl: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_bm25_vectorized(\n        dl: NDArray[np.float64], avgdl: float, b: float\n    ) -> NDArray[np.float64]:\n        return 1.0 - b + b * (dl / max(avgdl, 1.0))\n\n    @staticmethod\n    def length_norm_pivot(dl: float, pivot: float, b: float) -> float:\n        return 1.0 - b + b * (dl / max(pivot, 1.0))\n\n    @staticmethod\n    def length_norm_log(dl: float, avgdl: float, b: float) -> float:\n        ratio = dl / max(avgdl, 1.0)\n        return 1.0 + b * math.log(ratio) if ratio > 0 else 1.0\n\n    @staticmethod\n    def multiply(*args: float) -> float:\n        result = 1.0\n        for x in args:\n            result *= x\n        return result\n\n    @staticmethod\n    def add(*args: float) -> float:\n        return sum(args)\n\n    @staticmethod\n    def weighted_sum(values: list[float], weights: list[float]) -> float:\n        return sum(v * w for v, w in zip(values, weights, strict=False))\n\n    @staticmethod\n    def geometric_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        product = 1.0\n        for v in values:\n            if v <= 0:\n                return 0.0\n            product *= v\n        return product ** (1.0 / len(values))\n\n    @staticmethod\n    def harmonic_mean(values: list[float]) -> float:\n        if not values:\n            return 0.0\n        reciprocal_sum = sum(1.0 / (v + EvolvedParameters.epsilon) for v in values)\n        return len(values) / reciprocal_sum if reciprocal_sum > 0 else 0.0\n\n    @staticmethod\n    def soft_max(values: list[float], temperature: float = 1.0) -> float:\n        if not values:\n            return 0.0\n        max_val = max(values)\n        exp_sum = sum(math.exp((v - max_val) / temperature) for v in values)\n        return max_val + temperature * math.log(exp_sum)\n\n    @staticmethod\n    def query_weight_uniform(qtf: float, k3: float) -> float:\n        return 1.0\n\n    @staticmethod\n    def query_weight_frequency(qtf: float, k3: float) -> float:\n        return qtf\n\n    @staticmethod\n    def query_weight_saturated(qtf: float, k3: float) -> float:\n        return ((k3 + 1) * qtf) / (k3 + qtf + EvolvedParameters.epsilon)\n\n    @staticmethod\n    def coverage_bonus(matched_terms: int, total_query_terms: int) -> float:\n        if total_query_terms <= 0:\n            return 0.0\n        coverage = matched_terms / total_query_terms\n        # more tunable than fixed square; tends to help nDCG@10 by preferring fuller matches\n        return coverage ** max(1.0, EvolvedParameters.cov_power)\n\n    @staticmethod\n    def rarity_boost(idf: float, threshold: float = 3.0) -> float:\n        return 1.0 + (idf - threshold) * 0.1 if idf > threshold else 1.0\n\n\n# ----- EVOLVE: Term score (IDF \u00d7 TF, or your formula) -----\n\nclass TermScorer:\n    \"\"\"One term's contribution. Evolve the formula; invent new combinations or new math.\"\"\"\n\n    @staticmethod\n    def score(tf: float, df: float, N: int, dl: float, avgdl: float) -> float:\n        if tf <= 0:\n            return 0.0\n        k1, b = EvolvedParameters.k1, EvolvedParameters.b\n\n        # Term evidence IDF: Lucene is a strong default across BEIR/BRIGHT\n        idf = ScoringPrimitives.idf_lucene(df, N)\n        idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n        norm = ScoringPrimitives.length_norm_bm25(dl, avgdl, b)\n\n        # Blend: classic Lucene BM25-like sat + length-aware salience\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n        # Mild rare-term shaping (kept small)\n        if EvolvedParameters.gamma > 0:\n            tf_comp *= (1.0 + EvolvedParameters.gamma * math.tanh(idf))\n\n        return idf * tf_comp\n\n\n# ----- EVOLVE: Doc score (aggregation of term scores) -----\n\nclass DocumentScorer:\n    \"\"\"Aggregate term scores into document score. Evolve aggregation or add new terms.\"\"\"\n\n    @staticmethod\n    def score(\n        term_scores: list[float],\n        query_weights: list[float],\n        matched_count: int,\n        total_query_terms: int,\n        matched_rarity: float = 0.0,\n        total_rarity: float = 0.0,\n        matched_idf: float = 0.0,\n        total_idf: float = 0.0,\n    ) -> float:\n        if not term_scores:\n            return 0.0\n\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # Recall-friendly additive priors\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n\n        # Precision-friendly bounded soft-AND (doesn't zero out partial matches)\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            base *= ScoringPrimitives.coord_rarity_aware(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return base\n\n\n# ----- EVOLVE: Query handling -----\n\nclass QueryProcessor:\n    \"\"\"Turn raw query into (terms, weights). Evolve weighting or dedup strategy.\"\"\"\n\n    @staticmethod\n    def process(query: list[str]) -> tuple[list[str], list[float]]:\n        if not query:\n            return [], []\n        counts = Counter(query)\n        terms = list(counts.keys())\n        weights = [\n            ScoringPrimitives.query_weight_saturated(float(counts[t]), EvolvedParameters.k3)\n            for t in terms\n        ]\n        return terms, weights\n\n\n# ----- EVOLVE: Full pipeline (or replace with new structure) -----\n\ndef score_kernel(query: list[str], doc_idx: int, corpus: Corpus) -> float:\n    \"\"\"Orchestrate term/doc scoring. Evolve pipeline or replace with a different structure.\"\"\"\n    if not query:\n        return 0.0\n    query_terms, query_weights = QueryProcessor.process(query)\n    if not query_terms:\n        return 0.0\n    doc_tf = corpus.get_term_frequencies(doc_idx)\n    dl = corpus.doc_lengths[doc_idx]\n    avgdl = corpus.avgdl\n    N = corpus.N\n\n    term_scores: list[float] = []\n    used_weights: list[float] = []\n    matched_count = 0\n\n    # For bounded coordination + priors\n    total_rarity = 0.0\n    matched_rarity = 0.0\n    total_idf = 0.0\n    matched_idf = 0.0\n\n    for term in query_terms:\n        df = corpus.get_df(term)\n        total_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n        total_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    for term, w in zip(query_terms, query_weights, strict=False):\n        tf = doc_tf.get(term, 0)\n        if tf > 0:\n            matched_count += 1\n            df = corpus.get_df(term)\n            term_scores.append(TermScorer.score(tf, df, N, dl, avgdl))\n            used_weights.append(w)\n\n            matched_rarity += max(0.0, ScoringPrimitives.idf_balanced(float(df), N))\n            matched_idf += max(0.0, ScoringPrimitives.idf_lucene(float(df), N))\n\n    return DocumentScorer.score(\n        term_scores,\n        used_weights,\n        matched_count,\n        len(query_terms),\n        matched_rarity=matched_rarity,\n        total_rarity=total_rarity,\n        matched_idf=matched_idf,\n        total_idf=total_idf,\n    )\n\n\n# ----- Tokenization (fixed; do not evolve) -----\n\n_TOKENIZER: _BaseLuceneTokenizer | None = None\n\ndef _get_tokenizer() -> _BaseLuceneTokenizer:\n    global _TOKENIZER\n    if _TOKENIZER is None:\n        _TOKENIZER = _BaseLuceneTokenizer()\n    return _TOKENIZER\n\ndef tokenize(text: str) -> list[str]:\n    return _get_tokenizer()(text)\n\nclass LuceneTokenizer:\n    def __init__(self):\n        self._tokenizer = _BaseLuceneTokenizer()\n    def __call__(self, text: str) -> list[str]:\n        return self._tokenizer(text)\n\n\n# ----- Corpus (fixed structure; evaluator expects this interface) -----\n\nclass Corpus:\n    \"\"\"Preprocessed collection; inverted index + sparse matrix. Interface must stay stable.\"\"\"\n\n    def __init__(self, documents: list[list[str]], ids: list[str] | None = None):\n        self.documents = documents\n        self.ids = ids or [str(i) for i in range(len(documents))]\n        self._id_to_idx = {doc_id: i for i, doc_id in enumerate(self.ids)}\n        self.N = len(documents)\n        self.document_count = self.N\n        self.doc_lengths = np.array([len(d) for d in documents], dtype=np.float64)\n        self.avgdl = float(np.mean(self.doc_lengths)) if self.N > 0 else 1.0\n        self.average_document_length = self.avgdl\n\n        self._vocab: dict[str, int] = {}\n        term_idx = 0\n        for doc in documents:\n            for term in doc:\n                if term not in self._vocab:\n                    self._vocab[term] = term_idx\n                    term_idx += 1\n        self.vocab_size = len(self._vocab)\n\n        tf_matrix_lil = lil_matrix((self.vocab_size, self.N), dtype=np.float64)\n        self._inverted_index: dict[int, list[int]] = {i: [] for i in range(self.vocab_size)}\n        self._df = np.zeros(self.vocab_size, dtype=np.float64)\n        self._doc_tf_dicts: list[Counter[str]] = [Counter(doc) for doc in documents]\n\n        for doc_idx, doc in enumerate(documents):\n            term_counts = Counter(doc)\n            seen_terms = set()\n            for term, count in term_counts.items():\n                term_id = self._vocab[term]\n                tf_matrix_lil[term_id, doc_idx] = count\n                if term_id not in seen_terms:\n                    self._inverted_index[term_id].append(doc_idx)\n                    self._df[term_id] += 1\n                    seen_terms.add(term_id)\n\n        self.tf_matrix = csr_matrix(tf_matrix_lil)\n        self._posting_lists: dict[int, NDArray[np.int64]] = {\n            term_id: np.array(doc_ids, dtype=np.int64)\n            for term_id, doc_ids in self._inverted_index.items()\n            if doc_ids\n        }\n        del self._inverted_index\n\n        # Match TermScorer: lucene IDF for term evidence (balanced used only for coordination)\n        self.idf_array = ScoringPrimitives.idf_lucene_vectorized(self._df, self.N)\n        self.norm_array = ScoringPrimitives.length_norm_bm25_vectorized(\n            self.doc_lengths, self.avgdl, EvolvedParameters.b\n        )\n        self.document_frequency = Counter(\n            {term: int(self._df[term_id]) for term, term_id in self._vocab.items()}\n        )\n        self.document_length = self.doc_lengths\n\n    def __len__(self) -> int:\n        return self.N\n\n    @classmethod\n    def from_huggingface_dataset(cls, dataset) -> Corpus:\n        ids = [doc[\"id\"] for doc in dataset]\n        documents = [tokenize(doc[\"content\"]) for doc in dataset]\n        return cls(documents, ids)\n\n    def get_df(self, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 1\n        return max(1, int(self._df[term_id]))\n\n    def get_tf(self, doc_idx: int, term: str) -> int:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return 0\n        return int(self.tf_matrix[term_id, doc_idx])\n\n    def get_term_frequencies(self, doc_idx: int) -> Counter[str]:\n        return self._doc_tf_dicts[doc_idx]\n\n    def get_posting_list(self, term: str) -> NDArray[np.int64]:\n        term_id = self._vocab.get(term)\n        if term_id is None:\n            return np.array([], dtype=np.int64)\n        return self._posting_lists.get(term_id, np.array([], dtype=np.int64))\n\n    def get_term_id(self, term: str) -> int | None:\n        return self._vocab.get(term)\n\n    def id_to_idx(self, ids: list[str]) -> list[int]:\n        return [self._id_to_idx[doc_id] for doc_id in ids if doc_id in self._id_to_idx]\n\n    @property\n    def map_id_to_idx(self) -> dict[str, int]:\n        return self._id_to_idx\n\n    @property\n    def vocabulary_size(self) -> int:\n        return self.vocab_size\n\n    @property\n    def term_doc_matrix(self) -> None:\n        return None\n\n    @property\n    def term_frequency(self) -> list[Counter[str]]:\n        return self._doc_tf_dicts\n\n\n# ----- BM25 API (interface fixed for evaluator) -----\n\nclass BM25:\n    \"\"\"Scorer: uses score_kernel for single-doc; vectorized path for batch (same formula).\"\"\"\n\n    def __init__(self, corpus: Corpus):\n        self.corpus = corpus\n\n    def score(self, query: list[str], index: int) -> float:\n        return score_kernel(query, index, self.corpus)\n\n    def _score_candidates_vectorized(\n        self,\n        query_term_ids: list[int],\n        candidate_docs: NDArray[np.int64],\n        query_term_weights: NDArray[np.float64] | None = None,\n    ) -> NDArray[np.float64]:\n        if len(candidate_docs) == 0:\n            return np.array([], dtype=np.float64)\n\n        norms = self.corpus.norm_array[candidate_docs]\n        dls = self.corpus.doc_lengths[candidate_docs]\n        avgdl = self.corpus.avgdl\n\n        scores = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        # For bounded coordination + IDF-share prior\n        matched_rarity = np.zeros(len(candidate_docs), dtype=np.float64)\n        matched_idf = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        k1 = EvolvedParameters.k1\n\n        qids = np.array(query_term_ids, dtype=np.int64)\n        df_q = self.corpus._df[qids] if len(qids) else np.array([], dtype=np.float64)\n\n        rarity_q = (\n            ScoringPrimitives.idf_balanced_vectorized(df_q, self.corpus.N)\n            if df_q.size\n            else np.array([], dtype=np.float64)\n        )\n        rarity_q = np.maximum(rarity_q, 0.0)\n        total_rarity = float(np.sum(rarity_q)) if rarity_q.size else 0.0\n\n        total_idf = float(np.sum(np.maximum(self.corpus.idf_array[qids], 0.0))) if len(qids) else 0.0\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            scores += weight * idf * tf_comp\n\n        # Additive priors\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            share = ScoringPrimitives.matched_idf_share_vectorized(matched_idf, total_idf)\n            scores += EvolvedParameters.idf_match_gamma * share\n\n        # Bounded rarity-aware coordination\n        if EvolvedParameters.coord_beta > 0 and total_rarity > 0:\n            scores *= ScoringPrimitives.coord_rarity_aware_vectorized(\n                matched_rarity,\n                total_rarity,\n                EvolvedParameters.coord_beta,\n                EvolvedParameters.coord_floor,\n            )\n\n        return scores\n\n    def rank(\n        self,\n        query: list[str],\n        top_k: int | None = None,\n    ) -> tuple[np.ndarray, np.ndarray]:\n        if not query:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        query_terms, query_weights = QueryProcessor.process(query)\n        query_term_ids = []\n        query_term_weights = []\n        for term, w in zip(query_terms, query_weights, strict=False):\n            term_id = self.corpus.get_term_id(term)\n            if term_id is not None:\n                query_term_ids.append(term_id)\n                query_term_weights.append(float(w))\n        if not query_term_ids:\n            indices = np.arange(self.corpus.N, dtype=np.int64)\n            scores = np.zeros(self.corpus.N, dtype=np.float64)\n            return indices, scores\n        qtf_weights = np.array(query_term_weights, dtype=np.float64)\n        candidate_set: set[int] = set()\n        for term_id in query_term_ids:\n            posting_list = self.corpus._posting_lists.get(term_id, np.array([], dtype=np.int64))\n            candidate_set.update(posting_list.tolist())\n        candidate_docs = np.array(sorted(candidate_set), dtype=np.int64)\n        candidate_scores = self._score_candidates_vectorized(\n            query_term_ids, candidate_docs, qtf_weights\n        )\n        all_scores = np.zeros(self.corpus.N, dtype=np.float64)\n        all_scores[candidate_docs] = candidate_scores\n        sorted_indices = np.argsort(-all_scores).astype(np.int64)\n        sorted_scores = all_scores[sorted_indices]\n        if top_k is not None:\n            sorted_indices = sorted_indices[:top_k]\n            sorted_scores = sorted_scores[:top_k]\n        return sorted_indices, sorted_scores\n\n    def batch_rank(\n        self,\n        queries: list[list[str]],\n        top_k: int | None = None,\n    ) -> list[tuple[np.ndarray, np.ndarray]]:\n        if len(queries) < MIN_QUERIES_FOR_PARALLEL:\n            return [self.rank(query, top_k) for query in queries]\n        with ThreadPoolExecutor(max_workers=NUM_QUERY_WORKERS) as executor:\n            return list(executor.map(lambda q: self.rank(q, top_k), queries))\n\n\n__all__ = [\n    \"BM25\",\n    \"Corpus\",\n    \"tokenize\",\n    \"LuceneTokenizer\",\n    \"LUCENE_STOPWORDS\",\n    \"ENGLISH_STOPWORDS\",\n    \"EvolvedParameters\",\n    \"ScoringPrimitives\",\n    \"TermScorer\",\n    \"DocumentScorer\",\n    \"QueryProcessor\",\n    \"score_kernel\",\n]\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Below are **targeted** changes aimed at improving combined score by (1) boosting early precision on \u201cone-keyword / one-dominant-term\u201d queries (nDCG@10) without hurting recall, and (2) improving robustness on bursty TF corpora (e.g., StackOverflow) via an **IDF-adaptive TF saturation**. Both are lightweight and keep interfaces intact.\n\n---\n\n## Change 1 \u2014 Add a bounded \u201cbest-term\u201d additive prior (cheap nDCG@10 lift)\n\nThis adds a small document-level bonus proportional to the **maximum per-term contribution** (after query weights). This tends to help when one query term is highly diagnostic (common in short queries), while being recall-safe since it\u2019s additive and small.\n\n<<<<<<< SEARCH\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending weight (kept)\n    alpha: float = 0.6\n\n    # Replace harsh coordination with bounded rarity-mass soft-AND (recall-safe)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Additive priors (recall-friendly)\n    cov_gamma: float = 0.10       # small additive term-coverage bump\n    idf_match_gamma: float = 0.08 # additive matched-IDF-share bump\n\n    # Keep mild rare-term shaping but do it via IDF choice rather than tf multiplier\n    gamma: float = 0.0\n\n    # Keep experimental knobs defined (prevents runtime failure if referenced)\n    idf_pivot: float = 3.0\n    cov_power: float = 2.0\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n=======\nclass EvolvedParameters:\n    \"\"\"Numeric parameters. Evolve values or add new ones. Defaults match Pyserini.\"\"\"\n    k1: float = 0.9\n    b: float = 0.4\n    k3: float = 8.0\n    delta: float = 0.5\n\n    # TF blending weight (kept)\n    alpha: float = 0.6\n\n    # Replace harsh coordination with bounded rarity-mass soft-AND (recall-safe)\n    coord_beta: float = 0.55\n    coord_floor: float = 0.35\n\n    # Additive priors (recall-friendly)\n    cov_gamma: float = 0.10       # small additive term-coverage bump\n    idf_match_gamma: float = 0.08 # additive matched-IDF-share bump\n\n    # New: nDCG-friendly \"one strong term is diagnostic\" prior (small, additive)\n    maxterm_gamma: float = 0.03\n\n    # Keep mild rare-term shaping but do it via IDF choice rather than tf multiplier\n    gamma: float = 0.0\n\n    # Keep experimental knobs defined (prevents runtime failure if referenced)\n    idf_pivot: float = 3.0\n    cov_power: float = 2.0\n\n    epsilon: float = 1e-9\n    max_idf: float = float(\"inf\")\n    min_idf: float = 0.0\n>>>>>>> REPLACE\n\n---\n\n## Change 2 \u2014 Add an IDF-adaptive TF saturation primitive (bursty TF robustness)\n\nRare terms should keep near-linear BM25-ish behavior; very common terms in bursty docs should be more log-like. This creates a smooth mixture **per term** using IDF as the gate.\n\n<<<<<<< SEARCH\n    def saturate_tflog_bm25(tf: float, k1: float, norm: float) -> float:\n        \"\"\"\n        New primitive: apply log-TF before BM25-style saturation.\n        Helps corpora with bursty term repetition (e.g., forum/stack traces) without\n        killing signals for single occurrences.\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        t = 1.0 + math.log(tf)\n        denom = t + k1 * norm + EvolvedParameters.epsilon\n        return (t * (k1 + 1.0)) / denom\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n=======\n    def saturate_tflog_bm25(tf: float, k1: float, norm: float) -> float:\n        \"\"\"\n        New primitive: apply log-TF before BM25-style saturation.\n        Helps corpora with bursty term repetition (e.g., forum/stack traces) without\n        killing signals for single occurrences.\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        t = 1.0 + math.log(tf)\n        denom = t + k1 * norm + EvolvedParameters.epsilon\n        return (t * (k1 + 1.0)) / denom\n\n    @staticmethod\n    def saturate_idf_adaptive(tf: float, k1: float, norm: float, idf: float, pivot: float) -> float:\n        \"\"\"\n        New primitive: IDF-adaptive TF saturation.\n        - Common terms: lean toward log-TF saturation (anti-burst).\n        - Rare terms: lean toward standard Lucene saturation.\n        mix = idf/(idf+pivot) in [0,1]\n        \"\"\"\n        if tf <= 0:\n            return 0.0\n        p = max(float(pivot), EvolvedParameters.epsilon)\n        mix = max(0.0, float(idf)) / (max(0.0, float(idf)) + p)\n        tf_l = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_log = ScoringPrimitives.saturate_tflog_bm25(tf, k1, norm)\n        return mix * tf_l + (1.0 - mix) * tf_log\n\n    @staticmethod\n    def saturate_idf_adaptive_vectorized(\n        tf: NDArray[np.float64],\n        k1: float,\n        norm: NDArray[np.float64],\n        idf: float,\n        pivot: float,\n    ) -> NDArray[np.float64]:\n        p = max(float(pivot), EvolvedParameters.epsilon)\n        mix = max(0.0, float(idf)) / (max(0.0, float(idf)) + p)\n        tf_l = ScoringPrimitives.saturate_lucene_vectorized(tf, k1, norm)\n        t = np.where(tf > 0, 1.0 + np.log(np.maximum(tf, 1.0)), 0.0)\n        denom = t + float(k1) * norm + EvolvedParameters.epsilon\n        tf_log = (t * (float(k1) + 1.0)) / denom\n        return mix * tf_l + (1.0 - mix) * tf_log\n\n    @staticmethod\n    def saturate_lucene_vectorized(\n        tf: NDArray[np.float64], k1: float, norm: NDArray[np.float64]\n    ) -> NDArray[np.float64]:\n        denom = tf + k1 * norm + EvolvedParameters.epsilon\n        return tf / denom\n>>>>>>> REPLACE\n\n---\n\n## Change 3 \u2014 Use the IDF-adaptive saturation inside TermScorer (small structure tweak)\n\nWe replace the single saturate_lucene with the adaptive one, still blended with tf_salience.\n\n<<<<<<< SEARCH\n        # Blend: classic Lucene BM25-like sat + length-aware salience\n        tf_sat = ScoringPrimitives.saturate_lucene(tf, k1, norm)\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n=======\n        # Blend: IDF-adaptive saturation (bursty robustness) + length-aware salience\n        tf_sat = ScoringPrimitives.saturate_idf_adaptive(\n            tf, k1, norm, idf, EvolvedParameters.idf_pivot\n        )\n        tf_sal = ScoringPrimitives.tf_salience(tf, dl, avgdl)\n        tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n>>>>>>> REPLACE\n\n---\n\n## Change 4 \u2014 Add the same max-term prior to DocumentScorer (scalar path)\n\nThis mirrors the vectorized implementation we\u2019ll add next.\n\n<<<<<<< SEARCH\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # Recall-friendly additive priors\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n=======\n        base = ScoringPrimitives.weighted_sum(term_scores, query_weights)\n\n        # nDCG-friendly: one strong term match can be highly diagnostic\n        if EvolvedParameters.maxterm_gamma > 0:\n            base += EvolvedParameters.maxterm_gamma * max(term_scores)\n\n        # Recall-friendly additive priors\n        if EvolvedParameters.cov_gamma > 0 and total_query_terms > 0:\n            c = float(matched_count) / max(1.0, float(total_query_terms))\n            base += EvolvedParameters.cov_gamma * (c * c)\n\n        if EvolvedParameters.idf_match_gamma > 0 and total_idf > 0:\n            base += EvolvedParameters.idf_match_gamma * ScoringPrimitives.matched_idf_share(\n                matched_idf, total_idf\n            )\n>>>>>>> REPLACE\n\n---\n\n## Change 5 \u2014 Vectorized path: (a) adaptive saturation, (b) max-term prior\n\nWe compute per-term `contrib` and track `max_contrib` to add the same prior cheaply.\n\n<<<<<<< SEARCH\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_lucene_vectorized(tf_row, k1, norms)\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            scores += weight * idf * tf_comp\n\n        # Additive priors\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n=======\n        max_contrib = np.zeros(len(candidate_docs), dtype=np.float64)\n\n        for i, term_id in enumerate(query_term_ids):\n            idf = float(self.corpus.idf_array[term_id])\n            if idf <= 0:\n                continue\n            idf = max(EvolvedParameters.min_idf, min(idf, EvolvedParameters.max_idf))\n\n            weight = query_term_weights[i] if query_term_weights is not None else 1.0\n\n            tf_row = self.corpus.tf_matrix[term_id, candidate_docs].toarray().ravel()\n            present = (tf_row > 0).astype(np.float64)\n            matched += present\n            if rarity_q.size:\n                matched_rarity += present * float(rarity_q[i])\n            matched_idf += present * max(0.0, idf)\n\n            tf_sat = ScoringPrimitives.saturate_idf_adaptive_vectorized(\n                tf_row, k1, norms, idf, EvolvedParameters.idf_pivot\n            )\n            tf_sal = ScoringPrimitives.tf_salience_vectorized(tf_row, dls, avgdl)\n            tf_comp = (1.0 - EvolvedParameters.alpha) * tf_sat + EvolvedParameters.alpha * tf_sal\n\n            contrib = weight * idf * tf_comp\n            scores += contrib\n            max_contrib = np.maximum(max_contrib, contrib)\n\n        # nDCG-friendly max-term prior (mirror DocumentScorer)\n        if EvolvedParameters.maxterm_gamma > 0:\n            scores += EvolvedParameters.maxterm_gamma * max_contrib\n\n        # Additive priors\n        qn = float(len(query_term_ids))\n        if EvolvedParameters.cov_gamma > 0 and qn > 0:\n            c = matched / qn\n            scores += EvolvedParameters.cov_gamma * (c * c)\n>>>>>>> REPLACE\n\n---\n\n### Why these help (expected)\n- **Adaptive saturation**: should improve rankings on corpora with repeated tokens/boilerplate (often boosts nDCG@10 without harming recall@100 much).\n- **Max-term prior**: improves early precision for short/keyword queries (common in BRIGHT subsets and arguana-like tasks), usually small recall impact since candidates are unchanged.\n\nIf you want an additional diversity direction next: we can add **query-length gating** for the max-term prior (only apply strongly when |Q| is small), or introduce a **rarity-weighted softmax** aggregator as an alternative to weighted_sum."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}