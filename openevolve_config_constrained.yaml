# OpenEvolve configuration for Constrained BM25 Seed Program
#
# Seed Program: src/ranking_evolved/bm25_constrained.py (~527 lines)
#
# GOAL: Evolve within the classic BM25 paradigm with constrained search space
# - Optimize the standard BM25 formula (IDF × TF, summed over terms)
# - Find optimal parameters for the target domain
# - Discover improved IDF/TF formulations
# - Keep the fundamental BM25 structure intact
#
# This is the CONSTRAINED approach - smallest search space, fastest convergence.
# Matches Lucene/Pyserini defaults for easy comparison with baselines.
#
# Run with:
#   export OPENAI_API_KEY="your-key"
#   uv run openevolve-run src/ranking_evolved/bm25_constrained.py evaluator_parallel.py --config openevolve_config_constrained.yaml
#
# Environment variables for evaluation:
#   EVAL_BENCHMARKS=all          # all | bright | beir | bright+beir
#   EVAL_SAMPLE_QUERIES=20       # Sample N queries per dataset (0 = all)
#   EVAL_TOKENIZER=lucene        # simple or lucene
#   EVAL_MAX_WORKERS=0           # 0 = auto (based on cores/memory)
#   EVAL_THREADS_PER_WORKER=8    # Threads for tokenization

max_iterations: 200
log_level: "INFO"
random_seed: 42
diff_based_evolution: true
max_code_length: 20000  # bm25_constrained.py is ~527 lines (~17k chars)

llm:
  models:
    - name: "gpt-4o"
      weight: 1.0
  api_base: "https://api.openai.com/v1"
  api_key: null  # falls back to OPENAI_API_KEY env var
  temperature: 0.7
  top_p: 0.95
  max_tokens: 8192
  timeout: 180
  retries: 3
  retry_delay: 5

prompt:
  system_message: |
    You are evolving a CONSTRAINED BM25 ranking function evaluated on IR benchmarks.
    
    ## Evaluation Metrics
    You receive detailed metrics for each dataset:
    - nDCG@10: Normalized Discounted Cumulative Gain (ranking quality)
    - Recall@100: Coverage metric (retrieval completeness)
    - index_time_ms: Time to tokenize corpus and build index
    - query_time_ms: Time to evaluate all queries
    
    Aggregate metrics:
    - avg_ndcg@10: Macro-average across all datasets
    - avg_recall@100: Macro-average across all datasets
    - combined_score: (avg_ndcg@10 + avg_recall@100) / 2
    
    Use per-dataset metrics to identify weak spots and optimize accordingly.
    
    ## Philosophy
    You are working within the constrained BM25 paradigm. The goal is to optimize
    the standard BM25 formula while keeping its fundamental structure:
    
        Score(D, Q) = Σ IDF(t) × TF(t, D)
    
    Do NOT try to fundamentally restructure the algorithm. Instead, focus on:
    - Finding optimal parameter values
    - Improving the IDF and TF formulas
    - Better length normalization
    - Query term weighting strategies
    
    ## Evolution Targets (in bm25_constrained.py)
    
    1. `Parameters` - Numeric constants (Lucene defaults)
       - k1: TF saturation (default: 0.9, try 0.5-2.0)
       - b: Length normalization (default: 0.4, try 0.3-0.9)
    
    2. `IDFFormula.compute()` - IDF formula
       - Default: log(1 + (N - df + 0.5) / (df + 0.5))  [Lucene IDF]
       - Always non-negative (unlike classic BM25)
       - Can try: ATIRE IDF, Robertson IDF
    
    3. `TFFormula.compute()` - TF saturation formula
       - Default: tf / (tf + k1 * norm)  [Lucene formula, no (k1+1)]
       - Consider: log damping, BM25L, BM25+, double saturation
    
    4. `LengthNorm.compute()` - Document length normalization
       - Default: 1 - b + b * (dl / avgdl)
       - Consider: log-based, pivot normalization, sigmoid
    
    5. `QueryWeighting.compute_weights()` - Query term handling
       - Mode: "unique" (default), "sum_all", "saturated"
       - Default uses "unique" - each term contributes once
    
    6. `ScoreAggregation.aggregate()` - How to combine term scores
       - Mode: "sum" (default), "weighted_sum", "max", "mean"
       - Default uses simple sum
    
    7. `BM25.score_kernel()` - The main scoring function
       - Combines IDF, TF, and aggregation
       - Primary evolution target for formula changes
    
    ## Lucene BM25 Formula Reference (default)
    
    IDF(t) = log(1 + (N - df(t) + 0.5) / (df(t) + 0.5))
    
    TF(t, D) = f(t,D) / (f(t,D) + k1 × norm)
    
    norm = 1 - b + b × |D| / avgdl
    
    Score = Σ IDF(t) × TF(t, D)  for each query term t
    
    ## Guidelines
    - Keep changes targeted to one component at a time
    - Preserve the BM25 interface (Corpus, rank, score signatures)
    - No new dependencies - only numpy and re are available
    - Ensure numerical stability (add epsilon to denominators)
    - Look for sections marked "EVOLUTION TARGET"
    - Consider both ranking quality (nDCG) AND efficiency (index/query time)
    
    ## Promising Directions
    - Try log-damped TF: log(1 + tf_saturated)
    - Add small delta bonus for matching terms (BM25+)
    - Adjust parameters based on dataset characteristics
    - Higher k1 for verbose documents, lower for short

  evaluator_system_message: |
    You review candidate BM25 ranking code evaluated on multiple IR datasets.
    
    Check for:
    - Numerical stability (no division by zero, no NaN)
    - Interface compatibility (Corpus, rank, score methods work)
    - Meaningful ranking behavior (not all zeros or identical scores)
    - Preservation of BM25 structure (IDF × TF pattern)
    - Compatibility with Lucene-style defaults
    
    The LLM receives per-dataset metrics. Look for:
    - Datasets where nDCG improved/degraded significantly
    - Balance between ranking quality and speed
    - Generalization across different benchmark types

  num_top_programs: 3
  num_diverse_programs: 2
  use_template_stochasticity: true
  include_artifacts: true

database:
  in_memory: true
  population_size: 200
  archive_size: 50
  num_islands: 3
  migration_interval: 20
  migration_rate: 0.1
  feature_dimensions:
    - "complexity"
    - "diversity"
  feature_bins: 10
  log_prompts: true

evaluator:
  timeout: 3600  # 1 hour - multiple datasets can take time even with parallelism
  max_retries: 2
  cascade_evaluation: false
  parallel_evaluations: 1  # Evaluator handles parallelism internally

evolution_trace:
  enabled: true
  format: "jsonl"
  include_code: true
  include_prompts: false
