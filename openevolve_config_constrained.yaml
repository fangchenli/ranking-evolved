# OpenEvolve configuration for Constrained BM25 Seed
#
# Seed: src/ranking_evolved/bm25_constrained_fast.py (concise: Parameters + idf, tf_saturated, length_norm, query_weights)
#
# GOAL: Safe search over known BM25 primitives — no exploration of novel retrieval ideas.
# - Tune hyperparameters (k1, b, k3)
# - Swap IDF / TF saturation / length norm / query weighting for known alternatives
# - Meaningful combination of known primitives; efficient grid-search over a known search space
# - Stay with BM25; keep interface (BM25, Corpus, tokenize, LuceneTokenizer, rank, score)
#
# Run (align with commands.txt):
#   export EVAL_EXCLUDE_DATASETS="dl19,dl20,..."
#   uv run python -m openevolve.cli src/ranking_evolved/bm25_constrained_fast.py evaluator_parallel.py --config openevolve_config_constrained.yaml --output openevolve_output_constrained

max_iterations: 70
checkpoint_interval: 5
log_level: "DEBUG"
random_seed: 42
diff_based_evolution: true
max_code_length: 30000

llm:
  models:
    - name: "gpt-5.2"
      weight: 1.0
  api_base: "https://api.openai.com/v1"
  api_key: null  # falls back to OPENAI_API_KEY env var
  temperature: 0.7
  top_p: 0.95
  max_tokens: 8192
  timeout: 180
  retries: 3
  retry_delay: 5

prompt:
  system_message: |
    You are evolving a **constrained BM25** ranking function. We want **safe updates** only: tune hyperparameters, swap in known IDF/TF/length-norm/query-weight formulas, and combine these primitives meaningfully. Do **not** explore novel retrieval ideas — stick to BM25 and efficiently search the known space.

    ## What you optimize

    - **combined_score** = 0.8 × avg_recall@100 + 0.2 × avg_ndcg@10 (higher is better).
    - Per-dataset: nDCG@10, Recall@100, index_time_ms, query_time_ms. Use these to target weak datasets.

    ## What you can change (evolve)

    1. **Parameters** (k1, b, k3) — Tune within reasonable ranges: e.g. k1 in [0.5, 2.0], b in [0, 1], k3 in [0, 20].
    2. **idf(df, N)** — Swap for known formulas: Lucene (default), Robertson `log((N-df+0.5)/(df+0.5))`, ATIRE `log(N/df)`, BM25L `log((N+1)/(df+0.5))`, BM25+ `log((N+1)/df)`.
    3. **tf_saturated(tf, k1, norm)** — Swap for known formulas: Lucene `tf/(tf+k1*norm)` (default), Robertson `(k1+1)*tf/(tf+k1*norm)`, log-damped `log(1+tf)/(tf+k1*norm)`.
    4. **length_norm(doc_lengths, avgdl, b)** — Swap for known: pivoted `1-b+b*dl/avgdl` (default), none `1.0`, or other standard variants.
    5. **query_weights(query, k3, mode)** — Change mode or formula: "unique" (1 per term), "count" (qtf, default), "saturated" `(k3+1)*qtf/(k3+qtf)`.

    Use **SEARCH/REPLACE** diffs: SEARCH must exactly match the current code; REPLACE is your edit. Prefer **one component per edit** (e.g. only change k1/b, or only swap IDF formula).

    ## What you must keep

    - **Interface**: BM25, Corpus, tokenize, LuceneTokenizer; BM25.rank(query, top_k), BM25.score(query, doc_idx). Do not remove or rename these.
    - **BM25 structure**: Score = sum over query terms of (weight × IDF × saturated_TF). No new retrieval model.
    - **Numerical stability**: Avoid division by zero and NaNs (use a small epsilon where needed).

    ## Guidelines

    - Prefer small, targeted changes (e.g. one hyperparameter or one formula swap).
    - Use known alternatives from the IR literature; do not invent new formulas.
    - This is effectively an efficient grid-search over a known search space — stay within it.

  evaluator_system_message: |
    You review candidate constrained BM25 code evaluated on IR benchmarks.

    Check:
    - Code runs without errors; scores are valid (no NaN, not all zeros).
    - **Interface**: BM25, Corpus, tokenize, LuceneTokenizer present; BM25 has rank() and score().
    - **BM25 structure**: IDF × TF (or known variant); no novel retrieval model.
    - Numerical stability (no division by zero).

    Reject changes that introduce new retrieval ideas or break the BM25 interface. Approve safe updates: hyperparameter tweaks and swaps among known IDF/TF/length-norm/query-weight formulas.

  num_top_programs: 3
  num_diverse_programs: 2
  use_template_stochasticity: true
  include_artifacts: true

database:
  in_memory: true
  population_size: 200
  archive_size: 50
  num_islands: 3
  migration_interval: 20
  migration_rate: 0.1
  feature_dimensions:
    - "complexity"
    - "diversity"
  feature_bins: 10
  log_prompts: true

evaluator:
  timeout: 1800  # 30 minutes - fast datasets only
  max_retries: 2
  cascade_evaluation: false
  parallel_evaluations: 1  # Evaluator handles parallelism internally

evolution_trace:
  enabled: true
  format: "jsonl"
  include_code: true
  include_prompts: true   # Include prompts and LLM responses in evolution_trace.jsonl
