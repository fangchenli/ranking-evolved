# OpenEvolve configuration for Composable FAST BM25 Seed Program
#
# Seed Program: src/ranking_evolved/bm25_composable_fast.py
#
# GOAL: Discovery of new IR methods (lexical retrieval algorithms).
# The seed frames retrieval as PRIMITIVES (atoms: IDF, TF, saturation, norm, aggregation)
# plus STRUCTURE (how they are combined). Your job is to invent new algorithms:
# new formulas, new primitives, or new structure—not just recombine existing primitives.
#
# FAST EVALUATION: Set EVAL_EXCLUDE_DATASETS before running (align with commands.txt).
#
# Run (align with commands.txt):
#   export EVAL_EXCLUDE_DATASETS="dl19,dl20,fever,climate-fever,hotpotqa,dbpedia-entity,nq,quora,webis-touche2020,cqadupstack,leetcode,aops,theoremqa_questions,robotics,psychology,sustainable_living"
#   export OPENAI_API_KEY="your-key"
#   uv run python -m openevolve.cli src/ranking_evolved/bm25_composable_fast.py evaluator_parallel.py --config openevolve_config_composable.yaml --output openevolve_output_composable_fast
#
# Datasets evaluated (12): nfcorpus, scifact, pony, arguana, theoremqa_theorems, scidocs, biology, earth_science, economics, stackoverflow, fiqa, trec-covid
#
# Output: evolution_trace.jsonl (prompts + LLM responses when include_prompts: true), checkpoints/checkpoint_<N>/, best/

max_iterations: 10  # More iterations for larger search space
checkpoint_interval: 5
log_level: "DEBUG"
random_seed: 42
diff_based_evolution: true
max_code_length: 30000  # bm25_composable_fast.py

llm:
  models:
    - name: "gpt-5.2"
      weight: 1.0
  api_base: "https://api.openai.com/v1"
  api_key: null  # falls back to OPENAI_API_KEY env var
  temperature: 0.8  # Slightly higher for more exploration
  top_p: 0.95
  max_tokens: 8192
  timeout: 180
  retries: 3
  retry_delay: 5

prompt:
  system_message: |
    You are evolving a LEXICAL RETRIEVAL algorithm. The goal is discovery of new IR methods—invent new algorithms, not just recombine existing primitives.
    
    ## View: Primitives + Structure
    Lexical retrieval in this codebase is PRIMITIVES (atoms) + STRUCTURE (how they combine).
    - Primitives: IDF, TF, saturation, length norm, aggregation (weighted sum, means, etc.).
    - Structure: how term scores become doc scores, how the pipeline is organized.
    The seed is one structure (BM25-like). You can change parameters, add or invent new primitives, change formulas, or change structure. Prefer inventing new formulas or new primitives over only swapping which existing primitive is used where.
    
    ## Evaluation Metrics
    Per dataset: nDCG@10, Recall@100, index_time_ms, query_time_ms.
    Aggregate: avg_ndcg@10, avg_recall@100, combined_score = 0.8*avg_recall@100 + 0.2*avg_ndcg@10 (used to rank solutions).
    Use per-dataset metrics to find weak spots.
    
    ## Evolution Targets (bm25_composable_fast.py)
    
    ### 1. EvolvedParameters
    Numeric constants: k1, b, k3, delta, alpha, beta, gamma, epsilon, max_idf, min_idf. Change or add parameters via SEARCH/REPLACE.
    
    ### 2. ScoringPrimitives
    Atoms: idf_* (classic, lucene, atire, bm25plus, smooth), tf_* (raw, log, double_log, boolean, augmented), saturate_* (basic, lucene, bm25, bm25l, bm25plus, log), length_norm_* (bm25, pivot, log), multiply, add, weighted_sum, geometric_mean, harmonic_mean, soft_max, query_weight_*, coverage_bonus, rarity_boost.
    You can add new primitives with new formulas (e.g. a new saturation curve, a new IDF variant). Invent when it might help; don’t limit yourself to recombining only what’s there.
    
    ### 3. TermScorer.score(tf, df, N, dl, avgdl)
    One term’s contribution. Evolve the formula: new combination of primitives or new math (e.g. asymmetric term importance, different saturation).
    
    ### 4. DocumentScorer.score(term_scores, query_weights, matched_count, total_query_terms)
    Aggregate term scores into doc score. Evolve aggregation or add terms (e.g. coverage, rarity).
    
    ### 5. QueryProcessor.process(query) → (terms, weights)
    Evolve how the query is interpreted (weighting, dedup).
    
    ### 6. score_kernel(query, doc_idx, corpus)
    Full pipeline. You can restructure it (e.g. different flow, use DocumentScorer here, or a different orchestration).
    
    ## Guidelines
    - Invent new formulas or new primitives when it might improve retrieval; recombination alone is not the goal.
    - Keep interface compatibility: BM25(corpus).rank(query) and the Corpus/BM25 API must still work for the evaluator.
    - Preserve numerical stability (epsilon, safe division).
    - Use SEARCH/REPLACE diffs; SEARCH must exactly match the current program.
    - If you add vectorized primitives (e.g. for Corpus.idf_array / norm_array), add a *_vectorized variant in ScoringPrimitives and use it in BM25._score_candidates_vectorized so batch ranking stays consistent with score_kernel.

  evaluator_system_message: |
    You review candidate lexical retrieval code. Goal: discovery of new IR methods (invent algorithms, not just recombine primitives).
    
    Check for:
    - Correct use of primitives and formulas (arguments, types, numerical stability)
    - Interface compatibility (BM25.rank(), Corpus API) so the evaluator can run
    - Whether the change invents new formulas/primitives/structure vs only recombining existing primitives
    - Any new primitives are well-defined and numerically stable
    
    Use per-dataset metrics to assess: where nDCG/recall improved or regressed, balance with speed, generalization.

  num_top_programs: 3
  num_diverse_programs: 3  # More diversity for exploration
  use_template_stochasticity: true
  include_artifacts: true

database:
  in_memory: true
  population_size: 250  # Larger population for more exploration
  archive_size: 60
  num_islands: 4  # More islands for diversity
  migration_interval: 25
  migration_rate: 0.1
  feature_dimensions:
    - "complexity"
    - "diversity"
  feature_bins: 10
  log_prompts: true

evaluator:
  timeout: 1800  # 30 min - fast set (12 datasets) when EVAL_EXCLUDE_DATASETS set
  max_retries: 2
  cascade_evaluation: false
  parallel_evaluations: 1  # Evaluator handles parallelism internally

evolution_trace:
  enabled: true
  format: "jsonl"
  include_code: true
  include_prompts: true
